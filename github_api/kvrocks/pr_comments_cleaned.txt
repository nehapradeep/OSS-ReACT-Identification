@torwig @PragmaTwice - a tls test failed, could you see inside it? 

Update to 1.64.3 release with latest fix https://github.com/golangci/golangci-lint/releases/tag/v1.64.3

@PragmaTwice Ideally is to add a Go test to verify the behavior.

> @PragmaTwice Ideally is to add a Go test to verify the behavior.\r\n\r\nAdded.

@ltagliamonte-dd You could just control the compaction/flush thread number by configuring:\r\n\r\n- rocksdb.max_background_compactions (default is LOW priority)\r\n- rocksdb.max_background_flushes (default is HIGH priority)\r\n\r\nBut those two parameters might be deprecated in future versions. It will use 1/4 * max_background_jobs as the flush thread number if both of them are not set. See [GetBGJobLimits](https://github.com/facebook/rocksdb/blob/ce6065ef70fe8c55bdf74c2a4121c038a1f7da4b/db/db_impl/db_impl_compaction_flush.cc#L2935)\r\n

@git-hulk I may be reading this wrong, but where is it the correlation between this params (max_background_jobs/max_background_compactions/max_background_flushes) and the number of threads in rocksdb codebase?\r\n\r\nIn this example in the doc I see the setting of the threadpool size and max_background_jobs\r\nhttps://github.com/facebook/rocksdb/wiki/Thread-Pool

\r\n> where is it the correlation between this params (max_background_jobs/max_background_compactions/max_background_flushes) and the number of threads in rocksdb codebase?\r\n\r\nThe correlation, you could refer: [GetBGJobLimits](https://github.com/facebook/rocksdb/blob/ce6065ef70fe8c55bdf74c2a4121c038a1f7da4b/db/db_impl/db_impl_compaction_flush.cc#L2935)

Closing in favor of using max_background_compactions and max_background_flushes has the same effect.

https://github.com/apache/kvrocks/actions/runs/13107232513/job/36565873699?pr=2763#step:17:62\r\n\r\nInteresting that there\

I think it should be due to that parallal execution is introduced for slotmigrate test cases. Maybe we can revert it for slot migration first, and create an issue for this problem.

> . Don\

Do our runners have enough memory to spawn all instances of Kvrocks we need to execute those tests in parallel?

looking at the ci failure here should we increase the wait time in `TestReplicationShareCheckpoint` to 100s to avoid flakeness?\r\n\r\n```\r\n\trequire.Eventually(t, func() bool {\r\n\t\t\treturn master.LogFileMatches(t, ".*Using current existing checkpoint.*")\r\n\t\t}, 50*time.Second, 100*time.Millisecond)\r\n```

looks like slot migrate tests are unstable with parallel maybe i could drop the changes from this pr

> looking at the ci failure here should we increase the wait time in `TestReplicationShareCheckpoint` to 100s to avoid flakeness?\r\n> \r\n> ```\r\n> \trequire.Eventually(t, func() bool {\r\n> \t\t\treturn master.LogFileMatches(t, ".*Using current existing checkpoint.*")\r\n> \t\t}, 50*time.Second, 100*time.Millisecond)\r\n> ```\r\n\r\nwe might need to keep the parallelism to ensure `TestReplicationShareCheckpoint` has sufficient cpu

Hi, thank you for your contribution!\r\n\r\nFrom my understanding (not being a native English speaker), it\

I think - https://www.urbandictionary.com/define.php?term=OOPPS 

@PragmaTwice Hmm... I understand the use of `ooops` for emphasis in informal contexts. However, for error messages, I recommend using the standard `Oops!`.

I think, could be better use a more accuracy version, like alpine:3.21

@err931 Thank you for your contribution.

@PragmaTwice @git-hulk pushed unit tests.\r\nthanks for the amazing support!

@PragmaTwice @git-hulk Rename addressed via 6f28ae05e7679983703c566b9a64936feb014663

```\r\nfunc TestXXXWithRESP3(t *testing.T) {\r\n\tsrv := util.StartServer(t, map[string]string{\r\n\t\t"resp3-enabled": "yes",\r\n\t})\r\n\tdefer srv.Close()\r\n```\r\n\r\nNeed to change to:\r\n```\r\nfunc TestXXXWithoutRESP3(t *testing.T) {\r\n\tsrv := util.StartServer(t, map[string]string{\r\n\t\t"resp3-enabled": "no",\r\n\t})\r\n\tdefer srv.Close()\r\n```

Also for `redis.setresp(3);`, should resp3 be default in scripting?

@PragmaTwice Yes, a few test cases need to be fixed after switching to RESP3. Will do it later.

Hi @mapleFU ,\r\n\r\nSorry to interrupt you again. 😊\r\nCould you help take a review for latest update?\r\n\r\nHave a nice day!\r\n\r\nBest Regards,\r\nEdward

> General LGTM. We can first move on and add a go unittests\r\n\r\nHi @mapleFU ,\r\n\r\nThanks very much for your warm help and patience! 😊\r\nDo I need to add command with go integration tests in this PR or create a new PR?\r\n\r\nBest Regards,\r\nEdward

Thank you for your contribution!

Ready for review now.

After #2732 is merged, we can remove these arg size checks inside flag gen functions.

> After https://github.com/apache/kvrocks/pull/2732 is merged, we can remove these arg size checks inside flag gen functions.\r\n\r\nIt should be good to keep the argument number checked inside the flag gen functions. We can avoid depending on the order between CheckArity and FlagGen functions.

@rabunkosar-dd Thanks for your contribution. You use `./x.py format` to format your source codes after installing the clang-format-14\r\n

@rabunkosar-dd Could you please fix the lint issue?

Thank you for your contribution!

> Thank you for your contribution!\r\n\r\nThanks for the feedbacks and patience @PragmaTwice and @git-hulk 

@fukua95 Nice! Thank you for your contribution.

@ltagliamonte-dd Hi. Did you set this config `hash-field-expiration` to yes or run the unit tests in this branch?  If the field has an expiration, we have to check to the field expiration before each read operation.

@jjz921024 @ltagliamonte-dd Thanks for your great efforts. I took my first pass in a new PR and generally look in good shape from my perspective. Two nit points can be improved:\r\n\r\n1. Change `ExistValidField` to `GetValidFieldCount`, so that `Hash::Size` can also resue this function.\r\n2. Many places decode the hash metadata and check if it has any valid fields. We can avoid duplicate code by adding a dedicated function to this check. \r\n```C++\r\n    if (metadata.Type() == kRedisHash) {\r\n      HashMetadata hash_metadata(false);\r\n      s = hash_metadata.Decode(rocksdb::Slice(pin_values[i].data(), pin_values[i].size()));\r\n      if (!s.ok()) continue;\r\n      redis::Hash hash_db(storage_, namespace_);\r\n      if (hash_db.ExistValidField(ctx, slice_keys[i], hash_metadata)) {\r\n```

@ltagliamonte-dd Hi, I create a [PR #1]( https://github.com/ltagliamonte-dd/kvrocks/pull/1) in your kvrocks fork repo. There are some polish about this feature for your reference. \r\n\r\nThe purpose of this this commit https://github.com/apache/kvrocks/pull/2402/commits/aac63cc83d0457bc1c02061063975e6a1736b8cc is to avoid unnecessary field expiration checks when executing the hset/hmset command, which comes from https://github.com/apache/kvrocks/pull/2402#discussion_r1894554219 @PragmaTwice  suggestion.\r\n\r\nThe  purpose of this this commit https://github.com/apache/kvrocks/pull/2402/commits/73062319eccf80ec06317b3541af8fca3313fd41 is make `ExistValidField` method more general so that Hash::Size can also resue it. which comes from @git-hulk suggestion.\r\n\r\n

@git-hulk I gave a look at your suggestion to remove duplicated code, it looks like duplicated but if you look closely they are all slightly different checks... we may be able to refactor with something like this:\r\n```\r\nrocksdb::Status Hash::HandleRedisHash(engine::Context &ctx, const Metadata &metadata, const std::string &value,\r\n                                  const std::string &key, std::function<void(int)> on_valid_field_count);\r\n```\r\nso we pass an handleFunction into the refactored function\r\nHappy to hear if you have better/different ideas about it.

@PragmaTwice can you please re-trigger the CI it looks like a flaky test

Hello Folks any updates on review/merge this PR?\r\nCc @PragmaTwice @git-hulk ?

Now ready for review. One problem has been focused in an updated package - golang.org/x/exp (hmm, surprise!).\r\n\r\nThanks @torwig for help with this PR

Maybe we need a merge #2707 PR before fix this

Hmm, interesting error in the linting... @torwig do you have any idea about this? 

@Sandra-Amina-Boss Please stop such meaningless reviews and stop sending spam notifications to me.\r\n

You can run `./x.py format` from the root dir of the repo to format code via `clang-format`.\r\n\r\nAdditional, please ensure `clang-format` exists and is in version 14. (download from [here](https://github.com/llvm/llvm-project/releases/tag/llvmorg-14.0.0))\r\n\r\nRead https://kvrocks.apache.org/community/contributing for more information.

Thank you for your contribution!

> Please avoid introducing unrelated changes.\r\n> \r\n> Also please ensure the version of clang-format you are using is 14. You can download it from [here](https://github.com/llvm/llvm-project/releases/tag/llvmorg-14.0.0).\r\n\r\nthanks, I fixed it.

@weim0000 Thanks for your fix, would you mind adding a test case to cover this? https://github.com/apache/kvrocks/blob/unstable/tests/gocase/unit/type/strings/strings_test.go

> @weim0000 Thanks for your fix, would you mind adding a test case to cover this? https://github.com/apache/kvrocks/blob/unstable/tests/gocase/unit/type/strings/strings_test.go\r\n\r\nOk, no problem. I will add the test cases ASAP

@weim0000 Thanks for your fix.

@weim0000 Thank you for your contribution. Please, run `./x.py format` to format the code accordingly. 

Yes, the connection is registered in worker, so need to remove from the worker first and then free the connection.

@wanghenshui You can run `./x.py format` before committing changes to format the code.

> Would you like to add some go test cases to cover the changes?\r\n> \r\n> Related cases can be found here: https://github.com/apache/kvrocks/blob/unstable/tests/gocase/unit/type/incr/incr_test.go .\r\n\r\nOk, no problem. I will add the go test cases ASAP

> The code looks good to me.\n> \n> \n> \n> Would you like to write a go test case that calls redis command `COMMAND GETKEYS XREAD/XREADGROUP ...` to confirm the returned keys is correct?\n> \n> \n> \n> You can add a new test like https://github.com/apache/kvrocks/blob/unstable/tests/gocase/unit/command/command_test.go#L125.\n\nOf course! \n\nI will add unittest later today.😄\n\nBest Regards,\nEdward

Hi @PragmaTwice ,\r\n\r\nI have added related unit tests for different combinations of `XREAD` and `XREADGROUP`. 😊\r\n\r\nBest Regards,\r\nEdward\r\n

This fixes https://github.com/apache/kvrocks/issues/2633

@git-hulk @PragmaTwice \r\ncc @AntiTopQuark 

New typos detected. Please review them as well.

> New typos detected. Please review them as well.\r\n\r\nThanks! I will fix it asap

A bug exists in this refactor, will dive into it tomorrow

When would "master is not a node in cluster" happens?

> When would "master is not a node in cluster" happens?\r\n\r\noh, this issue is caused by the replica instead of the master was removed in the cluster, but its replication is still living.

TBH the version number of TBB is quite confusing..

`GET_OR_RET` is already a generalized macro for both `Status` and `StatusOr<T>`.\r\n\r\nYou can refer to my comment https://github.com/apache/kvrocks/issues/2521#issuecomment-2335096655.

You can check #2630 about how to implement `rocksdb::Status` forwarding without introducing a new macro.

I apologize, but upon reflection, I believe this PR offers minimal value. Additionally, due to my current busy schedule and limited energy, I am unable to continue this work. I apologize once again.\r\n\r\n\r\ncc @PragmaTwice 

Would you mind fix the lint?

> Would you mind fix the lint?\r\n\r\nSure but ...  \r\nFrom the logs, this is not just a lint issue, but rather due to setting RESP_OK as the type std::string_view. Some function parameters accept const string& instead of std::string_view. If we change everything to accept std::string_view, I think it might require significant modifications...\r\n\r\nSince std::string_view cannot be implicitly converted to std::string, and I want to implement RESP_OK as a constexpr variable, I changed it to use constexpr char[].\r\n\r\n

>  Since std::string_view cannot be implicitly converted to std::string, and I want to implement RESP_OK as a constexpr variable, I changed it to use constexpr char[].\r\n\r\nThis not fixes the problem, we can either use `const static inline std::string`, or just cast once on "Reply". `const char[]` to `std::string` also applies copy...

> > Since std::string_view cannot be implicitly converted to std::string, and I want to implement RESP_OK as a constexpr variable, I changed it to use constexpr char[].由于 std::string_view 无法隐式转换为 std::string,而且我想将 RESP_OK 实现为 constexpr 变量,因此我将其改为使用 constexpr char[]。\r\n> \r\n> This not fixes the problem, we can either use `const static inline std::string`, or just cast once on "Reply". `const char[]` to `std::string` also applies copy...这并不能解决问题,我们可以使用 `const static inline std::string` ，或者只在"Reply"上进行一次转换。 `const char[]` 对 `std::string` 也适用于复制...\r\n\r\nOk u\

Do we need to add "exclusive" flag to `flushdb` and `flushall`? cc @git-hulk 

> Do we need to add "exclusive" flag to `flushdb` and `flushall`? cc @git-hulk\r\n\r\nYes, they should. `FLUSHDB` and `FLUSHALL` will affect all keys in the same/all namespace. For example:\r\n\r\nThread 1:  Read a list key metadata ------------------> push an element and update metadata\r\nThread 2:  ------------------------------> FlushDB\r\n\r\nThen the list key will update on the old metadata.\r\n

Rest LGTM

> Could we perform a basic benchmark (e.g. via redis-benchmark, with a relatively high pressure) for this patch?\r\n\r\nIt might hard to play the benchmark, since this method enlarge the critical section a bit, it might get a little bit slower but might not affect too much...

> > Could we perform a basic benchmark (e.g. via redis-benchmark, with a relatively high pressure) for this patch?\r\n> \r\n> It might hard to play the benchmark, since this method enlarge the critical section a bit, it might get a little bit slower but might not affect too much...\r\n\r\nYeah I just want to confirm that the performance impact is not too large. A slight performance downgrade is expected and fine to me.

this patch\r\n```\r\nubuntu% redis-benchmark -p 6666 -n 100000 -c 50 -q\r\nWARNING: Could not fetch server CONFIG\r\nPING_INLINE: 25497.20 requests per second, p50=1.175 msec                   \r\nPING_MBULK: 26581.61 requests per second, p50=1.135 msec                   \r\nSET: 15659.25 requests per second, p50=2.551 msec                   \r\nGET: 23809.53 requests per second, p50=1.239 msec                   \r\nINCR: 13029.32 requests per second, p50=3.143 msec                   \r\nLPUSH: 10604.45 requests per second, p50=3.879 msec                   \r\nRPUSH: 10123.51 requests per second, p50=3.927 msec                   \r\nLPOP: 9820.29 requests per second, p50=4.039 msec                   \r\nRPOP: 9662.77 requests per second, p50=4.103 msec                   \r\nSADD: 13182.18 requests per second, p50=2.903 msec                   \r\nHSET: 9415.31 requests per second, p50=4.263 msec                   \r\nSPOP: 24003.84 requests per second, p50=1.255 msec                   \r\nZADD: 11944.58 requests per second, p50=3.431 msec                   \r\nZPOPMIN: 23490.72 requests per second, p50=1.295 msec                   \r\nLPUSH (needed to benchmark LRANGE): 11371.39 requests per second, p50=3.415 msec                   \r\nLRANGE_100 (first 100 elements): 14312.29 requests per second, p50=2.007 msec                   \r\nLRANGE_300 (first 300 elements): 8036.00 requests per second, p50=3.551 msec                   \r\nLRANGE_500 (first 500 elements): 5914.71 requests per second, p50=4.759 msec                  \r\nLRANGE_600 (first 600 elements): 5207.79 requests per second, p50=5.415 msec                  \r\nMSET (10 keys): 9639.48 requests per second, p50=4.311 msec                   \r\nXADD: 10528.53 requests per second, p50=3.751 msec\r\n\r\nubuntu% redis-benchmark -p 6666 -n 100000 -c 50 --threads 8 -q\r\nWARNING: Could not fetch server CONFIG\r\nPING_INLINE: 66269.05 requests per second, p50=0.367 msec                   \r\nPING_MBULK: 79744.82 requests per second, p50=0.351 msec                   \r\nSET: 14198.49 requests per second, p50=2.871 msec                   \r\nGET: 66137.57 requests per second, p50=0.415 msec                   \r\nINCR: 12865.05 requests per second, p50=3.151 msec                   \r\nLPUSH: 10751.53 requests per second, p50=3.783 msec                   \r\nRPUSH: 10487.68 requests per second, p50=3.679 msec                   \r\nLPOP: 9487.67 requests per second, p50=4.183 msec                    \r\nRPOP: 9496.68 requests per second, p50=4.087 msec                    \r\nSADD: 12845.22 requests per second, p50=3.079 msec                   \r\nHSET: 8131.40 requests per second, p50=4.847 msec                  \r\nSPOP: 33178.50 requests per second, p50=1.223 msec                   \r\nZADD: 12058.36 requests per second, p50=3.375 msec                   \r\nZPOPMIN: 33156.50 requests per second, p50=1.223 msec                   \r\nLPUSH (needed to benchmark LRANGE): 10205.12 requests per second, p50=3.951 msec                   \r\nLRANGE_100 (first 100 elements): 28457.60 requests per second, p50=1.327 msec                   \r\nLRANGE_300 (first 300 elements): 14684.29 requests per second, p50=2.871 msec                   \r\nLRANGE_500 (first 500 elements): 9878.49 requests per second, p50=4.183 msec                    \r\nLRANGE_600 (first 600 elements): 8396.31 requests per second, p50=4.943 msec                  \r\nMSET (10 keys): 9254.12 requests per second, p50=4.519 msec                   \r\nXADD: 10205.12 requests per second, p50=3.935 msec\r\n```\r\nunstable branch\r\n```\r\nubuntu% redis-benchmark -p 6666 -n 100000 -c 50 -q\r\nWARNING: Could not fetch server CONFIG\r\nPING_INLINE: 25510.20 requests per second, p50=1.151 msec                   \r\nPING_MBULK: 25713.55 requests per second, p50=1.151 msec                   \r\nSET: 17768.30 requests per second, p50=2.207 msec                   \r\nGET: 25150.90 requests per second, p50=1.151 msec                   \r\nINCR: 14430.01 requests per second, p50=2.839 msec                   \r\nLPUSH: 12640.63 requests per second, p50=3.247 msec                   \r\nRPUSH: 12396.18 requests per second, p50=3.223 msec                   \r\nLPOP: 10765.42 requests per second, p50=3.535 msec                   \r\nRPOP: 10266.94 requests per second, p50=3.799 msec                   \r\nSADD: 14102.38 requests per second, p50=2.831 msec                   \r\nHSET: 8261.05 requests per second, p50=4.791 msec                  \r\nSPOP: 24813.89 requests per second, p50=1.215 msec                   \r\nZADD: 13150.97 requests per second, p50=3.111 msec                   \r\nZPOPMIN: 24863.25 requests per second, p50=1.207 msec                   \r\nLPUSH (needed to benchmark LRANGE): 11687.71 requests per second, p50=3.471 msec                   \r\nLRANGE_100 (first 100 elements): 14455.04 requests per second, p50=2.079 msec                   \r\nLRANGE_300 (first 300 elements): 7792.41 requests per second, p50=3.671 msec                  \r\nLRANGE_500 (first 500 elements): 5730.66 requests per second, p50=4.943 msec                  \r\nLRANGE_600 (first 600 elements): 5166.89 requests per second, p50=5.487 msec                  \r\nMSET (10 keys): 9764.67 requests per second, p50=4.255 msec                   \r\nXADD: 11546.01 requests per second, p50=3.471 msec\r\n\r\nubuntu% redis-benchmark -p 6666 -n 100000 -c 50 --threads 8 -q\r\nWARNING: Could not fetch server CONFIG\r\nPING_INLINE: 79554.50 requests per second, p50=0.375 msec                   \r\nPING_MBULK: 79681.27 requests per second, p50=0.383 msec                   \r\nSET: 15306.90 requests per second, p50=2.703 msec                   \r\nGET: 66269.05 requests per second, p50=0.423 msec                   \r\nINCR: 13267.88 requests per second, p50=3.031 msec                   \r\nLPUSH: 11367.51 requests per second, p50=3.543 msec                   \r\nRPUSH: 11392.12 requests per second, p50=3.511 msec                   \r\nLPOP: 9944.31 requests per second, p50=4.031 msec                    \r\nRPOP: 9464.32 requests per second, p50=4.191 msec                    \r\nSADD: 12841.92 requests per second, p50=3.159 msec                   \r\nHSET: 7505.25 requests per second, p50=5.479 msec                  \r\nSPOP: 33200.53 requests per second, p50=1.151 msec                   \r\nZADD: 12056.91 requests per second, p50=3.447 msec                   \r\nZPOPMIN: 33156.50 requests per second, p50=1.223 msec                   \r\nLPUSH (needed to benchmark LRANGE): 11050.95 requests per second, p50=3.775 msec                   \r\nLRANGE_100 (first 100 elements): 23446.66 requests per second, p50=1.671 msec                   \r\nLRANGE_300 (first 300 elements): 14156.29 requests per second, p50=3.023 msec                   \r\nLRANGE_500 (first 500 elements): 8812.90 requests per second, p50=4.791 msec                  \r\nLRANGE_600 (first 600 elements): 7741.14 requests per second, p50=5.583 msec                  \r\nMSET (10 keys): 9966.12 requests per second, p50=4.159 msec                    \r\nXADD: 11050.95 requests per second, p50=3.631 msec\r\n```

Hmm I took a benchmark in my local and I think it may due to unstable measurements.

Will merge if no negative comments tomorrow

Is this to avoid not calling `flag_gen` fn if having?

> Is this to avoid not calling `flag_gen` fn if having?\r\n\r\nExactly.

Also I plan to make key_range and other related members private in the next PR.

Hi @nathanlo-hrt Seems the Go test is broken, would you mind fixing it to make it mergeable?

By the way, may I ask that is this only for write-heavy workload(maybe with long and compress-able key-values)?

I see, this make sense

From the CI log, we need to add `vector.reserve()` before the loop containing `vector.push_back()`.

> From the CI log, we need to add `vector.reserve()` before the loop containing `vector.push_back()`.\r\n\r\nthank you, didnt see it.\r\nfixed

@PragmaTwice what do you think about rename nocompression_for_first_n_levels  -> start_compression_level?

maybe we could specific compression setting by level? user could mark l0/l1 as snappy or lz4 compression style\r\n\r\nl0/l1 nocompression is common type of usage,  use or not, \r\n\r\nrocksdb.nocompression_for_first_n_levels  indicate user could make l2/l3... no compression, user may not know about rocksdb stuff, make them confused about what senerio to use it\r\n\r\ninstead, let user specifc what compression style level used, maybe more clear. 

> maybe we could specific compression setting by level? user could mark l0/l1 as snappy or lz4 compression style\r\n\r\nI think this is a good point\r\nSomething similar to the more advanced settings in kvrocks. By default, the "compression" settings are used, but if the new setting "compression_by_levels" is specified, it takes precedence, and the "compression" setting is ignored.\r\n\r\n\r\n\r\n\r\n\r\n\r\n

Maybe we should first check this in, would you mind fix the comments firstly?

> @PragmaTwice what do you think about rename nocompression_for_first_n_levels -> start_compression_level?\r\n\r\nNice. But seems compression_start_level or compression_begin_level is better to me.

> > @PragmaTwice what do you think about rename nocompression_for_first_n_levels -> start_compression_level?\r\n> \r\n> Nice. But seems compression_start_level or compression_begin_level is better to me.\r\n\r\nrenamed

@nathanlo-hrt Thank you for your contribution. Ideally, is to add a dedicated test for the case(s) you highlighted.

Currently, the Kvrocks port listening implementation has a race condition: since Kvrocks checks whether the port is in use before actually grabbing exclusive use of the port, it\

cc @mapleFU PTAL

Thank you for your contribution.

https://github.com/apache/kvrocks/actions/runs/11296291234/job/31435412588?pr=2590\r\n\r\nThe CI job via ASF self-hosted runner is cancelled. Not sure why.\r\n\r\n> The runner has received a shutdown signal. This can happen when the runner service is stopped, or a manually started runner is canceled.\r\nThe self-hosted runner: asf-arm-trzxd-rh9np lost communication with the server. Verify the machine is running and has a healthy network connection. Anything in your workflow that terminates the runner process, starves it for CPU/Memory, or blocks its network access can cause this error.\r\n\r\n\r\ncc @gmcdonald 

As I see, arm64 now public in Github - https://github.blog/changelog/2025-01-16-linux-arm64-hosted-runners-now-available-for-free-in-public-repositories-public-preview/

All test passed, maybe are ok to merge?

@sryanyuan Thank you for this fix.

@PragmaTwice I decided to switch it up the serialization on the implementation, I can include it on the pr. As for the field_alias, you can use this implementation to use index fields to look and see if the alias matches when it receives a non matching index in a command (which will be implemented in a GetAlias function).

thanks for taking a look at this!

@LindaSummer Nice change!

@LindaSummer Good catch!!!

@PragmaTwice @Beihao-Zhou @LindaSummer Feel free to leave changes!

Took out column temporarily as it was failing tests. Will put it back in the full implementation.

Thanks! Would it be possible to add details about the encoding and outline the next steps in this PR, similar to how it was done in [this PR](https://github.com/apache/kvrocks/pull/2368)? I think it would help make everything clearer. :)\r\n

Also can we move the encoding under folder `types` instead of `storage`? Maybe similar to https://github.com/apache/kvrocks/blob/0a43bade1a4c3c885947fdd42eacefac934bf1bd/src/types/redis_stream_base.h

@Beihao-Zhou I could, but I assumed that creating the time_encoding in storage is fine as it needs its own search and querying capabilities over subkeys unlike the stream data.

@jonathanc-n How to be sure that bug was fixed? Exactly, write a test :)\r\n \r\nThere is a classic flow for fixing a bug:\r\n1. Write a test that fails.\r\n2. Fix a bug.\r\n3. Assert that the test passes -> it will prevent any regression.

I see, thank you for letting me know! :)

@torwig The test is added\r\n

@torwig I made some changes to the stream, hopefully it passes all the checks now

@PragmaTwice Do you know why this might be failing? I am not too sure myself

Stream test time out after 30 minutes. Maybe there is a block or infinite wait (for example, waiting for a message that is never added).

@nathanlo-hrt Thank you for this catch.

> Stream test time out after 30 minutes. Maybe there is a block or infinite wait (for example, waiting for a message that is never added).\r\n\r\nSo.. is this another bug of XPENDING? Or we can just change the test case to fix it?

@nathanlo-hrt Good job! You can run `./x.py format` to format the code and make the linter happy.

```\r\nError: unit/type/stream/stream_test.go:2170:6: ineffectual assignment to err (ineffassign)\r\n\t\tr, err := rdb.XAck(ctx, streamName, groupName, "0-0").Result()\r\n```

@PragmaTwice Commited to change. Should be good to run workflow

Could you check the CI failure to see if it can be fixed?

@PragmaTwice CI, should be fixed. It was just incorrect return values for type mismatch.

The rest LGTM :)

@PragmaTwice One thing noted for this pr, is that it is not fully removing the key from index from being searched. After deletion is called, the key and data is deleted however when calling the search function again, the key is still looked for and will call a notfound error

@PragmaTwice Is there a particular way you would want key expiration to be handled? 

Never mind, this pr seems to messing with the original Lint/Check code process. Maybe I can open an issue to check why these changed lines from ./x.py format cause an error when clang-format is run. Anyone have thoughts on this?

To avoid such inconsistency, please use clang-format 14 which is the same version as in CI pipeline.\r\n\r\nClosed.

```\r\n--- FAIL: TestBitmap (846.21s)\r\n    --- FAIL: TestBitmap/SETBIT/GETBIT/BITCOUNT/BITPOS_boundary_check_(type_bitmap) (150.90s)\r\n        bitmap_test.go:204: \r\n            \tError Trace:\t/Users/runner/work/kvrocks/kvrocks/tests/gocase/unit/type/bitmap/bitmap_test.go:204\r\n            \tError:      \tReceived unexpected error:\r\n            \t            \tread tcp 127.0.0.1:50435->127.0.0.1:50429: i/o timeout\r\n            \tTest:       \tTestBitmap/SETBIT/GETBIT/BITCOUNT/BITPOS_boundary_check_(type_bitmap)\r\n```\r\n\r\nThe above check failure _seems_ to be an unrelated i/o timeout; is there a way to rerun the checks?

@PragmaTwice @git-hulk could you take another look at this? It seems the previous opportunity to merge was missed because new changes made it to the unstable branch.

@fstd2 It would be great if you could add a Go test case to prevent breaking in the future.

Running `./x.py format` should make the linter happy.

@jonathanc-n Thank you for this contribution. Great job!

Thanks for your contribution!\r\n\r\nHowever, I cannot find a proposal to state the design details and storage format of the new data structure.\r\n\r\nMay I ask you provide such a proposal before we continue to review this patch?

@PragmaTwice it is mentioned in #2539, I added some extra information to clarify some things.

@PragmaTwice Sorry, about this implementation, are you able to close this pr. I might come back to it with a completely different code implementation.

Sure.

Please delete, this was my bad when attempting to update my local fork

Test case is added + the node checking is more flexible

@jonathanc-n #2401 is caused by missing a newline between nodes, and omitting the newline at the end should be allowed.

@git-hulk Alright, it should be fully working now.

@jonathanc-n You can enhance this inside [parseClusterNodes](https://github.com/apache/kvrocks/blob/2a0c57ac7fc4499a210bc452884025a0010cb470/src/cluster/cluster.cc#L712)

@PragmaTwice @git-hulk This should be good I believe, I moved the changes to cluster.cc.

@jonathanc-n It would be better to check if the field is node id in line 778? https://github.com/apache/kvrocks/blob/2a0c57ac7fc4499a210bc452884025a0010cb470/src/cluster/cluster.cc#L778\r\n\r\nThe current solution is too tricky from my perspective.

@git-hulk I simplified as much as I thought I could for the issue to not persist. I believe it should be fine.

@git-hulk @PragmaTwice Pretty sure this should be good, it passed the workflow. Unless you see some changes that should be made.

@git-hulk Alright, should be fine now, sorry about that.

@jonathanc-n No worry, thanks for your great contribution.

@poipoiPIO thanks for your contribution.

@poipoiPIO Thank you.

> @poipoiPIO Thank you.\n\nThank you for reviewing!

Workflow should be good I believe

Retrigger CI, would merge if all ci passed

Was there a change made to GetOptions recently? There seems to be a problem after the merge.

> Was there a change made to GetOptions recently? There seems to be a problem after the merge.\r\n\r\nYes, the operations to db requires an options here.

I believe this pr should be good for further review

@mapleFU do the changes seem right? sorry for the bother.

@PragmaTwice Is the workflow able to run again?

@jonathanc-n would you mind fix the lint?

@git-hulk @PragmaTwice I think a workflow can be run again with the changes

@PragmaTwice Would you be able to review this pr?

@mapleFU something seems to be a little odd here, I ran clang-format on all changed files and its running into formatting issues

@mapleFU Test cases are implemented! Ty for the test suggestions.\r\n

@mapleFU Does this seem fine to move forward?

@mapleFU Alright, thanks for the reviews, I put the changes in for it.

@mapleFU Srorry about that, fixed the lint.

I just reach home and have some minor updates, would continue tomorrow. Sorry for delaying

@mapleFU Were you gonna make your own changes to the code?

I believe implementing an elegant solution to track and restrict client connection memory usage is challenging and could reduce code maintainability. \r\n\r\nInstead, I suggest exploring alternative approaches, like limiting the number of clients.

> I believe implementing an elegant solution to track and restrict client connection memory usage is challenging and could reduce code maintainability.\r\n> \r\n> Instead, I suggest exploring alternative approaches, like limiting the number of clients.\r\n\r\n\r\n\r\n> I believe implementing an elegant solution to track and restrict client connection memory usage is challenging and could reduce code maintainability.\r\n> \r\n> Instead, I suggest exploring alternative approaches, like limiting the number of clients.\r\n\r\nWhen I looked at Redis previously, it uses the maxclients configuration option to limit the number of clients, and maxmemory-clients to limit the total memory used by connections. If you only want to limit the output buffer, you can use client-output-buffer-limit to restrict the usage per connection.

@PragmaTwice  @git-hulk  Hi, should we continue pushing this PR forward, or can it be closed?

It appears that CI is experiencing an SSL certificate issue😥

> It appears that CI is experiencing an SSL certificate issue😥\r\n\r\nHmm, I quickly read a CI error. What do you mean about ssl issue? 

> > It appears that CI is experiencing an SSL certificate issue😥\r\n> \r\n> Hmm, I quickly read a CI error. What do you mean about ssl issue?\r\n\r\nIt failed randomly during the first round of testing, possibly due to a networking issue unrelated to the changes made here.

Hi, could someone please take a look at this PR as well?🥰

@c8ef I will merge this PR if no further in one hour.

@c8ef Thank you!

@AntiTopQuark Thanks for your efforts. Would you mind adding a Go test case for this?

> @AntiTopQuark Thanks for your efforts. Would you mind adding a Go test case for this?\r\n\r\nSure, I will complete this task later.

While adding cases, it was found that there were no corresponding error messages, so error codes were added and reported in many places involving batch processing

> While adding cases, it was found that there were no corresponding error messages, so error codes were added and reported in many places involving batch processing\r\n\r\nYes, that makes sense.

@AntiTopQuark One comment inline, rest are good to me. Could anyone also have a look at this PR? @PragmaTwice @torwig @caipengbo @mapleFU 

> Also cc @PragmaTwice\r\n> \r\n> Do we need some macro like `RETURN_NOT_OK`, `RETURN_NOT_OK_FROM_ROCKSDB`?\r\n\r\n@mapleFU @AntiTopQuark `RETURN_IF_ERR` is also good for this?\r\n

> Also cc @PragmaTwice\r\n> \r\n> Do we need some macro like `RETURN_NOT_OK`, `RETURN_NOT_OK_FROM_ROCKSDB`?\r\n\r\nNope. They can be done by GET_OR_RET.

> Nope. They can be done by GET_OR_RET.\r\n\r\nSo `GET_OR_RET` can also handle the case for pure "Status"?\r\n\r\nBesides, what about err from rocksdb?

> LGTM. But seems no regression tests to guard this manner ..\r\n\r\nUnit tests can cover this scenario. The reason why unit test was ineffective before is that callback was not registered for the item, which means `config set` command did not actually execute in rocksdb. See\r\n![image](https://github.com/user-attachments/assets/5c57822c-f2f7-4583-877a-0635c7b7b0e1)\r\n\r\n

We can first checkin, and adding a cfg testing, and everyone we add a new config, a go-side cfg test should be added to cover the case?

@AntiTopQuark Running `./x.py format` should make the linter happy.

> Maybe we should separate it into following PRs to make it easier to merge and review\r\n\r\nI agree with it.

@furkan-bilgin Thanks for your contribution. Could you please help to pass the CI lint according to https://github.com/apache/kvrocks/actions/runs/10441248380/job/28912245323?pr=2495

@furkan-bilgin Looks good if you can add some Go test cases to cover this. And another point is that we should disable the `EV_READ` for connection while pausing?

@git-hulk Alright got it, thanks for the feedback!

@git-hulk Are you able to check if this looks good? Thank you in advance!

@git-hulk Thanks for the review, sorry about the messy PR. I will make sure it is better in the future.

@jonathanc-n A few new comments, rest are good to me. Thank you.

Tried the benchmark framework today, which took more than one hour to upload, but didn\

@Beihao-Zhou Do you know if theres a place where you can configure your compute for kvrocks?

LGTM, we can add a test case to avoid unexpected changes on this?

> LGTM, we can add a test case to avoid unexpected changes on this?\r\n\r\nDone.

Do we need to provide oldest sequence number?

> Do we need to provide oldest sequence number?\r\n\r\nAFAIK, we don\

> we don\

> > we don\

@git-hulk @PragmaTwice this patch:\r\n1. Does some minor optimization in command implemention\r\n2. Fix the bug for `Command` impl\r\n3. Gocase testings

Thanks @PokIsemaine for telling me the bug in cmd here

> Would you mind try redis-benchmark to test some commands? I don\

@mapleFU This PR generally looks good to me. Huge thanks for your efforts @LindaSummer.

Hi @mapleFU ,\r\n\r\nSorry for delay reply.\r\nI have added constraints for `parallel_for` and `parallel_reduce`.\r\nIt should now only use one thread for execution.\r\n\r\nBest Regards,\r\nEdward

Generally looks good to me, only one comment inline.

@PragmaTwice would you mind check again?

@AntiTopQuark What you did in this PR did not match issue #2284, which intended to control the output buffer limit instead of the process memory. The current implementation also cannot promise maximum memory usage since we have many background threads and existing connections might request more memory than that.

> @AntiTopQuark What you did in this PR did not match issue #2284, which intended to control the output buffer limit instead of the process memory. The current implementation also cannot promise maximum memory usage since we have many background threads and existing connections might request more memory than that.\r\n\r\nI apologize for the oversight. I will revise the code to align with the specified requirements.\r\n\r\n

@AntiTopQuark No worry, thanks for your efforts.

Just ping me if ready for code review

I think maybe we can just use `lua_isinteger` to check before calling  `lua_tointeger`?

> I think maybe we can just use `lua_isinteger` to check before calling `lua_tointeger`?\r\n\r\nThis API seems to be supported by lua 5.3

@mapleFU To see if you have further comments? or we can merge this PR to avoid pending too long.

This PR is ready for review! @PragmaTwice \r\nI probably want to add the hybrid executor after implementing the IR expressions (See the Next section in the first conversation block) if this sounds good to you. <3

I think we need to add a `Setup Debian` step into `steps`, just like `Setup OpenSUSE` or `Setup ArchLinux`, to do some dependency installing.

And why Debian 11 instead of 12, our docker base images based on bookworm-slim image

Updated 👍, kindly have a look and suggest changes, if required \r\n\r\nBest\r\n@shikharvashistha 

Seems CI is stucked due to unknown reason. Let me retrigger it.

I will close this PR as there is no further progress. Feel free to reopen it.

@PragmaTwice Yes, the RESP functions are more about the Redis protocol than the connection between a client and the server.

https://github.com/apache/kvrocks/blob/unstable/tests/gocase/tls/tls_test.go#L57 This case would not return the NOAUTH error since it sent the plain command to the TLS port. So it will return the connection closed by peer error.

Please, review.

> Please, review.\r\n\r\nI suggest you can avoid using imperative sentences. It reads rudely.

> And we use the second bit of the flags in metadata to indicate whether encode the expiration into value.\r\n\r\nAs we specified in https://kvrocks.apache.org/community/data-structure-on-rocksdb#encoding-version-and-data-type, the first and second (and maybe more bits from the MSB) of `flags` is used to represent the "encoding version". `flags` `1 1 0 0 | 0 0 1 0` indicates that the encoding version is 3.\r\n\r\nSo it should not be used for field expiration. (Also, not all data types need the field expiration feature.)\r\n\r\nInstead, you can add a new enum value of data type, or add a new field in the metadata of hash.

@jjz921024 Thanks for your contributions. As @PragmaTwice mentioned, we cannot put the expired flag in the version field and we also need to adopt the SubkeyFilter in compact_filter.cc to recycle expired subkeys.

> we also need to adopt the SubkeyFilter in compact_filter.cc to recycle expired subkeys.\r\n\r\n@git-hulk Thanks for you reminding. I have added this feature in latest commit.\r\n\r\nAbout `encoding`, my previous thought was to use each bit for a different meaning, rather than as an incremental version.\r\nFor example:\r\n  - For all type, the first bit indicate use 64bit or 32bit encoding\r\n  - For hash type, the second bit of the `flags` to indicate whether encode the expiration into value (not affect for other types)\r\n\r\nI see this issues #2292. Maybe we should finish it first?\r\n\r\n\r\n

> For all type, the first bit indicate use 64bit or 32bit encoding\r\nFor hash type, the second bit of the flags to indicate whether encode the expiration into value (not affect for other types)\r\n\r\nNo. It\

@PragmaTwice Got it, i will change it.

> > For all type, the first bit indicate use 64bit or 32bit encoding\r\n> > For hash type, the second bit of the flags to indicate whether encode the expiration into value (not affect for other types)\r\n> \r\n> No. It\

@git-hulk Hi, I had renamed the `decodeFieldAndTTL` to `decodeExpireFromValue`, `encodeFieldAndTTL` to `encodeExpireToValue`.  And rewrite the `IsFieldExpired()` to make it clearer. please review.\r\n\r\n

Modification:\r\n1.  In `hlen` command, count the number of fields at hash expiration enable,  replace the previous method to avoid loading all of fields into memory.\r\n2. For most `Generic`  type commands (eg: ttl, type, copy, expire, scan...). If the type is hash, we still need to check each fields. If all of fields was expired, the GetMetadata should return not found.\r\n3. In `del` command, if delete a hash object that all of fields expired, so this hash object should be treated as empty and should not affect the deleted_cnt.

> As I mentioned before, we can add a config option (e.g. `hash-field-expiration yes/no`) for users to decide if they want to enable field expiration.\r\n> \r\n> Also our testing can benefit from this option since we can cover the old hash encoding.\r\n\r\n@PragmaTwice Thank for you tip, I have finished this feature. please review.

@git-hulk @PragmaTwice @torwig Sorry to bother your. I have changed the code as review required. May I ask your review it again? I think this is a very useful feature and hope it can be merge to kvrocks. \r\nIf there are any changes, please let me know. Thank.

I had fixed the gofmt error and passed all CI in my fork repo.\r\n\r\nhttps://github.com/jjz921024/kvrocks/actions/runs/10337719500

Seems we need to solve the conflict first?

> Seems we need to solve the conflict first?\r\n\r\nrebase this pr on the latest commit of unstable branch.

Seems there are failure in CI.  I will check it up

@jjz921024 I will take another pass in a few days, thanks for your contribution.

rebase this branch on the HEAD

LGTM, one comment inline.

Hi @git-hulk ,\r\n\r\nI find it seems hard to create unit tests for the error handling code and pass sonarcloud coverage.\r\nCould you give me some suggestions?\r\n\r\nBest Regards,\r\nEdward

@LindaSummer That’s fine for this PR, no need to add the extra tests.

Hi @git-hulk ,\r\n\r\nGot it! 😊\r\nThanks very much for your help and patience!\r\n\r\nBest Regards,\r\nEdward

The CI failure should be caused by EOL of CentOS 7: https://github.com/pypa/manylinux/issues/1641

@shoothzj thanks for your contribution.

@PokIsemaine \r\n> First, regarding item 2 on the TODO list, I find this situation a bit confusing because I am not sure if this is the intended design of kvrocks. If possible, please determine whether this is a normal situation.\r\n\r\nThis is indeed the case at the moment. The migrated slots is not checked when the migration is performed, and I think they should be checked.\r\n\r\n> Second, concerning items 1 and 3 on the TODO list, these are enhancements. Please evaluate whether these features are needed and if they should be included in this PR.\r\n\r\nI think these are good improvements, but we can add them in the subsequent PR. This PR is enough.\r\n

@caipengbo Ok, thanks for the suggestion. So for the second TODO, do we try to fix it at this PR or create another separate issue to track it?

@PokIsemaine I think this PR is ready to review now?

Could you fix these clang-tidy issues? Please make sure the code can be successfully compiled.\r\n```\r\n/home/runner/work/kvrocks/kvrocks/build/_deps/fmt-src/include/fmt/base.h:1619:3: error: static_assert failed due to requirement \

Will merge this PR if no further comments and CI is passed.

Nice work, thanks!

@git-hulk Thanks for your quick reply, I will figure out it why :)

@git-hulk Why do you think configuration not works? I guess ci is failed because I bump the typos-check version also, I can also fix new typos in this PR.\r\n\r\nI debug this actions in my fork repo. It seems paas dir successfully.\r\n\r\nhttps://github.com/shoothzj/kvrocks/actions/runs/9732469130/job/26858166901\r\n\r\n```\r\n2024-06-30 13:42:57 (250 MB/s) - ‘typos-v1.22.9-x86_64-unknown-linux-musl.tar.gz’ saved [5575207/5575207]\r\n\r\n./typos\r\njq: jq-1.6\r\n$ ./typos . --config ./.github/config/typos.toml\r\nWarning: "arange" should be "arrange".\r\nWarning: "arange" should be "arrange".\r\n```

> @git-hulk Why do you think configuration not works? I guess ci is failed because I bump the typos-check version also, I can also fix new typos in this PR.\r\n> \r\n> I debug this actions in my fork repo. It seems paas dir successfully.\r\n> \r\n> https://github.com/shoothzj/kvrocks/actions/runs/9732469130/job/26858166901\r\n> \r\n> ```\r\n> 2024-06-30 13:42:57 (250 MB/s) - ‘typos-v1.22.9-x86_64-unknown-linux-musl.tar.gz’ saved [5575207/5575207]\r\n> \r\n> ./typos\r\n> jq: jq-1.6\r\n> $ ./typos . --config ./.github/config/typos.toml\r\n> Warning: "arange" should be "arrange".\r\n> Warning: "arange" should be "arrange".\r\n> ```\r\n\r\n@shoothzj Thanks for your investigation. I found those typos weren\

> since it uses different versions of typos\r\n\r\nThis is the reason. Typos update its vocabulary about each month.

We can rename `arange` to `a_range`.

@git-hulk @PragmaTwice I downgraded the version, I think we can merge this first. As for `arange` `brange`, I will send a patch later this week.

Thanks for your contribution!

> LGTM. Ideally, it would be great to add the test case for the filtering by IDLE time.\r\n\r\nWill add it in another PR.  :)

https://github.com/apache/kvrocks/pull/2384 is merged, could you refactor this PR to avoid iterating the whole stream subkeys?

> #2384 is merged, could you refactor this PR to avoid iterating the whole stream subkeys?\r\n\r\nSure, will do it later.

Since now the protection on unstable branch is removed unexpectedly, I need to merge it to recover as soon as possible.

Should `StreamEntryID::Maximum` be `{UINT64_MAX - 1, ..}` to avoid the `-1` case here?

Can we merge this PR?

@Yangsx-1 You can merge PR once the CI passed

There is one issue that needs to be considered: after the modification, the case where `EID MS` is -1 needs to be excluded when traversing (iterating) all sub-keys (`EID MS|EID SEQ`).\r\n\r\nAs I mentioned [here](https://github.com/apache/kvrocks/pull/2384#issuecomment-2200221083).

Hi @torwig, could you please review this PR when you have some time?

@Yangsx-1 @PragmaTwice Since we have `UINT64_MAX` as a delimiter, should it be an invalid `seq` of the stream entry ID?\r\nShould we consider changing code [here](https://github.com/apache/kvrocks/blob/unstable/src/types/redis_stream_base.cc#L40), [here](https://github.com/apache/kvrocks/blob/unstable/src/types/redis_stream_base.cc#L182) and [here](https://github.com/apache/kvrocks/blob/unstable/src/types/redis_stream_base.cc#L219)?

It need to be picked to release 2.9.0 to fix a bug in 8547cfb48cb30dc3ad26214980e2c6a61c0e609a.

All macos 11 jobs in CI cannot be started normally.\r\n\r\nWe need to solve #2381 and upgrade to macos 12/13.

@jackyyyyyssss Nice catch!

> go Redis client checks count parameter before passing to server, so count == 0 test case can\

> > go Redis client checks count parameter before passing to server, so count == 0 test case can\

Hi @PragmaTwice ,\r\n\r\nI find that I can run GitHub Actions in my own repo first.\r\nI have run CI in my own repo and fix all build issues now.\r\n\r\nThanks very much.\r\n\r\nBest Regards,\r\nEdward

Hi @torwig ,\r\n\r\nI have updated related code.\r\nPlease take a look.\r\nThanks very much.\r\n\r\nBest Regards,\r\nEdward

Thank you for your contribution!

Hi @PragmaTwice , this PR is ready for review! :)

> The code looks fine to me.\r\n> \r\n> But there are some issues in CI:\r\n> \r\n> * one unit test case failed in macOS arm64,\r\n> * some memory issues (likely use-after-free) reported by ASan/TSan.\r\n> \r\n> Could you try to investigate them?\r\n\r\n@git-hulk @PragmaTwice \r\nThe issue was caused by the `ComputeSimilarity` calculates the distance between `VectorItem` based on the `HnswVectorMetadata`. In the unit test, I initialized `VectorItem` where its `vector` size less than `metadata->dim`, so looping through the vector causes memory leak. \r\n\r\nI changed the code with one `VectorItem::Create` to do this validation early. Let me know if this still looks good to you <3\r\n\r\nCR: https://github.com/apache/kvrocks/pull/2368/commits/224141f8b3eea9f07a5cc2ce7fd5c2027824aa72\r\nSuccessful workflow: https://github.com/Beihao-Zhou/kvrocks/actions/runs/9914854795\r\n

Awesome. Thank you for your contribution!

Hi @Beihao-Zhou , could you also open a tracking issue to track all issues and PRs for vector search in Kvrocks?

> Hi @Beihao-Zhou , could you also open a tracking issue to track all issues and PRs for vector search in Kvrocks?\r\n\r\nSure, will do that later today <3 

https://github.com/apache/kvrocks/blob/84a3559be87ef623d9a2cbca51018be7218835ce/src/cluster/replication.cc#L1003\r\n\r\nForget to change?

@PragmaTwice Thanks for your suggestions, I updated the code

@PragmaTwice Thanks, updated again the code

I added more tests for non interval syntax and renamed the `min_val` parameter

Sounds good to me. We can just have a `./x.py prepare`.

Thanks for your contribution @PokIsemaine!

@PragmaTwice Awesome!

awesome, thanks! 

Hi @torwig ,\r\n\r\n  Thanks for your suggestion.  I have updated the related code.\r\n\r\n  Please take a look.\r\n\r\n  Thanks very much.

CI failed is unrelated, rerun

Maybe I need to rebase on unstable branch rather than merge, it involved merge commit in this PR.

Thanks very much for your warm help. 😊\r\n\r\nThe merge commit seems to make coverage calculated more code blocks than my commits.\r\n\r\nDo I need to solve it with rebase in this PR to re-trigger the coverage workflow?

Would merge if ci passes

No worry about this flaky test failure, can rerun to avoid blocking the PR.

Thanks to @thbley about the some fix and optimizing from https://github.com/apache/kvrocks/issues/2344

@aleksraiden Thanks for taking care of this.

Some troubles in healthcheck on docker, try to fix today or tomorrow

And also, do you have any suggestion to select between debian and ubuntu?

> And also, do you have any suggestion to select between debian and ubuntu?\r\n\r\nTo me both are quite good and work great in production for many companies.\r\nUbuntu lts versions have longer support lifetime, more recent versions of packages (e.g. gcc, openssl), Debian more stable, less recent versions. Key for alpine is size and security, but no glibc.\r\n\r\nfor package version comparison:\r\nhttps://packages.ubuntu.com/\r\nhttps://www.debian.org/distrib/packages\r\nhttps://pkgs.alpinelinux.org/packages\r\n

Yeah, sorry, I have not any real experience with ubuntu, but over 10 y use Debian on all our servers, so use it as a matured server-side os.\r\n\r\nIn other hands, the idea of multiple images (like a community or non-official, in separate directory etc) with other base images my Ok and I can support this feature. \r\n\r\nIn my mind, ideally - alpine 3.20+ but we cant switch due to some dependency. So - Debian looks like good solution at now. 

> But what if there are some namespaces? We also need to consider this situation.\r\n\r\n`Rewrite()` will persist them if there are any namespaces(except the default), and then remove them from the config file. So it should be good for this scenario.

cc @apache/kvrocks-committers 

Thank you for your great finding.

> So generally we can leave it as is until we make a big number implementation available.\r\n\r\nWhat do you mean by directly adding a catch (const jsoncons:: json_runtime_error<std:: runtime_error>e){\r\nReturn {Status:: NotOK, e.what()};\r\n}Is that enough for this ？

> > So generally we can leave it as is until we make a big number implementation available.\r\n> \r\n> What do you mean by directly adding a catch (const jsoncons:: json_runtime_error<std:: runtime_error>e){ Return {Status:: NotOK, e.what()}; }Is that enough for this ？\r\n\r\nYeah, just preventing kvrocks from crash is enough.

![image](https://github.com/apache/kvrocks/assets/127465317/aa6f2c9f-b078-4f4c-bb30-284b1a297b2a)\r\nThese errors seem to have nothing to do with the changes I made this time\r\nplease help retry

@jackyyyyyssss Thanks for your fix.

NOTE: \r\nThis is a relatively large PR that involves most of the functional modules. I tried to split them into smaller PRs but failed because the changes are interrelated and propagate among each other. \r\n\r\nTherefore, to better advance this work, I believe we need to conduct the review gradually and multiple times. At the start of each review, I will post a `"REVIEW X"` comment to inform which parts are being reviewed and highlight any points that need attention or discussion. When this review is complete, please reply to the comment with `"REVIEW X ok"` and I will start the next review. If you have any questions or suggestions during this period, please include the prefix `"REVIEW X"` to indicate which stage it belongs to. \r\n\r\nAlthough there are still some `TODO` items, I plan to discuss and refine them during the review process as you become more familiar with the background of these `TODOs`. \r\n\r\nIf you think this approach is feasible, I will soon start the first review, change the PR from `Draft` to `Review` status, and then switch it back to Draft status for modifications. \r\nThe overall process is: `REVIEW 1 => DRAFT modification => REVIEW 1 ok => REVIEW 2 => DRAFT modification => REVIEW 2 ok...`.

**Update:**\r\n- Removed `LatestSnapshot` and `GetOptions`, replacing them with `Context`.\r\n- Added `NoTransactionContext` to `Context` along with some encapsulated APIs to obtain commonly used `ReadOptions`.\r\n- The `Context` uses a guard in its construction, destruction, and `ResetLatestSnapshot` methods, as a data race issue with the `storage` was discovered in the replication test cases.\r\n- Cleared all TODOs related to `Context`.\r\n    - In `slotmigrator`, the majority of TODOs suggest that `Context` is unnecessary, as there is already a setting for `slot_snapshot_`, which has low transaction requirements. Moreover, writing a large amount of data to `context batch` could severely impact performance.\r\n    - In the TODO for the `search` module: It seems that the basic functionality of the `search` section has been completed, so I attempted to add `Context`. The main goal was to determine the `Context` during the construction of `ExecutorContext`, allowing multiple `ExecutorNodes` on the same execution plan tree to use the same `Context`.\r\n- Added a test for `WriteBatchIndexer`\r\n\r\n\r\n**Review 2:**\r\n- Modifications under `types`  \r\n- `search` module  \r\n- Places where `no_txn_ctx` is used  \r\n\r\n

The speedb is no longer active in the public repository after acquiring, maybe we can remove it. What do you think? @apache/kvrocks-committers 

![image](https://github.com/user-attachments/assets/1f6b6bb9-95cd-4b4d-8189-de011eda8d8b)\r\nOccasional problems in CI are caused by https://github.com/apache/kvrocks/issues/2473. Temporarily use sleep to make the test pass. Remove sleep after fixing it.

@PokIsemaine Sorry for the late review, a few comments inline.

Unrelated to this pr: I think we can also add a server side conf to allow disable `is_txn_mode`, this is useful when we have some performance issue with DeleteRange or make something wrong. We can merge this first and add this in a separate patch

> Unrelated to this pr: I think we can also add a server side conf to allow disable `is_txn_mode`, this is useful when we have some performance issue with DeleteRange or make something wrong. We can merge this first and add this in a separate patch\r\n\r\nI agree, this PR has a wider impact and we should provide the option to turn it on and off. \r\n\r\nIn subsequent observations, in addition to performance and errors, memory usage also needs attention, because we have an additional `WriteBatchWithIndex`.

Will merge this after ci pass

If this PR https://github.com/apache/kvrocks/pull/2348 will be OK, we can update to use latest rocksdb (as I hope)))

Now we are ready to merge (wow!)

Need to confirm why there are go test failures on macos 12.

realpath util not included at macos-12 (see: https://apple.stackexchange.com/questions/450035/is-the-unix-realpath-command-distributed-with-macos-ventura)

realpath are fixed, some go integration test broken by i/o timeout

> There are several issues in this implemetation:\r\n> \r\n> * the result of `as_string().size()` is not equal to the real size, we should at least use `to_string()`\r\n> * but actually `to_string` is also wrong, since it maybe not equal to the result of `compact_json_string_encoder`. also it would be totally wrong if CBOR format is enabled.\r\n> * there may be multiple location found from one jsonpath, which is not handled in this impl\r\n> * if jsonpath is `$`, no json decoding is required\r\n\r\nThank you for your help. The modifications have been completed, please help me take a look again

@VasuDevrani Should we also fix those errors in the scope of this PR?

> @VasuDevrani Should we also fix those errors in the scope of this PR?\r\n\r\nmaybe yes (but they are a lot), I think the CI failing because of those.

Yeah we should fix them otherwise the PR will be blocked by CI failure.\r\n\r\nYou can use the attribute `[[maybe_unused]]`.

> You can use the attribute `[[maybe_unused]]`.\r\n\r\nWouldn\

> > You can use the attribute `[[maybe_unused]]`.\r\n> \r\n> Wouldn\

We already uses C++17, yeah?

@xiaobiaozhao You need to turn the default value of rocksdb.async_io to true as well. Another quick question: is the rocksdb community marking this feature as stable?

The feature has been unlabeled for experimentation, but rocksdb has not enabled it by default.

> Do you mean on older kernels? I just test on ubuntu 20/22/24 , it works well.\r\n\r\nOn legacy os it uses "Read" rather than ReadAsync api

The CI failure is not related to this PR, will take a look later.

related issue: https://github.com/apache/kvrocks/issues/2295

`./x.py format` should fix linter issues.

> `./x.py format` should fix linter issues.\r\n\r\ndone

Format problems occurs frequently, let me create an extra issue for that: https://github.com/apache/kvrocks/issues/2301

Use rand() function in unit testing. That check could be ignored, I guees.

Verified: All test passed with no ubsan error in My M1 MacOS with `-DENABLE_ASAN=ON -DENABLE_UBSAN=ON -DDISABLE_JEMALLOC=ON -DASAN_WITH_LSAN=OFF` . I can decrease `no-sanitize` later

Could we also add it to the worflow matrix in this PR?\r\nhttps://github.com/apache/kvrocks/blob/unstable/.github/workflows/kvrocks.yaml

> Could we also add it to the worflow matrix in this PR? https://github.com/apache/kvrocks/blob/unstable/.github/workflows/kvrocks.yaml\r\n\r\nPreviously I think we should add after some issue is fixed, let me add it now.\r\n\r\n

Damn: https://github.com/facebook/rocksdb/pull/12427/files#diff-084cbffee5bd519be89896acbf616c2d8435fbfe699f9dcac7e7ccb3fad7f586\r\n\r\nThis patch fix a ub in rocksdb 😅, the `IOOptions::verify_and_reconstruct_read` is not initialized yet

cc @jjz921024 @PragmaTwice @git-hulk 

@git-hulk @PragmaTwice Would you mind take a look?

@mapleFU Sure, will take a look today.

@jackyyyyyssss Good catch!

some test erro i will fix

@jackyyyyyssss Can use `require.ErrorIs(err, redis.Nil)`

@jackyyyyyssss  Running `./x.py format` should prevent lint errors.

These errors seem to have nothing to do with the changes I made this time  \r\n![image](https://github.com/apache/kvrocks/assets/127465317/af9ce521-963c-4efe-9dec-819b2aa40637)\r\n![image](https://github.com/apache/kvrocks/assets/127465317/54b5edf1-3697-46b3-bb43-6b44a08ddb5b)\r\n

Seems like some flaky tests.

> Seems like some flaky tests.\r\n\r\nYes, not related to this PR. Will rerun and merge after it passed.

Ready for review now : )

Could you write some test cases for your patch? Thank you!

> Could you write some test cases for your patch? Thank you!\r\n\r\nHere I just changed a coding method. The previous tests can still be used. I have now added some corner cases.

> > Could you write some test cases for your patch? Thank you!\r\n> \r\n> Here I just changed a coding method. The previous tests can still be used. I have now added some corner cases.\r\n\r\nGenerally, if we fixed a bug in a patch, we need to add a test case, that should fail without the patch, and succeed after the patch is applied.

@AntiTopQuark Thanks for your efforts.

I can confirm it also works in Redis 4:\r\n\r\n```\r\n127.0.0.1:6379> RESTORE list 0 "\\x0e\\x04\\x0e\\x0e\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x01\\x00\\x00\\x01d\\xff\\x0e\\x0e\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x01\\x00\\x00\\x01c\\xff\\x0e\\x0e\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x01\\x00\\x00\\x01b\\xff\\x0e\\x0e\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x01\\x00\\x00\\x01a\\xff\\x06\\x003\\x06\\xd4\\x86\\x93\\xfbvB"\r\nOK\r\n127.0.0.1:6379> lrange list 0 -1\r\n1) "d"\r\n2) "c"\r\n3) "b"\r\n4) "a"\r\n```

@AntiTopQuark Need to reformat codes.

@mapleFU Can we have int64_t instead of time_t everywhere? Maybe just rename these variables as `timestamp` and add a comment `// in seconds/in milliseconds`? WDYT?

Both is ok for me. Currently the different "time" is really confusing, any possible change is ok

@git-hulk @PragmaTwice Would you mind take a look? I can accept any idea here.

@mapleFU It looks great with the unit of time. But some places are using `_secs` and `_sec` as the suffix, perhaps we can keep them consistent. 

I think the best idea is to use some types with unit like chrono::duration, but the affected code range will be very large.\r\nSo currently you can add unit to variable names.

@git-hulk Hi, done it, please review

@jjz921024 Thank you! Generally looks good to me, you can fix the Go format to pass the CI.

> @jjz921024 Thank you! Generally looks good to me, you can fix the Go format to pass the CI.\r\n\r\nYes, I had fix it

Will merge in 1-days if no negative comments

@jjz921024 Thanks for your contributions.

I believe "lite mode" is not a suitable name as it does not clearly convey its purpose.\r\n\r\nWe should find a more specific name instead of labeling any random behavior as "lite mode".

> I believe "lite mode" is not a suitable name as it does not clearly convey its purpose.\r\n> \r\n> We should find a more specific name instead of labeling any random behavior as "lite mode".\r\n\r\nMaybe we can rename this option to `reduce_infrequently_column_memory_usage` so that explicitly tell the user that it is used to reduce the memory size of an infrequently used column family?\r\n

> > I believe "lite mode" is not a suitable name as it does not clearly convey its purpose.\r\n> > We should find a more specific name instead of labeling any random behavior as "lite mode".\r\n> \r\n> Maybe we can rename this option to `reduce_infrequently_column_memory_usage` so that explicitly tell the user that it is used to reduce the memory size of an infrequently used column family?\r\n\r\nIt can be `minor_columns_write_buffer_size num`.

@jjz921024 Maybe a simpler way is:\r\n1. Config an extra "rocksdb.minor_write_buffer_size" as extra arguments, it can be a bit smaller\r\n2. "default" and "metadata" cf is "major", other column families are "minor"\r\n3. Adjust the size for these two parts. There are two kinds of ways:  (1) using same default "rocksdb.minor_write_buffer_size", or even a smaller "rocksdb.minor_write_buffer_size", and separate the two args. This would nice for new user and most users, since it could reducing the arg here and just improve some performance (2) Using current way, this would be good for some extra user who adjusted the write_buffer_size previously\r\n\r\nPersonally (3) is a bit hard but I think these CF should be tuned themselves in the future, maintaining two configs both for "minor"( like if "rocksdb.write_buffer_size" is set and "rocksdb.minor_write_buffer_size"  doesn\

Update: bump rocksdb to 9.1.1 (bugfix release - https://github.com/facebook/rocksdb/releases/tag/v9.1.1)\r\n\r\n- Fixed a regression when ColumnFamilyOptions::max_successive_merges > 0 where the CPU overhead for deciding whether to merge could have increased unless the user had set the option ColumnFamilyOptions::strict_max_successive_merges

@git-hulk @PragmaTwice I need your superpower here when you have time. Please take a look at the build error. I tried to figure out the reason but failed. Just a regular RocksDB code was changed between 9.1.0 and 9.1.1 but the error somehow appeared.

Maybe can help us - https://github.com/jemalloc/jemalloc/issues/778 and https://github.com/google/tcmalloc/issues/150 (but tmalloc)

Now we can try update from upstream and check it

Hmm, not working, like a mystic )

succeeded by #2327.

Generally LGTM, only one comment is inline.

@proost Thanks for your contribution.

> I found sonar cloud tells us "Rename function "TestCopy_Error" to match the regular expression ^(_|[a-zA-Z0-9]+)$". Can you change those go test case function names to pascal Case (eg: `TestCopyError`)?\r\n> \r\n> https://sonarcloud.io/project/issues?resolved=false&pullRequest=2238&id=apache_kvrocks&open=AY7N7bb-6GZZ34cBM2MJ\r\n\r\nOK. I will also change the function name in rename_test.go by the way.

@git-hulk One ci test failed. Did it happen occasionally or something wrong?

@jackyyyyyssss CommandSubkeyScanBase should have the same issue which is used for HSCAN/SSCAN/ZSCAN: https://redis.io/docs/latest/commands/sscan/.

> @jackyyyyyssss CommandSubkeyScanBase should have the same issue which is used for HSCAN/SSCAN/ZSCAN: https://redis.io/docs/latest/commands/sscan/.\r\n\r\n\r\nOkay, can I use different PR for these modifications？

@jackyyyyyssss Can remove ParseMatchAndCountParam if no place is using it.

> @jackyyyyyssss Can remove ParseMatchAndCountParam if no place is using it.\r\n\r\nok

I will prepare a patch to succeed this PR.

> I will prepare a patch to succeed this PR.\r\n\r\nCan you help me take a look at the optimized version thks

@zjregee It seems like CI is OK now.

@AntiTopQuark Running the `./x.py format` should fix the linting issues.

> Since kvrocks does not support encoding methods such as listpack and lzf, most types calculate the dump payload string using the RAW method.\r\n\r\nI feel good about this, and we can make it work first and then improve. 

Hi @Chiro11, thanks for your PR.\r\nIn the issues, we think we need to add a new command. Can you move the code to a new command?\r\nhttps://github.com/apache/kvrocks/issues/1950#issuecomment-1963244776

> Hi @Chiro11, thanks for your PR. In the issues, we think we need to add a new command. Can you move the code to a new command? [#1950 (comment)](https://github.com/apache/kvrocks/issues/1950#issuecomment-1963244776)\r\n\r\nYou mean to use a new name? Do you have any idea?

> > Hi @Chiro11, thanks for your PR. In the issues, we think we need to add a new command. Can you move the code to a new command? [#1950 (comment)](https://github.com/apache/kvrocks/issues/1950#issuecomment-1963244776)\r\n> \r\n> You mean to use a new name? Do you have any idea?\r\n\r\nYes. @git-hulk @PragmaTwice @mapleFU @enjoy-binbin Do you have some idea about new command name?

> > > Hi @Chiro11, thanks for your PR. In the issues, we think we need to add a new command. Can you move the code to a new command? [#1950 (comment)](https://github.com/apache/kvrocks/issues/1950#issuecomment-1963244776)\r\n> > \r\n> > \r\n> > You mean to use a new name? Do you have any idea?\r\n> \r\n> Yes. @git-hulk @PragmaTwice @mapleFU @enjoy-binbin Do you have some idea about new command name?\r\n\r\nPerhaps MOVEX is a good name for aligning with the CLUSTERX command.

> > > > Hi @Chiro11, thanks for your PR. In the issues, we think we need to add a new command. Can you move the code to a new command? [#1950 (comment)](https://github.com/apache/kvrocks/issues/1950#issuecomment-1963244776)\r\n> > > \r\n> > > \r\n> > > You mean to use a new name? Do you have any idea?\r\n> > \r\n> > \r\n> > Yes. @git-hulk @PragmaTwice @mapleFU @enjoy-binbin Do you have some idea about new command name?\r\n> \r\n> Perhaps MOVEX is a good name for aligning with the CLUSTERX command.\r\n\r\n@jihuayu @git-hulk Modified as suggested.

@Chiro11 Thanks for your efforts and quick response.

Rest are good to me, one comment inline.

`MOVEX key ns_name ns_token`\r\n\r\nI think here the `ns_name` is useless.

> `MOVEX key ns_name ns_token`\r\n> \r\n> I think here the `ns_name` is useless.\r\n\r\nTheoretically it is. But I think this is more clear. @git-hulk @jihuayu What are the opinions of you two?

@baobaomaomeng Hi, can you resolve the conflicts?

In my opinion, we need a well-designed solution for implementing Redis glob-style patterns that is **scalable**, rather than just handling specific cases. If the design is not scalable, we may have to remove all modifications when aiming to support the complete pattern cases.\r\n

> In my opinion, we need a well-designed solution for implementing Redis glob-style patterns that is **scalable**, rather than just handling specific cases. If the design is not scalable, we may have to remove all modifications when aiming to support the complete pattern cases.\r\n\r\nThank you for your review, but this is not consistent with the goal I communicated at the beginning. I now need to rewrite my code, and I think I need to consider how to write it.Could you give me some reference suggestions？\r\n

closed by #2608.

From my perspective, I agree what @PragmaTwice mentioned. Expect for the design, the main obstacle for Kvrocks performance tracing is how to export the underlying storage metrics to users with an easy way. Maybe we can discuss this in another thread and +1 for this PR.

Seems that performance analysis should be for developers...

Generally looks good, you can fix the tidy issue according to https://github.com/apache/kvrocks/actions/runs/8526319228/job/23369018645?pr=2217#step:7:1294

@kinoute Thanks for your contribution.

Could you add a ctor to GetOptions?

Ready for review now.

Hi @Yangsx-1! I have a question about `MakeCmdAttr`. What are `first_key`, `last_key`, and `key_step` for here? Also for the `description` field, should I have `no-dbsize-check` for `XCLAIM`? Thanks a lot!!

Hi @Beihao-Zhou. \r\nFirst_key, last_key, and key_step are Redis key range. For this command, it will be 1 1 1.\r\nIn my opinion, this command could not add the `no-dbsize-check` flag, `no-dbsize-check` is only added to the command which can make the db smaller.

> Hi @Beihao-Zhou. First_key, last_key, and key_step are Redis key range. For this command, it will be 1 1 1. In my opinion, this command could not add the `no-dbsize-check` flag, `no-dbsize-check` is only added to the command which can make the db smaller.\r\n\r\nI see I seem thank you very much!!

@Beihao-Zhou Thanks for your patch.\r\nCan you fix the CI first? https://github.com/apache/kvrocks/actions/runs/8500498583/job/23284955556?pr=2202#step:7:1062

@Beihao-Zhou Done, thanks for your efforts.

@Beihao-Zhou You can enable action in your [fork repo](https://github.com/Beihao-Zhou/kvrocks/actions),it is free!

> @Beihao-Zhou You can enable action in your [fork repo](https://github.com/Beihao-Zhou/kvrocks/actions),it is free!\r\n\r\nThanks for telling me that!!

> You can fix the CI first.\r\n\r\nSure thing, will fix it and modify code this weekend!

@Yangsx-1 Hiii! Code and CI are fixed (CI succeeds on my fork). Could you please take a look at the PR again? Thanks!!

@Yangsx-1 Thanks for reviewing! Everything is fixed then :))

In this PR, I also removed the unused import_fd_ field in the importor.

Thanks for your review.

This PR is ready for review now : ) cc @git-hulk @mapleFU 

@PragmaTwice Seems the newer cmake version failed to install in CentOS 7: https://github.com/apache/kvrocks/actions/runs/8393130640/job/22987944193?pr=2184#step:8:967

Great job, looks good to me.

> Mark wal_compression feature as production-ready. Currently only compatible with ZSTD compression\r\n\r\nMaybe we can expose it in the config file.

> Important! format_version=6 is the new default setting in BlockBasedTableOptions, for more robust data integrity checking. DBs and SST files written with this setting cannot be read by RocksDB versions before 8.6.0\r\n\r\nWould this be dangerous? We need check which version does 2.0 kvrocks cluster uses?

> I wonder if we\

> > I wonder if we\

Instead, what test coverage we lack that prevents us to confidently upgrade ..

We can be radical in upgrade the bug fix versions, but maybe we should be more carefully when upgrading the big-version changing

> We can be radical in upgrade the bug fix versions, but maybe we should be more carefully when upgrading the big-version changing\r\n\r\nAs I think, an rocksdb team will not  continuous support older version, so if we not upgrade, we will use unsupported version - no patch and bugfix in 8.х 

> As I think, an rocksdb team will not continuous support older version, so if we not upgrade, we will use unsupported version - no patch and bugfix in 8.х\r\n\r\n@aleksraiden Yes, we exactly want to keep the rocksdb version as fresh as possible, the main concern is the timing to introduce this major upgrade.

Sigh, actually we\

Also, maybe we can support a build flag, e.g. -DUSEROCKSDB=8.11.3|9.0.0|upstream(latest commit) ? 

> Also, maybe we can support a build flag, e.g. -DUSEROCKSDB=8.11.3|9.0.0|upstream(latest commit) ?\r\n\r\nI think it could be a good choice for library, however, most users use default version or using our builded mirror. 🤔 It could be useful for only few "advanced" users

And so, what is our plan to do with this changes? As I see in rocksdb repository, yesterday they are starting an 9.1 branch. \r\n\r\nAt my side, in our project, we are switch to our unstable version of kvrocks which are included all latest unstable commits for core components (rocksdb, zstd, libevent, zlib and so on).

>Tests failed with Asan or MacOS. Seems the kvrocks2redis does not work properly.\r\n\r\nIt could be very helpful if you can open an issue for the test failure, with asan log.

I enable the CI for macos and with tsan/asan, we can check the failure first.

No worry. You can firstly work on this PR and make it merged, with ASAN/TSAN/macos.. disabled.\r\n\r\nAnd then feel free to investigate the memory issue in kvrocks2redis, if you are interested.

Created #2195 to track CI problem with tsan/asan and macos. Disabled for them....

And I found the `redis-server` compiles failed in macos14, seems related with https://github.com/redis/redis/issues/12585. Could we upgrade the redis from 6.2.7 to 6.2.14 in CI?

> And I found the `redis-server` compiles failed in macos14, seems related with [redis/redis#12585](https://github.com/redis/redis/issues/12585). Could we upgrade the redis from 6.2.7 to 6.2.14 in CI?\r\n\r\nI think it is OK! @Zakelly 

@jihuayu @PragmaTwice I think this PR is in good shape, and all the CI has passed for commit 2246fac0cd66da860ee1a9fc9860fb63eb2f68a7: https://github.com/Zakelly/kvrocks/actions/runs/8589761419 .\r\n\r\nPlease let me know if you have any other suggestions or comments, thanks!

CI failed is unrelated, rerun

@LiuYuHui Thanks for your contribution.

For review questions, please refer : https://github.com/apache/kvrocks/issues/1212#issuecomment-2000449365

@mapleFU, there is an issue that needs to be discussed : https://github.com/apache/kvrocks/issues/1212#issuecomment-2000449365\r\n\r\nPlease take a look at it when your free.

Rest LGTM, this part of logic is too tricky, thanks a lot for efforts

@sheharyaar Would you mind resolve conflict? I edit the `GetMetadata` in https://github.com/apache/kvrocks/pull/2174/files . Will approve after that

Will merge it tonight if no negative comments

Merged, thanks!

The similar issue in other project: https://github.com/cirruslabs/cirrus-ci-docs/issues/1241 😢 

The ubuntu image version that is failing is ` Version: 20240310.1.0` and the one that worked before was `Version: 20240304.1.0`\r\n\r\nvery recent change I guess by GH

The changes have been introduced in https://github.com/apache/kvrocks/tree/actions-m1.\r\n\r\nAlso the bug has already been fixed.

This issue was reported by the user `Asad Awadia` in Slack.

Thank you for the prompt fix\r\n\r\nWhat is the nightly docker img - i can try it out

> Thank you for the prompt fix\r\n> \r\n> What is the nightly docker img - i can try it out\r\n\r\n@asad-awadia Need to wait for this PR to be merged, and then you can have a try at `apache/kvrocks:nightly`.

Thanks for catching this!

```\r\nStatus: Downloaded newer image for apache/kvrocks:nightly\r\nI20240311 13:09:12.585850     1 main.cc:142] kvrocks unstable (commit 86d34ac)\r\n```\r\n\r\nTried the nightly image and works as expected with resp3 default in lettuce :)

I am testing this modification on my test project to see if it takes effect. This will take some time, please wait.

@jihuayu thanks about your investigation, you are absolutely correct, I see a docs about sonar cube, not a cloud version.

@aleksraiden \r\nThank you for your help. \r\nI am looking into this issue. Just a few more questions to deal.

Performance for sadd(On my WSL2 Ubuntu22, with gcc11.4, CPU AMD 3800X) with default release build:\r\n\r\n```\r\nbefore:\r\nSADD: 27173.91 requests per second\r\n\r\nafter:\r\nSADD: 30211.48 requests per second\r\n```\r\n\r\ncc @git-hulk @PragmaTwice 

@tutububug Thank you for your contribution. Running `./x.py format` should eliminate the linting issues.

Thank you for your contribution! Maybe you need to add a command parser to use the data structure with actual command like redis.

Thank you for your contribution!\r\n\r\nCould you include your design in the PR description? For example, explain how to encode the metadata and HLL data (subkeys), similar to what is shown on https://kvrocks.apache.org/community/data-structure-on-rocksdb.

> Thank you for your contribution! Maybe you need to add a command parser to use the data structure with actual command like redis.\r\n\r\nYes, I will give the commit later.\r\n\r\n> Thank you for your contribution!\r\n> \r\n> Could you include your design in the PR description? For example, explain how to encode the metadata and HLL data (subkeys), similar to what is shown on https://kvrocks.apache.org/community/data-structure-on-rocksdb.\r\n\r\nOK.\r\n

@PragmaTwice I create a PR(https://github.com/apache/kvrocks-website/pull/207) for describe hyperloglog storage format.

I suggest that we can store the number of registers in one rocksdb key in the metadata (for example, referred to as `register_number_in_one_key`), so that even if adjustments are made later for performance reasons, the compatibility of kvrocks data can be maintained.\r\n\r\n Currently, a fixed value for `register_number_in_one_key` can be hardcoded in the code.\r\n\r\ncc @tutububug 

>  So I think the question is, should we also introduce two mode of hll encoding (sparse and dense layout) and an auto switching policy between these two layout?\r\n\r\nI prefer to do this :-) But we can regard it as a further optimization

Other Looks ok to me

@mapleFU @PragmaTwice Thank you for carefully code review. Since the code for the algorithm part comes from redis, some function arguments and comments need to be adjusted. Regarding optimizing key encoding and the number of subkeys, I will refer to your suggestions.\r\ncc @git-hulk 

@tutububug Thanks for your great efforts.

@tutububug FYI:\r\n1. bitfield: https://github.com/apache/kvrocks/blob/unstable/src/common/bitfield_util.h#L33 . You can refer to these logic or rewrite the HLL logic using these yourself. A value in HLL using redis\

@PragmaTwice We can first implement that, and optimize writing to HLL step-by-step. Since 2.9 release would be a long time from now.

@mapleFU @PragmaTwice The code refactoring is ready for review, and the storage format description is sorted out at https://github.com/apache/kvrocks-website/pull/207/files. Please take a look if have time. \r\ncc @git-hulk @torwig  \r\n

@tutububug you can pull the unstable into this branch and have a try.

> @tutububug you can pull the unstable into this branch and have a try.\r\nfixed\r\n

LGTM, one comment to check if we need to add a notice for the source code.

@mapleFU Could you review it again?\r\n

> This looks much better than previous version. The remaining are mostly code-style problem\r\n\r\nfixed.

@torwig Does this PR look good to you? if yes, we can merge since @jihuayu has approved.

I believe this change will have a significant impact as it removes the global stdout buffer. (e.g. logging behavior, performance without buffer..)\r\n\r\nI suggest that we DO NOT pick this patch to version 2.8.0 until a thorough investigation is conducted.\r\n\r\ncc @git-hulk @caipengbo 

@PragmaTwice Thanks for raising this potential issue, I have linked this in the release proposal.

https://github.com/google/glog/issues/943\r\nFLAGS_logtostdout does not seem to follow FLAGS_logbuflevel \r\n是这个问题导致的，如果去修glog应该更好

Thanks for your review. @enjoy-binbin @torwig 

Impressive PR!

When #2123 is merged (fix a bug about `COMMAND` command), the unit tests will be passed :)\r\n

Can someone confirm whether this logic is correct? I am also unable to determine it.\r\nAlternatively, can we ignore whether the algorithm is correct and fix it when a bug arises?

> As mentioned by @jihuayu, the implementation of the algorithm should be put into redis_string.cc instead of cmd_string.cc.\r\n\r\nUnderstood, I have moved the algorithm to `redis_string.cc` as suggested. Please review the implementation and let me know if any further modifications are required.

Thank you for your contribution!

Hi @JoverZhang, can you add this command to our docs? \r\nYou can set `Since Version` with unstable.\r\nhttps://kvrocks.apache.org/docs/supported-commands#string-commands

> Hi @JoverZhang, can you add this command to our docs? You can set `Since Version` with unstable. https://kvrocks.apache.org/docs/supported-commands#string-commands\r\n\r\nOkay, I added it.\r\nhttps://github.com/apache/kvrocks-website/pull/208

cc @git-hulk @PragmaTwice 

> LGTM. I just wondering whether @PragmaTwice will advise to rename `sample_helper.h` to something with `_util.h` suffix like `random_util/random_sample_util/sample_util` :)\r\n\r\nI am fine since this file is not in common dir.

Done. Other parts will be submitted as further PRs. cc @git-hulk

Hmm, I think (thanks, @torwig) that in failed gotest kvrocks not starting yet. Need to investigate this

Great thanks to @torwig about rewrite a gotest for logger. Wuuf, now a longest PR ready to review

@aleksraiden Great!

@PragmaTwice @git-hulk 

> Metadata::ExpireMsToS function will use rounding to get a closer time value if\r\nMETADATA_64BIT_ENCODING_MASK was enabled.\r\n\r\nI think this function will not be used in the 64bit encoding mode?

> need to highlight this modification in our next release note.\r\n\r\nYes, this is an important enhancement and we should have highlighted it.

Repeated comment deleted.

https://github.com/wsehjk/kvrocks/actions/runs/7932113585.  CI has passed 

Code updated.

@wsehjk After taking a look at [addReplyVerbatim](https://github.com/search?q=repo%3Aredis%2Fredis%20addReplyVerbatim&type=code), only one place that needs to be changed in Kvrocks: use verbatim string while replying to the cluster command: https://github.com/redis/redis/blob/c854873746e9515f322d805f863e839ebfcefd62/src/cluster.c#L853\r\n\r\nAnd for the Lua part, I will do it in the next PR.\r\n

> @wsehjk After taking a look at [addReplyVerbatim](https://github.com/search?q=repo%3Aredis%2Fredis%20addReplyVerbatim&type=code), only one place that needs to be changed in Kvrocks: use verbatim string while replying to the cluster command: https://github.com/redis/redis/blob/c854873746e9515f322d805f863e839ebfcefd62/src/cluster.c#L853\r\n> \r\n> And for the Lua part, I will do it in the next PR.\r\n\r\nwhat about the `info` cmd and `client` cmd https://github.com/redis/redis/blob/c854873746e9515f322d805f863e839ebfcefd62/src/server.c#L6082\r\nhttps://github.com/redis/redis/blob/c854873746e9515f322d805f863e839ebfcefd62/src/networking.c#L3086

> > @wsehjk After taking a look at [addReplyVerbatim](https://github.com/search?q=repo%3Aredis%2Fredis%20addReplyVerbatim&type=code), only one place that needs to be changed in Kvrocks: use verbatim string while replying to the cluster command: https://github.com/redis/redis/blob/c854873746e9515f322d805f863e839ebfcefd62/src/cluster.c#L853\r\n> > And for the Lua part, I will do it in the next PR.\r\n> \r\n> what about the `info` cmd and `client` cmd https://github.com/redis/redis/blob/c854873746e9515f322d805f863e839ebfcefd62/src/server.c#L6082 https://github.com/redis/redis/blob/c854873746e9515f322d805f863e839ebfcefd62/src/networking.c#L3086\r\n\r\nYes, can fix them as well.

As discussed before, I replace `redis::BulkString` with `connection->VerbatimString()`reply in `info`, `client` and `cluster`command where in redis codebae `addReplyVerbatim` reply is used

Thanks

@wsehjk Thanks for your contributions! 

> At least in some test cases, we need to validate the indent and spaces of the output JSON. And `JSONEq` will just ignore all of them.\r\n> \r\n> For exmaple, all cases in `JSON.GET with options`.\r\n\r\nThank you, I fixed it.

@aleksraiden I have filed a ticket to the ASF team to allow using dorny/paths-filter@v3, refer: https://issues.apache.org/jira/browse/INFRA-25491 \r\n\r\ncc @tisonkun 

> @aleksraiden I have filed a ticket to the ASF team to allow using dorny/paths-filter@v3, refer: https://issues.apache.org/jira/browse/INFRA-25491\r\n> \r\n> cc @tisonkun\r\n\r\ndorny/paths-filter@v3 is ready to use now.

Try to close and reopen for CI.

> dorny/paths-filter@v3 is ready to use now.\r\n\r\nNope. From the conversation, we need to use `v3.0.0` instead of `v3`.

> > dorny/paths-filter@v3 is ready to use now.\r\n> \r\n> Nope. From the conversation, we need to use `v3.0.0` instead of `v3`.\r\n\r\nI saw the v3 branch is referred to as v3.0.0, so it should be good to use v3.\r\n\r\nhttps://github.com/dorny/paths-filter/tree/v3

> > > dorny/paths-filter@v3 is ready to use now.\r\n> > \r\n> > \r\n> > Nope. From the conversation, we need to use `v3.0.0` instead of `v3`.\r\n> \r\n> I saw the v3 branch is referred to as v3.0.0, so it should be good to use v3.\r\n> \r\n> https://github.com/dorny/paths-filter/tree/v3\r\n\r\nIt is not related to "refer", instead it is just regex matching.\r\n\r\nv3 and v3.0.0 is different.

~~Also, you can see the current CI status. It cannot be started.~~\r\n\r\nSeems the workflow file is not updated.

Try to use dorny/paths-filter v3.0.1

If I get it correctly, you should use exactly v3.0.0

Thanks @tisonkun and @PragmaTwice I fix version to v3.0.0 and all CI jobs are runned OK as I see

It looks like clang-tidy run failed. You need to fix it before merging.\r\nhttps://github.com/apache/kvrocks/actions/runs/7744539810/job/21124606184?pr=2087#step:8:1406

@jihuayu Thank you for review carefully, I will fix it when I am free ～

Others LGTM except https://github.com/apache/kvrocks/pull/2087#discussion_r1477628027

>  I think maybe the origin BITCOUNT which used BYTE index also has this bug\r\n\r\nYeah

@kay011 My edits are listed above

> @kay011 My edits are listed above\r\nGot it. Let me merge it on this commit.\r\n\r\n

cc @PragmaTwice @git-hulk 

Will merge if CI success...

As I see, something in cmake scripts, need a help in this point

A bug are fixed at https://github.com/madler/zlib/commit/f1f503da85d52e56aae11557b4d79a42bcaa2b86

Update to latest head, includes all current fix, instead of 1.3.1.

Thank you! @aleksraiden 

cc @git-hulk @PragmaTwice 

@git-hulk @PragmaTwice \r\n\r\ncc @kay011 

@caipengbo Great! Overall is quite clear and concise. Just a few questions inline.

> Just a few questions inline.\r\n\r\nI have modified the question you mentioned before. Are there any other questions? @git-hulk 

How can we list all indexes efficiently? 🤔\r\nRediSearch seems would tracking the records for insert and maintaining index on that, should we support some global tracking structure?

> How can we list all indexes efficiently? 🤔 RediSearch seems would tracking the records for insert and maintaining index on that, should we support some global tracking structure?\r\n\r\nLike RediSearch, we will track all HASH and JSON commands and adjust our indexes accordingly.

> Like RediSearch, we will track all HASH and JSON commands and adjust our indexes accordingly.\r\n\r\nI mean, when start the process, we should "LIST" all indexes

> > Like RediSearch, we will track all HASH and JSON commands and adjust our indexes accordingly.\r\n> \r\n> I mean, when start the process, we should "LIST" all indexes\r\n\r\nAhh I got your point.\r\n\r\nI think we can add a common prefix for all index key, or put them to a new CF. Currently I prefer the latter.

> > > Like RediSearch, we will track all HASH and JSON commands and adjust our indexes accordingly.\r\n> > \r\n> > \r\n> > I mean, when start the process, we should "LIST" all indexes\r\n> \r\n> Ahh I got your point.\r\n> \r\n> I think we can add a common prefix for all index key, or put them to a new CF. Currently I prefer the latter.\r\n\r\nThis skeleton code looks good to me. I also prefer the latter one if we want to keep the common prefix since the current column families are not for this purpose. But we need to do a little backward compatibility work while adding a new column family.

So only "tag" is support currently?

> So only "tag" is support currently?\r\n\r\nYeah, I also plan to add the support of numeric indexes.\r\n\r\nBut for text and vector fields, it is currently not in the plan.

> IMHO the `fetch-depth: 0` SHOULD NOT be removed. Otherwise the blame information cannot be uploaded to sonarcloud.\n> \n> ```\n> WARN: Shallow clone detected, no blame information will be provided. You can convert to non-shallow with \

for example, zunionstore, two range:\r\n- store key: {1, 1, 1}\r\n- non-store key: {3, 2 + num_key, 1}\r\n\r\nsomething like this?

Next time can we try to use the top comment (if it is useful) as a squash commit message and bring it into the commit?\r\n\r\nThis is useful for viewing the context of changes based on commit messages without having to open a web PR page.

> Next time can we try to use the top comment (if it is useful) as a squash commit message and bring it into the commit?\r\n> \r\n> This is useful for viewing the context of changes based on commit messages without having to open a web PR page.\r\n\r\nSure! Sorry for not caring about it.

Thank you for your review

@jihuayu @torwig Thanks for your good reviews. All are done, plz take a look again.

I think we need to inform developers that in versions of kvrocks before 2.8, some commands may cause server crashes.\r\nCan we add this to the release log and [supported-commands desc](https://kvrocks.apache.org/docs/supported-commands)? \r\n

@jihuayu Thanks for your warm reminder. Yes we should mention this in our next release note.

> LGTM. Do you mean we can parser this JSON in sonar action and get more information?\r\n\r\nYeah. You can use some cli tools like `jq`, which is already installed.

Will this logic confuse users?\r\nFor example, when the data size reaches the max-db-size, if the user deletes a large amount of data, it will cause the db size to significantly exceed the db-size they have configured.

> Will this logic confuse users?\r\n\r\n@jihuayu There is also a risk of confusion, which we can explain in the configuration file. In addition, these delete makers will be eliminated during compaction, and the data size will decrease. \r\n\r\nIf `max-db-size` is not reached, users deleting large amounts of data will also cause disk explosion, this can also bother users.

Seems lint failed, would you mind run `./x.py format` or using `clang-format` to format the edited file?

> Seems lint failed, would you mind run `./x.py format` or using `clang-format` to format the edited file?\r\n\r\ndone, thank you very much

> So they have same meaning, just a style enhancement?\r\n\r\nIt can avoid spurious awakenings.

Ok, my machine clang-format is version 17, let me install version 14.

Hmmm, the SonarCloud Code Analysis seems wrong. It tell me: \r\n\r\n> ## Quality Gate failed\r\n> Failed conditions\r\n> \r\n> [58.7% Coverage on New Code](https://sonarcloud.io/component_measures?id=apache_kvrocks&branch=unstable&metric=new_coverage&view=list) (required ≥ 80%)\r\n> [E Reliability Rating on New Code](https://sonarcloud.io/dashboard?id=apache_kvrocks&branch=unstable) (required ≥ A)\r\n> \r\n> [See analysis details on SonarCloud](https://sonarcloud.io/dashboard?id=apache_kvrocks&branch=unstable)\r\n\r\nhttps://github.com/apache/kvrocks/pull/2038/checks?check_run_id=20711991119

> This pr is to solve https://github.com/apache/kvrocks/issues/2014.\r\n\r\nHi @wsehjk, you can use resolve #2014 instead of solve #2014. It will make GitHub auto close issue when PR has been merged.\r\n\r\nhttps://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword

Please use `./x.py format` to format your code, otherwise the CI cannot pass.

Though a different sampling algorithm is used, general LGTM

You can enable the GitHub action in your repo (It is free). So that you can find the CI failing issues earlier and more easily.\r\n\r\nhttps://github.com/wsehjk/kvrocks/actions

\r\nThe script for pressing data into kvrocks: \r\n```\r\nmemtier_benchmark -t 4 -c 20 -h 127.0.0.1 -p 6666 --distinct-client-seed --key-minimum=1000000000 --key-maximum=10000000000 --random-data --key-prefix="" --data-size=50 -n 2000000 --pipeline=100 --command "mset key data key data key data key data key data key data key data key data key data key data"\r\n``` \r\n\r\nThe script for benchmarking: where the READ_RATIO is 0, 50, 90 or 100, respectively.\r\n```\r\nmemtier_benchmark -t 4 -c 20 -s 127.0.0.1 -p 6666 --distinct-client-seed --key-minimum=1000000000 --key-maximum=10000000000 --key-prefix="" -n 10000 --command "get key" --command-ratio=READ_RATIO --command "set key value" --command-ratio=100-READ_RATIO\r\n```\r\n\r\nHere is the P99, P999 and P9999 latency with difference read ratios for the SET and GET command in the benchmark above.\r\n\r\nSET: \r\n![latency_SET](https://github.com/apache/kvrocks/assets/53040712/63deb558-c38f-4da7-949d-1dd1be50d69b)\r\n\r\nGET: \r\n![latency_GET](https://github.com/apache/kvrocks/assets/53040712/820996ac-7a0a-43ee-b836-a6dfd5b96396)\r\n

> --data-size=50 -n 2 000 000\r\n\r\n50 * 2,000,000 * 20(c) = 2,000,000,000\r\n\r\nhow much rocksdb sst data your make, 2G? please make at least 10G data

I think the clock might has better concurrency and perhaps a bit worse cache hit rate. We can also report the cache hit rate etc for analyzing the performance here

> I think the clock might has better concurrency and perhaps a bit worse cache hit rate. We can also report the cache hit rate etc for analyzing the performance here\r\n\r\nThanks for your review! I would add these statistics later today.

As for the statistics, you can refer `Server::GetRocksDBInfo` for help

Sorry for late update. I found that the way I use `memtier_benchmark` is totally wrong before. \r\n\r\nHere is my current script for putting data and running benchmark: \r\n\r\nPutting data into kvrocks:\r\n```\r\nmemtier_benchmark -t 8 -c 20 -h 127.0.0.1 -p 6666 --distinct-client-seed --key-minimum=1000000000 --key-maximum=10000000000 --random-data --key-prefix="" --data-size\r\n=50 -n 2000000 --pipeline=100 --command "mset __key__ __data__ __key__ __data__ __key__ __data__  __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ \r\n__data__ __key__ __data__ __key__ __data__"\r\n```\r\n\r\nI failed to run the script above to complete because I am almost run out of disk space. After running the script above for about 20% of progress, the keyspace statistics of kvrocks is as follows. \r\n```\r\n#  Keyspace\r\n# Last DBSIZE SCAN time: Wed Jan 24 12:04:59 2024\r\ndb0:keys=341377306,expires=0,avg_ttl=0,expired=0\r\nsequence:615748420\r\nused_db_size:18388474802\r\nmax_db_size:0\r\nused_percent: 0%\r\n ```\r\n\r\nRunning benchmark:\r\n\r\n```\r\nmemtier_benchmark -t 4 -c 20 -s 127.0.0.1 -p 6666 --distinct-client-seed --key-minimum=1000000000  --key-maximum=10000000000 --key-prefix="" -n 50000 --ratio={100-read_ratio}:{read_ratio}\r\n```\r\n\r\nwhere the `read_ratio` above is one of 0, 50, 90 and 100.\r\n\r\nResult:\r\n\r\nGET Latency: \r\n\r\n![latency_GET](https://github.com/apache/kvrocks/assets/53040712/04a8fb94-5e80-4892-a2f4-fddd3b825d3d)\r\n\r\nSET Latency:\r\n\r\n![latency_SET](https://github.com/apache/kvrocks/assets/53040712/026e340e-0416-465d-b1b4-3b702330c3e6)\r\n\r\nGET Ops/sec\r\n\r\n![ops_GET](https://github.com/apache/kvrocks/assets/53040712/e106605c-a62a-4cc9-a131-6fe0fd1eb340)\r\n\r\n\r\nSET Ops/sec\r\n\r\n![ops_SET](https://github.com/apache/kvrocks/assets/53040712/b96a7d22-9a20-4a06-bf24-df8ca51ccbc4)\r\n\r\n\r\nOverall(Get+Set) Ops/sec\r\n\r\n![ops_Overall](https://github.com/apache/kvrocks/assets/53040712/0abef1cf-e189-4221-b18a-6f6dd8aee245)\r\n\r\nAccording to the results listed above, it seems that HCC does outperform LRU for latency and throughput in this simple benchmark.\r\n\r\nBlock Cache Hit Ratio:\r\n\r\n![cacheHit_SET](https://github.com/apache/kvrocks/assets/53040712/2c83d497-15c6-460f-ba9c-14f22d0f1a43)\r\n\r\nI\

Due to GitHub Actions limits, the newly-added workflow will take effect after this PR is merged.

I have a question about  last PR #2017, why put `MultiBulkString` to `Connection`? @git-hulk 

> LGTM.\r\n> \r\n> BTW, the line: ` *output = conn->SetLen(3);` made me stop and think for a while :) ("How do we set a length to a connection?")\r\n \r\nYes, it\

> That said, use RESP2 or RESP3 should be related to connection instead of developers.\r\n\r\nGot it !

@MaheshMadushan Thank you for the contribution. \r\nYou can format the codebase by running `./x.py format`.

> @MaheshMadushan Thank you for the contribution. You can format the codebase by running `./x.py format`.\r\n\r\nHi @torwig , formatting issue fixed. Thank you.

Hi @torwig . Thanks for the feedback. will add edge test cases for the feature.

@git-hulk Are we waiting for additional test in this PR or it will be a separate one?

> @git-hulk Are we waiting for additional test in this PR or it will be a separate one?\r\n\r\n@torwig ohh, sorry for missing your previous comment. @MaheshMadushan Could you please add the edge cases as @torwig mentioned in this PR?

> > @git-hulk Are we waiting for additional test in this PR or it will be a separate one?\r\n> \r\n> @torwig ohh, sorry for missing your previous comment. @MaheshMadushan Could you please add the edge cases as @torwig mentioned in this PR?\r\n\r\nwill add them asap.

Please retry analysis of this Pull-Request directly on SonarCloud

@MaheshMadushan Thanks for your contribution.

@git-hulk @torwig Thank you for feedback.

@git-hulk Please help review :)

@chrisxu333 Please, run `./x.py format` to format the codebase and CI will be happy :)

Hi @git-hulk could you also help diagnose why the newly introduced testcase is failed? It was successful when I did locally.

This PR involves many files, but most of them are renaming the function.

@torwig I changed NilString in the last commit for the new command ZRANDMEMBER which was introduced in #2016, please retake a look.

Thanks for @torwig @PragmaTwice review.

LGTM, except one new file need an extra newline. Thank you!

> except one new file need an extra newline. \r\n\r\nThanks for your review! I have added the newline to that file in my recent commit.\r\n

@JxLi0921 Thanks for your contribution.

Instead, if you want to fix #2012, I think a better way is to change `Metadata::ParseMetadata`.

@PragmaTwice \r\nI delay InvalidArgument error check after type match check in `Metadata::ParseMetadata`.\r\n\r\nFor the tests, I think it is necessary.

This test data is not easy to build, I will add tests in the subsequent PR related to data migration.

@caipengbo I can add a basic test case for this.

@caipengbo I have appended a commit to add the basic test cases, you can take a look.

@caipengbo Looks good, but need to resolve conflicts.

@raffertyyu Thanks for your contributions.

2.8version support all redis 7.0  pubsub???\r\n\r\n    MakeCmdAttr<CommandPublish>("publish", 3, "read-only pub-sub", 0, 0, 0),\r\n    MakeCmdAttr<CommandMPublish>("mpublish", -3, "read-only pub-sub", 0, 0, 0),\r\n    MakeCmdAttr<CommandSubscribe>("subscribe", -2, "read-only pub-sub no-multi no-script", 0, 0, 0),\r\n    MakeCmdAttr<CommandUnSubscribe>("unsubscribe", -1, "read-only pub-sub no-multi no-script", 0, 0, 0),\r\n    MakeCmdAttr<CommandPSubscribe>("psubscribe", -2, "read-only pub-sub no-multi no-script", 0, 0, 0),\r\n    MakeCmdAttr<CommandPUnSubscribe>("punsubscribe", -1, "read-only pub-sub no-multi no-script", 0, 0, 0),\r\n    MakeCmdAttr<CommandSSubscribe>("ssubscribe", -2, "read-only pub-sub no-multi no-script", 0, 0, 0),\r\n    MakeCmdAttr<CommandSUnSubscribe>("sunsubscribe", -1, "read-only pub-sub no-multi no-script", 0, 0, 0),\r\n    MakeCmdAttr<CommandPubSub>("pubsub", -2, "read-only pub-sub no-script", 0, 0, 0), )\r\n \r\n/* pubsub */\r\n{"psubscribe","Listen for messages published to channels matching the given patterns","O(N) where N is the number of patterns the client is already subscribed to.","2.0.0",CMD_DOC_NONE,NULL,NULL,COMMAND_GROUP_PUBSUB,PSUBSCRIBE_History,PSUBSCRIBE_tips,psubscribeCommand,-2,CMD_PUBSUB|CMD_NOSCRIPT|CMD_LOADING|CMD_STALE|CMD_SENTINEL,0,.args=PSUBSCRIBE_Args},\r\n{"publish","Post a message to a channel","O(N+M) where N is the number of clients subscribed to the receiving channel and M is the total number of subscribed patterns (by any client).","2.0.0",CMD_DOC_NONE,NULL,NULL,COMMAND_GROUP_PUBSUB,PUBLISH_History,PUBLISH_tips,publishCommand,3,CMD_PUBSUB|CMD_LOADING|CMD_STALE|CMD_FAST|CMD_MAY_REPLICATE|CMD_SENTINEL,0,.args=PUBLISH_Args},\r\n{"pubsub","A container for Pub/Sub commands","Depends on subcommand.","2.8.0",CMD_DOC_NONE,NULL,NULL,COMMAND_GROUP_PUBSUB,PUBSUB_History,PUBSUB_tips,NULL,-2,0,0,.subcommands=PUBSUB_Subcommands},\r\n{"punsubscribe","Stop listening for messages posted to channels matching the given patterns","O(N+M) where N is the number of patterns the client is already subscribed and M is the number of total patterns subscribed in the system (by any client).","2.0.0",CMD_DOC_NONE,NULL,NULL,COMMAND_GROUP_PUBSUB,PUNSUBSCRIBE_History,PUNSUBSCRIBE_tips,punsubscribeCommand,-1,CMD_PUBSUB|CMD_NOSCRIPT|CMD_LOADING|CMD_STALE|CMD_SENTINEL,0,.args=PUNSUBSCRIBE_Args},\r\n{"spublish","Post a message to a shard channel","O(N) where N is the number of clients subscribed to the receiving shard channel.","7.0.0",CMD_DOC_NONE,NULL,NULL,COMMAND_GROUP_PUBSUB,SPUBLISH_History,SPUBLISH_tips,spublishCommand,3,CMD_PUBSUB|CMD_LOADING|CMD_STALE|CMD_FAST|CMD_MAY_REPLICATE,0,{{NULL,CMD_KEY_NOT_KEY,KSPEC_BS_INDEX,.bs.index={1},KSPEC_FK_RANGE,.fk.range={0,1,0}}},.args=SPUBLISH_Args},\r\n{"ssubscribe","Listen for messages published to the given shard channels","O(N) where N is the number of shard channels to subscribe to.","7.0.0",CMD_DOC_NONE,NULL,NULL,COMMAND_GROUP_PUBSUB,SSUBSCRIBE_History,SSUBSCRIBE_tips,ssubscribeCommand,-2,CMD_PUBSUB|CMD_NOSCRIPT|CMD_LOADING|CMD_STALE,0,{{NULL,CMD_KEY_NOT_KEY,KSPEC_BS_INDEX,.bs.index={1},KSPEC_FK_RANGE,.fk.range={-1,1,0}}},.args=SSUBSCRIBE_Args},\r\n{"subscribe","Listen for messages published to the given channels","O(N) where N is the number of channels to subscribe to.","2.0.0",CMD_DOC_NONE,NULL,NULL,COMMAND_GROUP_PUBSUB,SUBSCRIBE_History,SUBSCRIBE_tips,subscribeCommand,-2,CMD_PUBSUB|CMD_NOSCRIPT|CMD_LOADING|CMD_STALE|CMD_SENTINEL,0,.args=SUBSCRIBE_Args},\r\n{"sunsubscribe","Stop listening for messages posted to the given shard channels","O(N) where N is the number of clients already subscribed to a shard channel.","7.0.0",CMD_DOC_NONE,NULL,NULL,COMMAND_GROUP_PUBSUB,SUNSUBSCRIBE_History,SUNSUBSCRIBE_tips,sunsubscribeCommand,-1,CMD_PUBSUB|CMD_NOSCRIPT|CMD_LOADING|CMD_STALE,0,{{NULL,CMD_KEY_NOT_KEY,KSPEC_BS_INDEX,.bs.index={1},KSPEC_FK_RANGE,.fk.range={-1,1,0}}},.args=SUNSUBSCRIBE_Args},\r\n{"unsubscribe","Stop listening for messages posted to the given channels","O(N) where N is the number of clients already subscribed to a channel.","2.0.0",CMD_DOC_NONE,NULL,NULL,COMMAND_GROUP_PUBSUB,UNSUBSCRIBE_History,UNSUBSCRIBE_tips,unsubscribeCommand,-1,CMD_PUBSUB|CMD_NOSCRIPT|CMD_LOADING|CMD_STALE|CMD_SENTINEL,0,.args=UNSUBSCRIBE_Args},\r\n

@MaheshMadushan Sure, thank you! 

Thank you for your contribution!

Please retry analysis of this Pull-Request directly on SonarCloud

> BTW, I think we can have an enum type in our code.\r\n\r\nUnder the namespace `engine` in `src/storage.h` following enum can be created.\r\n`enum class Type {\r\n  RocksDB,\r\n  Speedb,\r\n};\r\n`\r\n\r\nThen we can have macros for storage engine as following,\r\n`STORAGE_ENGINE=0 # for rocksdb`\r\n`STORAGE_ENGINE=1 # for speedb`\r\n\r\nThen we can determine the engine type as following,\r\n`if (STORAGE_ENGINE == static_cast<int>(engine::Type::RocksDB)) {} ...`\r\n\r\nWould that be ideal?

Thank you for your contribution!

@jihuayu Could you add a test case to make sure this issue was resolved?

@git-hulk No problem, I will add test cases for each type.

> @git-hulk No problem, I will add test cases for each type.\r\n\r\nThank you!

@jihuayu This bug should be resolved after https://github.com/apache/kvrocks/pull/1971, but can add test cases to make sure it works well if you like.

Many commands have this kind of constraint (cannot operate on keys of other types), and we may need a universal method to test them.\r\nI will find a better way that covers this issue. \r\nClose this PR.

Thanks for your effort!\r\n\r\n> It should be noted that since the timeout precision currently supported by kvrocks is seconds, the results obtained by executing PEXPIRETIME will be different from those obtained by redis.\r\n\r\nIf you build kvrocks with `-DENABLE_NEW_ENCODING=ON`, you can see that the precision of these expire time is in milliseconds.

@kay011 Thanks for your contribution and quick response. 

Thanks for your contribution @maochongxin!\r\n\r\nThe PR description is empty. Can you add some background so that reviewers can understand the motivation and how you plan to implement it?

> Thanks for your contribution @maochongxin!\r\n> \r\n> The PR description is empty. Can you add some background so that reviewers can understand the motivation and how you plan to implement it?\r\n\r\nThank you for the reminder. I have now provided the background information for this matter.

> The rest looks good to me! It would be great if you can add some test for it.\r\n\r\nAdded test for data consistency between the source and destination nodes.

> > The rest looks good to me! It would be great if you can add some test for it.\r\n> \r\n> Added test for data consistency between the source and destination nodes.\r\n\r\nGreat! Could you also add tests for the incremental data? e.g. kvrocks2redis can catch new data after starting if it is written to the source kvrocks node.

Thank you for your contribution!

Need to hold this PR till the speedb supports those functions.

Thank you for your effort! Good to see it.\r\n\r\nI think the original intension of #1929 is to add a new column into the website (https://kvrocks.apache.org/docs/supported-commands), rather than add a new file to the root directory of this project.\r\n\r\nSo technically, you can open pull requests to https://github.com/apache/kvrocks-website, instead of this repo.

Close since https://github.com/apache/kvrocks-website/pull/181 is merged. Thank you.

> do we still need to do this in this patch\r\n\r\nI agree it\

> > do we still need to do this in this patch\r\n> \r\n> I agree it\

Sure, sorry for missing this message. I will take a look recently.

@chrisxu333 Can rename `true_args_` to `command_args_`, rest are good to me. Sorry for missing this point.

Would you mind fix the lint first?

@chrisxu333  `./x.py format` should do the trick.

 \r\n\r\n> Would you mind fix the lint first?\r\n\r\nSounds good I just did.

Hi @git-hulk @PragmaTwice,\r\n\r\nCould you help review and approve this pr if possible? Thx!

@chrisxu333 Thanks for your follow-up. This PR looks good to me, to see if @PragmaTwice and @torwig has any comments, if not, we can move forward to merge.

CI failed on macOS, which is related to the case about libc++ rather than libstdc++ that I mentioned before. We need to add a check about the std lib impl that are currently using.

~~It is somewhat difficult to determine the versions of gcc/clang/libstdc++/libc++. We can first fix the case for gcc8, and then set a minimum version limit of 13 for Clang @PragmaTwice~~\r\n\r\nPassed CI tests, PTAL @PragmaTwice @mapleFU 

Please retry analysis of this Pull-Request directly on SonarCloud

@git-hulk  Maybe we need to add a note to the documentation, telling users that the next version（2.8）supports the SET option with KEEPTLE and GET options?

@jihuayu Sure, thank you!

It feels like it could also be included in #1929 and done together

Seems `rocksdb_ratelimiter_create_auto_tuned` is just a C API to create an auto-tuned GenericRateLimiter , which not high related to us..\r\n\r\n

should we use this opportunity to add a kvrocks_mode?

> should we use this opportunity to add a kvrocks_mode?\r\n\r\nI think so.

How did we lose this change when upgrading rocksdb? It seems like a good opportunity to think about how to handle the upgrade of dependency more safely.

Another new feature here https://github.com/danielaparker/jsoncons/issues/467#issuecomment-1826868711.\r\n\r\ncc @Qiaolin-Yu 

> Maybe we can also add this workaround in the website.\r\n\r\nGood point, will do it soon.

Agree, we can default enable it. But let user know we can disable it if it has bug?

> Add a new statistic `COMPACTION_CPU_TOTAL_TIME` that records cumulative compaction cpu time. This ticker is updated regularly while a compaction is running.\r\n\r\ncan we adding this if it exists?\r\n

> > Add a new statistic `COMPACTION_CPU_TOTAL_TIME` that records cumulative compaction cpu time. This ticker is updated regularly while a compaction is running.\r\n> \r\n> can we adding this if it exists?\r\n\r\nSure

@julic20s Can use `./x.py format` to format your code before pushing.

Thanks @julic20s . This patch is huge, and I need some time to review the whole patch. I just submit some small comments.

```cpp\r\n LockGuard(LockGuard &&guard) : lock_(guard.lock_) { guard.lock_ = nullptr; } // correct constructor\r\n LockGuard &operator=(LockGuard &&other) = default; // uncorrect assignment.\r\n```\r\n😭

@julic20s BTW, would you like to take anonymous functions(get/set/get_segment) from Bitmap::bitfield into named functions. This function is a bit hard to read for now.

done

@mapleFU Hi, I finished all changes you pointed, could you review them again? Thanks.

@PragmaTwice @mapleFU I think we can go ahead to merge this PR if overall looks good.

Thank you for your contribution!

@PragmaTwice Sonar Lint reports hotspot security due to using the stren function in daemon_util.h, but I guess it should be fine since the variable is always set the `\\0`.

> @PragmaTwice Sonar Lint reports hotspot security due to using the stren function in daemon_util.h, but I guess it should be fine since the variable is always set the `\\0`.\r\n\r\nThis `strlen` is not introduced by me lol.

> > @PragmaTwice Sonar Lint reports hotspot security due to using the stren function in daemon_util.h, but I guess it should be fine since the variable is always set the `\\0`.\r\n> \r\n> This `strlen` is not introduced by me lol.\r\n\r\naha, yes i know. What I mean is to ignore this hotspot security report.

@skyitachi Can you help to resolve conflicts before merging?

Could you check the clang-tidy report in CI and try to fix it?

CI failed on macOS and ASAN build (use-after-free behaviors are found).\r\n

> CI failed on macOS and ASAN build (use-after-free behaviors are found).\r\n\r\nOkay I went through the code and I thought it might be related to pinnable slice copy into bf_data_list container.

Hi @mapleFU , I saw all tests are passed for this pr. So do you mind helping merge this pr after I rebase it with the `unstable` branch

Thanks for the bugfix\r\n\r\nHowever, changing the style from Status to exception only in this submodule is so weird...

Close PR due to lack of active, free feel to reopen if the issue is still living.

@jihuayu Can ignore this wrong duplication report.

@jihuayu Let me have a check later.

You can try to build it to make sure it will pass the compiler before pushing.

> You can try to build it to make sure it will pass the compiler before pushing.\r\n\r\nissue fixed. 

> LGTM, thanks\r\n\r\nThanks for the feedback

@MaheshMadushan Thanks for your contribution.

Could you try to fix these issue reported by clang-tidy in CI?\r\n\r\nhttps://github.com/apache/kvrocks/actions/runs/6754872818/job/18363385655?pr=1874

> Could you try to fix these issue reported by clang-tidy in CI?\r\n\r\nFixed.\r\n\r\n

Thank you for your contribution!

@jihuayu I think only Go tests is ok if most of them can be covered.

> rstanding the difference in using between cpp test and go test in kvrocks. When should I add a cpp test, and when go test is\r\n\r\nNo, only Go is ok if the key path can be tested. And cpp test is used to cover the path which is hard to test through the redis protocol way.

Thank you for your contribution!

Please retry analysis of this Pull-Request directly on [SonarCloud](https://sonarcloud.io/project/issues?id=apache_kvrocks&pullRequest=1869).

@jihuayu Good catch!

Looks generally good to me, thank you!

The logic for when `index > array size` is missing here. I need to first check the behavior of the Redis stack in this situation.

Only one typo in the comment inline, rest are good to me.

@skyitachi Thank you for your contribution. To avoid lint error during the CI, please run `./x.py format` before the final commit and push.

> SonarCloud Quality Gate failed.\xa0 \xa0 [![Quality Gate failed](https://camo.githubusercontent.com/4ea51c1f64ee3746f631653a02ab678ca6a3efb5f5cb474402faed2e3dcf90b5/68747470733a2f2f736f6e6172736f757263652e6769746875622e696f2f736f6e6172636c6f75642d6769746875622d7374617469632d7265736f75726365732f76322f636865636b732f5175616c6974794761746542616467652f6661696c65642d313670782e706e67)](https://sonarcloud.io/dashboard?id=apache_kvrocks&pullRequest=1865)\r\n> \r\n> [![Bug](https://camo.githubusercontent.com/4c6102327f5a954f9c8acaf2e2714183157a9e41717b371b2cd585cf25057310/68747470733a2f2f736f6e6172736f757263652e6769746875622e696f2f736f6e6172636c6f75642d6769746875622d7374617469632d7265736f75726365732f76322f636f6d6d6f6e2f6275672d313670782e706e67)](https://sonarcloud.io/project/issues?id=apache_kvrocks&pullRequest=1865&resolved=false&types=BUG) [![A](https://camo.githubusercontent.com/1cba125a897d7fa47033a3b3b2be2bbee680d34d4f004a215564659b853fb201/68747470733a2f2f736f6e6172736f757263652e6769746875622e696f2f736f6e6172636c6f75642d6769746875622d7374617469632d7265736f75726365732f76322f636865636b732f526174696e6742616467652f412d313670782e706e67)](https://sonarcloud.io/project/issues?id=apache_kvrocks&pullRequest=1865&resolved=false&types=BUG) [0 Bugs](https://sonarcloud.io/project/issues?id=apache_kvrocks&pullRequest=1865&resolved=false&types=BUG) [![Vulnerability](https://camo.githubusercontent.com/3ba1ee49636ffc3427e38649a9f8a65ee392f28e8a662fcf96ce24cefbb520e9/68747470733a2f2f736f6e6172736f757263652e6769746875622e696f2f736f6e6172636c6f75642d6769746875622d7374617469632d7265736f75726365732f76322f636f6d6d6f6e2f76756c6e65726162696c6974792d313670782e706e67)](https://sonarcloud.io/project/issues?id=apache_kvrocks&pullRequest=1865&resolved=false&types=VULNERABILITY) [![A](https://camo.githubusercontent.com/1cba125a897d7fa47033a3b3b2be2bbee680d34d4f004a215564659b853fb201/68747470733a2f2f736f6e6172736f757263652e6769746875622e696f2f736f6e6172636c6f75642d6769746875622d7374617469632d7265736f75726365732f76322f636865636b732f526174696e6742616467652f412d313670782e706e67)](https://sonarcloud.io/project/issues?id=apache_kvrocks&pullRequest=1865&resolved=false&types=VULNERABILITY) [0 Vulnerabilities](https://sonarcloud.io/project/issues?id=apache_kvrocks&pullRequest=1865&resolved=false&types=VULNERABILITY) [![Security Hotspot](https://camo.githubusercontent.com/fb735cbe76f8d5e1679c76ce83b740ceb1eaf62de4f7bf88623dc9953261aff7/68747470733a2f2f736f6e6172736f757263652e6769746875622e696f2f736f6e6172636c6f75642d6769746875622d7374617469632d7265736f75726365732f76322f636f6d6d6f6e2f73656375726974795f686f7473706f742d313670782e706e67)](https://sonarcloud.io/project/security_hotspots?id=apache_kvrocks&pullRequest=1865&resolved=false&types=SECURITY_HOTSPOT) [![A](https://camo.githubusercontent.com/1cba125a897d7fa47033a3b3b2be2bbee680d34d4f004a215564659b853fb201/68747470733a2f2f736f6e6172736f757263652e6769746875622e696f2f736f6e6172636c6f75642d6769746875622d7374617469632d7265736f75726365732f76322f636865636b732f526174696e6742616467652f412d313670782e706e67)](https://sonarcloud.io/project/security_hotspots?id=apache_kvrocks&pullRequest=1865&resolved=false&types=SECURITY_HOTSPOT) [0 Security Hotspots](https://sonarcloud.io/project/security_hotspots?id=apache_kvrocks&pullRequest=1865&resolved=false&types=SECURITY_HOTSPOT) [![Code Smell](https://camo.githubusercontent.com/8fe18b2dfb6f7d4e44582f281b29f617eb5ae07c248d2002ca586e91da219212/68747470733a2f2f736f6e6172736f757263652e6769746875622e696f2f736f6e6172636c6f75642d6769746875622d7374617469632d7265736f75726365732f76322f636f6d6d6f6e2f636f64655f736d656c6c2d313670782e706e67)](https://sonarcloud.io/project/issues?id=apache_kvrocks&pullRequest=1865&resolved=false&types=CODE_SMELL) [![A](https://camo.githubusercontent.com/1cba125a897d7fa47033a3b3b2be2bbee680d34d4f004a215564659b853fb201/68747470733a2f2f736f6e6172736f757263652e6769746875622e696f2f736f6e6172636c6f75642d6769746875622d7374617469632d7265736f75726365732f76322f636865636b732f526174696e6742616467652f412d313670782e706e67)](https://sonarcloud.io/project/issues?id=apache_kvrocks&pullRequest=1865&resolved=false&types=CODE_SMELL) [1 Code Smell](https://sonarcloud.io/project/issues?id=apache_kvrocks&pullRequest=1865&resolved=false&types=CODE_SMELL)\r\n> \r\n> [![No Coverage information](https://camo.githubusercontent.com/86785eaa572e061f5ecb3f8bf1cbb7b8eca44925083f92d067805b4362cead0c/68747470733a2f2f736f6e6172736f757263652e6769746875622e696f2f736f6e6172636c6f75642d6769746875622d7374617469632d7265736f75726365732f76322f636865636b732f436f76657261676543686172742f4e6f436f766572616765496e666f2d313670782e706e67)](https://sonarcloud.io/component_measures?id=apache_kvrocks&pullRequest=1865) No Coverage information [![4.9%](https://camo.githubusercontent.com/63dc444def276c404504b2fbf78d2bb7ad9e26e3a37650ddf55039446bb8ab6b/68747470733a2f2f736f6e6172736f757263652e6769746875622e696f2f736f6e6172636c6f75642d6769746875622d7374617469632d7265736f75726365732f76322f636865636b732f4475706c69636174696f6e732f352d313670782e706e67)](https://sonarcloud.io/component_measures?id=apache_kvrocks&pullRequest=1865&metric=new_duplicated_lines_density&view=list) [4.9% Duplication](https://sonarcloud.io/component_measures?id=apache_kvrocks&pullRequest=1865&metric=new_duplicated_lines_density&view=list)\r\n> \r\n> ![idea](https://camo.githubusercontent.com/00150338e138e454e357462ed3d9cfceaf338ca50ccadcc1f3a75ad6a0e2aab2/68747470733a2f2f736f6e6172736f757263652e6769746875622e696f2f736f6e6172636c6f75642d6769746875622d7374617469632d7265736f75726365732f76322f636f6d6d6f6e2f6c696768745f62756c622d313670782e706e67) Catch issues before they fail your Quality Gate with our IDE extension ![sonarlint](https://camo.githubusercontent.com/e4a8f808c72d84f231c1400ce7f2c77a09afe3eadb6bde9ad2c208a46735d320/68747470733a2f2f736f6e6172736f757263652e6769746875622e696f2f736f6e6172636c6f75642d6769746875622d7374617469632d7265736f75726365732f76322f636f6d6d6f6e2f736f6e61726c696e742d313670782e706e67) [SonarLint](https://www.sonarsource.com/products/sonarlint/features/connected-mode/?referrer=sonarcloud-welcome)\r\n\r\nit seems no need to care about define const for JSON.ARRINDEX comman in json_test.go files ?

Hi @skyitachi , currently you do not need to care about the sonarcloud report, since we have not finished its set-up yet.\r\n\r\nBut other CI checks (lint / build / unit tests) are mandatory.

Seems the golangci-lint have failed. You can try to fix your golang code to make it pass.\r\n\r\nhttps://github.com/apache/kvrocks/actions/runs/6734552602/job/18306093590?pr=1865

Hi @skyitachi Would you mind resolving conflicts before merging.

@skyitachi Seems previous suggestions are disappeared, can you apply them again?

This PR opened before changing the sonar reconfigured, so just ignore it. cc @PragmaTwice 

> Have a glance and seems that all modified variables is Server\r\n\r\nYes, only changed the variable name of server from `svr` to `srv`.

(I mean nothing weird is changed, aha )

@isHuangXin Are you still working on this PR? 

@isHuangXin I have added the test case now.

Can you help me review this? @PragmaTwice 

I have updated this PR based on https://github.com/apache/kvrocks/pull/1911. PTAL @PragmaTwice .

Hi @Ziy1-Tan , before you push your code, you can run these c++ and golang unit tests on your local device.\r\nAnd please make sure these tests can PASS before pushing.\r\n\r\nPlease refer to https://kvrocks.apache.org/community/contributing for more details.

nice, long connect will not disconnect when change workers, I use redis-benchmark continuous write

Should we prevent from setting the thread-cnt to a dangerous value( .e.g `0`)?

> Should we prevent from setting the thread-cnt to a dangerous value( .e.g `0`)?\r\n\r\n@mapleFU Yes, IntField will check the value range before setting.\r\n\r\n`{"workers", true, new IntField(&workers, 8, 1, 256)},`

Got it, so finally resource is handled by dtor.

Thanks all, merging..

Have it found some already exists unused variables bug?

> Have it found some already exists unused variables bug?\r\n\r\nSee #1853.

The following error is reported when building version 2.7: \r\n\r\nkvrocks/src/cluster/cluster.cc:537:22: error: unused variable ‘_’ [-Werror=unused-variable]\r\n   for (auto &[_, info] : slots_infos) {

Seems you also need to change the method declaration to make it pass the compiler.

Is a SonarCloud analysis required for a PR to pass? It seems to display a duplication error for C++ test cases, although there are no duplicates.\r\n\r\n@PragmaTwice 

> Is a SonarCloud analysis required for a PR to pass? It seems to display a duplication error for C++ test cases, although there are no duplicates.\r\n> \r\n> @PragmaTwice\r\n\r\nNope. It is fine to leave it alone.\r\n\r\n@git-hulk Do you have free time to help review and merge this PR?

> > Is a SonarCloud analysis required for a PR to pass? It seems to display a duplication error for C++ test cases, although there are no duplicates.\r\n> > @PragmaTwice\r\n> \r\n> Nope. It is fine to leave it alone.\r\n> \r\n> @git-hulk Do you have free time to help review and merge this PR?\r\n\r\nSure, I would take a look soon.

Thank you! @2rueSid 

Hi @2rueSid , in the new version of jsoncons, some new features have been added for removing nodes.\r\n\r\nRefer to https://github.com/danielaparker/jsoncons/issues/467#issuecomment-1826868711.\r\n\r\nAre you willing to improve this MERGE command using that?

> Hi @2rueSid , in the new version of jsoncons, some new features have been added for removing nodes.\r\n> \r\n> Refer to [danielaparker/jsoncons#467 (comment)](https://github.com/danielaparker/jsoncons/issues/467#issuecomment-1826868711).\r\n> \r\n> Are you willing to improve this MERGE command using that?\r\n\r\nHello. Yes for sure! I will take a look tomorrow.

@2rueSid You can format your code with the `./x.py` command. It allows your code to pass the lint stage in the CI pipeline.

Rest are good to me, great work! @2rueSid 

Thanks for your contribution!

@guojidan You can run `./x.py format` to format your code before pushing (to pass the CI checks).

Hi @guojidan , thanks for your contribution.\r\n\r\nCould you add some golang test cases for this patch?

I fixed the problem you pointed out : )

Hi @guojidan, before we review this PR, you can first try to solve these conflicts with unstable.

> Hi @guojidan, before we review this PR, you can first try to solve these conflicts with unstable.\r\n\r\nhi @PragmaTwice Thank you very much for your help, now I already rebase this pr 

@guojidan Are you still working on this PR? 

> @guojidan Are you still working on this PR?\r\n\r\n\r\n\r\n> @guojidan Are you still working on this PR?\r\n\r\nyes, this pr wait review

@guojidan Cloud you please resolve conflicts again

> The current of CommandJsonArrAppend::Execute uses MultiBulkString. Is there a need to provide an MultiBulkInteger?\r\n\r\nhttps://github.com/redis/redis-specifications/blob/master/protocol/RESP2.md#resp-integers Redis has a resp protocol, and we don\

Thanks @lieck . The patch lgtm!\r\n\r\nBut you still need to fix the golang-lint:\r\n\r\n```\r\nunit/type/json/json_test.go:60: File is not `gofmt`-ed with `-s` (gofmt)\r\n\t    require.NoError(t, rdb.Do(ctx, "SET", "a", `1`).Err())\r\n        require.Error(t, rdb.Do(ctx, "JSON.ARRAPPEND", "a", "$", `1`).Err())\r\n        require.NoError(t, rdb.Do(ctx, "DEL", "a").Err())\r\n$ /home/runner/go/bin/golangci-lint run -v ./...\r\nTraceback (most recent call last):\r\n```\r\n\r\nYou can just run `go fmt` to format the file.

cc @git-hulk @PragmaTwice 

Thanks @lieck . Merged now

intrev64ifbe can be replaced with memrev64ifbe to keep consistent.

> Actually there are some difference in the query result between path $.x and .x (or x). It can be observe that query of path prefixed without $ will just get response with the first matched value instead of an array.\r\n\r\nThis patch is only a basic json legacy path support, the detail rule will be handled later.

Note: this patch doesn\

Close as stale. @mapleFU welcome to pick up and rebase on a new patch.

```\r\n--- FAIL: TestRedisCli (14.14s)\r\n    --- FAIL: TestRedisCli/test_interactive_cli (0.01s)\r\n        --- FAIL: TestRedisCli/test_interactive_cli/Status_reply (0.00s)\r\n            cli_test.go:111: \r\n                \tError Trace:\t/__w/kvrocks/kvrocks/tests/gocase/integration/cli/cli_test.go:111\r\n                \t            \t\t\t\t/__w/kvrocks/kvrocks/tests/gocase/integration/cli/cli_test.go:174\r\n                \tError:      \tNot equal: \r\n                \t            \texpected: "OK"\r\n                \t            \tactual  : "get_per_sec:0\\nseek_per_sec:0\\nnext_per_sec:0\\nprev_per_sec:0\\nis_bgsaving:no\\nis_compacting:no"\r\n                \t            \t\r\n                \t            \tDiff:\r\n                \t            \t--- Expected\r\n                \t            \t+++ Actual\r\n                \t            \t@@ -1 +1,6 @@\r\n                \t            \t-OK\r\n                \t            \t+get_per_sec:0\r\n                \t            \t+seek_per_sec:0\r\n                \t            \t+next_per_sec:0\r\n                \t            \t+prev_per_sec:0\r\n                \t            \t+is_bgsaving:no\r\n                \t            \t+is_compacting:no\r\n                \tTest:       \tTestRedisCli/test_interactive_cli/Status_reply\r\n        --- FAIL: TestRedisCli/test_interactive_cli/Integer_reply (0.00s)\r\n            cli_test.go:111: \r\n                \tError Trace:\t/__w/kvrocks/kvrocks/tests/gocase/integration/cli/cli_test.go:111\r\n                \t            \t\t\t\t/__w/kvrocks/kvrocks/tests/gocase/integration/cli/cli_test.go:178\r\n                \tError:      \tNot equal: \r\n                \t            \texpected: "(integer) 1"\r\n                \t            \tactual  : "OK"\r\n                \t            \t\r\n                \t            \tDiff:\r\n                \t            \t--- Expected\r\n                \t            \t+++ Actual\r\n                \t            \t@@ -1 +1 @@\r\n                \t            \t-(integer) 1\r\n                \t            \t+OK\r\n                \tTest:       \tTestRedisCli/test_interactive_cli/Integer_reply\r\n        --- FAIL: TestRedisCli/test_interactive_cli/Bulk_reply (0.00s)\r\n            cli_test.go:111: \r\n                \tError Trace:\t/__w/kvrocks/kvrocks/tests/gocase/integration/cli/cli_test.go:111\r\n                \t            \t\t\t\t/__w/kvrocks/kvrocks/tests/gocase/integration/cli/cli_test.go:183\r\n                \tError:      \tNot equal: \r\n                \t            \texpected: "\\"foo\\""\r\n                \t            \tactual  : "(integer) 1"\r\n                \t            \t\r\n                \t            \tDiff:\r\n                \t            \t--- Expected\r\n                \t            \t+++ Actual\r\n                \t            \t@@ -1 +1 @@\r\n                \t            \t-"foo"\r\n                \t            \t+(integer) 1\r\n                \tTest:       \tTestRedisCli/test_interactive_cli/Bulk_reply\r\n        --- FAIL: TestRedisCli/test_interactive_cli/Multi-bulk_reply (0.00s)\r\n            cli_test.go:111: \r\n                \tError Trace:\t/__w/kvrocks/kvrocks/tests/gocase/integration/cli/cli_test.go:111\r\n                \t            \t\t\t\t/__w/kvrocks/kvrocks/tests/gocase/integration/cli/cli_test.go:189\r\n                \tError:      \tNot equal: \r\n                \t            \texpected: "1) \\"foo\\"\\n2) \\"bar\\""\r\n                \t            \tactual  : "\\"foo\\""\r\n                \t            \t\r\n                \t            \tDiff:\r\n                \t            \t--- Expected\r\n                \t            \t+++ Actual\r\n                \t            \t@@ -1,2 +1 @@\r\n                \t            \t-1) "foo"\r\n                \t            \t-2) "bar"\r\n                \t            \t+"foo"\r\n                \tTest:       \tTestRedisCli/test_interactive_cli/Multi-bulk_reply\r\n        --- FAIL: TestRedisCli/test_interactive_cli/Parsing_quotes (0.00s)\r\n            cli_test.go:111: \r\n                \tError Trace:\t/__w/kvrocks/kvrocks/tests/gocase/integration/cli/cli_test.go:111\r\n                \t            \t\t\t\t/__w/kvrocks/kvrocks/tests/gocase/integration/cli/cli_test.go:196\r\n                \tError:      \tNot equal: \r\n                \t            \texpected: "OK"\r\n                \t            \tactual  : "1) \\"foo\\"\\n2) \\"bar\\""\r\n                \t            \t\r\n                \t            \tDiff:\r\n                \t            \t--- Expected\r\n                \t            \t+++ Actual\r\n                \t            \t@@ -1 +1,2 @@\r\n                \t            \t-OK\r\n                \t            \t+1) "foo"\r\n                \t            \t+2) "bar"\r\n                \tTest:       \tTestRedisCli/test_interactive_cli/Parsing_quotes\r\nFAIL\r\nexit status 1\r\n```\r\n\r\nHmm does this happen before?

We can create an issue to track other new commands. And others can help to implement the new commands if they like.

Tests will be added in further PRs to make an example/reference for other issues in the tracking issue : )

Thanks for your contribution!\r\n\r\nSeems there is a failure in the golang test cases, could you try to locate and fix it?

> Thanks for your contribution!\r\n> \r\n> Seems there is a failure in the golang test cases, could you try to locate and fix it?\r\n\r\nFixed.

left one comment, others are good to me. Thank you!

Thanks all, merging..

> Hi @xq2010 Thanks for your excellent contribution first!↳\r\n> \r\n> About how to load the RDB file, I\

> If we add the \

Thanks for your contribution!\r\n\r\nAgree with @git-hulk, I think it would be better to just implement it like a command inside the kvrocks process instead of a new program.

@git-hulk @PragmaTwice Okay, I will add a command to load the RDB file. The command might be rdb load \\<path\\> [namespace]. Do you have any suggestions or advice?

> @git-hulk @PragmaTwice Okay, I will add a command to load the RDB file. The command might be rdb load <path> [namespace]. Do you have any suggestions or advice?\r\n\r\nThank you! Sounds good to me, but [namespace] seems unnecessary since it knows the current namespace once it enters the command.

> Perhaps using a parameter to specify which db to load would be better. If so, the command would be rdb load <path> [database number (default 0)].\r\n\r\n@xq2010 Yes, I think this is a good solution.

Thanks for your review, I will review it later

I would like to merge this PR today if no more comments.

Hi @LiuYuHui , is there any progress?

Description below is incomplete:\r\n\r\n```\r\nFix a bug with atomic_flush=true that can cause DB to stuck after a flush fails (https://github.com/facebook/rocksdb/pull/11872).\r\n```\r\n\r\nRest LGTM

> Description below is incomplete:\r\n> \r\n> ```\r\n> Fix a bug with atomic_flush=true that can cause DB to stuck after a flush fails (https://github.com/facebook/rocksdb/pull/11872).\r\n> ```\r\n> \r\n> Rest LGTM\r\n\r\nYou are right, my mistake, I try to shorter a desc for PR.

> > Statistics rocksdb.sst.read.micros now includes time spent on multi read and async read into the file\r\n> \r\n> Since this can help our statistics if we use this. (But seems that this is unused)\r\n\r\nIn all latest release, as I see, a rocksdb team has an intensive work about extending stats and added a lot of new metrics.

LGTM, This PR is very good.

@xiaobiaozhao Would you like to have a test?

> @xiaobiaozhao Would you like to have a test?\r\n\r\nAll right, let me make time for performance test and stability test.

Happy to assist in general tuning and specific Speedb features.\r\nOnce you have a full run completed and results, please share the logs with statistics.\r\npreferably in Speedb [Discord](https://discord.com/invite/deMznf8aZZ)  \r\n

<img width="617" alt="image" src="https://github.com/apache/kvrocks/assets/52393536/1d4a7a8b-e52c-4a48-8e0c-9a83edbdcb98">\r\n\r\n\r\n```\r\n# insertDB\r\ndocker run -d --rm redislabs/memtier_benchmark:latest -t 4 -c 20 -s 192.168.0.252 -p 6666 --distinct-client-seed  --key-minimum=1000000000 --key-maximum=10000000000 --random-data  --key-prefix="" --data-size=50 -n 100000000 --pipeline=100 --command "mset  __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__"\r\n\r\ndocker run -d --rm redislabs/memtier_benchmark:latest -t 4 -c 20 -s 192.168.0.252 -p 6666 --distinct-client-seed  --key-minimum=2000000000 --key-maximum=20000000000 --random-data  --key-prefix="" --data-size=50 -n 5000000 --pipeline=100 --command "mset  __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__"\r\n\r\n# stressDB\r\n# test set only\r\ndocker run --rm redislabs/memtier_benchmark:latest -t 2 -c 20 -s 192.168.0.252 -p 6666 --key-minimum=2000000000 --key-maximum=20000000000 --random-data --key-prefix="" --data-size=50 -n 100000 --ratio=1:0\r\n\r\ndocker run --rm redislabs/memtier_benchmark:latest -t 2 -c 20 -s 192.168.0.253 -p 6666 --key-minimum=2000000000 --key-maximum=20000000000 --random-data --key-prefix="" --data-size=50 -n 100000 --ratio=1:0\r\n\r\n# test get only\r\ndocker run --rm redislabs/memtier_benchmark:latest -t 2 -c 20 -s 192.168.0.252 -p 6666 --key-minimum=2000000000 --key-maximum=20000000000 --random-data --key-prefix="" --data-size=50 -n 100000 --ratio=0:1\r\n\r\ndocker run --rm redislabs/memtier_benchmark:latest -t 2 -c 20 -s 192.168.0.253 -p 6666 --key-minimum=2000000000 --key-maximum=20000000000 --random-data --key-prefix="" --data-size=50 -n 100000 --ratio=1:0\r\n\r\n# test set:get 1:1\r\ndocker run --rm redislabs/memtier_benchmark:latest -t 2 -c 20 -s 192.168.0.252 -p 6666 --key-minimum=2000000000 --key-maximum=20000000000 --random-data --key-prefix="" --data-size=50 -n 100000 --ratio=1:1\r\n\r\ndocker run --rm redislabs/memtier_benchmark:latest -t 2 -c 20 -s 192.168.0.253 -p 6666 --key-minimum=2000000000 --key-maximum=20000000000 --random-data --key-prefix="" --data-size=50 -n 100000 --ratio=1:1\r\n\r\n# test set:get 1:10\r\ndocker run --rm redislabs/memtier_benchmark:latest -t 2 -c 20 -s 192.168.0.252 -p 6666 --key-minimum=2000000000 --key-maximum=20000000000 --random-data --key-prefix="" --data-size=50 -n 100000 --ratio=1:10\r\n\r\ndocker run --rm redislabs/memtier_benchmark:latest -t 2 -c 20 -s 192.168.0.253 -p 6666 --key-minimum=2000000000 --key-maximum=20000000000 --random-data --key-prefix="" --data-size=50 -n 100000 --ratio=1:10\r\n\r\n# test set:get 10:1\r\ndocker run --rm redislabs/memtier_benchmark:latest -t 2 -c 20 -s 192.168.0.252 -p 6666 --key-minimum=2000000000 --key-maximum=20000000000 --random-data --key-prefix="" --data-size=50 -n 100000 --ratio=10:1\r\n\r\ndocker run --rm redislabs/memtier_benchmark:latest -t 2 -c 20 -s 192.168.0.253 -p 6666 --key-minimum=2000000000 --key-maximum=20000000000 --random-data --key-prefix="" --data-size=50 -n 100000 --ratio=10:1\r\n```\r\n\r\n[stress.LOG](https://github.com/apache/kvrocks/files/12857057/stress.LOG)\r\n192.168.0.252 rocksdb\r\n192.168.0.253 speed\r\n\r\n<img width="1706" alt="image" src="https://github.com/apache/kvrocks/assets/52393536/86b98520-1b6f-4f7a-a376-519e6100099d">\r\n

We are trying to recreate the issue of the compaction filter (as far as we know we did not touch this area in our OS version).  @xiaobiaozhao if you may please send us the benchmark that you run. We will try to see if any of our features helps and try to create a more comprehensive benchmark (more data, more threads, more tests). Please note that without enabling those features we should have the same performance as rocksdb (only more stable)  \r\nThanks 

> e logs and reason is indeed trivial move only despite kForceOptimized.\r\n> Speedb is currently based on rocksdb 8.1.1 and I get the same 2 unit test failures when trying to run kvrocks with rocksdb 8.1.1.\r\n\r\nHi @speedbmike, thanks for your investigation. And it makes sense to compact twice to workaround this in our test cases. \r\n\r\n\r\n\r\n@xiaobiaozhao Put his benchmark test script inside: https://github.com/apache/kvrocks/pull/1792#issuecomment-1754970288 cc @hilikspdb 

There should be some changes to NOTICE before we merge it.\r\n\r\nLet me address it quickly.

> There should be some changes to NOTICE before we merge it.\r\n> \r\n> Let me address it quickly.\r\n\r\nmaybe we can mentioned that we use speedb as the rocksdb alternative in README.

We are working on enabling the speedb features in the vrocks code. The patch will be ready today. Initial performance tests show significant improvements. \r\n 

> maybe we can mentioned that we use speedb as the rocksdb alternative in README.\r\n\r\nDone.

We have seen nice improvements in the code that manually enable the speedb features. Since we are about to release a  minor version that will do this in a single call we will wait with this patch and add a pull request (to both upgrade the speedb and to call the method) ... Hilik 

@hilikspdb Thanks for your information.

I am not sure if we can just disable these two function in TSAN since maybe we can bear some data race here.

Thanks all, merging...

> so this is something like a typo error? (or other) anyway, can you also update the top commen to mention the reason (and fix)? thanks.\r\n\r\nYeah, it is a typo.

It just come to me that if we firstly set `repl-namespace-enabled` to yes, use kvrocks in a period of time, and then change it to no, will it process correctly? e.g. will these tokens in the storage be removed?

> It just come to me that if we firstly set `repl-namespace-enabled` to yes, use kvrocks in a period of time, and then change it to no, will it process correctly? e.g. will these tokens in the storage be removed?\r\n\r\nGood point, we should remove the DB key if this option switches from yes to no. But this may introduce another issue if the slave enables this configuration. So I think we can disallow changing it to `no` after turning it to yes.

Hi @HolyLow , thank you for your contribution.\r\n\r\nCould you consider to rewrite it in our new helper class [BlockingCommander](https://github.com/apache/kvrocks/blob/unstable/src/commands/blocking_commander.h)?\r\n\r\nYou can refer to [CommandBPop](https://github.com/apache/kvrocks/blob/unstable/src/commands/cmd_list.cc#L237) for how to use it.

@PragmaTwice refactored to use `BlockingCommander`.

Thank you for your contribution!

@xiaofan8421 Thanks for your good catch! Leave a comment inline.

@xiaofan8421  Hi, would you like to finish the final modification according to @git-hulk before the 2.6.0 cherry-picking period? (refer to https://github.com/apache/kvrocks/discussions/1767)\r\n\r\nIf it can be merged before the deadline, it can be shipped to kvrocks 2.6.0 smoothly : )\r\n\r\nIf you are busy, we can help you to do that.

> @xiaofan8421 Hi, would you like to finish the final modification according to @git-hulk before the 2.6.0 cherry-picking period? (refer to #1767)\r\n> \r\n> If it can be merged before the deadline, it can be shipped to kvrocks 2.6.0 smoothly : )\r\n> \r\n> If you are busy, we can help you to do that.\r\n\r\nThanks for your mention. Update PR. Please review it.

The patch LGTM. But CI still failed lol

> The patch LGTM. But CI still failed lol\r\n\r\nYes, maybe I should print the replication first when failed.

I add the `getBFDataList` function to get all the bloomfilter data at once. And update the `bloomCheck` interface for only check the data.

About `BF.MADD` command, it will be likely to return error reply in a array. But I have no way to check it in gocase, for the string type is inconsistent to the error type. It seems have not enough API to transform it. How should I solve it ? \r\n\r\ncc @mapleFU

You can maintain a protect/private constructor, and when build from non-owned, you can use a `unique_ptr<const BlockSplit>CreateNonOwned()`, and it can only call constant fn.

Thanks for your contribution!

> Seems subclass of `Database` still suffer from not having `[[nodiscard]]`? Should we add thm later?\r\n\r\nI actually want to replace all of them from rocksdb::Status to our Status or StatusOr, then we can just have implicit `[[nodiscard]]`s instead of adding them to all functions.\r\n\r\nHowever, it requires additional effort, so I have not yet begun.

@PragmaTwice It is a great idea to replace `rocksdb::Status` with `Status/StatusOr` in the `Database` class. 

@enjoy-binbin Need to format the Go tests

@darionyaphet Thanks for your contribution.

@darionyaphet You can run `/.x.py format` to format the code.

@darionyaphet Would you like to push forward this PR?

This has been resolved in https://github.com/apache/kvrocks/pull/1962, close this PR.

Also cc @zncleon 

This patch is LGTM. Would you mind add a tiny test in `tests/gocase/unit/type/bloom/bloom_test.go` for this?

> This patch is LGTM. Would you mind add a tiny test in `tests/gocase/unit/type/bloom/bloom_test.go` for this?\r\n\r\ndone

> BTW, can we enable some clang checkings or other lint checkings after this patch is merged?\r\n\r\nIt is enabled, but it cannot find every missing specifier.

@GoGim1 Thanks for your good catch.

@git-hulk Would the expired subkey finally cleared by compaction filter?

> \r\n\r\n\r\n\r\n> @git-hulk Would the expired subkey finally cleared by compaction filter?\r\n\r\nYes, exactly.

Changes have been made in response to the comments above. PTAL. :)

Thanks for your contribution!\r\n\r\nPlease refer to https://kvrocks.apache.org/community/contributing/ to format, lint and test your code. It is hard for us to merge it until the CI passes.\r\n\r\nBTW, it would be better to add golang test cases for slowlog changes.

Others are good to me, thank you!

Will merge if all CI passed

Thank you for catching this @GoGim1 

I want to complain about the PR title, which is not formal and clear enough to me. A better message may be **"Fix error messages in parsing of bit commands"**.\r\n\r\n@mapleFU You can adjust the commit message before you merge it, please do not miss the chance.\r\n\r\nAnd please do not use spoken English and abbreviation, and avoid grammatical errors in command messages.

It is forbidden by project settings to force push to the unstable branch.\n\nJust be careful next time : )

Thanks for your contribution!\r\n\r\nPlease refer to https://kvrocks.apache.org/community/contributing/ to format, lint and test your code : )

Thanks for your contribution!

```go\r\nfunc RandString(min, max int, typ RandStringType) string {\r\n\treturn RandStringWithSeed(min, max, typ, rand.Int63())\r\n}\r\n```\r\n\r\nThe change LGTM, however, would it better to change `RandString` as above? Since `WithSeed` should better have a stable output with the same seed.

@mapleFU Great, thanks for catching this.

Thanks, @mapleFU!  I think, we can change our defaults to rocksdb defaults. I can add in this patch, if other reviewers are OK.

Yes. All these bugfixes LGTM. But there are some changes over compaction, I think having a benchmark or others would be better

@aleksraiden @mapleFU How about upgrading to v8.3.3 first? since those critical fixes are also fixed in this version. And then we can have more tests before upgrading to the next minor version.\r\n\r\nhttps://github.com/facebook/rocksdb/releases/tag/v8.3.3\r\n\r\n## Bug Fixes\r\n- Fix a bug where if there is an error reading from offset 0 of a file from L1+ and that the file is not the first file in the sorted run, data can be lost in compaction and read/scan can return incorrect results.\r\n- Fix a bug where iterator may return incorrect result for DeleteRange() users if there was an error reading from a file.\r\n- Fixed a race condition in GenericRateLimiter that could cause it to stop granting requests\r\n

Maybe we can upgrade to 8.3.3 before releasing our 2.6.0, and then upgrade to 8.5 in the unstable branch.\r\n\r\nOFF TOPIC: I think we now need a release manager for 2.6.0.

> OFF TOPIC: I think we now need a release manager for 2.6.0.\r\n\r\n@apache/kvrocks-committers Not sure if any committer has time to drive this process recently? Can refer [how to create a Kvrocks release](https://kvrocks.apache.org/community/create-a-release)

Yes, I also think we upgrade the underlying engine is too radical. I agreed to upgrade to 8.3.3, one of the bugs about `GenericRateLimiter` we encountered in production and I reported it(https://github.com/facebook/rocksdb/issues/11742).\r\n\r\nLike @mapleFU suggested, we should set `level_compaction_dynamic_level_bytes` to true in kvrocks, this has a lot of benefits, and [rocksdb8.2.0](https://github.com/facebook/rocksdb/blob/main/HISTORY.md#820-04242023) can be easily to migrate `level_compaction_dynamic_level_bytes` from false to true( https://github.com/facebook/rocksdb/pull/11321)\r\n

Okey, many thanks about discussion. I changed PR to support only 8.3.3 and send new PR in next release. Also, config options level_compaction_dynamic_level_bytes changed to true.

If error and return after `CreateBloomfilter`, it will left zombie bloomfilter. So I Update the function interfaces about `CreateBloomfilterInBatch `and `BloomAdd`. Using only one write will also get high performance.\r\n\r\ncc @mapleFU

Oh seems Twice has refactor the storage, so you mind need to solve the conflicts.

Thanks for your contribution!

Thank you!

We may have the deadlock in ZSet::InterStore(as well as other similar APIs) if the dst key was inside the source key list since the OverWrite function will also lock the dst key: https://github.com/apache/kvrocks/blob/unstable/src/types/redis_zset.cc#L635 \r\n\r\nThis issue was found by TSAN in CI: https://github.com/apache/kvrocks/actions/runs/6022265431/job/16338859035?pr=1712\r\n

@git-hulk Good point.

I guess we should lock all the keys including the destination inside the `ZSet::InterStore` and similar methods (right now we lock the destination inside the `Overwite` method). Will it work?

> I guess we should lock all the keys including the destination inside the `ZSet::InterStore` and similar methods (right now we lock the destination inside the `Overwite` method). Will it work?\r\n\r\nYes, I think it can resolve this issue.

Or can it be ok without locking the dst key? since we are doing a overwrite

@enjoy-binbin I think we need to serialize write operations on the key to avoid "lost updates". However, you made me rethink the previous sentence because we deal with overwriting :)

Is it possible to lock the dst key only and use the read snapshot like SDIFFSTORE/SINTERSTORE?

> Is it possible to lock the dst key only and use the read snapshot like SDIFFSTORE/SINTERSTORE?\r\n\r\n@git-hulk can you describe this a bit more? (for example posting a pseudo code)

> > Is it possible to lock the dst key only and use the read snapshot like SDIFFSTORE/SINTERSTORE?\r\n> \r\n> @git-hulk can you describe this a bit more? (for example posting a pseudo code)\r\n\r\nSure. For now, SDIFFSTORE is implemented in two steps:\r\n\r\n- The first step is to calculate the diff results without locking source keys\r\n- Then overwrite the dst key with locking the dst key\r\n\r\nPlease refer https://github.com/apache/kvrocks/blob/unstable/src/types/redis_set.cc#L433

actually now we will lock the source keys. it was added in #1700, maybe we should remove the source keys locks?\r\n\r\n```\r\nrocksdb::Status Set::Diff(const std::vector<Slice> &keys, std::vector<std::string> *members) {\r\n  std::vector<std::string> lock_keys;\r\n  lock_keys.reserve(keys.size());\r\n  for (const auto key : keys) {\r\n    std::string ns_key = AppendNamespacePrefix(key);\r\n    lock_keys.emplace_back(std::move(ns_key));\r\n  }\r\n  MultiLockGuard guard(storage_->GetLockManager(), lock_keys);\r\n```\r\n\r\nedit: ohh, i think i got your point. maybe we should split InterStore to two function, one do the inter and the other one do the store, just like the Set apis. I wanted to do it (adding a ZINTER command) before, but because we already had a pending #1517

> > > Is it possible to lock the dst key only and use the read snapshot like SDIFFSTORE/SINTERSTORE?\r\n> > \r\n> > \r\n> > @git-hulk can you describe this a bit more? (for example posting a pseudo code)\r\n> \r\n> Sure. For now, SDIFFSTORE is implemented in two steps:\r\n> \r\n> * The first step is to calculated the diff result without locking source keys\r\n> * Then overwrite the dst key with locking the dst key\r\n\r\n\r\n\r\n> actually now we will lock the source keys. it was added in #1700, maybe we should remove the source keys locks?\r\n> \r\n> ```\r\n> rocksdb::Status Set::Diff(const std::vector<Slice> &keys, std::vector<std::string> *members) {\r\n>   std::vector<std::string> lock_keys;\r\n>   lock_keys.reserve(keys.size());\r\n>   for (const auto key : keys) {\r\n>     std::string ns_key = AppendNamespacePrefix(key);\r\n>     lock_keys.emplace_back(std::move(ns_key));\r\n>   }\r\n>   MultiLockGuard guard(storage_->GetLockManager(), lock_keys);\r\n> ```\r\n> \r\n> edit: ohh, i think i got your point. maybe we should split InterStore to two function, one do the inter and the other one do the store, just like the Set apis. I wanted to do it (adding a ZINTER command) before, but because we already had a pending #1517\r\n\r\nGreat, thank you!

i pushed a commit [c51975b](https://github.com/apache/kvrocks/pull/1712/commits/c51975b4e243705584f7ae1429d022aeee48c6b7)\r\n\r\nsplit InterStore to two function, one do the inter and the other one do the store, just like the Set apis. to see if the CI can pass

i see the CI is passed, so the split change can fix the deadlock, i am going to submit another PR to make the change, am revert the change in this PR (after we merge the new PR see #1726, we can re-merge it)

lock-order-inversion found in TSan\r\n\r\nhttps://github.com/apache/kvrocks/actions/runs/6062050794/job/16447962572?pr=1712

@zncleon BTW, according to the Redis documentation, the BF.EXISTS command returns 0 if the key doesn\

> @zncleon BTW, according to the Redis documentation, the BF.EXISTS command returns 0 if the key doesn\

Thanks all, merging

another question, now run the no-script flag command will response this error:\r\n```\r\n127.0.0.1:6666> eval "return redis.call(\

@Yangsx-1 Could you please add the Go tests for this command?

> @Yangsx-1 Could you please add the Go tests for this command?\r\n\r\nOf course.

@Yangsx-1 Can you or someone extract some content from #1586 and #1654 and put it in the top comment?\r\nsince there are some comments in the discussion, that will be great if we have a final version in the top comment

> @Yangsx-1 Can you or someone extract some content from #1586 and #1654 and put it in the top comment? since there are some comments in the discussion, that will be great if we have a final version in the top comment\r\n\r\nadded

Sorry for misleading, I mean maybe we can have some "upgrading test", since now we evolution our Stream Metadata. Users in 2.5.1 using stream would have shorter metadata. Maybe we need to test create group on an existing stream without `group_number` (or just not write it if it\

Can anyone help merge this pr?

Thanks all!

Hi @xq2010, thanks for your contribution. Could you please fix the issue in CI: https://github.com/apache/kvrocks/actions/runs/5974247406/job/16208064626#step:8:1361

@xq2010 You need to modify the function signature not only in declaration but also in definition.

The new implemention keeps consistent with Redis. But I think we need to fix the lint :-)

> @xq2010 You need to modify the function signature not only in declaration but also in definition.↳\r\n\r\nthanks!

@xq2010 The CI failed, and there are also conflicts against the unstable branch.\r\n\r\nYou need to solve the conflict and fix the code to make CI pass. Sorry but currently I have no time to help you check the root cause. You can check the CI log or just run go test in your local.

For BfMetadata, I change it to the `default_cf`. If stored it in  `metadata_cf`, it will influence the `DEL` and` KEYS`. Should we design a new cf for it, or just place it in `default_cf`.\r\n\r\ncc @mapleFU

Thanks all, merging...

I think we can import xxhash via FetchContent instead of put them to vendor directory.\r\n\r\nhttps://github.com/Cyan4973/xxHash\r\n\r\nYou can refer to our other dependences in `cmake/` directory, e.g. https://github.com/apache/kvrocks/blob/unstable/cmake/fmt.cmake.\r\n\r\nOnly if FetchContent does not apply, or we need to modify the external sources, we then put them to vendor.

If you do not familiar with cmake, I can help you to import xxhash. @zncleon 

I have submit a patch #1694 to import xxhash.\r\n\r\nYou can just use xxhash without any step after it is merged.

It is merged. You can now just use xxhash without bringing any sources or changing cmake code.

Oh, since it is ported from Apache Parquet, I am not sure whether we need to put a statement to NOTICE file for license issue. cc @tisonkun 

>  ported from Apache Parquet\r\n\r\nSince C++ Parquet is owned by Apache Arrow, maybe we require a notice lincense for apache arrow

Thanks all, merging...

> Thank you!\r\n> \r\n> Do we have other settings to use this script for debugging?\r\n> \r\n> Or may you or @git-hulk can tell how we use this file previously?\r\n\r\nWe previously use Valgrind to test if any memory leaks on the local side, and I guess no one does it now since we have ASAN in CI.

cc @zncleon we can also use "vendor" directory to vendor the xxhash algorithm.

Thank you.

PR is ready to be reviewed, but I need some time to add test cases for all encoding types.

For compatibility testing, I think we can start a redis service in CI and do some mutual en/decoding? e.g. `KvrocksDecode(RedisEncode(x)) = x` or `RedisDecode(KvrocksEncode(y)) = y`

> For compatibility testing, I think we can start a redis service in CI and do some mutual en/decoding? e.g. `KvrocksDecode(RedisEncode(x)) = x` or `RedisDecode(KvrocksEncode(y)) = y`\r\n\r\nAha, my test case values are dumped from the Redis, it should be difficult since those encoding types are located in many Redis versions, for example, the zip map exists in Redis 2.x.

Sorry for the big PR but the good news is it only affects the restore command itself :)

@git-hulk Yeah, a massive PR! But I believe we will handle it :)

I have tested the [RedisShake](https://github.com/tair-opensource/RedisShake) to migrate Redis data into Kvrocks and it works well now.

Thanks all, I will merge this PR and can submit an issue to followup if you found any issues.

@enjoy-binbin That issue was assigned to another person by me (almost 2 months ago) and I think @JoverZhang just took the task and did it.

I did some stupid things with my GitHub App.

@git-hulk Thanks, I get it now. No wonder I also think there was duplicate coverage.

Oh I get it. `Kill` is just for timeout handling. And most of time it will not happen.\r\n\r\nWhen it start to `Kill`, we expect a graceful shutdown?

Oh got it! Thanks!

#1663 has removed this unit, close this PR.

The topic of #1663 is not related to this, i think it can be kept

We should comment out and add an inline comment. Even we should open an issue to dig out, instead of remove the matrix item.

This is the related issue - https://github.com/apache/kvrocks/issues/1662

why bring this into that typo ci? we can merge this one first and then refresh the typo ci one

So the problem here is that we have background network IO, which might cause error much more-frequently than disk io error?\r\nThis idea looks ok, but I prefer a default disabled config for this. Because it might cause some unexpected behavior.

IMHO, it will not affect the correctness of the data. The possible effect is that users tend to overlook background errors they encounter.  HDYT @git-hulk @PragmaTwice

It should be fine since this only impacts the io error scenario, and the instance also would be broken if got those errors.

This try failed. I will try to solve it later (maybe after several days). So I close the PR now.

Maybe you also need to update the default value in https://github.com/apache/kvrocks/blob/unstable/src/config/config.cc .

> Maybe you also need to update the default value in https://github.com/apache/kvrocks/blob/unstable/src/config/config.cc .\r\n\r\nYou are right, done

Wanted to cancel my comments but failed :)

Thanks, @torwig for helps with config.cc

Currently "Install typos-cli" step takes 2-3 minutes 😲\r\n\r\nThis takes 1 second 🐎\r\nhttps://github.com/szepeviktor/byte-level-care/blob/b2e8a838c14be862f95f1bb3486d3b2479b1bc79/.github/workflows/spelling.yml#L34-L46\r\n

> Currently "Install typos-cli" step takes 2-3 minutes 😲\r\n\r\nI saw similar trick in OpenDAL. Let me try it out.\r\n\r\nActually we may make use of Rust GHA cache. But it\

> Maybe you can switch to this solution also.\r\n\r\n@tisonkun Here are some facts for you.\r\n\r\n- that action is very complicated - but you see only 2 lines in your workflow\r\n- that action works on the filesystem, not on git index\r\n- that action [runs `typos` twice](https://github.com/crate-ci/typos/blob/ef5fcf92dfbd679f97c0371159e143852f7b1eb1/action/entrypoint.sh#L64-L65) ❗\r\n\r\nThese made me rewrite it [in 6 commands](https://github.com/szepeviktor/byte-level-care/blob/b2e8a838c14be862f95f1bb3486d3b2479b1bc79/.github/workflows/spelling.yml#L38-L49).\r\n

I am sorry. I cannot contribute here.\r\n\r\nMy solution needs hard-wired excludes etc.

ASF repo does not allow to use action like `crate-ci/typos@master`. Maybe you need to specify the commit hash or version tag, like `@1234567` or `@1.0`, which is allowed.

@torwig @PragmaTwice Kvrocks failed to build. Do you have any idea - https://github.com/apache/kvrocks/actions/runs/5827809599/job/15805697484?pr=1663?

since this one is a bit small, i skipped the test. (i can add it if needed)

Seems cmake ci failed @PragmaTwice , is this a flaky test?

Recorded https://github.com/apache/kvrocks/issues/1662

It is not related to our code, not sure why.

I am not sure whether the apache license is in conflict with this (https://github.com/codespell-project/codespell), please help me to check.\r\nIf conflict, i will remove the CI and refresh the PR with only the typo fix.\r\n\r\nhere is am example: https://github.com/apache/kvrocks/actions/runs/5818954638/job/15776396946?pr=1660#step:5:7\r\n```\r\nUsed config files:\r\n    1: ./.codespell/.codespellrc\r\n./kvrocks.conf:569: Leve ==> Level, Levee\r\n```\r\n\r\nmore info:\r\n```\r\n# A CI action that using codespell to check spell.\r\n# .codespell/.codespellrc is a config file.\r\n# .codespell/wordlist.txt is a list of words that will ignore word checks.\r\n# More details please check the following link:\r\n# https://github.com/codespell-project/codespell\r\n```\r\n\r\nused to mention in https://github.com/apache/kvrocks/pull/1591#issuecomment-1633629161

> I am not sure whether the apache license is in conflict with this (https://github.com/codespell-project/codespell), please help me to check.\r\n\r\nIt is OK to use a GPL licensed project to do spell check, as we also use GCC in CI to build kvrocks : )

> It is OK to use a GPL licensed project to do spell check, as we also use GCC in CI to build kvrocks : )\r\n\r\ngood to know, thanks for the confirmation!

closing in favor of #1663

@enjoy-binbin Thank you for the links.

I guess I should consider introducing a new command for this instead of changing the `PUBLISH` command.

> I guess I should consider introducing a new command for this instead of changing the PUBLISH command.\r\n\r\nAgree with it

Do you already want to use this feature? One case is we can pipeline it in this monment (Waiting for Redis to actually implement it if not in a hurry).\r\n\r\nor implementing `MPUBLISH channel message1 message2 ...`. This should be possible since it should be implemented in Redis 8 (i see two Redis core-team member agree with it)

Thanks, @enjoy-binbin @torwig \r\n\r\nYes, variant `MPUBLISH channel message1 message2` will be awesome

Implemented the feature as a separate command `MPUBLISH`.

@ChrisZMF Very happy to see you back again!

> So, this is because the input is first input in batch, and then, write to store. Should we insert to set only when the key is not existed, and should be insert to batch?\r\n\r\nIt may have a partial commit if the instance is broken in the middle of the operation.

IMO, if you already have a fix, there is no need to open an issue here.\r\nAlthough there is already an issue associated with the PR, please also update the top comment (it can be copied from issue, pictures are not so convenient, those can actually be copied with block text)\r\n\r\nGood catch btw, did not CR

This code LGTM. Would you mind format the code using `./x.py format`? Seems that CI failed here.

Thanks @mapleFU @PragmaTwice 

>  is `-clang-analyzer-cplusplus.InnerPointer` a new rule here?\r\n\r\nit is removed due to a false positive.\r\nhttps://github.com/apache/kvrocks/actions/runs/5767527255/job/15637332818\r\n

This commit ( https://github.com/apache/kvrocks/pull/1632/commits/acf50104fbbecc84e363dfd88f750bf52c46f0e7 ) looks ok, but would you mind create an single patch for it?\r\n\r\nYou can also add an issue as a tracker for BloomFilter

Close as doing in other patch

After a round of review I get to understand this patch. There is a problem:\r\n\r\n```c++\r\nclass MultiLockGuard {\r\n public:\r\n  explicit MultiLockGuard(LockManager *lock_mgr, const std::vector<std::string> &keys) : lock_mgr_(lock_mgr) {\r\n    locks_ = lock_mgr_->MultiGet(keys);\r\n    for (const auto &iter : locks_) {\r\n      iter->lock();\r\n    }\r\n  }\r\n\r\n  ~MultiLockGuard() {\r\n    // Lock with order `A B C` and unlock should be `C B A`\r\n    for (auto iter = locks_.rbegin(); iter != locks_.rend(); ++iter) {\r\n      (*iter)->unlock();\r\n    }\r\n  }\r\n\r\n  MultiLockGuard(const MultiLockGuard &) = delete;\r\n  MultiLockGuard &operator=(const MultiLockGuard &) = delete;\r\n\r\n private:\r\n  LockManager *lock_mgr_ = nullptr;\r\n  std::vector<std::mutex *> locks_;\r\n};\r\n```\r\n\r\nPreviously, the lock happens per-key. It was a bit wierd but would not introduce problem. Now, `exclusive` is removed, so it might introduce the deadlock problem.\r\n\r\nA proposal is that using std::lock( https://en.cppreference.com/w/cpp/thread/lock ) instead of loop in `MultiLockGuard`.\r\n\r\n@PragmaTwice would that be possible?

@PragmaTwice сould you leave a little doc on how to use TLS for replication?\r\n\r\nWith this config\r\non master\r\n```\r\nkvrocks --tls-key-file ... --tls-cert-file ... --tls-ca-cert-file ...\r\n```\r\nand on slave\r\n```\r\nkvrocks --tls-replication yes --tls-key-file ... --tls-cert-file ... --tls-ca-cert-file ...\r\n```\r\n\r\nI get such errors\r\n```\r\nE20230814 08:22:48.497459 957356 redis_connection.cc:103] [connection] Going to remove the client: ...:59388, while encounter error: Success, SSL Error: error:0A0000C7:SSL routines::peer did not return a certificate\r\nE20230814 08:22:49.507014 957356 redis_connection.cc:103] [connection] Going to remove the client: ...:59392, while encounter error: Success, SSL Error: error:0A0000C7:SSL routines::peer did not return a certificate\r\n```

@sheyt0 Sure. Here are my test steps:\r\n\r\n1. Generate or obtain some certs (e.g. using [minica](https://github.com/jsha/minica)):\r\n- ca.crt\r\n- server.crt\r\n- server.key\r\n\r\n(all of them are in PEM format)\r\n\r\n2. Start a kvrocks server instance\r\n```sh\r\n./kvrocks --dir datadir1 --port 6666 --tls-port 6676 --log-dir stdout --tls-cert-file cert/server.crt --tls-key-file cert/server.key --tls-ca-cert-file cert/ca.crt\r\n```\r\n\r\n3. Start a replica (in same host, for convenience)\r\n```sh\r\n./kvrocks --dir datadir2 --port 6667 --tls-port 6677 --log-dir stdout --tls-cert-file cert/server.crt --tls-key-file cert/server.key --tls-ca-cert-file cert/ca.crt --tls-replication yes --slaveof "127.0.0.1 6676"\r\n```\r\n\r\n4. Test\r\n\r\n```sh\r\n➜  build git:(tls-replica) ✗ redis-cli -p 6666\r\n127.0.0.1:6666> get a\r\n(nil)\r\n127.0.0.1:6666> set a 1\r\nOK\r\n127.0.0.1:6666> set b 2\r\nOK\r\n127.0.0.1:6666> set c 3\r\nOK\r\n127.0.0.1:6666> exit\r\n➜  build git:(tls-replica) ✗ redis-cli -p 6667\r\n127.0.0.1:6667> get a\r\n"1"\r\n127.0.0.1:6667> get b\r\n"2"\r\n127.0.0.1:6667> get c\r\n"3"\r\n127.0.0.1:6667>\r\n```\r\n\r\nServer logs:\r\n```\r\nWARNING: No config file specified, using the default configuration. In order to specify a config file use \

@PragmaTwice \r\n>Currently we do not support splitting cert between server and client.\r\n\r\nDo I understand correctly that one set of certs should be used (same certs for master and slave)?\r\nOr is it just about config options?

> @PragmaTwice\r\n> \r\n> > Currently we do not support splitting cert between server and client.\r\n> \r\n> Do I understand correctly that one set of certs should be used (same certs for master and slave)? Or is it just about config options?\r\n\r\nYeah, certs for master and slave must be the same.

> Yeah, certs for master and slave must be the same.\r\n\r\nIn the current implementation, this is not suitable for us. We need mTLS, not just TLS.

> In the current implementation, this is not suitable for us. We need mTLS, not just TLS.\r\n\r\nAlthough I previously do not plan to support it in this PR, the feature is not hard to implement.\r\n\r\nI will try to support it later.

@sheyt0 Sorry, I think I mis-understand the problem.\r\n\r\nThe certs of master and slave can be different. I have tried and it works well.\r\n\r\nBut you need to be noticed that the server connection (listen to kvrocks clients) and the client connection (connect to the master) of the slave must use the same cert, which currently is not able to split into different certs.\r\n\r\nSo internally it includes four part of SSL context:\r\n- master: server-side cert, client-side cert\r\n- slave: server-side cert, client-side cert\r\n\r\nThe two certs in master cannot be different (and also in slave), but the server-side cert in master and client-side cert in slave can be different.

@sheyt0 And, I have found why you cannot start TLS connection between master and slave.\r\n\r\nCurrently slave must have a tls-port to have TLS connection to master, this is a BUG since it is unnecessary.\r\n\r\nI will fix it soon.

> Currently slave must have a tls-port to have TLS connection to master, this is a BUG since it is unnecessary.\r\n\r\nIt is fixed. Now I think the slave can disable the tls-port (if you want).

> It is fixed. Now I think the slave can disable the tls-port (if you want).\r\n\r\nNow it works

<img width="1114" alt="image" src="https://github.com/apache/kvrocks/assets/125778860/382d4d86-cabc-446c-94c2-6bf9756404a0">\r\nI use minica generate ca file.\r\nhttps://github.com/jsha/minica

> <img alt="image" width="1114" src="https://user-images.githubusercontent.com/125778860/282232164-382d4d86-cabc-446c-94c2-6bf9756404a0.png"> I use minica generate ca file. https://github.com/jsha/minica\r\n\r\nYou need to build kvrocks with cmake option `ENABLE_OPENSSL=ON`.\r\n\r\nPlease refer to https://github.com/apache/kvrocks#build for details.

tls-replication params I shout config on master node or slave node?

I config tls-replication on slave node,then I login on master node, info comand:\r\n<img width="654" alt="image" src="https://github.com/apache/kvrocks/assets/125778860/dc73367f-3cd4-41dd-97de-222eec9a2e91">\r\nReplication slave node info is port 6666,why not 6676(master node tls port)

You need to configure the `slaveof` port to 6676 by yourself.\r\n\r\nAlso, you can open a discussion or question in slack channel rather than reply in this PR thread.

hope to get your approval or comments :)

@tisonkun @PragmaTwice Does this sound good to you? with the merge button would also make the release process more smooth.

I do not add [kvrocks-operator](https://github.com/RocksLabs/kvrocks-operator) since I do not know whether it is ready for use or not.

Seems some fixes are missing:\r\n- https://github.com/apache/kvrocks/commit/d839d163284a7cda61127a285587a489d243812e\r\n- https://github.com/apache/kvrocks/commit/53b7e879e395653651680f76e29c9e4a63b72f10

Seems that VERSION.txt is not changed.\r\nYou can run `./x.py package ...` to append a new commit.

> Seems that VERSION.txt is not changed. You can run `./x.py package ...` to append a new commit.\r\n\r\nYes, I think we can propose commits and get approved first, then merge them into 2.5 and append the version commit.

@enjoy-binbin The merge/rebase button is controlled by: https://github.com/apache/kvrocks/blob/unstable/.asf.yaml#L32 and it seems no way to open for some branches only.\r\n\r\n

ohh, in this way, maybe we can also enable the merge button? i think we can go beyond some PRs.\r\nwe can control it (when we are doing merge, usually squash is used, and in some cases we need to keep commits we use merge)\r\nthat is, the choice of squash or merge is the responsibility of the person for merging the PR

i raise a PR to enable the plain merge in #1624, we can move the discussion there

we need to update the VERSION.txt as well

i see the tag we made is v2.5.1, we used to tag it without v. see https://github.com/apache/kvrocks/releases\r\n\r\nbut in here it is 2.5.1, so https://github.com/apache/kvrocks/releases/tag/2.5.1 is a 404 link\r\n<img width="657" alt="image" src="https://github.com/apache/kvrocks/assets/22811481/6922921b-1a23-4682-b7ee-016db1ef04c4">\r\n\r\n

I think it is a bug of GitHub. Our release tag is `v2.5.1`, and our release note title is `2.5.1`.\r\n\r\nIt is not required to be equal. No problem here for ourselves.

> Or even add the extra test step at -\r\n\r\nSeems a good idea, but I do not know how to get the docker image in the previous step (`docker/build-push-action@v3`) since we do not push it.\r\n\r\nhttps://github.com/apache/kvrocks/blob/f3d3105b5ad1a6b276cfc305a57e7cc5233e4fed/.github/workflows/kvrocks.yaml#L289

And, a failed try by me, is that `uses: docker://apache/kvrocks:nightly` does not work, seems the asf github settings have blocked them.

@PragmaTwice I already link to [this snippet](https://github.com/korandoru/hawkeye/blob/75da87606f8f303c740d8d8d6738d8271af90d0a/.github/workflows/ci.yml#L94-L119) which has a step to "Build and load".\r\n\r\n```yml\r\n      - name: Build and load\r\n        uses: docker/build-push-action@v3\r\n        with:\r\n          context: .\r\n          file: ${{matrix.file}}\r\n          tags: ${{matrix.name}}:ci\r\n          outputs: type=docker\r\n      - name: Sanity check\r\n        run: docker run --rm -v $(pwd):/github/workspace ${{matrix.name}}:ci check\r\n```\r\n\r\nTake care of `outputs` and the sanity check step we use `docker run`.

> @PragmaTwice I already link to [this snippet](https://github.com/korandoru/hawkeye/blob/75da87606f8f303c740d8d8d6738d8271af90d0a/.github/workflows/ci.yml#L94-L119) which has a step to "Build and load".\r\n> \r\n> ```yaml\r\n>       - name: Build and load\r\n>         uses: docker/build-push-action@v3\r\n>         with:\r\n>           context: .\r\n>           file: ${{matrix.file}}\r\n>           tags: ${{matrix.name}}:ci\r\n>           outputs: type=docker\r\n>       - name: Sanity check\r\n>         run: docker run --rm -v $(pwd):/github/workspace ${{matrix.name}}:ci check\r\n> ```\r\n> \r\n> Take care of `outputs` and the sanity check step we use `docker run`.\r\n\r\nGot it. Done.

@git-hulk @PragmaTwice With these changes, does the data race still exist? I mean the `Cluster::slots_nodes_` variable. Do the read operations obtain an exclusive lock when reading from that variable?

i am not very familiar with it, so hopefully it is a right fix

thanks for the link. so these warnings are false positives? do we have any other option to ignore it?

I guess this would be fixed with a higher version of compiler or clang-tidy. https://en.cppreference.com/w/cpp/language/copy_elision C++17 has some gurantees\r\n\r\n![image](https://github.com/apache/kvrocks/assets/24351052/c6415c13-c61d-4b8d-929e-add55dbb421f)\r\n\r\nEven if NRVO not work well, the compiler will not copy. By the way, would you mind print the version of clang and clang-tidy? Maybe it would make things clear

> By the way, would you mind print the version of clang and clang-tidy?\r\n\r\nsure, i will take a look when i get home. (i am using different PC when i am working)

```\r\n$ clang -v\r\nApple clang version 12.0.0 (clang-1200.0.32.29)\r\nTarget: x86_64-apple-darwin19.6.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n```

Seems your clang version is too old. Apple Clang 12 is based on LLVM/Clang 10, but the latest version of LLVM is 16.\r\n\r\nBTW I recommend that you use LLVM/Clang rather than Apple Clang.

ci link: https://github.com/apache/kvrocks/actions/runs/5652375120/job/15311891588?pr=1607#step:9:14

redis doc:\r\nhttps://redis.io/commands/zrank/\r\nhttps://redis.io/commands/zrevrank/

Seems ninja does not support something like `$(MAKE)`, but I current have no idea how to workaround it.

Oh, I think one workaround is to check CMAKE_GENERATOR.

> What about we set `MAKE_COMMAND` at the top level?\r\n> \r\n> nit: revert whitespace changes.\r\n\r\ndone

Thanks for your contribution @Yangsx-1! Merging...

@aleksraiden This PR only changes the docker image PORTABLE value to 1. I think this makes a lot of sense since we cannot promise users are always running on the intel CPU like @ovaistariq mentioned.

Inherited by #1805.

It\

> It\

Not sure if anyone would rely on and use used_memory_human, i am leaning towards no...\r\nit is a potential breaking change\r\n\r\nNote our doc also describes it as used_memory_rss_human in https://kvrocks.apache.org/docs/info-sections

> Not sure if anyone would rely on and use used_memory_human, i am leaning towards no... it is a potential breaking change\r\n> \r\n> Note our doc also describes it as used_memory_rss_human in https://kvrocks.apache.org/docs/info-sections\r\n\r\nIt should be fine since used_memory_rss_human is only for human reading.

Thanks. Merging...

ping @git-hulk @PragmaTwice 

Thanks for your catch!

Thanks all, merging...

maybe we can add a spell check ci?\r\n\r\nI have added this spell check ci to Redis in the past: https://github.com/redis/redis/blob/unstable/.github/workflows/spell-check.yml\r\n\r\nRedis spell check PR link: https://github.com/redis/redis/pull/8890\r\n\r\nI can try to add it if I get the conceptual approval.\r\n\r\nhere is an example: https://github.com/redis/redis/actions/runs/5309794414\r\nci checks take very little time (tens of seconds)

I ran it locally and it passed, hope that we can run it a few more times in github CI to verify it

... and thanks for your contribution!

Have you tested how much performance can be improved?

@git-hulk Would you mind take a look?

> @git-hulk Would you mind take a look?\r\n\r\nLooks good to me.\r\n\r\n@xiaobiaozhao I think this PR is only for follow up #1215 \r\n

Hi @enjoy-binbin, thanks for taking care of this. Maybe we should fix it in https://github.com/apache/kvrocks/blob/unstable/src/types/redis_string.cc#L320

@enjoy-binbin Out of curiosity, do you have a fuzzer to test out Kvrocks? Your recent great fixes seem like a combo rather than random findings :D

They were actually random findings.\r\nI may be sensitive to this, doing code review and found them (also based on my familiarity with redis)

@git-hulk when doing the -1 * increment_, the overflow is happended, so i can not do the check\r\n```\r\n    auto s = string_db.IncrBy(args_[1], -1 * increment_, &ret);\r\n```

> They were actually random findings.\r\n\r\nHaha, cool!

> @git-hulk when doing the -1 * increment_, the overflow is happended, so i can not do the check\r\n> \r\n> ```\r\n>     auto s = string_db.IncrBy(args_[1], -1 * increment_, &ret);\r\n> ```\r\n\r\nGot it, thanks

Thanks all, merging...

I think the `SetCompressionOption` parameter value is the name of the compression algorithm. We get `rocksdb::CompressionType` by `map[name]`(name->CompressionType), and then pass it to `SetOptions`, in `SetOptions` static_cast it to `rocksdb::CompressionType`.

@PragmaTwice @xiaobiaozhao @caipengbo I minor refactor the PR, PTAL again.

I have tested if the config set command works on my side:\r\n\r\nSet the compression to LZ4:\r\n```\r\n2023/07/14-20:45:08.518174 6158610432 [db/db_impl/db_impl.cc:1195] SetOptions() on column family [default], inputs:\r\n2023/07/14-20:45:08.518210 6158610432 [db/db_impl/db_impl.cc:1198] compression_per_level: kNoCompression:kNoCompression:kLZ4Compression:kLZ4Compression:kLZ4Compression:kLZ4Compression:kLZ4Compression\r\n```\r\n\r\nthen use `redis-benchmark -p 6666  -n 1000000 -r 5000000 -t set -d 1024` to write the data, and check the db LOG to see if the compression was used:\r\n\r\n```\r\n2023/07/14-20:50:15.181736 6156890112 (Original Log Time 2023/07/14-20:50:15.181594) [db/compaction/compaction_job.cc:888] [metadata] compacted to: files[0 1 13 0 0 0 0] \r\nmax score 0.93, MB/sec: 39.1 rd, 4.5 wr, level 2, files in(1, 1) out(1 +0 blob) MB \r\nin(58.0, 2.6 +0.0 blob) out(7.1 +0.0 blob), read-write-amplify(1.2) write-amplify(0.1) OK, \r\nrecords in: 89927, records dropped: 3779 output_compression: LZ4\r\n``` 

I think we should testing rather than copying the config directly from Redis. Redis store all parts in memory, and has too many small objects, like sds, skiplist node and so on. So small object would be a severe problem for it.\r\nHowever, kvrocks stores these objects on a log-structure merge tree, and data is organized as sst/blocks. I guess there would be something different.

In my quick tests, these changes brought a slight improvement in terms of better freeing up memory when kvroks is idle.

@aleksraiden Could you paste your test result here?

@aleksraiden Thanks for your information. I feel good after looking through https://github.com/jemalloc/jemalloc/issues/1098. Another question is why disable the tcache as well, did I miss anything?

Seems that by default, if we disable `cache-oblivious`, we will\r\n1. Not align the large chunk, which make 16kb page just uses 16kb of memory, rather than about 20kb, which brings more usable memory to fill the "block cache" required by system\r\n2. May harms some cache-obilivious. But seems that we don\

Seems that CI compile failed :-)

Updated. So, now we can set options in build, e.g. -D=DISABLE_CACHE_OBLIVIOUS=0|1 where 0 are default and only set to 1 add this options into jemalloc compile process.

Thanks @torwig for help with cmake

Thanks all, merging...

@git-hulk By the way, when would kvrocks use `fill_cache = true`? Seems that only when point get, we would have that, and read_ahead / fill_cache is usally disabled for performance?

Could you add some tests?

> Could you add some tests?\r\n\r\nof course

@JackTan25  You can format the code before commit, see details in https://kvrocks.apache.org/community/contributing/

> @JackTan25 You can format the code before commit, see details in https://kvrocks.apache.org/community/contributing/\r\n\r\nthanks

Hi @JackTan25, this was done in https://github.com/apache/kvrocks/pull/1681, I will close this PR. Great thanks for your efforts.

@git-hulk @PragmaTwice \r\nthis one need to pay attention, it may be a security issue, which will cause the server to crash directly:\r\n```\r\n127.0.0.1:6666> eval \

Thanks for your good catch. @enjoy-binbin 

Great catch! Thanks.

Thanks for your contribution!

> And maybe we can enable all bugprone-* and do some NOLINT or style fixing in the future?\r\n\r\nWe need to specify the concrete name of checkers to prevent different checkers in different version of clang-tidy.

You can also add some go tests.

> You can also add some go tests.\r\n\r\nI will do it later

Seems here are some clang-tidy reports that need to be fixed.\r\n\r\nhttps://github.com/apache/kvrocks/actions/runs/5542464594/jobs/10117205769?pr=1565

\r\n\r\n\r\n\r\n> Rest LGTM!\r\n> \r\n> By the way, I noticed that you have written:\r\n> \r\n> ```\r\n> // TODO: Add test to verify randomness of the selected random fields\r\n> ```\r\n> \r\n> Would this be added in this patch or later?\r\n\r\nThis patch will not implement that

> fixes :#1510\r\n> \r\n> * Add parse for hrandfield\r\n> * Implemented the case where the requested data is a positive number greater than the size of the hash.\r\n\r\nClose #1510 is better.

@zevin02 Thanks for your contribution and patience and greater thanks to reviewers: @mapleFU @PragmaTwice @Yangsx-1 

ok now i see DISCARD also have the same problem, should I fix it together in this PR, or in a new PR?

> ok now i see DISCARD also have the same problem, should I fix it together in this PR, or in a new PR?\r\n\r\nIt’s fine to append a new commit.☺️

i see there is a flaky test:\r\nhttps://github.com/apache/kvrocks/actions/runs/5495230368/jobs/10014556657?pr=1562#step:12:114\r\n```\r\n--- FAIL: TestString (46.13s)\r\n    server.go:109: \r\n        \tError Trace:\t/Users/runner/work/kvrocks/kvrocks/tests/gocase/unit/type/strings/server.go:109\r\n        \t            \t\t\t\t/Users/runner/work/kvrocks/kvrocks/tests/gocase/unit/type/strings/server.go:112\r\n        \t            \t\t\t\t/Users/runner/work/kvrocks/kvrocks/tests/gocase/unit/type/strings/server.go:101\r\n        \t            \t\t\t\t/Users/runner/work/kvrocks/kvrocks/tests/gocase/unit/type/strings/strings_test.go:742\r\n        \tError:      \tAn error is expected but got nil.\r\n        \tTest:       \tTestString\r\nFAIL\r\nexit status 1\r\n```\r\n\r\nBut from the error message, it seems that it is hard to see which case failed (or which line failed). or i am searching it wrong?

this could be a security issue if there is a release that includes this command\r\ni see 2.5 RC include it, please make sure patch it

Thanks all, merging...

@xiaobiaozhao Why force push cf80ff7d443e42c08f73c1f89caf8996965132bf?

> @xiaobiaozhao Why force push [cf80ff7](https://github.com/apache/kvrocks/commit/cf80ff7d443e42c08f73c1f89caf8996965132bf)?\r\n\r\nrebase ？

> > @xiaobiaozhao Why force push [cf80ff7](https://github.com/apache/kvrocks/commit/cf80ff7d443e42c08f73c1f89caf8996965132bf)?\r\n> \r\n> rebase ？\r\n\r\nIt is very weird. DO NOT force push to other contributor\

@xiaobiaozhao And I would like to state that I believe this is a very serious violation of the "unspoken agreement" among committers in git operations.

i doing a force-push since it is a trivial diff.\r\nFor me, reset --hard unstable + cherry-pick + a new commit is clearer (and easier)

Thanks all, merging...

Oh, so that :\r\n\r\n```\r\nif ((CMAKE_CXX_COMPILER_ID STREQUAL "GNU") OR (CMAKE_CXX_COMPILER_ID STREQUAL "Clang"))\r\n```\r\n\r\nwill not step into. So no flags would be added here. Sounds reasonable

Thanks all, merging...

Close this PR due to lack of activity.

Others are generally good for me, thanks for your efforts.

@git-hulk `rocksdb.share_metadata_and_subkey_block_cache` is completely ignored after the change (it will always be ON, despite whether the option is ON or OFF). Is it OK?

Thanks for your contribution!

Hi guys, i see we are doing squash merge but the commit message is empty.\r\nPersonally, I value the commit message. To be precise, i value the git blame log.\r\nIt is very helpful for us to track the PR changes, the change details etc.\r\nMy suggestion is to add appropriate message to express a certain commit when the code is merged.\r\n\r\nIn Redis community, we usually maintain the top comment of the PR. Both the proposer of the PR and the maintainer of the project have the right to modify the top comment (to reflect the latest status of PR). and when the time is come, maintainers can directly use top comment as commit message and do the squash merge. We can simply edit the top comment, copy those plain text and then do the squash merge\r\n\r\nPS. it was discussed in https://github.com/apache/kvrocks/discussions/1553

This may not be that important, since there may be other commands that have this problem (redis itself also have it),\r\nbut at least for ZMPOP and BZMPOP, this behavior can be consistent (i just happened to see it, a cleanup)

cc @aleksraiden

LGTM, thanks!

Thanks for your contribution!

Seems that when key not found or expired, `Database::Type(const Slice &user_key, RedisType *type)` would return `kRedisNone`, and get `Disk::GetKeySize` with `kRedisNone` would raise not found rather than return `Nil`?\r\n\r\n(It\

Merging...\r\n\r\nThanks for the quick fix :D

cool, thanks for your great efforts!

It seems some memory leaks are found by ASAN.\r\n\r\nhttps://github.com/apache/kvrocks/actions/runs/5527836049/jobs/10085132766?pr=1534

> I recommend you to add unique_ptr wrappers for redisContext and redisReply pointers. You can refer to [event_util.h](https://github.com/apache/kvrocks/blob/unstable/src/common/event_util.h).\r\n\r\n@PragmaTwice I wanted to use unique_ptr to encapsulate `redisContext` and `redisReply`, but I found that it is only used for free. Currently hiredis does not use much, maybe simply handle it. We can better encapsulate hiredis when we use it more in the future.\r\n\r\n

For introducing hiredis, here are something need to be clarified:\r\n- how is TLS support of hiredis? can we directly pass SSL_CTX and ssl_st to it?\r\n- as hiredis is a c library, we need a c++ wrapper. **kvrocks is not written in C**, and we need to follow modern C++ principles. raw C style is not welcomed.\r\n- will the existing code (like, in replication) be replaced with hiredis? or more concrete, how hiredis can help with the client connection reading/writing?

> There is no TLS requirement\r\n\r\nTLS support for replication is an ongoing effort, and we will support TLS for slot migration in the future. So we need to consider these problems now, to prevent TLS being more hard to support.

In the future, if there is a need, we can import other libraries and use this library on other existing code. For now, we can start with a simple library.

> In the future, if there is a need, we can import other libraries and use this library on other existing code.\r\n\r\nWe cannot introduce libraries so casually.\r\n\r\nWhen introducing a library, we need to make sure that it can indeed solve the requirements/simplify the code and is easy to maintain. We will not randomly introduce a library in the early stages and then introduce other similar libraries later when problems arise: this is not something a well-managed project should do, as it would make the code worse by having multiple libraries doing the same thing. We need to carefully consider before introducing any library.\r\n\r\nI am not against introducing hiredis, but I believe that these issues should be considered clearly when introducing it. **Instead of "randomly introducing a simple library and considering what comes next later."**\r\n\r\n[hiredis seems to support TLS](https://github.com/redis/hiredis/blob/master/hiredis_ssl.h), but I\

> We certainly can\

[WIP] Bug fixed in the radius section of RADIUS searches.\r\n\r\n- [x] Work on GEOSEARCH BYBOX tests\r\n- [x] Find the bug and check if expected implementation work similar to redis search.\r\n- [x] Implement GEOSEARCHSTORE.

@git-hulk @PragmaTwice @infdahai , this MR is ready for review, thanks for your patience :D 

> @git-hulk @PragmaTwice @infdahai , this MR is ready for review, thanks for your patience :D\r\n\r\nCongrats! It seems you need firstly resolve the conflict in geo_test.go.

@PragmaTwice done.

Sorry for pending so long, will review in a few days.

@uds5501 Could you please run the `./x.py format` command to format the code with `clang`? 

@uds5501 PR generally looks good to me, can help to format like what @torwig mentioned.

@git-hulk / @torwig will update this today.

Apologies for the delay, @git-hulk @torwig could you please take a look now?

Rebased branch, @git-hulk / @torwig / @PragmaTwice please take a look whenever you get time, thank you!

@torwig thanks for taking out time to review, I have made changes and replied to a couple of comments, kindly check.

@torwig updated the MR with suggestions, please check.

Thanks all, merging...

@uds5501 Thanks for your support again!

No changes to these commands, just a change of order to make the format reasonable.

Merging...\r\n\r\nThank you!

@torwig There is something wrong in CI with Darwin Clang, what can i do to deal with it?

@Yangsx-1 You can try casting `timeout_microsecond` to `int` by using static_cast<int>(timeout_microsecond) because according to the warning, the second argument should be `int` not `int64`.

@Yangsx-1 According to this:\r\n\r\n```\r\nstruct timeval {\r\n   time_t      tv_sec;   // Number of whole seconds of elapsed time\r\n   long int    tv_usec;  // Number of microseconds of rest of elapsed time minus tv_sec. Always less than one million\r\n};\r\n```\r\n\r\nThe number of microseconds is always less than one million so `int` is OK. Or what is your concern?

> @Yangsx-1 According to this:\r\n> \r\n> ```\r\n> struct timeval {\r\n>    time_t      tv_sec;   // Number of whole seconds of elapsed time\r\n>    long int    tv_usec;  // Number of microseconds of rest of elapsed time minus tv_sec. Always less than one million\r\n> };\r\n> ```\r\n> \r\n> The number of microseconds is always less than one million so `int` is OK. Or what is your concern?\r\n\r\nOh, i understand. There is no need to worry about overflow!

Thanks all, merging...

Has been done in #1983, close this PR.

In this version of rocksdb changes a compiler flag PORTABLE, so I changes it into Cmake and Docker for default value 0. In this case all of features (related to CPU) are in runtime. \r\n\r\nSo, now PORTABLE option must be:\r\n\r\n- 0 and compiler work as best as it can\r\n- 1 usually nothing to do; compiler default is typically the most general\r\n- <ARCH> and we can set a CPU family for build best optimal code, e.g.: nocona core2 nehalem corei7 westmere sandybridge corei7-avx ivybridge core-avx-i haswell core-avx2 broadwell skylake skylake-avx512 cannonlake icelake-client rocketlake icelake-server cascadelake tigerlake cooperlake sapphirerapids alderlake bonnell atom silvermont slm goldmont goldmont-plus tremont knl knm x86-64 x86-64-v2 x86-64-v3 x86-64-v4 eden-x2 nano nano-1000 nano-2000 nano-3000 nano-x2 eden-x4 nano-x4 k8 k8-sse3 opteron opteron-sse3 athlon64 athlon64-sse3 athlon-fx amdfam10 barcelona bdver1 bdver2 bdver3 bdver4 znver1 znver2 znver3 btver1 btver2 native\r\n\r\nDetailed, we can see in this code diff: https://github.com/facebook/rocksdb/pull/11419/commits/5cb625142c77c84111f1f65b6f35f2318c602802\r\n

Merging...\r\n\r\nThank you!

Thanks all, merging...

Thanks for @aleksraiden taking care of this, merging...

This PR needs to closed, because it has been finished in https://github.com/apache/kvrocks/pull/2016

Hi @gloof11 can you write a cpp unit test? (probably in https://github.com/apache/incubator-kvrocks/blob/unstable/tests/cppunit/config_test.cc)\r\n

Thanks for your contribution!\r\n\r\nThe code need to be formatted to pass the CI, refer to https://kvrocks.apache.org/community/contributing#code-style.

It seems the code is still not formatted properly.\r\n\r\nAnd I think you need to give a reason why you do not just check `iter == fields_.end()`. TBH I cannot understand your code.

Just made a commit. I read up on how maps **actually** work in Cpp. In the refactor, if the key that was passed to "iter" was not a valid key via "find()", then it would be equivalent to "past-the-end". It simply checks if both values are considered "past-the-end", and throws an error message if that\

@infdahai Yes, I also think that a test, in this case, is too fanatic.

You need to revert the change of `src/common/status.h` to make the CI pass.\r\n\r\n![image](https://github.com/apache/incubator-kvrocks/assets/20042607/b665e83b-bdf5-4bfb-b42d-32a915055a27)

Hi @gloof11 , any progress or problem here?

@PragmaTwice You slashed the clang linter with the long line :)

@PragmaTwice The syntax `ZDIFFSTORE destination numkeys key [key ...]` is sourced from https://redis.io/commands/zdiffstore/. The CommandKeyRange is divided into two parts by `numkeys`. Do we need to add a vector for skipping or just ignore this?

need to write tests.

This PR needs to close, because it has been finished in https://github.com/apache/kvrocks/pull/2021

@torwig Thanks for your quick review.

Could you specify the location of another Join call of the task runner?

> Could you specify the location of another Join call of the task runner?\r\n\r\nSure, the TaskRunner destructor will join as well: https://github.com/apache/incubator-kvrocks/blob/unstable/src/common/task_runner.h#L49

> LGTM. Although a dtor with a long time waiting may cause some misunderstanding.\r\n\r\nYes, I think we can remove the Cancel and Join in dtor since the Server::Stop will cancel the task runner first. Can take a look again. @PragmaTwice @torwig 

@git-hulk Yes, `Server::Stop` will cancel and `Server::Join` will join all threads.\r\nAdditionally, here https://github.com/apache/incubator-kvrocks/blob/unstable/src/common/task_runner.cc#L48 should we stop joining threads when the first call to `util::ThreadJoin` or just log an error and continue joining other threads?

@git-hulk My idea was to change the loop to the following:\r\n\r\n```\r\nfor (auto &thread : threads_) {\r\n  if (auto s = util::ThreadJoin(thread); !s) {\r\n    LOG(WARNING) << "Failed to join thread: " << s.Msg();\r\n    continue;\r\n  }\r\n}\r\n```\r\n\r\nWhat do you think about that? @PragmaTwice What is your opinion on that?

> @git-hulk My idea was to change the loop to the following:\r\n> \r\n> ```\r\n> for (auto &thread : threads_) {\r\n>   if (auto s = util::ThreadJoin(thread); !s) {\r\n>     LOG(WARNING) << "Failed to join thread: " << s.Msg();\r\n>     continue;\r\n>   }\r\n> }\r\n> ```\r\n> \r\n> What do you think about that? @PragmaTwice What is your opinion on that?\r\n\r\nYes, it\

> @git-hulk My idea was to change the loop to the following:\r\n> \r\n> ```\r\n> for (auto &thread : threads_) {\r\n>   if (auto s = util::ThreadJoin(thread); !s) {\r\n>     LOG(WARNING) << "Failed to join thread: " << s.Msg();\r\n>     continue;\r\n>   }\r\n> }\r\n> ```\r\n> \r\n> What do you think about that? @PragmaTwice What is your opinion on that?\r\n\r\nSeems good to me.

Thanks all, merging...

```sh\r\n=== RUN   TestZset/ZRANGE_basics_-_skiplist\r\n    zset_test.go:445: \r\n                Error Trace:    ~/incubator-kvrocks/tests/gocase/unit/type/zset/zset_test.go:445\r\n                Error:          Not equal: \r\n                                expected: []string{}\r\n                                actual  : []string(nil)\r\n                            \r\n                                Diff:\r\n                                --- Expected\r\n                                +++ Actual\r\n                                @@ -1,3 +1,2 @@\r\n                                -([]string) {\r\n                                -}\r\n                                +([]string) <nil>\r\n                                 \r\n                Test:           TestZset/ZRANGE_basics_-_skiplist\r\n```\r\nHow can I solve this?

Can I ask if we use nilString and we use what to match the value or the err?

```sh\r\nok  \tgithub.com/apache/incubator-kvrocks/tests/gocase/unit/type/strings\t45.094s\r\n--- FAIL: TestZset (6.78s)\r\n    --- FAIL: TestZset/ZRANGE_basics_-_skiplist (0.00s)\r\n        zset_test.go:445: \r\n            \tError Trace:\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/unit/type/zset/zset_test.go:445\r\n            \tError:      \tNot equal: \r\n            \t            \texpected: []string{}\r\n            \t            \tactual  : []string(nil)\r\n            \t            \t\r\n            \t            \tDiff:\r\n            \t            \t--- Expected\r\n            \t            \t+++ Actual\r\n            \t            \t@@ -1,3 +1,2 @@\r\n            \t            \t-([]string) {\r\n            \t            \t-}\r\n            \t            \t+([]string) <nil>\r\n            \t            \t \r\n            \tTest:       \tTestZset/ZRANGE_basics_-_skiplist\r\n    --- FAIL: TestZset/ZRANGEBYLEX_with_LIMIT (0.00s)\r\n        zset_test.go:778: \r\n            \tError Trace:\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/unit/type/zset/zset_test.go:778\r\n            \tError:      \tNot equal: \r\n            \t            \texpected: []interface {}([]interface {}{})\r\n            \t            \tactual  : <nil>(<nil>)\r\n            \tTest:       \tTestZset/ZRANGEBYLEX_with_LIMIT\r\nFAIL\r\n```\r\n\r\nHi, if I return the nil string in the redis server, how can I match this msg in go-redis? \r\njust Use Errorf("redis: nil")? 

@infdahai You can use expression like `require.EqualError(t, r.Err(), redis.Nil.Error())`\r\n

Thanks all, merging..

@Yangsx-1 You can use the `./x.py format` command before committing the code to avoid errors during CI.

> @Yangsx-1 You can use the `./x.py format` command before committing the code to avoid errors during CI.\r\n\r\nThanks, fixed.

> Thanks for your contribution!\r\n> \r\n> I notice there are some duplicated code segments, could you try to refactor to avoid these trivially repeated code?\r\n\r\nRefactored.

@torwig Is there anything i need to modify? :)

@Yangsx-1 I made a PR with my changes to your branch. You can approve it and merge so they should be visible here.

> @Yangsx-1 I made a PR with my changes to your branch. You can approve it and merge so they should be visible here.\r\n\r\nThanks for your patience and modification!

Thanks all, merging...

Thanks for @Yangsx-1 contribution, you can add the command in https://github.com/apache/incubator-kvrocks-website/blob/main/docs/supported-commands.md if you like. Also thanks for the great review comments from @torwig and @PragmaTwice.

Thanks for your contribution!\r\n\r\nSeveral points after a short glance:\r\n- seems the ring buffer is shared between all connections, so just be curious that, will the functionality of `SCAN` get break if we have lots of client connections simultaneously? And could we make it duplicate per connections or able to adjust the buffer size dynamically?\r\n- if some keys are deleted while the `SCAN` operation is executed, will the key properties of `SCAN` be preserved? (refer to "Scan guarantees" in https://redis.io/commands/scan/#scan%20guarantees)\r\n- self-increase cursor may cause informantion leaks: one client can retrieve some informantion of the current executed commands from another client (partially same as the first point)\r\n- seems the mutex can be replaced by rwlock or atomic variables\r\n\r\ncc @mapleFU @torwig 

@git-hulk We may need to know how the N value is determined in actual project groups. If N=1024 and the average key name length is 40, the memory usage would be 1024*(40+40)=80KB.  \r\n\r\n

@git-hulk @PragmaTwice \r\nHello, this PR is ready for review. I have change [top comment](https://github.com/apache/incubator-kvrocks/pull/1489#issue-1750786287) for explanation of the changes . Thanks

@mapleFU We add hash to avoid informantion leaks\r\n\r\n> We prevent other users from guessing the data traversed by adjacent cursors by adding the hash value of the keyname to the cursor. If a user tries to obtain adjacent cursor information by traversing the hash, the cursor will become an invalid cursor before the traversal is complete because the size of the 32-bit space is much larger than the length of the cursor_dict_.

I modified the format of the cursor.  https://github.com/apache/incubator-kvrocks/pull/1489/commits/47fb25a63b4f2c207f9544cb0089e35477778d93\r\n- Placed the `counter` in the lowest bit \r\n- set the highest bit of the `hash` to `1`. \r\n- Modified the content of the `static_assert` from `static_assert(CURSOR_DICT_SIZE < (1 << 15),...);` to `static_assert(CURSOR_DICT_SIZE <= (1 << 15),...);`\r\n\r\n\r\n@infdahai @mapleFU \r\n\r\n

So after that the hashing is 31bits?

> So after that the hashing is 31bits?\r\n\r\nSeems right, and [with my idea](https://github.com/apache/incubator-kvrocks/pull/1489#discussion_r1234790169) hash can be 27bit.

> If we encoded cursor_type to the cursor, hash will only have 27 bits left.\r\n\r\nBut I think the hash value here is not an important thing, it is just used as [check digits](https://en.wikipedia.org/wiki/Check_digit), and actually the command type can also be regarded as check digits.\r\n\r\nSeems a 16bit hash is already enough here for me.

This PR generally looks good to me, to see if others have further comments.

Thanks for the review. Does anyone have any other suggestions?\r\n

Thanks all, merging...

very neat design !

Thanks @uds5501, you need to fix the clang-tidy error: https://github.com/apache/incubator-kvrocks/actions/runs/5199313230/jobs/9376587287?pr=1488#step:8:1166 to pass the CI. And can you add some Go tests to make sure it works?\r\n

@git-hulk , added integration test and clang-tidy error

> @git-hulk , added integration test and clang-tidy error\r\n\r\nThanks for your efforts

Thanks all, merging...

@uds5501 Thanks for your contribution.

Thanks all. Merging...

Thanks all, merging...

Hi @jihuayu Thanks for your contribution again.\r\n\r\nFor the merged new command, you can help to add it to the [supported commands](https://github.com/apache/incubator-kvrocks-website/blob/main/docs/supported-commands.md)

Good catch! My bad.

@git-hulk Thanks for reproducing and adding the appropriate test.

Thanks all, merging...

@jihuayu In `CommandSIsMember` you changed `int` to `bool` which is good. However, the line `*output = redis::Integer(ret);` means the boolean value is implicitly converted to an integer value. Am I right?

@jihuayu How about adding the `redis::Bool` function to convert the boolean value to RESP?

@jihuayu I think instead of adding a new function we can simply write `*output = redis::Integer(ret ? 1 : 0);` where `ret` is boolean.

Hello, I have fixed all mentioned above. Is there anything else that needs to be corrected?

@jihuayu Thanks for your contribution.

cc @aleksraiden may you verify this patch also.

> cc @aleksraiden may you verify this patch also.\r\n\r\nAwesome! LGTM

Merging...\r\n\r\nThanks for your review!

@wy-ei Cool, thanks 

IMHO I think maybe we should just use bulk string reply rather than simple string to solve this problem, and for me these string escaping in this patch is just like a workaround.

https://redis.io/commands/monitor/\r\n\r\nSeems redis itself already has some bad practice in solving these escaping problem, which is sad since we need to be compatible to redis.\r\n\r\nFor me, escaping is not need at all, we can just send an arrays with bulk strings, all in RESP.

Thanks all, merging...

Hi @zncleon \r\n\r\n>In addition, srandmember not consider the case of a negative count and return an empty list or set，should we give it a definite action?\r\n\r\nIn this case, we can behave consistently with the Redis.

a nice contribution!

 ok，@git-hulk\r\n> In this case, we can behave consistently with the Redis.\r\n\r\nnow `srandmember` only returns always first N members if not changed.\r\nI will implement its(as well as `zrandmember` #1459) random algorithm according to Redis. \r\n\r\n

yeah, you can do this(or give a new PR about this special problem).  

> ok，@git-hulk\r\n> \r\n> > In this case, we can behave consistently with the Redis.\r\n> \r\n> now `srandmember` only returns always first N members if not changed. I will implement its(as well as `zrandmember` #1459) random algorithm according to Redis.\r\n\r\n@zncleon Cool, can submit an issue to track this.

Thanks all, mergin...

docker run apache/kvrocks --some-key some-value --other-key other-value ....\r\n\r\nwill run , but if you write docker-compose.yml and need read env for a .env, how to write ? change cmd ? Such an operation is very unintuitive.\r\n\r\nThese bash scripts can be run on all platforms with docker and bash installed.\r\n\r\nConfiguring docker through environment variables I think is a very common practice\r\n\r\nfor exampe nginx : https://hub.docker.com/_/nginx\r\n\r\n<img width="818" alt="image" src="https://github.com/apache/incubator-kvrocks/assets/130831741/f5e9aab2-33ea-4f81-b9a4-4808fd6780bf">\r\n\r\nfor example postgres : https://hub.docker.com/_/postgres\r\n<img width="929" alt="image" src="https://github.com/apache/incubator-kvrocks/assets/130831741/9509c93d-18bc-47c9-a00e-07945627248d">\r\n\r\nfor example redis : https://hub.docker.com/_/redis\r\n<img width="1050" alt="image" src="https://github.com/apache/incubator-kvrocks/assets/130831741/dc03feae-832d-418b-9895-0e665e96ba0d">\r\n

I acknowledge your idea to pass env vars as options, but IMHO the shell script composed by `sed` and some other text substitution tools looks terrible to me since it is more like workaround than a real solution.\r\n\r\nBTW, shell scripts are hard to maintain and be general-purpose enough. Our development scripts are all written in python, refer to `x.py`.\r\n\r\nFor [docker/build.sh](https://github.com/apache/incubator-kvrocks/pull/1474/files#diff-efe74abc7213e95246105b4a7b029557ef16ff997e9cc13eadcfffa18e6757c0), I think we do not need the script. You can learn how  our community build and release docker images from https://kvrocks.apache.org/community/create-a-release .\r\n\r\nAnd for [docker/docker-compose.yml](https://github.com/apache/incubator-kvrocks/pull/1474/files#diff-423deb13b7c401b1a7f41ee91c77f722e11d2f317d6a66b546524e8a04cc8b03), I think we do not need an example compose file, and I really do not know why these arguments are chosen and are suitable for various kvrocks users, e.g. `mem_limit: 2G`  `memswap_limit: -1`.

<img width="1918" alt="image" src="https://github.com/apache/incubator-kvrocks/assets/130831741/e96211b3-af6b-4336-8410-01d3adfd6731">\r\nI changed code, read command line args from KVROCKS_ARGS

Close the PR due to inactive, feel free to reopen if you like.

@infdahai Do you want to investigate the reason for an error by using 2/3 lines instead of one?

Yeah, we should distinguish between CI timeouts and program logic errors. Is there any brief way?\r\n\r\nIf there is no good one, we can wrap these calls.\r\n

How can we tell if a command is in its right place?

I find the command list in`https://kvrocks.apache.org/docs/supported-commands/` is ordered with `redis commands`. So the PR is needless and I close this. @git-hulk  

great job!\r\n1. Please add some test case\r\n2. Please explain the difference with the redis official `ZMPOP` command\r\n    1. syntax\r\n    2. Complexity

> Issue number: #1458\r\n\r\nPlease use `close` or `fixes`. The related site is https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue\r\n\r\n

> > Issue number: #1458\r\n> \r\n> Please use `close` or `fixes`. The related site is https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue\r\n\r\nFixed.

Others are good to me

Thanks all, merging...

Thanks @MizukiCry again.

LGTM. Please add the related issue (#1455) to your commit to help others understand why the test is written. 

Thanks all, merging...

Done by #2020 

@torwig I fixed the sync timeout test case as well in the last commit: https://github.com/apache/incubator-kvrocks/pull/1464/commits/bcc7b8509ab69f432f42c5cdeda43855ea066f4f, please take a look again. After changing the timeout to int, then use the float timeout would return an error.

Thanks all, merging...

Thanks @torwig, merging..

I think I can add it

@zevin02 Could you find or add a testcase for https://github.com/apache/incubator-kvrocks/issues/1455 ?

> @zevin02 Could you find or add a testcase for #1455 ? @zevin02 你能为 #1455 找到或添加一个测试用例吗？\r\n\r\nWhat does it mean? Do I need to add a test case for reproducing this issue? I think the PR I just modified already fixed this issue.

> Sorry, miss the NotFound part\r\n\r\n😊 No problem. Great thanks for your review.

> What does it mean? Do I need to add a test case for reproducing this issue? I think the PR I just modified already fixed this issue.\r\n\r\nIt means you can just write a test to record this process. (Add a test to the hash test)\r\nhttps://github.com/apache/incubator-kvrocks/blob/fb0e3d42031a6e0691c7a736711d0a01d72bdb1a/tests/gocase/unit/type/hash/hash_test.go#L53\r\n\r\nThe test is just used to avoid this situation reproducing due to the code that follows.

```\r\n--- FAIL: TestSlotMigrateSync (47.71s)\r\n    --- FAIL: TestSlotMigrateSync/MIGRATE_-_Migrate_sync_with_(or_without)_all_kinds_of_timeouts (0.01s)\r\n        slotmigrate_test.go:425: \r\n            \tError Trace:\t/home/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/integration/slotmigrate/slotmigrate_test.go:425\r\n            \tError:      \tNot equal: \r\n            \t            \texpected: string("OK")\r\n            \t            \tactual  : <nil>(<nil>)\r\n            \tTest:       \tTestSlotMigrateSync/MIGRATE_-_Migrate_sync_with_(or_without)_all_kinds_of_timeouts\r\nFAIL\r\nexit status 1\r\n```\r\n\r\nIs the unittest failed related?

> ```\r\n> --- FAIL: TestSlotMigrateSync (47.71s)\r\n>     --- FAIL: TestSlotMigrateSync/MIGRATE_-_Migrate_sync_with_(or_without)_all_kinds_of_timeouts (0.01s)\r\n>         slotmigrate_test.go:425: \r\n>             \tError Trace:\t/home/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/integration/slotmigrate/slotmigrate_test.go:425\r\n>             \tError:      \tNot equal: \r\n>             \t            \texpected: string("OK")\r\n>             \t            \tactual  : <nil>(<nil>)\r\n>             \tTest:       \tTestSlotMigrateSync/MIGRATE_-_Migrate_sync_with_(or_without)_all_kinds_of_timeouts\r\n> FAIL\r\n> exit status 1\r\n> ```\r\n> \r\n> Is the unittest failed related?\r\n\r\n\r\nI did not make any changes to the relevant code and I don\

It is related to #1452.

emm,So what should I do\r\n\r\n> It is related to #1452.\r\n\r\n

\r\n\r\n\r\n\r\n> > What does it mean? Do I need to add a test case for reproducing this issue? I think the PR I just modified already fixed this issue.\r\n> \r\n> It means you can just write a test to record this process. (Add a test to the hash test)\r\n> \r\n> https://github.com/apache/incubator-kvrocks/blob/fb0e3d42031a6e0691c7a736711d0a01d72bdb1a/tests/gocase/unit/type/hash/hash_test.go#L53\r\n> \r\n> The test is just used to avoid this situation reproducing due to the code that follows.\r\n\r\nI can add this test case later

Thanks all, the patch looks good to be merged.

Thanks all, merging...

@tisonkun I modify the issue.

The flaky test still exists after applying #1449 and it should be caused by the retry in Go redis client.

ready for reviews.

@infdahai Thanks, no hurry. Can ping back while your PR is ready to review.

@git-hulk I was resurrected. Ready to review.

https://github.com/apache/incubator-kvrocks/actions/runs/5234808336/jobs/9452175246\r\n\r\nDo we need to ignore this?

> https://github.com/apache/incubator-kvrocks/actions/runs/5234808336/jobs/9452175246\r\n> \r\n> Do we need to ignore this?\r\n\r\nNope. The build procedure need to succeed in Darwin. It cannot be ignored.\r\n\r\nYou can add some type cast before the std::min call to make it compile successfully.

Thanks all, merging...

@infdahai Can add the SINTERCARD command to https://github.com/apache/incubator-kvrocks-website/blob/main/docs/supported-commands.md

Thanks all, I will merge and summary this PR change.

I also encountered this.

Thanks @torwig, merging...

CI failure: https://github.com/apache/incubator-kvrocks/actions/runs/4939330152/jobs/8830171808?pr=1436\r\nJust for record

There seems some compile error while using fmt 10 in CI. Could you try to investigate it? Or you can wait me for some free time to handle it (maybe next week).

A lot of thanks @torwig about fix issue

@aleksraiden Thanks for your contribution.

For less dependencies and small image, what about using [distroless/cc](https://github.com/GoogleContainerTools/distroless/blob/main/cc/README.md) as base for the final image?

Could you provide a preview image on docker hub for quick review?

> Could you provide a preview image on docker hub for quick review?\r\n\r\nhttps://hub.docker.com/repository/docker/aleksraiden/kvrocks/general

I have work on a few patch to improve docker with new alpine image, please waiting before merge

So, new version are ready:\r\n\r\n- Use redis from official alpine repo instead of manual build (stable 7.х version - 7.0.11)\r\n- Add ninja for build dependency (if you want to use it)\r\n- Use UTC timezone as default in run image too\r\n- Change healthcheck start delay to 5 sec.\r\n\r\nIn result we have:\r\n\r\n- ~10% less container size (34.9 Mb vs. 40 Mb)\r\n- 30% faster container build

@aleksraiden Great result!

Pushed to Docker Hub - https://hub.docker.com/layers/aleksraiden/kvrocks/alpine-latest-v2/images/sha256-8a59b0ab7c79bf11375a4e41919b3ecfefb6c41d8f36536cb12e9b5c37cb3a4e?context=explore

I try out to work with distroless + glibc. The result image size is still 40MB. No need to invest more time since that base image is good for small size only - it lacks of most functions.

Merging...

> Add manual check Backtrace package and library for support building from source in Alpine Linux env.\r\n\r\nOut of curiosity, why do we have to do this in alpine environment?

@tisonkun "Why" is a good question :) Perhaps, because Alpine Linux uses `musl-libc` instead of `libc`.

Merging...

Tested at https://github.com/apache/incubator-kvrocks/actions/runs/4900989972/jobs/8752006763

To make the CI pass, you can run `./x.py format` in your local before pushing your code.

@xiaobiaozhao  please benchmark this in `compaction with heavy write` scenario

This LGTM. Let wait some benchmarking for it

@xiaobiaozhao Did you test this PR? 

> \r\n\r\n\r\n\r\n> @xiaobiaozhao Did you test this PR?\r\nI will test it this weekend.\r\n

Press data into kvrocks script\n\n> docker run -d --rm redislabs/memtier_benchmark:latest -t 4 -c 20 -s 172.27.255.244 -p 6666 --distinct-client-seed  --key-minimum=1000000000 --key-maximum=10000000000 --random-data  --key-prefix="" --data-size=50 -n 100000000 --pipeline=100 --command "mset  __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__ __key__ __data__"\n\nwrite script\n> docker run --rm redislabs/memtier_benchmark:latest -t 4 -c 20 -s 172.27.255.244 -p 6666 --distinct-client-seed  --key-minimum=1000000000 --key-maximum=10000000000 --key-prefix="" -n 1000 --command "set  __key__ __data__"\n\n![mmexport1683951102262.png](https://github.com/apache/incubator-kvrocks/assets/52393536/e59a3ecb-4826-4e5d-b1d5-8310e2c41b94)\n\n![mmexport1683951105206.png](https://github.com/apache/incubator-kvrocks/assets/52393536/834cb41d-4d29-4fcf-a199-ebd14322afe4)\n\n![mmexport1683951131259.png](https://github.com/apache/incubator-kvrocks/assets/52393536/1f7e2664-42c5-4d9e-831c-2f718e5c1d77)\n\n\n\n

Thanks! Do you have idea that why the performance decline in normal p100(And does all performance optimization and decline comes from unstable test result)?

> Thanks! Do you have idea that why the performance decline in normal p100(And does all performance optimization and decline comes from unstable test result)?\r\n\r\nI think this is bias of a test, p99 p9999 should be more convincing\r\n

Thanks all, merging...

Merging...

Thanks all. Merging...

@mapleFU Thanks for reminding! Have fixed it.

Rest LGTM

The release manual should be updated later also - https://github.com/apache/incubator-kvrocks-website/blob/f2ecdd3743fbbeddbbe293ced5fd49d6d74a0626/community/create-a-release.md?plain=1#L173

Thanks @ZENOTME , merging...

@git-hulk @xiaobiaozhao @PragmaTwice @torwig @ShooterIT 

@torwig @PragmaTwice The reason why I use `shared_ptr` of `SyncMigrateContext` as a field of  `CommandClusterX` is that I have to keep the lifetime of `SyncMigrateContext` as long as the command object.\r\n\r\nWhen the `slot_migrator` wake up the `SyncMigrateContext` with a migration result status, it has to release the pointer so that no context will "occupy" the migrator any more (see [here](https://github.com/apache/incubator-kvrocks/pull/1418/files#diff-6a88559b7b94f9543cde3449bda77668eba37d8696153da4f244b250054e809aR1113)). \r\nHowever, at this time point, the `WriteCB` have not been triggered by the event loop yet, thus the context cannot be deconstructed yet. Using a shared pointer will bind the life cycle of the `SyncMigrateContext` with the `CommandClusterX` object, which will solve this issue.

@ellutionist Thanks for your explanation.\r\n\r\nI have not dived into these changes deeply, but if you just want to make lifetime of SyncMigrateContext as long as the command object, you can store an instance of SyncMigrateContext in CommandClusterX directly (`class CommandClusterX { SyncMigrateContext ctx_; ... }`), and pass some non-owning pointer (like raw pointer, no ownership) to the migrator.

> @ellutionist Thanks for your explanation.\r\n> \r\n> I have not dived into these changes deeply, but if you just want to make lifetime of SyncMigrateContext as long as the command object, you can store an instance of SyncMigrateContext in CommandClusterX directly (`class CommandClusterX { SyncMigrateContext ctx_; ... }`), and pass some non-owning pointer (like raw pointer, no ownership) to the migrator.\r\n\r\nI agree with that

> @ellutionist Thanks for your explanation.\r\n> \r\n> I have not dived into these changes deeply, but if you just want to make lifetime of SyncMigrateContext as long as the command object, you can store an instance of SyncMigrateContext in CommandClusterX directly (`class CommandClusterX { SyncMigrateContext ctx_; ... }`), and pass some non-owning pointer (like raw pointer, no ownership) to the migrator.\r\n\r\n@PragmaTwice @xiaobiaozhao Thank you for the advice. This is another reasonable solution. However, there are two things to note here:\r\n1. The `CommandClusterX` does not always have a `SyncMigrateContext` (only when migrating with a "sync" flag). Making the context a non-pointer member of the `CommandClusterX` object may not make good sense here. If we decide to do this, I suggest to adopt `std::optional<SyncMigrateContext>`.\r\n2. Due to the concern of memory safety, I am not a fan of raw pointers. Of course, in this PR we could use it, but we may set up some potential risk for the future. Because not every maintainer could get to know  that "the`CommandClusterX` object must live long enough to complete the migration". If some change is made to "how the command objects work" in the future, it may lead to crash.

> The CommandClusterX does not always have a SyncMigrateContext (only when migrating with a "sync" flag). Making the context a non-pointer member of the CommandClusterX object may not make good sense here. If we decide to do this, I suggest to adopt std::optional<SyncMigrateContext>.\r\n\r\nSure, you can use something like `std::optional` or `std::unique_ptr`.\r\n\r\n> Due to the concern of memory safety, I am not a fan of raw pointers. Of course, in this PR we could use it, but we may set up some potential risk for the future. Because not every maintainer could get to know that "theCommandClusterX object must live long enough to complete the migration". If some change is made to "how the command objects work" in the future, it may lead to crash.\r\n\r\nI think in kvrock, we use `std::shared_ptr` if and only if the object need to be **"shared"** between multiple references, and the lifetime of this object cannot be determined **statically** (e.g. given an object `o` and two reference `a` and `b`, the lifetime of `o` is equal to the maximum of lifetime of `a` and `b`, which need to be determined **dynamically**).\r\n\r\nSo currently, just *"in the future ..."* seems not to be an accredited reason to me, since if this reason holds, we may need to change many raw pointer in the codebase to `shared_ptr`. If you just need a non-owning pointer (or called an observer/view/span, in modern C++), raw pointer is currently the available choice, since [`std::observer_ptr`](https://en.cppreference.com/w/cpp/experimental/observer_ptr) or `std::optional<T&>` (optional with reference type) is not in the standard.

> > The CommandClusterX does not always have a SyncMigrateContext (only when migrating with a "sync" flag). Making the context a non-pointer member of the CommandClusterX object may not make good sense here. If we decide to do this, I suggest to adopt std::optional.\r\n> \r\n> Sure, you can use something like `std::optional` or `std::unique_ptr`.\r\n> \r\n> > Due to the concern of memory safety, I am not a fan of raw pointers. Of course, in this PR we could use it, but we may set up some potential risk for the future. Because not every maintainer could get to know that "theCommandClusterX object must live long enough to complete the migration". If some change is made to "how the command objects work" in the future, it may lead to crash.\r\n> \r\n> I think in kvrock, we use `std::shared_ptr` if and only if the object need to be **"shared"** between multiple references, and the lifetime of this object cannot be determined **statically** (e.g. given an object `o` and two reference `a` and `b`, the lifetime of `o` is equal to the maximum of lifetime of `a` and `b`, which need to be determined **dynamically**).\r\n> \r\n> So currently, just _"in the future ..."_ seems not to be an accredited reason to me, since if this reason holds, we may need to change many raw pointer in the codebase to `shared_ptr`. If you just need a non-owning pointer (or called an observer/view/span, in modern C++), raw pointer is currently the available choice, since [`std::observer_ptr`](https://en.cppreference.com/w/cpp/experimental/observer_ptr) or `std::optional<T&>` (optional with reference type) is not in the standard.\r\n\r\nThank you. I got the point of the difference between dynamic and static lifetime. From this point of view the `share_ptr` is not proper in this case. Perhaps the raw pointer is the best solution here.

Hi @ellutionist Need to use `./x.py` format to correct the code format 

> Hi @ellutionist Need to use `./x.py` format to correct the code format\r\n\r\n> Hi @ellutionist Need to use `./x.py` format to correct the code format\r\n\r\nDone. It took me a while to adapt to the recent changes from [#1420]. \r\n\r\nAlso some more go test cases were added.

Seems need run `./x.py check golangci-lint`.\r\n\r\n> Those functions in SyncMigrateContext can be private:\r\n\r\nCurrently we need to make them public for the CRTP base classes to use them.\r\n

Looks like the migration test case has a segment fault. https://github.com/apache/incubator-kvrocks/actions/runs/4951001502/jobs/8855626634?pr=1418

Merging... Thanks for your contribution!

Thanks @ColinChamber , merging...

> Thanks for your contribution!\r\n> \r\n> You can run `./x.py format` to format the code and let the CI proceed.\r\n\r\nIf I exec `./x.py format`, I get a irrelevant result.(Not important though, just ignored).\r\n![image](https://user-images.githubusercontent.com/30547248/235607033-0051de12-08f9-499a-830a-a3e3ddc1879e.png)\r\n

> If I exec `./x.py format`, I get a irrelevant result.(Not important though, just ignored). ![image](https://user-images.githubusercontent.com/30547248/235607033-0051de12-08f9-499a-830a-a3e3ddc1879e.png)\r\n\r\nCould you provide the version of clang-format you are using?\r\n\r\nAnyway, I think you can firstly manually ignore this unrelated change as a workaround.

Sorry for the wrong close-button click. :rofl:

> > If I exec `./x.py format`, I get a irrelevant result.(Not important though, just ignored). ![image](https://user-images.githubusercontent.com/30547248/235607033-0051de12-08f9-499a-830a-a3e3ddc1879e.png)\r\n> \r\n> Could you provide the version of clang-format you are using?\r\n> \r\n> Anyway, I think you can firstly manually ignore this unrelated change as a workaround.\r\n\r\nyes, I will propose a new pr for this `clang-tidy`.

CI seems still blocked in format. Not sure your clang-format version, but you can try clang-format 12, which is used in CI.\r\n\r\nhttps://github.com/llvm/llvm-project/releases/tag/llvmorg-12.0.1\r\n\r\nAnd I can confirm that clang-format 14 on my local has also no issue.\r\n\r\n```\r\n/home/runner/work/incubator-kvrocks/incubator-kvrocks/src/cluster/cluster.cc:474:39: error: code should be clang-formatted [-Wclang-format-violations]\r\n  for (const auto &id : n->replicas) {      // replicas\r\n                                      ^\r\n```

Thanks all, merging...

Can we have lint to find that some cast is useless?

> Can we have lint to find that some cast is useless?\n\nCurrently no.

Thanks all. Merging...

Seems ok, RocksDB also use way like that https://rocksdb.org/blog/2019/03/08/format-version-4.html\r\nShould we mark extract for "metadata encoding version"? Or just "encoding version" is ok?

> Seems ok, RocksDB also use way like that https://rocksdb.org/blog/2019/03/08/format-version-4.html Should we mark extract for "metadata encoding version"? Or just "encoding version" is ok?\r\n\r\nDone.

Merging. Thanks for your contribution.

Seems that only "hello" support authenticate with user, should  https://redis.io/commands/auth/ support that?

> Seems that only "hello" support authenticate with user, should https://redis.io/commands/auth/ support that?\r\n\r\nI think it is not urgent as HELLO, since HELLO in redis just does not allow the username to be omitted, but `AUTH password` is valid in redis.

Ok, reasonable. LGTM!

Awesome feature, we are waiting for this, lot of thanks! For example, we can use this streamID for checking latency for each message (if consumer after processing, store an result with a same ID, but new autogenerated part of time). 

And some of futures - this feature will be pretty use with XMOVE/XCOPY next command planned too... ;) 

Merging...

Hi @Venjy, thanks for your contribution!\r\n\r\nWhile it is glad to have a GUI for operations on kvrocks, we currently have no plan to maintain a web application in this repo (apache/incubator-kvrocks).\r\n\r\nI think you can consider to maintain it by your self, extract it from a kvrocks fork as a single web app repo, or maybe contact [KvrocksLabs](https://github.com/KvrocksLabs) for any desire to accept it (mail to dev@kvrocks.apache.org).\r\n

> \r\n\r\nPlease create a discussion before you do development

Hi @Venjy\r\n\r\nMany thanks for your efforts first, As @PragmaTwice mentioned, you can start a discussion to show or discuss what you do.

@torwig thanks for help in discovering this bug

Thanks all, merging...

- please write a more descriptive PR title (done by me, but please do that next time);\r\n- please add some PR description.

Thanks all. Merging...

I tested it locally (with the CONFIG REWRITE)\r\nand the ref: https://stackoverflow.com/questions/5605125/why-is-iostreameof-inside-a-loop-condition-i-e-while-stream-eof-cons

@enjoy-binbin Thanks for your efforts. ❤️ 

Here is a simple test to demonstrate this point:\r\n```\r\n❯ cat y.cpp\r\n#include <fstream>\r\n#include <iostream>\r\n#include <string>\r\n\r\nint main() {\r\nstd::ifstream file("hello.txt");\r\nstd::string line;\r\nwhile (std::getline(file, line)) {\r\n    if (file.eof()) break;\r\n    std::cout << "> " << line << std::endl;\r\n}\r\n}\r\n\r\n❯ python -c "file=open(\

@PragmaTwice thanks for the detailed explanation! do you have any suggestions? i may need some time to figure it out\r\ndo you think if i put the `if (file.eof()) break;` in the last will help?\r\n\r\n

> @PragmaTwice thanks for the detailed explanation! do you have any suggestions? i may need some time to figure it out do you think if i put the `if (file.eof()) break;` in the last will help?\r\n\r\nNot sure the eof check is necessary, maybe something like `while(file.good() && getline(...))` is ok.\r\n\r\nThe call to `getline` will return the stream itself, so `operator bool` of the stream is called, and it is equal to `!stream.fail()`.

i change to use `while(file.good() && getline(...))` for now, i also tested it in local, it is fine

Thanks all, merging...

having trouble with how to add a time test case...

@git-hulk thanks. i got it now

Thanks all, merging...

Hi @zevin02, thanks for your contribution!\r\n\r\nWe setup lots of code checks in our CI, which requires you to pass for better code quality.\r\n\r\nFor example, from a glance to your code, maybe these checks will fail: \r\n- local variables must be lower_case with underscore,\r\n- NULL must be replaced with nullptr\r\n\r\nYou can check the CI log for more details and for how to modify your code to pass the CI.\r\n\r\nIt is recommended to try some commands to solve these problems, but maybe not all problems:\r\n- `./x.py format`\r\n- `./x.py check tidy --fix`\r\n\r\nAnother thing is that, it would be better to have some unit tests for this function before it is merged : )

@PragmaTwice I think I solved all the problems, can you give me a review

> @PragmaTwice I think I solved all the problems, can you give me a review\r\n\r\nSure! But as I mentioned in https://github.com/apache/incubator-kvrocks/pull/1392#issuecomment-1511603694, you need first to make the CI pass, which currently failed.\r\n\r\n```\r\n./x.py format\r\n```

> The rest looks good to me. cc @git-hulk\r\n\r\n@PragmaTwice I do a minor refactor, can help to take a look again.

Thanks for your contribution! Merging...

Thanks all. Merging...

Thanks all. Merging...

Thanks. Merging...

Thanks all, merging...

another question, i see the code i added, now cluster info is below rocksdb sestion, the location feels a bit strange, should i move it?

Thanks all, merging...

Good, thanks! Now I use a redisInsight tool im work and we have two separated service, redis and kvroks, and this features very good to me!

Thanks all, merging...

i want to rewrite code and checker failed. @PragmaTwice

@torwig when the config is deprecated, all the invalid code need to delete? I will check and delete no used code

@bruceqiwang Yes, you are right, there is no reason to delete that code for now.

Thanks for your contribution. Merging...

@PragmaTwice Could you please have a look at my PR? As @tisonkun advised, I can split it into several distinct PRs to make the review more convenient. What do you think?

Thanks all, merging...

Merging..

@io55555 Thanks for picking it up! Could you share the motivation for doing this upgrade? We don\

1, Correct a rare corruption bug in high compression mode.\r\n2, Improves decompression speed on Linux (+50%), wildly accelerates compression, doubling speed on large files (~1GB).\r\n3, 5.6x faster compression for small fields (1K).\r\n4, Faster compression at level 5-11 (6.2x-16x). \r\n

Refer to https://github.com/facebook/zstd/releases/tag/v1.5.5, it seems the performance improvement is pretty impressive.\r\n\r\nSo it is ok for me.

Merging...\r\n\r\nThank you @io55555!

I remember I review a similar patch before, but I cannot find it now..\r\n\r\nGood to merge for this one.

@tisonkun Yeah, I also had a strong feeling of déjà vu.

Yeah, I used to submit this patch in #1330, but there is always an ICE in gcc9 during LTO.\r\nIt has been workarounded in d13dbfd2c0bab0cdc2bc4b3e8f512041b5173633, so this patch can be merged.

Update: I have tested if the prefix extractor can bring better performance when seeking, but the result doesn\

Do we have some benchmark or micro-benchmark here? Seems MyRocks requires that, so RocksDB implements it. Can we take some benchmark from MyRocks?

Thanks @mapleFU, do you have any references?

1. MyRocks Paper, Section 3.2.2.1: https://vldb.org/pvldb/vol13/p3217-matsunobu.pdf\r\n2. https://www.slideshare.net/matsunobu/myrocks-deep-dive/58\r\n3. https://www.cidrdb.org/cidr2017/papers/p82-dong-cidr17.pdf\r\n\r\n> Preﬁx Bloom ﬁlters. Bloom ﬁlters do not help with range queries. We have developed a preﬁx Bloom ﬁlter that helps with range queries, based on the observation that many range queries are often over a preﬁx; e.g., the userid part of a (userid,timestamp) key or postid of a (postid,likerid) key. We allow users to deﬁne preﬁx extractors to deterministically extract a preﬁx part of the key from which we construct a Bloom ﬁlter. When querying a range, the user can specify that the query is on a deﬁned preﬁx. We have found this optimization reduces read ampliﬁcation (and attendant CPU overheads) on range queries by up to 64% on our systems.

Close this PR since we cannot find any performance improvements. Can reopen it if anyone is interested in this.

Thanks all. Merging...

Thanks. Merging...

Random thoughts - such service config can work better if integrated with package manager like AUR (pacman) or Homebrew.

Merging...

@PragmaTwice Impressive optimization!

Thanks all. Merging...

demo output\r\n```\r\nStarting 30001\r\ninstance is not ready wait 1/5s\r\nOK\r\nOK\r\nStarting 30002\r\ninstance is not ready wait 1/5s\r\nOK\r\nOK\r\nStarting 30003\r\ninstance is not ready wait 1/5s\r\ninstance is not ready wait 2/5s\r\nOK\r\nOK\r\nStarting 30004\r\ninstance is not ready wait 1/5s\r\nOK\r\nOK\r\nStarting 30005\r\ninstance is not ready wait 1/5s\r\ninstance is not ready wait 2/5s\r\nOK\r\nOK\r\nStarting 30006\r\ninstance is not ready wait 1/5s\r\ninstance is not ready wait 2/5s\r\nOK\r\nOK\r\n```

Cool, Thanks for your contribution. @enjoy-binbin We can use the command `./x.py format` to auto-format and make the linter happy.

Thanks all, merging..

Good catch, thanks

Thanks @enjoy-binbin and @torwig, merging...

Thanks all. Merging...

i am new to kvrocks / c++ / go, so I pick a simple task to do first.\r\nplease let me know if i do something wrong or i can do something to make the code better. thanks

Thanks for your contribution! Merging...

@torwig Thank you. So for the XSETID command, I should parse the last_added_id / entries_added / max_deleted_id, then translate it into the XSETID. Is it right?

BTW, do we implement https://github.com/apache/incubator-kvrocks/pull/577#issuecomment-1128483342?

@git-hulk Yes, absolutely.

Thanks all, merging..

```\r\n./x.py test go build -deleteOnExit true\r\n```

Thanks for your contributing. \r\n\r\nAs mentioned by @tisonkun , we already have an option for testing without temp files. And these files are useful when some tests are failed (or when the program crashed).\r\n\r\nBut feel free to do other improvements : )

> If you press ctrl + c in the middle, will it be cleaned up? I tried it and it doesn’t seem to work, if there is an error or ctrl+c in the middle.\r\n\r\nCurrently we need to cleanup these files manually (if we really do not need them to debug), but you are welcome to improve it (with regarding to the `deleteOnExit` option, althought I think it is a bit tricky, and maybe not a vital thing).

@PragmaTwice Wow! Awesome feature, especially 64-bit size.

Awesome, I very need this feature!!

It is ready for review now.

Wow! Awesome feature

Hi @torwig, thanks for your testing. \r\n\r\nIn this PR, the `metadata.expire` is now always in milliseconds (whatever it is in 64bit mode or not). And only when we encode it into some raw data in non-64bit mode, it will be converted to seconds.\r\n\r\nThis design is aimed to make the code simple. If the `metadata.expire` can be either in milliseconds or seconds, it can be a mess.

@PragmaTwice So there is a mistake in my test?

> @PragmaTwice So there is a mistake in my test?\r\n\r\nThe correct version is like this:\r\n```c++\r\nTEST(Metadata, MetadataDecodingBackwardCompatibleSimpleKey) {\r\n  auto expire_at = (Util::GetTimeStamp() + 10) * 1000;\r\n  Metadata md_old(kRedisString, true, false);\r\n  EXPECT_FALSE(md_old.Is64BitEncoded());\r\n  md_old.expire = expire_at;\r\n  std::string encoded_bytes;\r\n  md_old.Encode(&encoded_bytes);\r\n\r\n  Metadata md_new(kRedisNone, false, true); // decoding existing metadata with 64-bit feature activated\r\n  md_new.Decode(encoded_bytes);\r\n  EXPECT_FALSE(md_new.Is64BitEncoded());\r\n  EXPECT_EQ(md_new.Type(), kRedisString);\r\n  EXPECT_EQ(md_new.expire, expire_at);\r\n}\r\n```\r\n\r\nYou need to use milliseconds for `metadata.expire`.\r\n

@torwig Thanks! Added.

Thanks @torwig and @git-hulk. Merging...

Can some stale versions of kvrocks recognize this format? If user ENABLE it, and find other bug, and revert kvrocks to a old version. Would user get segment fault on these keys?

> Can some stale versions of kvrocks recognize this format? If user ENABLE it, and find other bug, and revert kvrocks to a old version. Would user get segment fault on these keys?\r\n\r\nFirstly, we currently do not enable this new encoding by default.\r\n\r\nAnd if users write some data via the new encoding, they cannot revert their kvrocks version to 2.3.0 or earlier. But if we release 2.4.0 and keep `ENABLE_NEW_ENCODING=OFF`, and then release 2.5.0 to turn on it as default, the users can revert their version from 2.5.0 to 2.4.0 without any data breaking.\r\n\r\nBTW, I think any encoding changes may be a pain to users, which is not specially related to this PR.

Thanks all. Merging...

a taste problem, why use `cOK` instead of `kOK`, for const variables, we use `k` prefix but this one 🙃

> a taste problem, why use `cOK` instead of `kOK`, for const variables, we use `k` prefix but this one 🙃\r\n\r\nMaybe you need to ask the author of Status. I just use the original name.

😭 This should be a typo from me, we copied the status from rocksdb and forgot to remove the `c` prefix.

@PragmaTwice Hi, i suppose you can do the review, thanks.

Thanks, merging...

Thanks all. Merging...

Thanks all, merging...

> Could you elaborate a bit how you fix this race condition logically? Generally, "wait for a while" is not logically correct but best effort.\r\n\r\nHi @tisonkun , this race is triggered by modifying the expire time of a key while compacting. Rare in real-world but indeed a race. \r\n\r\nIn my view, since it is hard and expensive to add locking to both compacting and command executing, waiting is the currently best way to avoid the race.

> > Could you elaborate a bit how you fix this race condition logically? Generally, "wait for a while" is not logically correct but best effort.\r\n> \r\n> Hi @tisonkun , this race is triggered by modifying the expire time of a key while compacting. Rare in real-world but indeed a race.\r\n> \r\n> In my view, since it is hard and expensive to add locking to both compacting and command executing, waiting is the currently best way to avoid the race.\r\n\r\nThank you. Then let\

Thanks all, merging...

Thanks for your contribution again @dongdongwcpp 

Good job! Could you append `modernize-return-braced-init-list` to `WarningsAsErrors` in `.clang-tidy`?

@PragmaTwice Update .clang-tidy and merged unstable in my branch.

Great thanks for @aleksraiden proposing this change.

Thanks all, merging...

There are still ICE:\r\n\r\n```\r\nlto1: internal compiler error: in add_symbol_to_partition_1, at lto/lto-partition.c:153\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.\r\nlto-wrapper: fatal error: /usr/bin/g++ returned 1 exit status\r\ncompilation terminated.\r\n/usr/bin/ld: error: lto-wrapper failed\r\ncollect2: error: ld returned 1 exit status\r\n```

> There are still ICE:\r\n> \r\n> ```\r\n> lto1: internal compiler error: in add_symbol_to_partition_1, at lto/lto-partition.c:153\r\n> Please submit a full bug report,\r\n> with preprocessed source if appropriate.\r\n> See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.\r\n> lto-wrapper: fatal error: /usr/bin/g++ returned 1 exit status\r\n> compilation terminated.\r\n> /usr/bin/ld: error: lto-wrapper failed\r\n> collect2: error: ld returned 1 exit status\r\n> ```\r\n\r\nsad

Will split this PR to multiple ones against ICE.

close this pr, new pr is  https://github.com/apache/incubator-kvrocks/pull/1374

Merging...\r\n\r\nThanks for your review :)

@ellutionist can you help to resolve conflicts?

Thanks all, merging...

Thanks for @ellutionist contribution again.

Thanks all.

Do we plan to optimize more places with TBB in the future?

> Do we plan to optimize more places with TBB in the future?\r\n\r\nSure.

Merging...

Merging.. Thanks all.

@torwig @caipengbo I extended the sequence limitation to require the checkpoint sequence must be greater or equal to the WAL sequence instead of strictly matching.\r\n\r\n```\r\n  if (seq < wal_seq) {\r\n    return {Status::NotOK, fmt::format("checkpoint seq: {} is smaller than the WAL seq: {}", seq, wal_seq)};\r\n  }\r\n```\r\n\r\nAnd I have tested if this check works correctly on my side.\r\n\r\n

Thanks all, merging...

Thanks @torwig , merging...

Thanks @torwig, merging...

Thanks all, merging...

Thanks for your contribution and patient again. @paragor 

Thanks all. Merging...

Thanks all, merging...

@DenizPiri Thanks for your good catch and contribution again.

@aleksraiden Thanks for your contribution.

@guoxiangCN Thanks for your contribution!

Just parallel exec can be put into a "Session.Context" rather than `Storage`. A exec can write to it own session, and read first it own session.\r\n\r\nThough it\

> Just parallel exec can be put into a "Session.Context" rather than Storage. A exec can write to it own session, and read first it own session.\r\n\r\nYes, the transaction state indeed is a connection state, but it needs some refactoring work to support binding transaction conetext in underlying storage type implementation. We won\

In general, LGTM.

Thanks all, merging…

@jjz921024 Merged. Thanks for your contribution again!

Thanks all. Merging...

This PR is generally good for me, to see if other folks have comments. @torwig @ShooterIT @caipengbo 

Thanks all. Merging...\r\n\r\nFeel free to open an issue if there is anything you are concerning about this PR later.

Great work!

Thanks all. Merging...

Thanks all, merging...

Thanks all, merging...

Thanks all. Merging...

> Generally looks good. One comment here:\r\n> \r\n> As we have #1037 now I\

Hi @git-hulk, I remember that, while I run some docker images like caddy, there are several volumes and even a single file volume for configuration, e.g.\r\n```\r\ndocker run -p 80:80 -p 443:443 \\\r\n    -v somepath/Caddyfile:/etc/caddy/Caddyfile \\\r\n    -v somepath:/data -v somepath:/config caddy/caddy\r\n```\r\n\r\nSo I think maybe the current file structure is enough?\r\n\r\nBTW, I think maybe `/kvrocks/data` is more simple that `/var/lib/kvrocks` for data volume.

> BTW, I think maybe /kvrocks/data is more simple that /var/lib/kvrocks for data volume.\r\n\r\nYes, `/kvrocks/data` is a better path name, but we cannot change the path now since it will affect old users.\r\n\r\n

Thanks all. Merging...

Merging...

Thanks all. Merging...

Thanks for your contribution!

Thanks all. Merging...

Thanks all. Merging...

Thanks all, merging...

Thanks all. Merging...

Thanks all, merging...\r\n\r\n@caipengbo Can help to push a new tag to have a try, the DOCKER_USERNAME and DOCKER_PASSWORD had existed before if my memory serves.

This is already discussed at:\r\n\r\n* https://github.com/apache/incubator-kvrocks/pull/588#issuecomment-1131320266\r\n* https://github.com/apache/incubator-kvrocks/pull/752#issuecomment-1194936068

Thanks all. Merging...

Though, the current one is acceptable for me.

How about dropping the prefix?

> How about dropping the prefix?\r\n\r\nI think the prefix can make them more distinguishing than other source files (in this dir) in alphabetical order, e.g.\r\n```\r\na  (cmd)\r\nb\r\nc  (cmd)\r\nd\r\ne  (cmd)\r\n```\r\nthan\r\n```\r\nb\r\ncmd_a\r\ncmd_c\r\ncmd_e\r\nd\r\n```

@caipengbo @hulk I did some rename in cd17abfd6b2bd3219f00e843f7b402e3af6cd510:\r\n```\r\nscan_util.h -> scan_base.h\r\nredis_cmd.{h|cc} -> commander.{h|cc}\r\nredis_cmd_*.{h|cc} -> cmd_*.{h|cc}\r\nredis_cmd_error.h -> error_constants.h\r\n```

Thanks all. Merging...

Thanks all. Merging...

Thanks all, merging...

Thanks all. Merging...

> For confirmation, will MORE_BUILD_ARGS override ENABLE_OPENSSL and PORTABLE if specified?\r\n\r\nYeah.

Thanks all. Merging...

Merging...

Thanks all. Merging...

Merging...

I think maybe we cannot bump the version of snappy due to some unknown bugs, refer to #788 and #790.

@PragmaTwice yes, sorry, I forgot about this bug. Revert this asap.

Thanks all, summarizing and merging...

Great thanks to @aleksraiden again

Thanks, @torwig \r\n\r\nNow all works fine!

Thanks all. Merging...

I notice that gcc implemented floating-point version `std::from_chars` since 11.1, which is a sad story. It means we cannot use `std::from_chars` because it comes too late and the gcc version is too new for some users. So I will implement it using `strto*` in c std library instead.

> I notice that gcc implemented floating-point version `std::from_chars` since 11.1, which is a sad story. It means we cannot use `std::from_chars` because it comes too late and the gcc version is too new for some users. So I will implement it using `strto*` in c std library instead.\r\n\r\ndone

Thanks all. Merging...

> May you add a description on what is fixed and the different before and after?\r\n\r\n\r\nhttps://github.com/apache/incubator-kvrocks/blob/f1f7c10a30dc3387f6ffd77fe3c6efe2f393ed38/src/server/server.cc#L135\r\n\r\nFor example, before the PR, the error message in this line can never be dumped into log. So users can hardly know why the server fail to start.

Merging...

Merging...

Thanks all. Merging...

I have some comments on the file format. \r\n\r\nI think it is a new file format that relies on what is in the comments (starting with `#`) as part of the parsing, which is puzzling. I think we can try not to create a new file format as well as a new parsing logic. If this file is named `*.conf` (it is weird to have two different file format in one program that both have suffix `.conf`), I think we could better refer to the previous kvrocks conf file format, i.e.\r\n\r\n```\r\n# a comment that does not affect parsing...\r\nversion 1\r\nid 0123456789012345678901234567890123456789\r\nnode 07c37dfeb235213a872192d90877d0cd55635b91 127.0.0.1 63262 master -  0-2 4-8193 10000 10002-11002 16381-16383\r\nnode ...\r\n```\r\n\r\nAnd there are some util functions in `config/config_util.h` that can help parsing text in such file format.\r\n

I have a suspicion that all the members of the `Cluster` instance are not thread-safe, e.g. simultaneous calls of `SetClusterNodes` and `GetClusterNodes` can lead to a data race. Am I right?

> I have a suspicion that all the members of the `Cluster` instance are not thread-safe, e.g. simultaneous calls of `SetClusterNodes` and `GetClusterNodes` can lead to a data race. Am I right?\r\n\r\nYes, we should reconsider making those commands exclusive.

all cluster  writing commands have `exclusive`, so they are safe now.

is there a way which allow we persist the cluster topology into some rocksdb CF?

another issue, since after rebooting, replica would load topology and try to sync with master, so when master receive the sync request from slave, master should check if the replica is in its current cluster topology, if yes, master allow, if no, it should reject.

> another issue, since after rebooting, replica would load topology and try to sync with master, so when master receive the sync request from slave, master should check if the replica is in its current cluster topology, if yes, master allow, if no, it should reject.\r\n\r\nWe have no node id or port in the replication process, so it cannot identify the replica and reject it on the master side. But we can compare the cluster version before connecting on the replica side.

> all cluster writing commands have exclusive, so they are safe now.\r\n\r\nMy bad, I forget the `cluster` and `clusterx` will be in exclusive mode even if it has no exclusive flag.

It seems CI was not triggered.  We can make an empty commit to retry.

> It seems CI was not triggered. We can make an empty commit to retry.\r\n\r\nok\r\n

Thanks all, merging...

@PragmaTwice Could you please link the motivation of adding such a feature?

> @PragmaTwice Could you please link the motivation of adding such a feature?\r\n\r\nIt is needed in #953, and the implementation in #953 is not good to me so I do not want to refactor these code after merge it.

Thanks all. Merging...

Since this option is marked as `Experimental` in the `rocksdb` codebase, I agree to make this option configurable.\r\n@xiaobiaozhao Thank you for the benchmarks. Nice job!

Good feature, any updates? In latest rocksdb update fixed some bugs in async_io support, so this PR is very good point to improve our performance. 

> Good feature, any updates? In latest rocksdb update fixed some bugs in async_io support, so this PR is very good point to improve our performance.\r\n\r\n@xiaobiaozhao has updated the PR, to see if @torwig and @PragmaTwice have comments.

I\

@xiaobiaozhao Good research.\r\nIf you update your branch, and make a final change as mentioned @git-hulk, I think we can merge this PR.

Thanks all, merging...

Hi @xiaobiaozhao , Excuse me again, one more question.\r\nIs the response and request order consistent at the same link? For example, send request 1, request 2 and expect to receive response 1, response 2 .

@jishengming1 Yes, exactly. The order of the response is always exactly the same as the request.

> Yes, exactly. The order of the response is always exactly the same as the request.\r\n\r\nThank you very much.

> \r\nExcuse me again, one more question.\r\nIn the same link, if request 1 blocks, will all subsequent requests block? Is request2 affected by request1?

Hi @jishengming1 I think we can move to the discussion to make the PR context clear.

Great changes. Since all warnings are fixed, could you modify the CMakeFiles to make the warning as error? You can just follow this TODO, and put `-Werror=unused-result` to L181.\r\n\r\nhttps://github.com/apache/incubator-kvrocks/blob/09d3d18dd9d4e2afebdda13560a8de392b0540d1/CMakeLists.txt#L180-L181

@PragmaTwice Done.

Thanks all. Merging...

Thanks all, merging...

Merging...

Closing...\r\n\r\nSee https://github.com/apache/incubator-kvrocks/pull/1206#pullrequestreview-1229819085.

Benchmark with wikipedia data: **109GB**, **38.5 M** rows, average **2.8 KB** per row.\r\n\r\nHardware: aliyun 2c16g, in random read, we also tested with 8c16g, disk is high performance ESSD(iops up to 60k).\r\n\r\nRAM is 16G to make it can not fit all (compressed) data.\r\n\r\n**community** conf [kvtopling-community.json](https://github.com/rockeet/incubator-kvrocks/blob/adapt-toplingdb/kvtopling-community.json) using BlockBasedTable(block size is 8K).\r\n**enterprise** conf [kvtopling-enterprise.json](https://github.com/rockeet/incubator-kvrocks/blob/adapt-toplingdb/kvtopling-enterprise.json) using ToplingZipTable.\r\n\r\n![image](https://github.com/apache/incubator-kvrocks/assets/1574991/2fef6caa-3a73-4eff-be27-18281843946a)\r\n\r\n![image](https://github.com/apache/incubator-kvrocks/assets/1574991/4f2a5caa-ce3d-4835-af1b-3faae3e1e40d)\r\n\r\nRandom read after full compaction, in random read, it is CPU bounded, rand 8c means 8c16g.\r\n\r\nblock cache is 1G for BlockBasedTable, we have tried block cache 4G & 8G, but performance getting worse.\r\n![image](https://github.com/apache/incubator-kvrocks/assets/1574991/3bf52672-5a4b-4e64-86fe-3eb184542762)\r\n\r\n\r\n\r\n\r\n

Thanks for your clarification.

https://github.com/wacfork/incubator-kvrocks/tree/unstable/docker-compose#readme\r\nkvrocks + rocksdb / topling 社区版 性能测试对比

> For chart 3, it is 16 threads(16 client connections) in which each thread use `mget` with 64 keys per request.\r\n\r\nhi , for chart 3,any idea why when blockcache was 4G & 8G, but performance will get worse.\r\n

> > For chart 3, it is 16 threads(16 client connections) in which each thread use `mget` with 64 keys per request.\r\n> \r\n> hi , for chart 3,any idea why when blockcache was 4G & 8G, but performance will get worse.\r\n\r\nI think it should because larger block cache evict OS page cache memory, thus page cache hit rate becomming lower, and OS page cache data is compressed, which can hold more data in same memory.

Now distributed compaction is supportted for `metadata`, which support `strings` data type and `main key` for other data types.\r\n\r\ndistributed compaction for subkey is not supportted, which needs `DB::Get`, thus can not be efficiently supportted.

Merging...

@PragmaTwice I suggest you involve those users directly here to test the implementation, since such local archive server is a customized syntax to use.

> @PragmaTwice I suggest you involve those users directly here to test the implementation, since such local archive server is a customized syntax to use.\r\n\r\nGood advice, I will follow it.

Thanks all. Merging...

Thanks all. Merging...

Thanks all. Merging...

Thanks all, merging...

Thanks all. Merging...

Thanks all. Merging...

Thanks all. Merging...

Thanks all, merging...

Thanks for your contribution. Merging...

Does CI fail because timeout is too short? @git-hulk 

I found some problems with `ScopeExit` and `SockConnect(timeout)` @git-hulk @PragmaTwice 

Thanks all, merging...

General looks good to me, except for two suggestions.

> General looks good to me, except for two suggestions.\r\n\r\nthanks

@IoCing Would you mind adding Go test cases for this configuration, to make sure it can be changed in-flight?

It seems Travis CI takes a longer time to finish. Perhaps we should restore the GitHub Actions + QEMU solution XD.\r\n\r\nMerging...

@IoCing Thank you!

Thanks all, merging...

Thanks all, merging...

@PragmaTwice @ShooterIT Does it look good to you?

Thanks all, merging...

Thanks all, merging...

Thanks all. Merging...

Thanks all. Merging...

Thanks for your contribution. Merging...

Thanks all. Merging...

`x.py` is only invoked from the project root so using relative dir in `x.py` seems to be ok.

> @PragmaTwice Another approach is that we thoroughly remove the `VERSION` file but pass the version by CMake option and default to `999.999.999` :)\r\n\r\nIt seems hard to do so in a source distribution, while users are required to specify a version number to build.

> It seems hard to do so in a source distribution, while users are required to specify a version number to build.\r\n\r\nYep..

Thanks for your contribution! Merging...

Thanks all. Merging...

> Can we add some linter/checker rules to prevent regressions?\r\n\r\nHeaders are actually hard to precisely check by clang-tidy due to some reason, maybe I will dive into them later.

Thanks all. Merging...

@PragmaTwice Changes are good to me, but we need to update the NOTICE as well.

> @PragmaTwice Changes are good to me, but we need to update the NOTICE as well.\r\n\r\nDone

Thanks all, merging...

@PragmaTwice make sense. Perhaps `ARG BUILD_PARALLELISM`. You can read https://docs.docker.com/engine/reference/builder/#arg.

I think 4 seconds is long enough since the default read/write timeout of the Redis client is 3 seconds. 

How about introduce some mutexes? like `map(key -> mutex)`. (After a cursory glance)

Merging as a net win...

I like this change

https://app.travis-ci.com/github/apache/incubator-kvrocks/jobs/588512318 Compilation failed. Please take a look @PragmaTwice .

> https://app.travis-ci.com/github/apache/incubator-kvrocks/jobs/588512318 Compilation failed. Please take a look @PragmaTwice .\r\n\r\nI notice that `cpu_count` returns `80` in travis ci, and then `make -j80` is run, causing OOM. It is weird.

> > https://app.travis-ci.com/github/apache/incubator-kvrocks/jobs/588512318 Compilation failed. Please take a look @PragmaTwice .\r\n> \r\n> I notice that `cpu_count` returns `80` in travis ci, and then `make -j80` is run, causing OOM. It is weird.\r\n\r\nMaybe the `getcpu()` retrieves the number of physical host cores, but the building process is running in the docker environment with limited resources.

> > > https://app.travis-ci.com/github/apache/incubator-kvrocks/jobs/588512318 Compilation failed. Please take a look @PragmaTwice .\r\n> > \r\n> > \r\n> > I notice that `cpu_count` returns `80` in travis ci, and then `make -j80` is run, causing OOM. It is weird.\r\n> \r\n> Maybe the `getcpu()` retrieves the number of physical host cores, but the building process is running in the docker environment with limited resources.\r\n\r\nI also suspect so. But I do not know how to retrieve the correct core number. :rofl:

> I also suspect so. But I do not know how to retrieve the correct core number. 🤣\r\n\r\nTry this? @PragmaTwice \r\nhttps://donghao.org/2022/01/20/how-to-get-the-number-of-cpu-cores-inside-a-container/\r\n

> > I also suspect so. But I do not know how to retrieve the correct core number. 🤣\r\n> \r\n> Try this? @PragmaTwice https://donghao.org/2022/01/20/how-to-get-the-number-of-cpu-cores-inside-a-container/\r\n\r\nThanks! I will try it. It would be nice if it can be cross-platform (e.g. linux/mac).

```\r\n$ cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us\r\ncat: /sys/fs/cgroup/cpu/cpu.cfs_quota_us: No such file or directory\r\n```\r\n\r\nIt is weird. It always reports "no such file" in my local host machine and docker container.\r\n\r\n

I think it is hard to find a sound & robust way to retrieve current core number with respect to cpuset. \r\n\r\n`psutil` did this, but its implementation is complex.

> I think it is hard to find a sound & robust way to retrieve current core number with respect to cpuset.\r\n> \r\n> `psutil` did this, but its implementation is complex.\r\n\r\nIt should be not worth to do that if we need to import a new module or add too much complexity. 

> ```\r\n> $ cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us\r\n> cat: /sys/fs/cgroup/cpu/cpu.cfs_quota_us: No such file or directory\r\n> ```\r\n> \r\n> It is weird. It always reports "no such file" in my local host machine and docker container.\r\n\r\nIn cgroup2, it seems to have been migrated to `cpu.max`.\r\n\r\nhttps://stackoverflow.com/questions/65551215/get-docker-cpu-memory-limit-inside-container/65554131#65554131

Thanks for your contribution!

Sorry guys, I ve removed old commented code, renamed variables, formatted code and updated .clang-tidy

Thanks for your contribution!

I will try to finish this pr as soon as possible before the big refactoring.

Thanks all. Merging...

I want to explain why kvrocks crashes when @tanruixiang is using `unique_ptr`.\r\n\r\nFirstly, he creates `ZrangeCommon` and some derived structs of it, like this:\r\n```\r\nstruct ZrangeCommon { ... };\r\nstruct ZrangeLex : ZrangeCommon { std::string x };\r\nstruct ZrangeIndex : ZrangeCommon { ... };\r\n```\r\n\r\nThen, he use `unique_ptr` to store `ZrangeCommon`:\r\n```\r\nunique_ptr<ZrangeCommon> spec_;\r\n\r\n...\r\n\r\nif (...) {\r\n  spec_ = make_unique<ZrangeLex>(...);\r\n} else if (...) {\r\n  spec_ = make_unique<ZrangeIndex>(...);\r\n}\r\n```\r\n\r\nNote: Unlike `shared_ptr`, `unique_ptr` does not do type erasure, which means that the default deleter in `unique_ptr` is determined at compile time for the destructor. In this case, destructor of `ZrangeCommon` will be called when `spec_` is destructed. Note that the actual object type at this point may be `ZRangeLex`, and calling the wrong destructor may not only lead to resource leaks, but also to other catastrophic consequences due to UB.\r\n\r\nThe design of `shared_ptr` is different in that `shared_ptr` does type erasure and its deleter is stored in runtime, which makes it store the exact function pointer of the destructor at construction (which, of course, adds runtime overhead), so there is no such problem.\r\n\r\nSo a better solution is to use a virtual destructor: `shared_ptr` is not needed here, because `shared_ptr` has extra overhead and other usefulness:\r\n\r\n```\r\nstruct ZrangeCommon {\r\n  ...\r\n  virtual ~ZrangeCommon() {}\r\n};\r\n```\r\n\r\n

@PragmaTwice Thanks for the analysis. I have another question, why clang can help us to solve this problem.

> @PragmaTwice Thanks for the analysis. I have another question, why clang can help us to solve this problem.\r\n\r\nCalling wrong dtor is an UB, and UB can cause anything.\r\n\r\nMay crash, may not. Also may spent $1 from your digital wallet : )

I will try to finish this pr as soon as possible before the big refactoring.

Closing as conflicts...\r\n\r\nAny progress requires rework now.

Thank you very much for your contribution. Maybe you also need to add the go test, you can refer to `incubator-kvrocks/tests/gocase/unit/type/set/set_test.go`

Everybody, Thank you for all your suggestions, I will recheck related codes.

@tisonkun Resolved. However, this feature is not ready to be merged.

Hi @boatrainlsz , do you still have time to continue this PR?

Close this PR due to out-of-date and no response.

Thanks all. Merging...

Thanks all. Merging...

Thanks for your contribution!

I think we can still use range-based for loop and remove these comment to continue with this PR if it is confirmed that these comments are out-of-date and not correct now. \r\ncc @git-hulk 

Thanks for your contribution!

Thanks for your contribution!

Thanks for your contribution!

GitHub Actions was not being triggered, which is weird. I will try to reopen this PR.

Thanks. Merging...

Thanks for your contribution!

Thanks! Merging...

> BTW, if you configure your editor/IDE correctly (vim/vscode/emacs with clangd or clion), you should see a `fix` button in the pop-up window in every code segment where a report exists here. Some of them may also provide a button to fix all reports in the current source file.\r\n\r\nYes, vscode just need to download `Clang-Tidy` and take effect without any configuration.\r\n\r\n

Thanks all. Merging...

Thanks all. Merging...

@PragmaTwice Why we should support ZLIB, IIRC, ZLIB has poor performance, we  already have there compression algorithms, snappy is general algorithm from leveldb period, not bad in everything, also is default compression in kvrocks. lz4 has powerful performance, and zstd has good compression ratio and not bad performance. So i think current compression algorithms are enough.

> @PragmaTwice Why we should support ZLIB, IIRC, ZLIB has poor performance, we already have there compression algorithms, snappy is general algorithm from leveldb period, not bad in everything, also is default compression in kvrocks. lz4 has powerful performance, and zstd has good compression ratio and not bad performance. So i think current compression algorithms are enough.\r\n\r\nI think there is no cost to support zlib since rocksdb already supports zlib. And if we really need high performance, we can just replace zlib with zlib-ng, which is several to ten times faster than zlib and compatible with APIs of zlib.

> Looks good to me.\r\n> \r\n> I just wonder if there is any check whose reports in kvrocks are all fixed. And then we can put them in `.clang-tidy`, according to #1029.\r\n\r\nI am fixing them in file order. I will try to see which checks have been fully fixed when I have time.

> > Looks good to me.\r\n> > I just wonder if there is any check whose reports in kvrocks are all fixed. And then we can put them in `.clang-tidy`, according to #1029.\r\n> \r\n> I am fixing them in file order. I will try to see which checks have been fully fixed when I have time.\r\n\r\nGot it. Thanks.

Thanks all. Merging...

@xiaobiaozhao Looks like you wrongly merged the previous commits and they appeared as new code. Please fix them first.

> @xiaobiaozhao Looks like you wrongly merged the previous commits and they appeared as new code. Please fix them first.\r\n\r\ndone

@git-hulk @tisonkun Merge?

does this change damage performance?\r\n\r\nmaybe we can add a new section which include these statistics, only show these when calling `info all` or `info $new_section`

> I don\

i found we already have "stats" command, it can report lots of rocksdb info.

\r\n\r\n\r\n> i found we already have "stats" command, it can report lots of rocksdb info.\r\n\r\n@git-hulk What do you think?

Yes, can move them into the stat command

Hi @xiaobiaozhao did you see `stats` command output? it reports lots of info, is it enough?

I think `stats` is enough

@PragmaTwice @git-hulk \r\n```\r\nI20221103 15:03:19.033007 40160 replication.cc:903] LogData(3 6)Put(\x0b__namespace\r\n\x037Ok/d�\x0fAA?\x02\x7f�������, 4373)PutCF(1, \x0b__namespace\r\n7Ok, \x03/d�\x0fAA?\x02\x11\x16\x7f�������\x7f�������)\r\nI20221103 15:03:19.033021 40160 replication.cc:903] LogData(3 6)Put(\x0b__namespace\r\n\x037Ok/d�\x0fAA?\x02\x7f�������, 4374)PutCF(1, \x0b__namespace\r\n7Ok, \x03/d�\x0fAA?\x02\x11\x17\x7f�������\x7f�������)\r\nI20221103 15:03:19.033035 40160 replication.cc:903] LogData(3 6)Put(\x0b__namespace\r\n\x037Ok/d�\x0fAA?\x02\x7f�������, 4375)PutCF(1, \x0b__namespace\r\n7Ok, \x03/d�\x0fAA?\x02\x11\x18\x7f�������\x7f�������)\r\nI20221103 15:03:19.033063 40160 replication.cc:903] LogData(3 6)Put(\x0b__namespace\r\n\x037Ok/d�\x0fAA?\x02\x7f�������, 4376)PutCF(1, \x0b__namespace\r\n7Ok, \x03/d�\x0fAA?\x02\x11\x19\x7f�������\x7f�������)\r\nI20221103 15:03:19.033077 40160 replication.cc:903] LogData(3 6)Put(\x0b__namespace\r\n\x037Ok/d�\x0fAA?\x02\x7f�������, 4377)PutCF(1, \x0b__namespace\r\n7Ok, \x03/d�\x0fAA?\x02\x11\x1a\x7f�������\x7f�������)\r\n```\r\n\r\nAfter inpecting hook, I think `WriteBatch::Handler` may works well, because program still coredump on `InternalKey`

cool, the batch looks correct, it may be caused by the uninitialized value. Can have a look at #1068 

@mapleFU I think we can keep this PR, coz it can make the batch debug easier.

@git-hulk update and making it as a tool

> But we can improve the copyright info layout and add an original link here. \r\n\r\n@tisonkun updated, PTAL

Merging...

Many thanks for the great core file from #1065, also thanks to folks who spent time to locate this issue. cc @PragmaTwice @mapleFU @torwig @tanruixiang

LGTM, but why does these unit tests fails in these few days? Maybe we still need to bisect the changes?

@torwig I got your point, will append a commit to enhance.

@git-hulk You are right, the logic was added 3 months ago, but issues with uninitialized variables or data races could be "hidden/invisible" for a long time. At least, from my experience, data races are very hard to reproduce :)

Thanks all, the CI failure disappeared after applying this patch, will merge it first to avoid blocking other PRs.

@PragmaTwice Good catch! I needed a few minutes to see the dangling reference in the code.

It seems it does not solve the frequent crashing in CI. Merging...

> Please update the PR description.\r\n\r\nDone. Thanks.

Merging...

@ShooterIT Very excite to see you back again. 😆 ❤️ 

@tisonkun Good observation.\r\n@PragmaTwice Thank you for your hints.\r\nDone.

Also reformatted `x.py` in a separate commit.

@torwig may I ask what tools you use to format the python code?

@tisonkun It was just `Pycharm` (IDE) :)

Thanks all. Merging...

@git-hulk @PragmaTwice No idea why CI failed...

> @git-hulk @PragmaTwice No idea why CI failed...\r\n\r\n@mapleFU Yes, we found a frequently crash test case and it is no related to current PR even it would also block the merge. @PragmaTwice has uploaded the coredump file to artifacts in [PR](https://github.com/apache/incubator-kvrocks/pulls/1065), still investigating the reason now.

Thanks @mapleFU, merging..

@manchurio Thanks for your report and PR, I think the root cause is using before the glog dir was specified in config::Load, so it will send logs to the default dir(/tmp), so this fix is not exactly correct.

> @manchurio Thanks for your report and PR, I think the root cause is using before the glog dir was specified in config::Load, so it will send logs to the default dir(/tmp), so this fix is not exactly correct.\r\n\r\n@git-hulk Thanks for the explanation, do you have any suggestion?

We do not need to log the first assignment of backup dir:\r\n```diff\r\n--- if (!previous_backup.empty()) { \r\n+++ if (srv && !previous_backup.empty()) { \r\n```

@manchurio #1061 has fixed this bug and will close this PR, feel free to reopen it if need.

Thanks all. Merging...

Thanks all. Merging...

I suggest putting the test results and conclusions here or in the issue.

> I suggest putting the test results and conclusions here or in the issue.\r\n\r\nHere is a simple test report\r\n\r\nhttps://github.com/apache/incubator-kvrocks/discussions/1013#discussioncomment-4005680

I think we can push forward to upgrading since the RocksDB community has confirmed the bad performance when compacting in 6.x and fixed in 7.5.3. Related issue: https://github.com/facebook/rocksdb/issues/9423

Merged. Thank you!

Thanks all. Merging...

> Any description (what is to be implemented) of this change? It seems miscellaneous in one patch.\r\n\r\nAdded.

Thanks all. Merging...

Thanks all. Merging...

Thanks @torwig @PragmaTwice, merging...

https://github.com/apache/incubator-kvrocks/blob/debd8988d6f6978593ad35b7e06d0c3e773ee825/src/config/config.cc#L767-L771\r\n\r\nSince **ALL `MultiStringField` cannot be rewritten**, so I think you need to change it to something like\r\n```\r\nif (iter->second.IsMulti())\r\n```\r\nin case of someone use `MultiStringField` but forget to include the config key in this check.

Thanks @tanruixiang @PragmaTwice, merging...

Thanks all, merging...

Thanks all, merging..

Thanks all, merging...

Hi, `clock_gettime` is a posix api. maybe `std::steady_clock` is better here.

> Hi, `clock_gettime` is a posix api. maybe `std::steady_clock` is better here.\r\n\r\n@mapleFU Yes, we are using C++ `std::chrono::system_clock` API instead of `clock_gettime` directly, can see `Util::GetTimeStamp*`.

> > Hi, `clock_gettime` is a posix api. maybe `std::steady_clock` is better here.\r\n> \r\n> @mapleFU Yes, we are using C++ `std::chrono::system_clock` API instead of `clock_gettime` directly, can see `Util::GetTimeStamp*`.\r\n\r\nMaybe it is [steady_clock](https://en.cppreference.com/w/cpp/chrono/steady_clock), which is a monotonic clock rather than [system_clock](https://en.cppreference.com/w/cpp/chrono/system_clock) (a system-wide real time wall clock) in c++11 and later.

@mapleFU But I have a question. I notice that unlike `system_clock`, standard of `steady_clock` does not specify that it measures Unix Time.

> But I have a question. I notice that unlike `system_clock`, standard of `steady_clock` does not specify that it measures Unix Time.\r\n\r\n`time_point` can call `time_since_epoch`. Like the case here: https://github.com/facebook/rocksdb/blob/main/env/env_posix.cc#L142-L162

> > But I have a question. I notice that unlike `system_clock`, standard of `steady_clock` does not specify that it measures Unix Time.\r\n> \r\n> `time_point` can call `time_since_epoch`. Like the case here: https://github.com/facebook/rocksdb/blob/main/env/env_posix.cc#L142-L162\r\n\r\n`time_since_epoch` DOES NOT imply the Unix epoch.

But another problem is that, even in c++20, the `system_clock` is guaranteed to use Unix Time, but it seems `steady_clock` does not have this guarantee.

Thanks all. Merging...

wow, so speedy.

Thanks all. Merging...

@PragmaTwice After looking through the PR, I was a bit worry that it needs to take some time for most developers(include myself) to understand how to use it and how it works. And for Redis command arguments, there\

> @PragmaTwice After looking through the PR, I was a bit worry that it needs to take some time for most developers(include myself) to understand how to use it and how it works. And for Redis command arguments, there\

@PragmaTwice Thanks for your explanation.\r\n\r\n> We cannot always move next: for example, to parse (EX v1) | (PX v2) | v3, we need first peek the token (EX or PX), then we can move next, otherwise we may lose v3. For a parser, moving next at every step will severely damage its parsing ability.\r\n\r\nYes, I got your point. What if we use the parser to iterator all tokens instead of only flags. I will take `ZADD` command as example:\r\n\r\n```C++\r\nwhile(token = parser.next()) {\r\n  case "NX":\r\n    _flags = nx;\r\n  case "INCR":\r\n    _flags = incr;\r\n  default:\r\n    break;\r\n}\r\nwhile(parse.has_next()) {\r\n   status = parser.expected<double>()\r\n   parse.next()\r\n   status = parser.expected<string>()\r\n}\r\n```\r\n\r\n> We need a method to forward error: this is where the sample code is idealized, error handling needs to be abstracted\r\n\r\nYes, it\

@git-hulk There is an [example in unit tests](https://github.com/apache/incubator-kvrocks/blob/85ae20ddff43bb71e8370b7a2b19c51c746b6871/tests/cppunit/command_parser_test.cc#L26) which parses some command in the syntax `[ HELLO i1 v1 | HI v2 ] [X i2 | Y]` (where `i1` `i2` are integers and `v1` `v2` are strings), and I think it demonstrate how to use the CommandParser well. I think from this example, the interface provided by the current framework can be quickly understood.

Hi everyone, any new thoughts on this PR?

Thanks all. Merging...

Hi @mapleFU, after a glance, I have two simple ideas:\r\n1. Could we add a new seperate mutex for directory changing? e.g. `backup_mu` and `backup_dir_mu`.\r\n2. If we use only one mutex `backup_mu`, could we use `try_lock` on the directory changing? e.g.\r\n```\r\nif(backup_mu.try_lock()) exchange ...\r\nelse report("backuping, cannot change...")\r\n```

> 1. Could we add a new seperate mutex for directory changing? e.g. `backup_mu` and `backup_dir_mu`.\r\n\r\nIt\

@git-hulk @PragmaTwice @tanruixiang update some comments, and the patch is ready to be merged now.

Thanks all, merging...

May I ask do the comments here also need to be modified?\r\nhttps://github.com/apache/incubator-kvrocks/blob/unstable/src/common/event_util.h#L47\r\n```\r\nstruct UniqueEvbufReadln : UniqueFreePtr<char[]> {\r\n  // cppcheck-suppress uninitMemberVar\r\n  UniqueEvbufReadln(evbuffer *buffer, evbuffer_eol_style eol_style)\r\n      : UniqueFreePtr(evbuffer_readln(buffer, &length, eol_style)) {}\r\n\r\n  size_t length;\r\n};\r\n```

> May I ask do the comments here also need to be modified? https://github.com/apache/incubator-kvrocks/blob/unstable/src/common/event_util.h#L47\r\n> \r\n> ```\r\n> struct UniqueEvbufReadln : UniqueFreePtr<char[]> {\r\n>   // cppcheck-suppress uninitMemberVar\r\n>   UniqueEvbufReadln(evbuffer *buffer, evbuffer_eol_style eol_style)\r\n>       : UniqueFreePtr(evbuffer_readln(buffer, &length, eol_style)) {}\r\n> \r\n>   size_t length;\r\n> };\r\n> ```\r\n\r\nSure, we can remove the suppression.

Let me create an issue for it.

You can just run `./x.py format` to format the source code rather than manual formatting.

Oh, I notice the cppcheck false alarm. I think you can change `--std=c++11` to `17` here: https://github.com/apache/incubator-kvrocks/blob/unstable/x.py#L162\r\n\r\n@manchurio 

> You can just run `./x.py format` to format the source code rather than manual formatting.\r\n\r\ngot it,thanks.

> Oh, I notice the cppcheck false alarm. I think you can change `--std=c++11` to `17` here: https://github.com/apache/incubator-kvrocks/blob/unstable/x.py#L162\r\n> \r\n> @manchurio\r\n```\r\nsrc/commands/redis_cmd.cc:2432:19: warning: Suspicious condition. The result of find() is an iterator, but it is not properly checked. [stlIfFind]\r\n    if (auto it = options.find(option); it != options.end()) {\r\n                  ^\r\n```\r\nIt seems cppcheck  wrongly reports @PragmaTwice 

It is sad that cppcheck still fail and report a FALSE ALARM.\r\n\r\nWe will replace cppcheck by clang-tidy later, and I think you can now workaround it by `auto iter = ...; if(iter != end()) ...`\r\n\r\ncc @git-hulk 

@manchurio Need to use gofmt to format the test case

> If you only need to determine whether the key exists, you can use count directly, the iterator is redundant. Also doing this will not trigger an error from cppcheck. (By the way, C++20 has [contains](https://en.cppreference.com/w/cpp/container/unordered_map/contains) to do this thing. )\r\n\r\nHi @tanruixiang, \r\n\r\nActually I like the original code more, since the iterator is used in the true-branch block to avoid a double search (the upper bound is actually not O(1) due to collision, and even O(1) DOES NOT imples no cost). \r\n\r\nAnd I think this change does not improve readability, as the use of iterators is natural and easy to understand in c++. The false positives of cppcheck are entirely due to his own problems, and we should not modify the code logic just for that.\r\n\r\nAnd in my opinion, I prefer just a comment rather than "request for changes" for just a trivial and purely code-smell review message, in order to be nice to new contributors.

> And in my opinion, I prefer just a comment rather than "request for changes" for just a trivial and purely code-smell review message, in order to be nice to new contributors.\r\n\r\nGot it, thanks a lot for your suggestion. It should be that I misunderstood this "request for changes". I originally thought that only use "request for changes"  can be clicked on github to modify. (Because if someone else suggests a little modification, I prefer to click on github to complete the modification🤣)

> > And in my opinion, I prefer just a comment rather than "request for changes" for just a trivial and purely code-smell review message, in order to be nice to new contributors.\r\n> \r\n> Got it, thanks a lot for your suggestion. It should be that I misunderstood this "request for changes". I originally thought that only use "request for changes" can be clicked on github to modify. (Because if someone else suggests a little modification, I prefer to click on github to complete the modification🤣)\r\n\r\nIn the github review page, you can select "comment", "request for changes" or "approve". I think the first option is relatively "milder".

@manchurio By the way, where are the unit tests? :)

The file permission of `x.py` has changed from 755 to 644, which is weird. Could you revert it?

> @manchurio By the way, where are the unit tests? :)\r\n\r\nThe unit tests of zadd options are  in zset_test.go

> > @manchurio By the way, where are the unit tests? :)\r\n> \r\n> The unit tests of zadd options are in zset_test.go\r\n\r\nHi. What he meant maybe the `tests/cppunit/t_zset_test.cc`.

@manchurio @tanruixiang  Yes, I meant the unit tests that are written in C++. Related to the sorted set unit tests are located in `tests/cppunit/t_zset_test.cc` and test only one component - `class ZSet`.\r\nOn the other hand, tests in Go/TCL test the entire `kvrocks` (storage + protocol + network).

> The file permission of `x.py` has changed from 755 to 644, which is weird. Could you revert it?\r\n\r\nThe file permission of `x.py` has changed from 644 to 755 now. @PragmaTwice  @tisonkun 

@manchurio You can also consider using something like the following to encapsulate the logic with options and eliminate code duplication:\r\n\r\n```\r\nstruct ZAddOptions {\r\n  ZAddOptions() {}\r\n  explicit ZAddOptions(uint8_t flags) : flags(flags) {}\r\n\r\n  bool IsXX() const { return (flags & kZSetNX) != 0; }\r\n  // other methods to ask if the specific flag was set\r\n\r\n  uint8_t flags = 0;\r\n};\r\n```\r\n\r\nOf course, you can add methods like `SetXX` to encapsulate set operations as well (to not spread bit operations over the codebase).

> @manchurio You can also consider using something like the following to encapsulate the logic with options and eliminate code duplication:\r\n> \r\n> ```\r\n> struct ZAddOptions {\r\n>   ZAddOptions() {}\r\n>   explicit ZAddOptions(uint8_t flags) : flags(flags) {}\r\n> \r\n>   bool IsXX() const { return (flags & kZSetNX) != 0; }\r\n>   // other methods to ask if the specific flag was set\r\n> \r\n>   uint8_t flags = 0;\r\n> };\r\n> ```\r\n> \r\n> Of course, you can add methods like `SetXX` to encapsulate set operations as well (to not spread bit operations over the codebase).\r\n\r\n\r\nThis refactoring has been completed

Sorry, touched wrong button. :rofl:\r\nThe new branch generated by the revert button has been deleted.

```\r\n--- FAIL: TestBitmap (403.30s)\r\n    --- FAIL: TestBitmap/SETBIT/GETBIT/BITCOUNT/BITPOS_boundary_check_(type_bitmap) (15.33s)\r\n        bitmap_test.go:146: \r\n            \tError Trace:\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/unit/type/bitmap/bitmap_test.go:146\r\n            \tError:      \tNot equal: \r\n            \t            \texpected: int(1)\r\n            \t            \tactual  : int64(0)\r\n            \tTest:       \tTestBitmap/SETBIT/GETBIT/BITCOUNT/BITPOS_boundary_check_(type_bitmap)\r\n```\r\n\r\n@PragmaTwice @git-hulk I don\

> ow why it keeps failing...Cannot reproduce locally and should not be related to this patch (although I try to "fix" it, it fail\r\n\r\nYes, not related to current PR, I\

@git-hulk No. Golang only runs tests in parallel if you write `t.Parallel()`.

> ```\r\n> --- FAIL: TestBitmap (403.30s)\r\n>     --- FAIL: TestBitmap/SETBIT/GETBIT/BITCOUNT/BITPOS_boundary_check_(type_bitmap) (15.33s)\r\n>         bitmap_test.go:146: \r\n>             \tError Trace:\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/unit/type/bitmap/bitmap_test.go:146\r\n>             \tError:      \tNot equal: \r\n>             \t            \texpected: int(1)\r\n>             \t            \tactual  : int64(0)\r\n>             \tTest:       \tTestBitmap/SETBIT/GETBIT/BITCOUNT/BITPOS_boundary_check_(type_bitmap)\r\n> ```\r\n> \r\n> @PragmaTwice @git-hulk I don\

>@vmihailenco do you have any ideas here? \r\n\r\nPerhaps try `maxOffset/8 + 1`?

After I add back `-bench=.` in `x.py`, tests passed. Really weird >_<

Thank you!

@tanruixiang ok , I get it ,thank you

> Thanks for your contribution! LGTM. Waiting for CI reports.\r\n\r\n@tisonkun @tanruixiang  Thanks for your help！！

```\r\n[TIMEOUT]: clients state report follows.\r\nsock5637433bc850 => (IN PROGRESS) BLPOP with same key multiple times should work (issue #801)\r\nKilling still running Redis server 43573\r\n\r\n                   The End\r\n\r\nExecution time of different units:\r\n\r\n!!! WARNING The following tests failed:\r\n\r\n*** [TIMEOUT]: clients state report follows.\r\n$ /usr/bin/tclsh tests/test_helper.tcl --server-path /home/runner/work/incubator-kvrocks/incubator-kvrocks/build/kvrocks --tls --single unit/tls --dont-clean\r\nTraceback (most recent call last):\r\n  File "/home/runner/work/incubator-kvrocks/incubator-kvrocks/./x.py", line 397, in <module>\r\n    args.func(**arg_dict)\r\n  File "/home/runner/work/incubator-kvrocks/incubator-kvrocks/./x.py", line 265, in test_tcl\r\n    run(tclsh, \

It seems like something wrong with `test_helper`.

I compiled successfully on Macos! Thank you very much for your contribution.❤️

Can I upgrade rocksdb to 7.x at the same time?

Thanks all. Merging...

@git-hulk migrated two more test cases.

Closed as partially completed.

Please reformat code :)

> Please reformat code :)\r\n\r\nDone, I saw the lint error

```\r\n--- FAIL: TestReplicationWithLimitSpeed (10.10s)\r\n    client.go:72: \r\n        \tError Trace:\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/integration/replication/client.go:72\r\n        \t            \t\t\t\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/integration/replication/replication_test.go:209\r\n        \tError:      \tReceived unexpected error:\r\n        \t            \ti/o timeout\r\n        \tTest:       \tTestReplicationWithLimitSpeed\r\n```\r\n\r\nUnstable test. @git-hulk perhaps release the read timeout for the specific case.

@tisonkun Thanks for your review, it has been revised.

@ShooterIT @caipengbo Can have a look the bug which mentioned in #996 

@git-hulk I have no problem with the #996 bug fix. However, there are two things in this PR, I think the title should be modified or split into two PR.

Thank you! @tanruixiang 

please assign to me\r\n

@xiaobiaozhao and after discussing, the solution was not to modify the version string itself but add one more with the redis_version pointing to the supported protocol which is 4.0

> and after discussing, the solution was not to modify the version string itself but add one more with the redis_version pointing to the supported protocol which is 4.0\r\n\r\nOK，Please continue to deal with this PR

> `log_collector` is not a network thing, and i think it should be put under `common`\r\n\r\nI think both `common` and `network` is ok, I put it into `network` because I find that this file relies on `redis_reply` and is using redis protocol. I will move it to `common`.

@PragmaTwice  log_collector is not a network-related thing, it is a statistical thing, how about adding a `stats` directory with `log_collector.h` and `stats.h` or even `redis_disk.h` (or can rename it `disk_stats.h`?)

> @PragmaTwice log_collector is not a network-related thing, it is a statistical thing, how about adding a `stats` directory with `log_collector.h` and `stats.h` or even `redis_disk.h` (or can rename it `disk_stats.h`?)\r\n\r\nGreate idea. I will do it.

> log_collector is not a network-related thing, it is a statistical thing, how about adding a stats directory with log_collector.h and stats.h or even redis_disk.h (or can rename it disk_stats.h?)\r\n\r\nWe renamed `redis_disk.h` to `disk_stats.h`, do you think that is reasonable? @tanruixiang 

Current source structure:\r\n\r\n```\r\n$ tree src\r\n├── cluster\r\n│   ├── cluster.cc\r\n│   ├── cluster.h\r\n│   ├── redis_slot.cc\r\n│   ├── redis_slot.h\r\n│   ├── replication.cc\r\n│   ├── replication.h\r\n│   ├── slot_import.cc\r\n│   ├── slot_import.h\r\n│   ├── slot_migrate.cc\r\n│   └── slot_migrate.h\r\n├── commands\r\n│   ├── redis_cmd.cc\r\n│   └── redis_cmd.h\r\n├── common\r\n│   ├── cron.cc\r\n│   ├── cron.h\r\n│   ├── db_util.h\r\n│   ├── encoding.cc\r\n│   ├── encoding.h\r\n│   ├── event_util.h\r\n│   ├── fd_util.h\r\n│   ├── parse_util.h\r\n│   ├── rand.cc\r\n│   ├── rand.h\r\n│   ├── rocksdb_crc32c.h\r\n│   ├── rw_lock.h\r\n│   ├── scope_exit.h\r\n│   ├── sha1.cc\r\n│   ├── sha1.h\r\n│   ├── status.h\r\n│   ├── task_runner.cc\r\n│   ├── task_runner.h\r\n│   ├── util.cc\r\n│   └── util.h\r\n├── config\r\n│   ├── config.cc\r\n│   ├── config.h\r\n│   ├── config_type.h\r\n│   ├── config_util.cc\r\n│   └── config_util.h\r\n├── main.cc\r\n├── network\r\n│   ├── redis_connection.cc\r\n│   ├── redis_connection.h\r\n│   ├── redis_reply.cc\r\n│   ├── redis_reply.h\r\n│   ├── redis_request.cc\r\n│   ├── redis_request.h\r\n│   ├── server.cc\r\n│   ├── server.h\r\n│   ├── tls_util.cc\r\n│   ├── tls_util.h\r\n│   ├── worker.cc\r\n│   └── worker.h\r\n├── stats\r\n│   ├── disk_stats.cc\r\n│   ├── disk_stats.h\r\n│   ├── log_collector.cc\r\n│   ├── log_collector.h\r\n│   ├── stats.cc\r\n│   └── stats.h\r\n├── storage\r\n│   ├── batch_extractor.cc\r\n│   ├── batch_extractor.h\r\n│   ├── compact_filter.cc\r\n│   ├── compact_filter.h\r\n│   ├── compaction_checker.cc\r\n│   ├── compaction_checker.h\r\n│   ├── event_listener.cc\r\n│   ├── event_listener.h\r\n│   ├── lock_manager.cc\r\n│   ├── lock_manager.h\r\n│   ├── redis_db.cc\r\n│   ├── redis_db.h\r\n│   ├── redis_metadata.cc\r\n│   ├── redis_metadata.h\r\n│   ├── redis_pubsub.cc\r\n│   ├── redis_pubsub.h\r\n│   ├── scripting.cc\r\n│   ├── scripting.h\r\n│   ├── storage.cc\r\n│   ├── storage.h\r\n│   ├── table_properties_collector.cc\r\n│   └── table_properties_collector.h\r\n├── types\r\n│   ├── geohash.cc\r\n│   ├── geohash.h\r\n│   ├── redis_bitmap.cc\r\n│   ├── redis_bitmap.h\r\n│   ├── redis_bitmap_string.cc\r\n│   ├── redis_bitmap_string.h\r\n│   ├── redis_geo.cc\r\n│   ├── redis_geo.h\r\n│   ├── redis_hash.cc\r\n│   ├── redis_hash.h\r\n│   ├── redis_list.cc\r\n│   ├── redis_list.h\r\n│   ├── redis_set.cc\r\n│   ├── redis_set.h\r\n│   ├── redis_sortedint.cc\r\n│   ├── redis_sortedint.h\r\n│   ├── redis_stream_base.cc\r\n│   ├── redis_stream_base.h\r\n│   ├── redis_stream.cc\r\n│   ├── redis_stream.h\r\n│   ├── redis_string.cc\r\n│   ├── redis_string.h\r\n│   ├── redis_zset.cc\r\n│   └── redis_zset.h\r\n├── valgrind.sup\r\n└── version.h.in\r\n\r\n8 directories, 104 files\r\n```

> The current diff in github review page seems to be a mess, maybe because I rebase some commits.\r\n\r\nSolved by re-rebase and force-push.\r\n\r\nCurrent source structure:\r\n```\r\nsrc\r\n├── cluster\r\n│   ├── cluster.cc\r\n│   ├── cluster.h\r\n│   ├── redis_slot.cc\r\n│   ├── redis_slot.h\r\n│   ├── replication.cc\r\n│   ├── replication.h\r\n│   ├── slot_import.cc\r\n│   ├── slot_import.h\r\n│   ├── slot_migrate.cc\r\n│   └── slot_migrate.h\r\n├── commands\r\n│   ├── redis_cmd.cc\r\n│   └── redis_cmd.h\r\n├── common\r\n│   ├── cron.cc\r\n│   ├── cron.h\r\n│   ├── db_util.h\r\n│   ├── encoding.cc\r\n│   ├── encoding.h\r\n│   ├── event_util.h\r\n│   ├── fd_util.h\r\n│   ├── parse_util.h\r\n│   ├── rand.cc\r\n│   ├── rand.h\r\n│   ├── rocksdb_crc32c.h\r\n│   ├── rw_lock.h\r\n│   ├── scope_exit.h\r\n│   ├── sha1.cc\r\n│   ├── sha1.h\r\n│   ├── status.h\r\n│   ├── task_runner.cc\r\n│   ├── task_runner.h\r\n│   ├── util.cc\r\n│   └── util.h\r\n├── config\r\n│   ├── config.cc\r\n│   ├── config.h\r\n│   ├── config_type.h\r\n│   ├── config_util.cc\r\n│   └── config_util.h\r\n├── main.cc\r\n├── server\r\n│   ├── redis_connection.cc\r\n│   ├── redis_connection.h\r\n│   ├── redis_reply.cc\r\n│   ├── redis_reply.h\r\n│   ├── redis_request.cc\r\n│   ├── redis_request.h\r\n│   ├── server.cc\r\n│   ├── server.h\r\n│   ├── tls_util.cc\r\n│   ├── tls_util.h\r\n│   ├── worker.cc\r\n│   └── worker.h\r\n├── stats\r\n│   ├── disk_stats.cc\r\n│   ├── disk_stats.h\r\n│   ├── log_collector.cc\r\n│   ├── log_collector.h\r\n│   ├── stats.cc\r\n│   └── stats.h\r\n├── storage\r\n│   ├── batch_extractor.cc\r\n│   ├── batch_extractor.h\r\n│   ├── compact_filter.cc\r\n│   ├── compact_filter.h\r\n│   ├── compaction_checker.cc\r\n│   ├── compaction_checker.h\r\n│   ├── event_listener.cc\r\n│   ├── event_listener.h\r\n│   ├── lock_manager.cc\r\n│   ├── lock_manager.h\r\n│   ├── redis_db.cc\r\n│   ├── redis_db.h\r\n│   ├── redis_metadata.cc\r\n│   ├── redis_metadata.h\r\n│   ├── redis_pubsub.cc\r\n│   ├── redis_pubsub.h\r\n│   ├── scripting.cc\r\n│   ├── scripting.h\r\n│   ├── storage.cc\r\n│   ├── storage.h\r\n│   ├── table_properties_collector.cc\r\n│   └── table_properties_collector.h\r\n├── types\r\n│   ├── geohash.cc\r\n│   ├── geohash.h\r\n│   ├── redis_bitmap.cc\r\n│   ├── redis_bitmap.h\r\n│   ├── redis_bitmap_string.cc\r\n│   ├── redis_bitmap_string.h\r\n│   ├── redis_geo.cc\r\n│   ├── redis_geo.h\r\n│   ├── redis_hash.cc\r\n│   ├── redis_hash.h\r\n│   ├── redis_list.cc\r\n│   ├── redis_list.h\r\n│   ├── redis_set.cc\r\n│   ├── redis_set.h\r\n│   ├── redis_sortedint.cc\r\n│   ├── redis_sortedint.h\r\n│   ├── redis_stream_base.cc\r\n│   ├── redis_stream_base.h\r\n│   ├── redis_stream.cc\r\n│   ├── redis_stream.h\r\n│   ├── redis_string.cc\r\n│   ├── redis_string.h\r\n│   ├── redis_zset.cc\r\n│   └── redis_zset.h\r\n├── valgrind.sup\r\n└── version.h.in\r\n\r\n8 directories, 104 files\r\n```

@PragmaTwice please rebase on the latest unstable branch.\r\n\r\n@ShooterIT @caipengbo could you give a review on this patch? Perhaps we should move it forward in a timely manner to avoid further conflicts.

> @PragmaTwice please rebase on the latest unstable branch.\r\n\r\nDone.

@tisonkun Thanks for your help , I will pay attention to this kind of problems !

@IoCing already you did a good job!

Hi. Is it better to use `constexpr` here too?\r\nhttps://github.com/apache/incubator-kvrocks/blob/unstable/src/redis_cmd.cc#L63

> Hi. Is it better to use `constexpr` here too? https://github.com/apache/incubator-kvrocks/blob/unstable/src/redis_cmd.cc#L63\r\n\r\nSure. I think at least it should be typed `const char * const` (if no `constexpr`), since the top-level `const` qualifier matters for constants (in linkage and storage durations).

Thanks all. Merging...

@PragmaTwice Good job! 

@tisonkun Perhaps I\

> LGTM. Although, as we write more and more code, it can be a good time to try modularizing source files into folders.\r\n\r\nGood idea, I will try to do this.

> I think you should update the version constant on `x.py` correspondingly:\r\n> \r\n> https://github.com/apache/incubator-kvrocks/blob/unstable/x.py#L31\r\n\r\nOh thanks.

Just for record: https://github.com/apache/incubator-kvrocks/actions/runs/3219906408/jobs/5265964053\r\n```\r\n    --- FAIL: TestExpire/PEXPIRE/PSETEX/PEXPIREAT_can_set_sub-second_expires (20.86s)\r\n```

@PragmaTwice Thank you! I think expire tests heavily depend on time relativity and sleep. We may spend some time to make it stable.

@PragmaTwice Complete the refactoring according to your suggestion.

Thanks all, merging...

Can review commits one by one, this PR also solves some typos and naming issue.

Thanks all, merging...

Thank you! Merging...

Something wrong in CI-mode, when I try to start default kvrocks server  from x.py

@torwig Could you please check my dirty Python and clean-up in? As I think we maybe can add all options from original redis-benchmark into options params to bench stage? 

@aleksraiden I suggest you take a look at how `./x.py test go` implement. From my perspective, these two commands can be very similar.

cc @git-hulk @ShooterIT @ChrisZMF please help with reviewing.

@git-hulk @PragmaTwice @torwig @caipengbo Do you have any better ideas? After discussing a suitable method, I will follow up the replacement work.

I think the function generally looks fine, but it seems we now need a well-designed parsing framework rather than some non-generic util functions now.\r\n\r\nI am making some attempts at this, although I may not be able to devote much time to it these days.\r\n\r\n

> I think the function generally looks fine, but it seems we now need a well-designed parsing framework rather than some non-generic util functions now.\r\n> \r\n> I am making some attempts at this, although I may not be able to devote particularly much time to it these days.\r\n\r\nI agree. If we need to implement a very generalized function, a single util function is not enough. \r\n\r\nMaybe you can split your work into some issues for everyone to implement together?

> > I think the function generally looks fine, but it seems we now need a well-designed parsing framework rather than some non-generic util functions now.\r\n> > I am making some attempts at this, although I may not be able to devote particularly much time to it these days.\r\n> \r\n> I agree. If we need to implement a very generalized function, a single util function is not enough.\r\n> \r\n> Maybe you can split your work into some issues for everyone to implement together?\r\n\r\nOf cause, if everything goes well I will come up with some design ideas and plan out some implementation tasks in the next week or so.

I think it can be roughly classified into several types of parameters. Can it be classified by some features such as SFINAE?

Another concern is that, there are so many unnecessary string copy in both current code and this PR. Maybe we need to find a way to reduce them, and take advantages of move semantics of `std::string` and usage of `std::string_view`.

One idea I have is that we implement syntactic state machine for each command, and then use a utility function to parse the state machine.

cool, thanks for your contribution.\r\n\r\n@maochongxin Can we put the ttl argument parse into a single function? so that we can avoid duplicate codes, the expire argument can be translated into ttl as well.

> cool, thanks for your contribution.\r\n> \r\n> @maochongxin Can we put the ttl argument parse into a single function? so that we can avoid duplicate codes, the expire argument can be translated into ttl as well.\r\n\r\n@git-hulk Complete the refactoring, but Is it waiting for #963  conclusion

```\r\nSET key value [NX | XX] [GET] [EX seconds | PX milliseconds |\r\n  EXAT unix-time-seconds | PXAT unix-time-milliseconds | KEEPTTL]\r\n\r\nGETEX key [EX seconds | PX milliseconds | EXAT unix-time-seconds |\r\n  PXAT unix-time-milliseconds | PERSIST]\r\n```\r\n\r\nI think we should notice their difference: there is no `PERSIST` in `SET`, and no `NX`, `XX` in `GETEX`, hence we should give a syntax error to user e.g. while encountering `NX` in `GETEX`.\r\n\r\nSo I do not think the whole parsing procedure for `SET` and `GETEX` can be put in one function (or you can try it, but I think it might make the logic more complicated), you could just unify the EX/PX/EXAT/PXAT part.

> It seems these TCL tests are migrated to golang, so maybe we should add these GetEX tests in go test cases again. cc @maochongxin\r\n\r\nOk, let me add it

> > It seems these TCL tests are migrated to golang, so maybe we should add these GetEX tests in go test cases again. cc @maochongxin\r\n> \r\n> Ok, let me add it\r\n\r\ndone

Just for record: https://github.com/apache/incubator-kvrocks/actions/runs/3231715201/jobs/5291557196\r\n```\r\nFAIL: TestString/Extended_SET_EXAT_option\r\n```

Thanks all, merging...

submitted the doc updated PR: https://github.com/apache/incubator-kvrocks-website/pull/19

@ColinChamber Can you help to add more background information?

@git-hulk Thank you! Updated.

orz, thanks masters, I will continue to contribute 

@maochongxin thank you!

I do not quite love the current design for configuration, I think a better way would be something like:\r\n\r\n```\r\nrocksdb.db_paths /mnt/ssd /mnt/hdd\r\nrocksdb.db_paths_size 10 10\r\n```\r\n\r\nOr simply:\r\n```\r\nrocksdb.db_paths /mnt/ssd 10 /mnt/hdd 10\r\n```\r\n\r\nSince we have no quoted multiple field configuration now, I think we can just forbid pathes that include spaces.

> I do not quite love the current design for configuration, I think a better way would be something like:\r\n> \r\n> ```\r\n> rocksdb.db_paths /mnt/ssd /mnt/hdd\r\n> rocksdb.db_paths_size 10 10\r\n> ```\r\n> \r\n> Or simply:\r\n> \r\n> ```\r\n> rocksdb.db_paths /mnt/ssd 10 /mnt/hdd 10\r\n> ```\r\n> \r\n> Since we have no quoted multiple field configuration now, I think we can just forbid pathes that include spaces.\r\n\r\nI think this is better\r\nrocksdb.db_paths "/mnt/ssd 10; /mnt/hdd 10"

maybe 10Gb / 100Mb - with size dimensions? 

> maybe 10Gb / 100Mb - with size dimensions?\r\n\r\nYes, I think it will be better if we can set the unit explicitly. \r\n\r\n`rocksdb.db_paths /mnt/ssd 10Gb /mnt/hdd 10Mb` or `/mnt/ssd 10Gb; /mnt/hdd 10Mb` is good.

Good idea. I think just something like `1` `1k` `1M` `1G` `1T` is fine, without the `B`.

> Good idea. I think just something like `1` `1k` `1M` `1G` `1T` is fine, without the `B`.\r\n\r\nAgree with this, looks more intuitive.

> I think this option is useful if multiple storage media (Such as SSD and HDD) of different speeds are used.\r\n\r\n@tanruixiang can we control which data are written to a specific path? It seems not and then when I config multiple paths, the only purpose to me is that the data volume is large while one disk cannot serve it and I need another ones.

> > I think this option is useful if multiple storage media (Such as SSD and HDD) of different speeds are used.\r\n> \r\n> @tanruixiang can we control which data are written to a specific path? It seems not and then when I config multiple paths, the only purpose to me is that the data volume is large while one disk cannot serve it and I need another ones.\r\n\r\n`Rocksdb` will be stored in the order of `db_path`, and the low-level sstable will be placed in the earlier path. For example, if we have an SSD and an HDD, we always want the low-level sstable to be in the SSD.

@xiaobiaozhao The only issue is only parts of DB paths are used which are mentioned in https://github.com/apache/incubator-kvrocks/pull/953#issuecomment-1286391126, it looks still no follow-up from the RocksDB community.

Hi all, is there any updates? This feature is very interesting for us. Could we (with @torwig) helping for this PR?

Close this PR due to lack of active, feel free to reopen it if want to continue working on this.

You possibly need to remove the entry of string tests for TCL test helper. Otherwise, the TCL suite can fail.\r\n\r\nSee https://github.com/apache/incubator-kvrocks/commit/43cca6a1ec50bfc850b8d9d08c552a61de8c0034#diff-44b16c150e71ef9382d728c10c9098bf483e57731d98f84f94a394e81c9172c3 as an example.

https://github.com/apache/incubator-kvrocks/pull/944#discussion_r986722195\r\n\r\ni think use `Do("SET", "foo", "1", "xx")` method is better than `SetXX`  method in `Extended SET XX option` unit test, since we are testing the `set` command with option instead of `setxx` command, and the two commands don\

\r\n\r\n\r\n> Review part of this patch. Comments inline.\r\n> \r\n> Please try to keep the logic identical - that is, no eager refactor. Otherwise, it\

1. It seems something wrong with [TestExpire](https://github.com/apache/incubator-kvrocks/actions/runs/3210776460/jobs/5251928178#step:11:202)?\r\n2. No ideas why [this](https://github.com/apache/incubator-kvrocks/actions/runs/3210776460/jobs/5251928178#step:11:987) happened. Maybe due to the unstable test?

> 1. It seems something wrong with [TestExpire](https://github.com/apache/incubator-kvrocks/actions/runs/3210776460/jobs/5251928178#step:11:202)?\r\n> 2. No ideas why [this](https://github.com/apache/incubator-kvrocks/actions/runs/3210776460/jobs/5251928178#step:11:987) happened. Maybe due to the unstable test?\r\n\r\nI remember having similar unstable results before, long time no see after tison fix.  Do we need to use the same settings as List here? https://github.com/apache/incubator-kvrocks/pull/869 @tisonkun 

Thank you @GoGim1!

Lot of thanks @torwig for awesome help and fix cpp linter. Please, review

Yes @tisonkun your work on Go test are awesome!

Thank you! Merged.

@tisonkun Thanks! 

Thank you @aleksraiden! Merging...

@PragmaTwice updated.

Closed for now. Probably pick it up later.

@tanruixiang Thank you!

@torwig please, review

@aleksraiden Quick action!

Found one missing in streams module too

@caipengbo Can you add some tests for this fix?

Sorry, I have been delayed by some things. I will do it as soon as possible.

@git-hulk @PragmaTwice Thank you so much. They have been fixed.

LGTM

@PragmaTwice @tanruixiang may I ask why `ParseInt` is better than `stoi` and `atoi`? I can see we write more boilerplate code.

> \r\n\r\n\r\n\r\n> @PragmaTwice @tanruixiang may I ask why `ParseInt` is better than `stoi` and `atoi`? I can see we write more boilerplate code.\r\n\r\nMy opinion is that the string to number process can be managed uniformly. What the `ParseInt` does is essentially the Wrapper of `std::strt*`. In fact, if you use `std::strt*` directly, you also need boilerplate code (at least try catch), but during the replacement process, we found that try catch was not done in some places, so it was supplemented.

> @PragmaTwice @tanruixiang may I ask why `ParseInt` is better than `stoi` and `atoi`? I can see we write more boilerplate code.\r\n\r\nWe currently prefer to use `Status`/`StatusOr` for error handling for several reasons:\r\n- There are some misuses of exceptions in the current code: for example, a lot of `try...catch` statements catch `std::exception` directly, but apparently this catches additional exceptions, such as `std::bad_alloc`. Also, some uses of functions such as `atoi`/`stoi` do not take into account parsing failures at all, which have been fixed in this PR, so the amount of code has increased.\r\n- `StatusOr` is able to work better with `Status` to forward error messages from lower layers, and has better composability, which is not very obvious at the moment, but will become more obvious as `StatusOr` is used more and more.

Thank you!

@git-hulk  @PragmaTwice  Hi. Do you have time to review?

> @git-hulk @PragmaTwice Hi. Do you have time to review?\r\n\r\nYes, will take a look tonight.

> Yes, will take a look tonight.\r\n\r\nThank you very much.\r\n

Hi @tanruixiang , you can also replace `atol`, `atoll` and other `sto*` or `ato*` in this PR if you like. \r\n\r\nThey belong to one function series, you can check cppreference to ensure all functions in this series are included.

> Hi @tanruixiang , you can also replace `atol`, `atoll` and other `sto*` or `ato*` in this PR if you like.\r\n> \r\n> They belong to one function series, you can check cppreference to ensure all functions in this series are included.\r\n\r\nThank you very much. All of these I found have been replaced(After checking it did miss one), and not all of them are written in the title.

This is because the redis build uses cache, and you need to update cache-redis to add redis-server to it \r\nhttps://github.com/apache/incubator-kvrocks/blob/unstable/.github/workflows/kvrocks.yaml#L137-L143

Thank you for your support @Ranxy 

It seems that redis-server has been successfully moved to the `$HOME/local/bin/` directory. [Here](https://github.com/apache/incubator-kvrocks/actions/runs/3144465670/jobs/5110553092#step:5:5). But why is it unsuccessful when starting redis-server? [Here](https://github.com/apache/incubator-kvrocks/actions/runs/3144465670/jobs/5110553092#step:11:558). In addition, redis-server has not been compiled successfully in other environments. \r\nCan you give me some help? Thanks for your time. @git-hulk @PragmaTwice 

> It seems that redis-server has been successfully moved to the `$HOME/local/bin/` directory. [Here](https://github.com/apache/incubator-kvrocks/actions/runs/3144465670/jobs/5110553092#step:5:5). But why is it unsuccessful when starting redis-server? [Here](https://github.com/apache/incubator-kvrocks/actions/runs/3144465670/jobs/5110553092#step:11:558). In addition, redis-server has not been compiled successfully in other environments. Can you give me some help? Thanks for your time. @git-hulk @PragmaTwice\r\n\r\ncc @caipengbo Could you help me?\r\n

Sorry for the late reply. #942 This feature needs a Redis server to check if the `dump.rdb` can be loaded correctly. Thanks for your help. @tisonkun 

For example, start a Redis server with:\r\n\r\n```go\r\n// populate with map with your configs\r\nsrv := util.StartRedisServer(t, map[string]string{})\r\n```\r\n\r\n... and in the workflow file:\r\n\r\n```bash\r\n./x.py test go build -redisBinPath=/path/to/redis-server\r\n```

I notice that CI is down in darwin clang. Maybe `#include <chrono>` is needed to use namespace `std::chrono_literals`.

> I notice that CI is down in darwin clang. Maybe `#include <chrono>` is needed to use namespace `std::chrono_literals`.\r\n\r\nYes, should include `<chrono>`, fixed.

hi @PragmaTwice, seems the namespace of `chrono_literals` is a bit different between compilers, do you suggest rollbacking to use the previous way?

> hi @PragmaTwice, seems the namespace of `chrono_literals` is a bit different between compilers, do you suggest rollbacking to use the previous way?\r\n\r\nSure. Maybe it need c++14 :rofl:

Thanks all, merging...

Merging...

Cool！How much performance improvement did you get from testing?

l did an incomplete test. For 1 millon times short range query, the query time is reduced more than 20%.

https://github.com/apache/incubator-kvrocks/pull/508  shows the usage of prefix extractor (FixedPrefixTransform provided by Rocksdb) @git-hulk. 

@shangxiaoxiong So I think we can merge what #508 does into this PR and use the new prefix extractor. 

> @shangxiaoxiong So I think we can merge what #508 does into this PR and use the new prefix extractor.\r\n\r\nl think so and l can merge https://github.com/apache/incubator-kvrocks/pull/508 if there is no problem about the new prefix extractor.

Got it, thanks a lot.

Thank you for your contribution, it would be better if you could explain it in detail in this PR, which can help us to see the code and understand the design.

@caipengbo is right, we can add more informations to make this easier to review.

> @caipengbo is right, we can add more informations to make this easier to review.\r\n\r\nha ha ha ! On the way.

Hi @shangxiaoxiong! Any progress on this pull request?

Hi @shangxiaoxiong , do you still have time to continue this PR?

I will raise another PR to do this.

Do we need to enable TLS in docker images? Currently TLS is enabled in nightly dockerhub images.

> Do we need to enable TLS in docker images? Currently TLS is enabled in nightly dockerhub images.\r\n\r\nYes, we need to enable TLS in docker image, let me fix it later.

Thanks all, merging..

The problem we had was that when we migrated, we had a lot of disk reads, but with this optimized version, these abnormal disk reads disappeared, so I think GetMetaData calls the Get interface

> The problem we had was that when we migrated, we had a lot of disk reads, but with this optimized version, these abnormal disk reads disappeared, so I think GetMetaData calls the Get interface.\r\n\r\n@datavisorxiaobiaozhao Yes. During the iteration, we set the cache not to be filled:\r\n\r\nhttps://github.com/apache/incubator-kvrocks/blob/e8721a270d23345f385af6db0617c3549d6e6812/src/slot_migrate.cc#L273-L277\r\n\r\nso that when we call `DB::Get()`, we are likely to read the disk.

There may be some problem. The Get interface may aim to get the latest expire time of the key. 

> There may be some problem. The Get interface may aim to get the latest expire time of the key.\r\n\r\n@shangxiaoxiong Oh Yes, I missed that. Then we only need GetMetaData for those with expiration time. Or we can record the time snapshot was created and subtract the expiration time to determine whether it is expired!

During sending snapshot, exipre time of some key may be changed to larger by clients. GetMetaData is needed to deal with this case.

l think GetMetaData is needed for complex key with expire time to get the latest expire time.

> During sending snapshot, exipre time of some key may be changed to larger by clients. GetMetaData is needed to deal with this case.\r\n\r\nDuring sending snapshot:\r\n- for simple type, user changes to expire are incremental data and will be synchronized through WAL.\r\n- for complex types, we really should get the latest metadata.

> > During sending snapshot, exipre time of some key may be changed to larger by clients. GetMetaData is needed to deal with this case.\r\n> \r\n> During sending snapshot, user changes to expire are incremental data and will be synchronized through WAL.\r\n\r\nSimple key is ok but complex key is not.

Indeed, the whole process is not atomic.

Yes, the root cause is the snapshot can be different between writers and background threads. I prefer to use current implementation, then we can try to seek another way to resolve this issue completely. How do you think? @caipengbo @shangxiaoxiong 

Current implementation makes slotmigration faster.  Faster slotmigration is, smaller risk of losing key is.  On the whole, current implementation may be good.

I think OK, I will continue to think about how to solve this kind of problem.

I think this is the best solution for now, merge this pr first, and then contionue to look for new solutions.

@tanruixiang why i can\

@IoCing  Hi. You can merge the newest unstable branch, and compile again.

Many thanks for @IoCing great contribution, overall is good to me. You can help to resolve the conflict.

LGTM. Thank you very much for your contribution.

@IoCing Thank you!

@tanruixiang @git-hulk @tisonkun thanks for your help

@ellutionist Can you help to resolve the conflict in redis_cmd.cc

> @ellutionist Can you help to resolve the conflict in redis_cmd.cc\r\n\r\n@git-hulk Conflict resolved.

@ellutionist Thank you very much for your contribution. LGTM. Do you have time to add a unit test for cpp? It should be in `t_string_test.cc`.

> @ellutionist Thank you very much for your contribution. LGTM. Do you have time to add a unit test for cpp? It should be in `t_string_test.cc`.\r\n\r\n@tanruixiang Since this PR does not bring any new interface like `SetEX` or `MSet` to the class `Redis::String`, I cannot see any proper entry point for cpp unit test. Any good idea?

Many thanks for your contribution, will merge if there are no more comments by tomorrow. cc @PragmaTwice @caipengbo 

Thank you!

@PragmaTwice  Hi. The original code uses `std:: stol` , but using `ParseInt<long int> `instead the lint will report an error. Do you have any better suggestions?

> That is to say, the type used in the original code shoule be the same here. In this pr, regardless of whether the type can be adjusted logically. If the type can be modified, we can do it in the next Pr?\r\n\r\n@PragmaTwice  HI.  What do you think about this. If you want to modify it in this pr, I will modify it immediately, otherwise I will modify it to another pr.

> > That is to say, the type used in the original code shoule be the same here. In this pr, regardless of whether the type can be adjusted logically. If the type can be modified, we can do it in the next Pr?\r\n> \r\n> @PragmaTwice HI. What do you think about this. If you want to modify it in this pr, I will modify it immediately, otherwise I will modify it to another pr.\r\n\r\nI think it is ok to change it, since it is trivial.

> > > That is to say, the type used in the original code shoule be the same here. In this pr, regardless of whether the type can be adjusted logically. If the type can be modified, we can do it in the next Pr?\r\n> > \r\n> > \r\n> > @PragmaTwice HI. What do you think about this. If you want to modify it in this pr, I will modify it immediately, otherwise I will modify it to another pr.\r\n> \r\n> I think it is ok to change it, since it is trivial.\r\n\r\nOk, I will change it .

Hi @tanruixiang, I notice there is also some code using `std::stoul` or `std::stoull` in kvrocks, do you want to also replace them in this PR? Or it is OK to merge it and replace them in another PR.

> Hi @tanruixiang, I notice there is also some code using `std::stoul` or `std::stoull` in kvrocks, do you want to also replace them in this PR? Or it is OK to merge it and replace them in another PR.\r\n\r\nOK I am willing to do this task. I think this PR information is a bit too much, we can continue to do this in the next PR.

> > Hi @tanruixiang, I notice there is also some code using `std::stoul` or `std::stoull` in kvrocks, do you want to also replace them in this PR? Or it is OK to merge it and replace them in another PR.\r\n> \r\n> OK I am willing to do this task. I think this PR information is a bit too much, we can continue to do this in the next PR.\r\n\r\nThanks. Maybe there are also some `atoi` and `std::stoi`, you can replace them too in another PR.

Thanks. I will merge it soon.

Thanks for @tanruixiang contribution, this PR looks good to me. Do you mind adding the `gocase` for the hrange command as well first, then we can migrate the other test cases later.

> Thanks for @tanruixiang contribution, this PR looks good to me. Do you mind adding the `gocase` for the hrange command as well first, then we can migrate the other test cases later.\r\n\r\nYes. Originally, I wanted to complete the migration of `hash.tcl` by the way. But I see that issue seems to be assigned. ~Can I add to gocase after he is done migrating?~ I think I can write gocase for hrange first.

Thanks all, merging...

BTW, it seems there are some issues with local time in your device, the commit is with a timestamp of `Sep 20, 2022, 4:48 AM GMT+8`.\r\n\r\nThis value is from the future.

IIRC @suica suggested removing the whole folder on tests passed optionally https://github.com/apache/incubator-kvrocks/issues/862. This PR can be related :) That is, by adding logics on Server closed, that suggestion can be achieved.

> @PragmaTwice please check whether the title change is correct. I suggest we write the solution/expected state on PRs, while the issue/error state on issues.\r\n\r\nYeah, and it is more accurate than before. Thanks!

Hi guys, I think it is ready now : )

🤔 @tanruixiang  I think I need to return the old Write interface and add a method to return default write options from storage. Is it ok?

> 🤔 @tanruixiang I think I need to return the old Write interface and add a method to return default write options from storage. Is it ok?\r\n\r\nWe can discuss it. @git-hulk @PragmaTwice What form do you think is the best?\r\n

I also prefer to add the `WriteWithDefault` to avoid creating the option everywhere, then we can keep the `Write` method to be consistent with RocksDB.

As @PragmaTwice  said. I think it is possible to increase the configuration of default writeoption in the configuration. Doing this eliminates the need to recompile the code every time the writeoption is modified.

> As @PragmaTwice said. I think it is possible to increase the configuration of default writeoption in the configuration. Doing this eliminates the need to recompile the code every time the writeoption is modified.\r\n\r\nhmmm, I can add `sync`, `no_slowdown`, `low_pri`, `memtable_insert_hint_per_batch`. But `ignore_missing_column_families` and `timestamp` looks useless to setting through the config. do you agree with this?

> > As @PragmaTwice said. I think it is possible to increase the configuration of default writeoption in the configuration. Doing this eliminates the need to recompile the code every time the writeoption is modified.\r\n> \r\n> hmmm, I can add `sync`, `no_slowdown`, `low_pri`, `memtable_insert_hint_per_batch`. But `ignore_missing_column_families` and `timestamp` looks useless to setting through the config. do you agree with this?\r\n\r\nI think these parameters can be reserved, and synchronization in the config file(For example, adding an interface to change the `default writeoption`. If the configuration file is configured with `default writeoption`, then make corresponding modifications.). Of course, if it is not in the configuration file, the default value is used.

> I think these parameters can be reserved, and synchronization in the config file(For example, adding an interface to change the `default writeoption`. If the configuration file is configured with `default writeoption`, then make corresponding modifications.). Of course, if it is not in the configuration file, the default value is used.\r\n\r\nAfter this modification, if you want to `disableWAL`, you only need to modify the configuration file.\r\n\r\n

I need a decision about exposing write options. \r\nI think `ignore_missing_column_families` and `timestamp` are unneeded for configuring. Others can be useful

Thanks @tanruixiang 

@PragmaTwice Hi. Do you have any more suggestions on reducing redundant code?

Is it possible that it is a bug of rocksdb? I can get the value of the corresponding key, but the size occupied by the key is 0. And this problem occurs occasionally.

> OK, got it. I need to read this PR first and try to reproduce it on my side.\r\n\r\n@git-hulk  I used the parameters configured for `GetApproximateSizes` testing in `rocksdb`. However, there is still a probability of zero results. Do you have any suggestions?

> > OK, got it. I need to read this PR first and try to reproduce it on my side.\r\n> \r\n> @git-hulk I used the parameters configured for `GetApproximateSizes` testing in `rocksdb`. However, there is still a probability of zero results. Do you have any suggestions?\r\n\r\nSorry for missing this comment, I have no enough time to inspect this case yet.

@git-hulk  I think I already know why. According to [this](https://github.com/facebook/rocksdb/issues/10733), to prevent the situation of 0 size during the test, it is necessary to save the data in different data blocks of sst as much as possible. This requires us to enter a lot of data when testing.

> @git-hulk I think I already know why. According to [this](https://github.com/facebook/rocksdb/issues/10733), to prevent the situation of 0 size during the test, it is necessary to save the data in different data blocks of sst as much as possible. This requires us to enter a lot of data when testing.\r\n\r\nNice!

@git-hulk @torwig @caipengbo Hi. Do you have time to review?

@PragmaTwice @caipengbo @torwig Do you have any suggestion on this PR? Or I will merge it if we have no further comment. 

LGTM

Thanks for @tanruixiang contribution and patient, also thanks to everyone who reviewed this PR. Merging...

We also need to update our website :) @git-hulk 

Yes, GitHub actions will deploy the update automatically.

Yes, you can do it :) @tanruixiang 

@git-hulk mind take a look? I need some help here

Hi, @mapleFU, many thanks for your contribution.\r\n\r\n> I don\

@git-hulk Now I extract the setting logic from `CommandAuth`, and implement it in `HELLO`.\r\n\r\nBut I wonder how can I testing `hello`, and would it be ambigious if `setName` is called when `conn` is an admin? And should we update the `tokens` here? And should I add some extra return values?

Hi, I\

It will be no problem when i change `ADD_CMD("hello" ...` to `ADD_CMD("HELLO", ...` or `"ciallo"` , which really makes me mad...

Oh, finally I find out the reason. `go-redis` will send `hello 3` to kvrocks, and it will be filtered by auth checking...

```\r\n[TIMEOUT]: clients state report follows.\r\n```\r\n\r\nmaybe I should rerun the test?

Thanks all, will merge this PR after the CI becomes green.

Thanks @mapleFU again

@git-hulk @PragmaTwice Hi. Do you have time to review for me? Thank you very much for your help.

> @git-hulk @PragmaTwice Hi. Do you have time to review for me? Thank you very much for your help.\r\n\r\nSure, thanks for your contribution.

Thank you!

refer to #857 

Thanks for your contribution!

@PragmaTwice  I guess maybe the go-redis not supports quit ([Redis 3 commands except QUIT, MONITOR, and SYNC.](https://github.com/go-redis/redis)).As @IoCing said on the pr.

@PragmaTwice thanks for your comments

Hi, the first `rdb := srv.NewTCPClient()` is not used, I think u can move it into every `t.Run` .Thanks for your great contribution.

I am wondering whether we could use the pipeline support in go-redis instead of current way. Any ideas?\r\n\r\nRefer to https://github.com/go-redis/redis/blob/master/pipeline.go\r\n\r\nJust an idea, I am ok with current way : )

> I am wondering whether we could use the pipeline support in go-redis instead of current way. Any ideas?\r\n> \r\n> Refer to https://github.com/go-redis/redis/blob/master/pipeline.go\r\n> \r\n> Just an idea, I am ok with current way : )\r\n\r\n\r\n@PragmaTwice  I\

> > I am wondering whether we could use the pipeline support in go-redis instead of current way. Any ideas?\r\n> > Refer to https://github.com/go-redis/redis/blob/master/pipeline.go\r\n> > Just an idea, I am ok with current way : )\r\n> \r\n> @PragmaTwice I\

> > I am wondering whether we could use the pipeline support in go-redis instead of current way. Any ideas?\r\n> > Refer to https://github.com/go-redis/redis/blob/master/pipeline.go\r\n> > Just an idea, I am ok with current way : )\r\n> \r\n> @PragmaTwice I\

@PragmaTwice It will panic because of EOF. We do get results for this case if we ignore panic.\r\n```\r\npipe := srv.NewClient().Pipeline()\r\ntmp1 := pipe.Do(ctx, "quit")\r\ntmp2 := pipe.Do(ctx, "ping")\r\n_, err := pipe.Exec(ctx)\r\nif err != nil { // eof error\r\n\tpanic(err)\r\n}\r\nt.Log(tmp1.Val()) //  OK\r\nt.Log(tmp2.Val()) //  nil\r\n```

> @PragmaTwice It will panic because of EOF. We do get results for this case if we ignore panic.\r\n> \r\n> ```\r\n> pipe := srv.NewClient().Pipeline()\r\n> tmp1 := pipe.Do(ctx, "quit")\r\n> tmp2 := pipe.Do(ctx, "ping")\r\n> _, err := pipe.Exec(ctx)\r\n> if err != nil { // eof error\r\n> \tpanic(err)\r\n> }\r\n> t.Log(tmp1.Val()) //  OK\r\n> t.Log(tmp2.Val()) //  nil\r\n> ```\r\n\r\nA little confused. I think the `panic` is added by yourself rather than in `pipe.Exec`?

> > @PragmaTwice It will panic because of EOF. We do get results for this case if we ignore panic.\r\n> > ```\r\n> > pipe := srv.NewClient().Pipeline()\r\n> > tmp1 := pipe.Do(ctx, "quit")\r\n> > tmp2 := pipe.Do(ctx, "ping")\r\n> > _, err := pipe.Exec(ctx)\r\n> > if err != nil { // eof error\r\n> > \tpanic(err)\r\n> > }\r\n> > t.Log(tmp1.Val()) //  OK\r\n> > t.Log(tmp2.Val()) //  nil\r\n> > ```\r\n> \r\n> A little confused. I think the `panic` is added by yourself rather than in `pipe.Exec`?\r\n\r\n[Example code](https://redis.uptrace.dev/guide/go-redis-pipelines.html#pipelines)  shows we should check this err. I do not know if we sholud ignore it.

> > > @PragmaTwice It will panic because of EOF. We do get results for this case if we ignore panic.\r\n> > > ```\r\n> > > pipe := srv.NewClient().Pipeline()\r\n> > > tmp1 := pipe.Do(ctx, "quit")\r\n> > > tmp2 := pipe.Do(ctx, "ping")\r\n> > > _, err := pipe.Exec(ctx)\r\n> > > if err != nil { // eof error\r\n> > > \tpanic(err)\r\n> > > }\r\n> > > t.Log(tmp1.Val()) //  OK\r\n> > > t.Log(tmp2.Val()) //  nil\r\n> > > ```\r\n> > \r\n> > \r\n> > A little confused. I think the `panic` is added by yourself rather than in `pipe.Exec`?\r\n> \r\n> [Example code](https://redis.uptrace.dev/guide/go-redis-pipelines.html#pipelines) shows we should check this err. I do not know if we sholud ignore it.\r\n\r\n@PragmaTwice @tanruixiang Should I now use `pipeline.do` instead of `tcpclient` reimplementation ?

> @PragmaTwice @tanruixiang Should I now use `pipeline.do` instead of `tcpclient` reimplementation ?\r\n\r\nI prefer `pipeline.do`, although I think both of them are OK to me.

@IoCing Hi,I just try `QUIT returns OK`, If we igrone the err, It can run as expected. You can try others.

> > @PragmaTwice @tanruixiang Should I now use `pipeline.do` instead of `tcpclient` reimplementation ?\r\n> \r\n> I prefer `pipeline.do`, although I think both of them are OK to me.\r\n\r\nok  ,  i will try to use pipeline.Do to implement

> @IoCing Hi,I just try `QUIT returns OK`, If we igrone the err, It can run as expected. You can try others.\r\n\r\nok,thanks for your help 

@git-hulk thanks! I rerun the CI for now.

Closed as inactive. Code conflict by the way. @suica you can resubmit the patch when it gets completed.

Thanks, @torwig

@tisonkun Yes, I tested it locally. Short manual testing I would say. 

Waiting for Travis CI reports...

Thanks all, merging...

@tisonkun @git-hulk  @PragmaTwice  HI.  I found that the results of CI were a bit random. My same code failed in different places the first two times, and I passed it the last time.

```\r\nBenchmarkLTRIM\r\nBenchmarkLTRIM/LTRIM_stress_testing_-_zipList\r\nBenchmarkLTRIM/LTRIM_stress_testing_-_zipList-3         \t       1\t10113128988 ns/op\r\nBenchmarkLTRIM/LTRIM_stress_testing_-_linkedList\r\n    list_test.go:80: \r\n        \tError Trace:\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/unit/type/list/list_test.go:80\r\n        \t            \t\t\t\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/unit/type/list/benchmark.go:193\r\n        \t            \t\t\t\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/unit/type/list/benchmark.go:233\r\n        \t            \t\t\t\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/unit/type/list/asm_amd64.s:1594\r\n        \tError:      \tReceived unexpected error:\r\n        \t            \ti/o timeout\r\n        \tTest:       \tBenchmarkLTRIM/LTRIM_stress_testing_-_linkedList\r\n--- FAIL: BenchmarkLTRIM/LTRIM_stress_testing_-_linkedList\r\n--- FAIL: BenchmarkLTRIM\r\nFAIL\r\n```\r\n\r\nSeems a new unstable test.

> ```\r\n> BenchmarkLTRIM\r\n> BenchmarkLTRIM/LTRIM_stress_testing_-_zipList\r\n> BenchmarkLTRIM/LTRIM_stress_testing_-_zipList-3         \t       1\t10113128988 ns/op\r\n> BenchmarkLTRIM/LTRIM_stress_testing_-_linkedList\r\n>     list_test.go:80: \r\n>         \tError Trace:\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/unit/type/list/list_test.go:80\r\n>         \t            \t\t\t\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/unit/type/list/benchmark.go:193\r\n>         \t            \t\t\t\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/unit/type/list/benchmark.go:233\r\n>         \t            \t\t\t\t/Users/runner/work/incubator-kvrocks/incubator-kvrocks/tests/gocase/unit/type/list/asm_amd64.s:1594\r\n>         \tError:      \tReceived unexpected error:\r\n>         \t            \ti/o timeout\r\n>         \tTest:       \tBenchmarkLTRIM/LTRIM_stress_testing_-_linkedList\r\n> --- FAIL: BenchmarkLTRIM/LTRIM_stress_testing_-_linkedList\r\n> --- FAIL: BenchmarkLTRIM\r\n> FAIL\r\n> ```\r\n> \r\n> Seems a new unstable test.\r\n\r\nLooks like bench test may cause Kvrocks high latency and then client got timeout error.

@git-hulk see #870 .

I think, if we want to support command line options for configurations, we need to implement it in a more direct way, rather than `sed`, which is more like a workaround.

@tanruixiang I suggest we focus on the design decision before jump into implementation details.\r\n\r\n@PragmaTwice @git-hulk @ShooterIT @usrtax a quick idea on passing override from environment variable is taking a namespace said `kvrocks.` and when start the server, read from the env vars and overwrite `xxx` with `kvrocks.xxx`.\r\n\r\nAnyway, I hope this initiative goes through a design instead of rushing into patches.

Hi everyone, I think this PR is ready to review now : )

> Generally looks good. One comment:\r\n> \r\n> From https://redis.io/docs/manual/config/ it is valid:\r\n> \r\n> ```\r\n> keyword argument1 argument2 ... argumentN\r\n> ```\r\n> \r\n> but it seems in Kvrocks we support only one argument?\r\n\r\nI used to support multiple value configuration at the first (can see my first commit, it can parse multiple value configuration and even has a more simple state machine). But it involved too many changes, and the current configuration reading/dumping/rewriting/hot-loading mechanism in kvrocks do not handle multiple value configuration well, so I give up and move to support just one value configuration.\r\n

Hi. When we parse `"a\

> Hi. When we parse `"a\

> > Hi. When we parse `"a\

> > > Hi. When we parse `"a\

> \r\n\r\n\r\n\r\n> > > > Hi. When we parse `"a\

Hi all, I will merge it if no further discussion, feel free to review it.

Thanks for @tisonkun carefully review, will merge after the CI become green.

Thanks @tisonkun ! Merging...

Thank you! Merging...

Thanks all, merging...

Thank you!

@tisonkun I noticed you were online. Can you open the CI for me?

@tanruixiang cool, thanks for your contribution.

Merging...

You can rebase on latest unstable version to catch up #834 for golangci-lint reports.

Verified linter violence can trigger CI failure correctly https://github.com/tisonkun/incubator-kvrocks/runs/8228711764?check_suite_focus=true.

FYI, i removed the deprecated golangci linters: structcheck, deadcode and varcheck, see https://github.com/apache/kvrocks/pull/1608

For the first time I know that Go files can have `-` in name :)

> For the first time I know that Go files can have `-` in name :)\r\n\r\nMaybe we need to use `list_common.go` & `list_2_test.go` to comply the Go naming conventions?

@tisonkun Oh yes, that naming would be more concise and consistent. \r\nAnd since the file is about the cases from `list-2.tcl`, how about keeping the number in its name like this?\r\n\r\n```\r\nlist\r\n- common.go\r\n- list_2_test.go\r\n```\r\n

@suica I send a patch onto your PR https://github.com/suica/incubator-kvrocks/pull/1.\r\n\r\ncc @PragmaTwice I notice that test like "LTRIM stress testing" can adopt a Go benchmark flavor and when you wire it into `x.py`, you may think of add `-bench=.` if possible.

> cc @PragmaTwice I notice that test like "LTRIM stress testing" can adopt a Go benchmark flavor and when you wire it into `x.py`, you may think of add `-bench=.` if possible.\r\n\r\nThe script will forward all unparsed options, e.g.\r\n```\r\n./x.py test go <build-dir> -bench=...\r\n```\r\nequals to\r\n```\r\ncd ...\r\nKVROCKS_BIN_PATH=... GO_CASE_WORKSPACE=... go test -v ./... -bench=...\r\n```\r\n\r\n---\r\n\r\nOh, I think you may mean that we can make `-bench=.` a default option?

@PragmaTwice yes. We can discuss in detail in the dedicated PR. Just FYI.

Thanks for your contribution! Merging...\r\n\r\n

@tisonkun Added `-bench=.`

Thanks all! Merging...

Thanks @xiaobiaozhao. Please review - @git-hulk @PragmaTwice @tisonkun   @torwig 

Not so trivial, need to use macros for define options only for CMake > 3.24.0. I will patch this asap

Sorry, I Am newbie in CMake, but tried to make this clear as possible. In dedicated policy.cmake we can add any options in futures.

Lot of thanks @torwig 

@tisonkun Done, lot of thanks

@aleksraiden merged. Thanks for your contribution!

Awesome, @torwig, lot of thanks! Please, review @PragmaTwice @ShooterIT @git-hulk 

> In the [RESP spec](https://redis.io/docs/reference/protocol-spec/) it seems only CRLF("\\r\\n") is the allowed delimiter. Does Redis implement a different manner actually?\r\n\r\nYes, RESP only allows CRLF but inline mode allows the LF end of line as well: https://github.com/redis/redis/blob/unstable/src/networking.c#L2080

Thanks @tisonkun\r\n\r\nThe tcl is really hard to debug for myself. Go or Rust are good to me but I think Go will be easier for developers to learn and participate. Can see other guys have any feedback on this topic, 

I looked through this PR and it’s clear enough for me. To see other guys have any questions? @PragmaTwice @ShooterIT @Alfejik @caipengbo 

@torwig @aleksraiden Hi, could you guys help me to check whether this PR really fixed the CMake error on Fedora or Debian? So thanks!\r\n\r\nHere is a simple script for reference to check it:\r\n```sh\r\ngit clone -b fix-792 https://github.com/PragmaTwice/incubator-kvrocks\r\ncd incubator-kvrocks && ./x.py build\r\n```

@tisonkun I can confirm that the build process goes smoothly on Fedora 35 now.

@torwig Thanks for your confirmation!\r\n\r\nMerging...

@tisonkun I can confirm too, that the build process on Debian 11 are OK.

Thanks all, merging...

@torwig Are you in Kvrocks slack channel?

@git-hulk Yes, joined a few minutes ago :)

> @git-hulk Yes, joined a few minutes ago :)\r\n\r\nOK, thanks. Will contact you on slack.

Hi all, this PR is now ready for review!\r\n\r\nThere is an error in TCL TLS test only in macOS, and I think I test a lot to ensure it works well in linux(centos or archlinux, openssl 1.0 or 1.1, tls v1 v1.1 v1.2 or v1.3).\r\nI will try to figure out the error in mac, and I will be very appreciated if you help me, since I have no Mac device.\r\n\r\ncc @git-hulk @ShooterIT @tisonkun @caipengbo 

> Hi all, this PR is now ready for review!\r\n> \r\n> There is an error in TCL TLS test only in macOS, and I think I test a lot to ensure it works well in linux(centos or archlinux, openssl 1.0 or 1.1, tls v1 v1.1 v1.2 or v1.3). I will try to figure out the error in mac, and I will be very appreciated if you help me, since I have no Mac device.\r\n> \r\n> cc @git-hulk @ShooterIT @tisonkun @caipengbo\r\n\r\nThanks @PragmaTwice, I will try to locate macOS issue, not sure whether it caused by macOS ssl configured to forbid the older version like 1.2

> > Hi all, this PR is now ready for review!\r\n> > There is an error in TCL TLS test only in macOS, and I think I test a lot to ensure it works well in linux(centos or archlinux, openssl 1.0 or 1.1, tls v1 v1.1 v1.2 or v1.3). I will try to figure out the error in mac, and I will be very appreciated if you help me, since I have no Mac device.\r\n> > cc @git-hulk @ShooterIT @tisonkun @caipengbo\r\n> \r\n> Thanks @PragmaTwice, I will try to locate macOS issue, not sure whether it caused by macOS ssl configured to forbid the older version like 1.2\r\n\r\nSo thanks! The version of openssl in mac is probably 3.0, and you can check here to see the error:\r\n\r\nhttps://github.com/apache/incubator-kvrocks/runs/8110693005?check_suite_focus=true\r\n\r\nTo reproduce the error, you can just follow `kvrocks.yml`:\r\n```\r\n            git clone https://github.com/jsha/minica\r\n            cd minica && go build && cd ..\r\n            ./minica/minica --domains localhost\r\n            mkdir -p tests/tcl/tests/tls\r\n            cp localhost/cert.pem tests/tcl/tests/tls/server.crt\r\n            cp localhost/key.pem tests/tcl/tests/tls/server.key\r\n            cp localhost/cert.pem tests/tcl/tests/tls/client.crt\r\n            cp localhost/key.pem tests/tcl/tests/tls/client.key\r\n            cp minica.pem tests/tcl/tests/tls/ca.crt\r\n            ./x.py test tcl build --tls --single unit/tls --single unit/command --dont-clean\r\n```

@git-hulk So thanks for your warm help! \r\n\r\nAs discussed earlier, maybe tcl-tls is linked against the builtin openssl in mac, thus it behaves weirdly. I will temporarily disable the TCL TLS tests on MacOS so this PR can move on. 

does replication support TLS?

> does replication support TLS?\r\n\r\nCurrently no. This PR does not provide any TLS client features.\r\n\r\nIt will be supported in subsequent PRs.\r\n

@PragmaTwice I think we can merge this PR if no further discussion and other folks also can review and call a new discussion if they come back. cc @ShooterIT @caipengbo 

> @PragmaTwice I think we can merge this PR if no further discussion and other folks also can review and call a new discussion if they come back. cc @ShooterIT @caipengbo\r\n\r\nGreat, I will merge it today.

Thanks all. Merging. Feel free to open issues or discussions if you have any problem or suggestion after merge.

Thanks all. Merging...

@git-hulk BTW, after reverting snappy version I have Cmake error on Fedora 35\r\n\r\n```\r\nCMake Error at /usr/lib64/cmake/GTest/GTestTargets.cmake:37 (message):\r\n  Some (but not all) targets in this export set were already defined.\r\n\r\n  Targets Defined: GTest::gtest;GTest::gtest_main\r\n\r\n  Targets not yet defined: GTest::gmock;GTest::gmock_main\r\n\r\nCall Stack (most recent call first):\r\n  /usr/lib64/cmake/GTest/GTestConfig.cmake:42 (include)\r\n  /usr/share/cmake/Modules/FindGTest.cmake:187 (find_package)\r\n  build/_deps/snappy-src/CMakeLists.txt:40 (find_package)\r\n```\r\nHowever, I see that all builds you have in your CI are successful.

> @git-hulk BTW, after reverting snappy version I have Cmake error on Fedora 35\r\n> \r\n> ```\r\n> CMake Error at /usr/lib64/cmake/GTest/GTestTargets.cmake:37 (message):\r\n>   Some (but not all) targets in this export set were already defined.\r\n> \r\n>   Targets Defined: GTest::gtest;GTest::gtest_main\r\n> \r\n>   Targets not yet defined: GTest::gmock;GTest::gmock_main\r\n> \r\n> Call Stack (most recent call first):\r\n>   /usr/lib64/cmake/GTest/GTestConfig.cmake:42 (include)\r\n>   /usr/share/cmake/Modules/FindGTest.cmake:187 (find_package)\r\n>   build/_deps/snappy-src/CMakeLists.txt:40 (find_package)\r\n> ```\r\n> \r\n> However, I see that all builds you have in your CI are successful.\r\n\r\nThere are some cmake changes from Snappy 1.1.7 to 1.1.9, `find_package(gtest)` always exists in 1.1.7 regardless of `SNAPPY_BUILD_TESTS` being set to `OFF`, which is fixed in 1.1.9.\r\n\r\n@torwig Could you open an issue for this? I will fix it.

And if you want to workaround it now, you can just remove `find_package(GTest QUIET)` in `<build dir>/_deps/snappy-src/CMakeLists.txt`.

@ShooterIT Can have a look again and merge if have no comment.

> If it can only be reproduced in centos, I wonder if it is some bug or incompatible in old version gcc compiler, libstdc++ or glibc.\r\n\r\nYes, I also suspected that. I read through compressions code in RocksDB, it do nothing special except using RawCompress and RawUncompress, so I think it may cause by the behavior old version compiler since the compress/uncompression are user space behaviors.

@tisonkun Will merge this PR first since @corningsun also helped confirm it works after applying this patch. For how to aware the similar issue, my general idea is to add the daily\r\nCI to run benchmark.\r\n\r\nThanks @corningsun again for reporting issue and testing, also everyone who reviewed this PR.

> Could you link this PR to the master issue? And is there a description about "the new command parsing module"?\r\n\r\nHi @tisonkun , there is an old issue #598 to describe it but maybe a little out-dated, and I will create a new issue.

Thanks all! Merging...

Thanks all. Merging...

@xiaobiaozhao would you like to rebase this patch onto latest unstable? Then I can find some time to review it :)

@xiaobiaozhao Can help to resolve conflicts and take a look at why the CI NOT works.

> @xiaobiaozhao Can help to resolve conflicts and take a look at why the CI NOT works.\r\ndone

Thanks, will take a look soon.

@ShooterIT @PragmaTwice @caipengbo This PR will be merged in tomorrow if we have no more comments.

@git-hulk this patch has a code conflict with #900. But I can simply update after this patch merged :)

@PragmaTwice this manner seems by design for me O_O\r\n\r\nGenerally, Kvrocks doesn\

> @PragmaTwice this manner seems by design for me O_O\r\n> \r\n> Generally, Kvrocks doesn\

> > @PragmaTwice this manner seems by design for me O_O\r\n> > Generally, Kvrocks doesn\

@git-hulk @PragmaTwice the unstable go cases can be resolved by https://github.com/apache/incubator-kvrocks/pull/899. Please help with reviewing.

Hi, thanks for your contribution.\r\n\r\nSince there is no `std::make_unique` in c++11, you can use `Util::MakeUnique` in `util.h` as an alternative.

Hi, as mentioned by @git-hulk, I think you could minimize your PR, and you can check here to see why cpplint failed: https://github.com/apache/incubator-kvrocks/runs/7902217149?check_suite_focus=true

> Hi, as mentioned by @git-hulk, I think you could minimize your PR, and you can check here to see why cpplint failed: https://github.com/apache/incubator-kvrocks/runs/7902217149?check_suite_focus=true\r\n\r\nthanks for your review! I have removed all unnecessary changes.

cool, thanks

Thanks for your contribution again! Merging...

Good catch, thanks for your contribution.

Thanks all, merging...

Thanks to @mathspanda contribution and @torwig review, merging...

By the way, can someone tell me that how can I run CI on my PC?

> By the way, can someone tell me that how can I run CI on my PC?\r\n\r\nHi, you could try this command on root path of the repo:\r\n```\r\n./x.py check -h\r\n```

Let me fix this ci...

Thanks for your contribution! Merging...

Thank you!

@mathspanda Thanks for finding out and solving this issue, left a few suggestions. You can commit them if look good to you.

Thanks all, merging...

@Ranxy Thanks for your contribution~

Thanks @Ranxy, will update `GetDel` command to the supported commands soon. cc @ShooterIT 

In fact, I think `StatusOr` is not only more expressive, but also performs better than `Status` in some cases. \r\n\r\nThis is because `Status` is a large structure (`std::string` is usually larger than two words because SSO (small string optimization) is generally used in the implementation of `std::string`, for example, the implementation of `std::string` in libstdc++ is 32 bytes under 64 bits), so just `Status` is 40 bytes long, not including pointers for output. And Status inevitably constructs a `std::string` for any execution path.\r\n\r\nBut `StatusOr` instance like `StatusOr<int>` is only 16 bytes long and does not initialize `std::string` when no error occurs, and it initializes the resulting integer directly in-place inside these 16 bytes.

To be honest, this change looks a bit complex at first glance. \r\nBut after taking a look at how the Status/StatusOr works and uses, \r\nI think it can simplify a lot on how we add a new status code\r\nand return a status with value. \r\n\r\nFor myself, I would happy to see that we can use the modern way\r\nto improve our codebase, even it needs some time to learn for guys\r\ncame from C or legacy C++ code style like me.

Hi everyone, feel free to review and ask questions. \r\n\r\nIf there is no further discussion, I will merge it. :rocket:

Please wait a minute, i still need some time to put my thoughts

> C++23 std::expected [2] is used to contains expected value or an error, does this a more generic way, Can we implement same thing like this. We can combine Status and the expected class do the things StatusOr does.\r\n\r\nHi @wy-ei , `StatusOr<T>` has same purpose and similar implementation with `std::expected<T, E>`. Actually I has implemented something equal to `std::expected` for [oneflow](https://github.com/Oneflow-Inc/oneflow), and you can find its source code [here](https://github.com/Oneflow-Inc/oneflow/tree/master/oneflow/maybe), `oneflow::maybe::Maybe<T, E>`, `oneflow::maybe::Variant<T...>` and `oneflow::maybe::Optional<T>` is the basic error handling method in oneflow.\r\n\r\nYou can check Oneflow-Inc/oneflow/pull/6820 for more details.\r\n\r\nBut obviously, the implementation of `std::expected` is only more complicated and difficult to understand, while the implementation of `StatusOr` is relatively simple.\r\n\r\n> For StatusOr I have one question. StatusOr use a char[] to store T or Status, I don\

Yes, I also think `StatusOr` is more intuitive than before

Merging...

Thanks all, merging...

Thanks all! Merging...

It seems GCC builds in linux work well but Clang builds fail in linking phase (https://github.com/apache/incubator-kvrocks/runs/7631095274?check_suite_focus=true). \r\n\r\nI have not tried it manually but I guess it maybe due to ld, which cannot recognize `*.o` generated by Clang with `-flto` enabled (it is actually LLVM bytecode in LTO mode, not native object file).\r\n\r\nHence you can try to replace ld with lld while clang is used. I can help you to do that if you do not familiar with those stuff. 

I think we should add some descriptions for these options, why do we use, their advantage or disadvantages

> I think we should add some descriptions for these options, why do we use, their advantage or disadvantages\r\n\r\nLink-time optimization (LTO) is an important optimization technology in modern compilers.\r\n\r\nAs we already known, C/C++ compilers generate native object files for every translation unit (TU), which indicates compiler can only get information within a certain TU, i.e. it know nothing about another TU (except declarations). But obviously most compiler optimization approach requires the information in function definitions, like constant propagation, reachable analysis, interprocedual dataflow analysis, inlining, loop invariant analysis, pointer analysis, etc.\r\n\r\nSo if compiler cannot retrieve such information, the optimization will be just discarded. Obviously, this is a huge loss. LTO postpone the optimization procedures to link-time so (almost) every definition in the program is available to the optimizer. I do not think LTO has any disadvantages other than possibly slowing down compilation.\r\n\r\nI do not know why CMake call it interprocedure optimization, since interprocedure optimization can be done without LTO (but limited since it loss information in definitions).

Hi everyone, is there any other thought on this PR? I will merge it if no further discussion 🚀 

oh, thanks, is there a performance comparison? 

> oh, thanks, is there a performance comparison?\r\n\r\nYou can check [this review](https://www.phoronix.com/review/gcc10-lto-tr) and [this paper](https://arxiv.org/pdf/1010.2196.pdf).

Hi @ShooterIT, I agree with your argument that benchmark matters.\r\n\r\nI think we can wait for a benchmark before merge it or set `ENABLE_IPO` to OFF on default since LTO is practical in lots of database projects like [mysql](https://github.com/mysql/mysql-server/blob/8.0/CMakeLists.txt#L921), [clickhouse](https://github.com/ClickHouse/ClickHouse/blob/22.7/CMakeLists.txt#L399) and [mongodb](https://github.com/mongodb/mongo/blob/master/SConstruct#L217).

for test, currently we can use redis-benchmark\r\nhttps://github.com/apache/incubator-kvrocks#2--qps-on-different-payloads

Closed as no consensus. @wanghenshui I suggest you create an issue first and share the performance benchmark so that our maintainers can be sure whether this change brings good.

The merge conflict is resolved now.

I think we can merge it now since the benchmark looks good.

> I think we can merge it now since the benchmark looks good.\r\n\r\nYes

Thanks all, merging...

Thanks to @wanghenshui contribution again.

Thanks to @tisonkun, merging…

Thanks all, merging...

```\r\n*** [err]: requests on cluster are ok in tests/integration/cluster.tcl\r\nExpected \

Hello everyone, any thought on this PR? ❤️ 

Thanks all! Merging...

Thanks! Merging...

LGTM

Thanks all for review! Merging...

Thanks all, merging...

typo swith -> switch 😆 

@tisonkun good catch, I fixed the commit message before merging.. ahaha

Good to know XD. I notice this typo when watching your video on https://www.bilibili.com/video/BV1QY4y1A74H. When I saw you clicked "create pull request" with a highlight typo underline, I went here to see if it retains.

@tisonkun Aha, really meticulous. Very happy that tison can notice this video, I learn many good points from [your video](https://www.bilibili.com/video/BV1w54y1Z7VP?share_source=weixin_web&share_times=1).

I tried to fix the release workflow and upload packages automatically to a draft release in #751. \r\n\r\n@tisonkun suggested that we can directly remove the release workflow and move packaging scripts to x.py.\r\n\r\nI do not know whether the release vote process and the  release workflow are compatible, maybe we can have a discussion.

I think rename x.py to  build.py is better filename for user.

> I think rename x.py to build.py is better filename for user.\r\n\r\nHi @xiaobiaozhao, this script is versatile for not just build, for example it can execute code inspection tools, package source code, generate rpm packages, etc. (and more and more features are being added) So naming it `build.py` may not express this meaning well, you can refer to https://github.com/apache/incubator-kvrocks/pull/725 for details.

Hi @tisonkun, could you take a look at this PR while free?

I think we should disable releasing from GitHub Actions Workflow though.

> I think we should disable releasing from GitHub Actions Workflow though.\r\n\r\nI think we can genrate some draft releases in github actions to build `rpm`/`deb` packages,\r\nonce the release vote passes, we can make the draft release to a public release.\r\n\r\nIf we disable the release workflow, all these `rpm`/`deb` packages and docker images should be built and uploaded manually,\r\nwhich is inconvenient, error prone and unsafe (to both packager and users) in my view.

Sounds good. I will move these packaging procedures to `x.py` in another PR.

Thanks all. Merging...

Aha, no worry, my review also misses this issue.

Thanks all. Merging...

Thanks all. Merging...

Can you add some design documents so that people can understand your design ideas :)

@caipengbo Yes, sure, I will :)\r\n

@caipengbo Added design notes.\r\n\r\n

> @caipengbo Added design notes.\r\n\r\nMany thanks to @torwig detail explanation and bring this good feature into Kvrocks community, we will take a look recently.

@git-hulk Fixed possible crashes via access to command non-existing command arguments by index. Created column family for streams. I will use it also for replication purposes, to unblock blocked readers.\r\n

@git-hulk Added batch handling on replicas to unblock clients that were blocked by the XREAD command by the respective XADD. 

> @git-hulk Added batch handling on replicas to unblock clients that were blocked by the XREAD command by the respective XADD.\r\n\r\nThanks for your great contribution, I will try and review the PR in a few days.

I have reviewed this PR, implementation, test coverage and design are good to me. I will approve this PR after above conversations were resolved.\r\n\r\nPTAL @ShooterIT @Alfejik @caipengbo @PragmaTwice, I think we can merge this PR after it got >= 3 approves.

Sorry, there are some small things recently. I will carry out CR within this week :)

LGTM

@torwig Can you help to resolve the conflict?

@git-hulk Done

Hello, thanks  to @torwig brings this awesome feature for Kvrocks community and everyone who reviewed this PR. I will summary and merge this PR and welcome to create a new thread to further discussion. Cheers!!!

Thanks @torwig and @aleksraiden again.

> some little typos, and seems luaJIT is missing : )\r\n\r\nThanks twice, we won’t release luajit in 2.1.0, so will add it later.

I think we should also describe kvrocks uses some redis codes, and already put copyright on every file.

Thanks all, merging...

Very love this feature, users can use the latest version if they want to have a try.

Thanks all. Merging...

Thanks all, merging...

Too many conflicts, will checkout from unstable instead of 2.0

I tried on my side and also found it missing the `incubating` keyword

@tisonkun No sorry, you have helped the community a lot. Many thanks for your great contribution.

Merged, thanks @tisonkun again.

Thanks @tisonkun

Thanks for your reviews.\r\n\r\nMerging...

Merging...

Thanks for your reviews! Merging...

Thanks! \r\nHas fixed it.\r\n

Thanks all, merging PR...

Merging...\r\n\r\nThanks for your contribution @jackwener !

LGTM

Thanks @AvivPl 

Last failure was caused by the monitor command, will fix it

Merged. Thanks for you contribution!

> ASAN:DEADLYSIGNAL\r\n=================================================================\r\n==30388==ERROR: AddressSanitizer: SEGV on unknown address 0x0000000000f0 (pc 0x55c5e3f1823b bp 0x7fb31cbe58b0 sp 0x7fb31cbe5888 T103)\r\n==30388==The signal is caused by a READ memory access.\r\n==30388==Hint: address points to the zero page.\r\n\r\nemmm, does this caused by my commits? No idea why this occurs

> This mr solves issue https://github.com/apache/incubator-kvrocks/pull/709\r\n\r\n@mapleFU there is no "issue" #709 .

> @mapleFU there is no "issue" #709 .\r\n\r\nSo sorry for making the mistake, I have update it to https://github.com/apache/incubator-kvrocks/issues/708. PTAL\r\n\r\n

Thanks all, merging...

Thanks for @mapleFU contribution, merging...

@PragmaTwice I don\

> @PragmaTwice I don\

Thanks @jackwener, merging...

```\r\ntests/tmp/server.20197.3/stderr:==21201==ERROR: AddressSanitizer: heap-use-after-free on address 0x608000014068 at pc 0x0000006a79ef bp 0x7f33e0b7e9f0 sp 0x7f33e0b7e9e8 \r\ntests/tmp/server.20197.3/stderr:SUMMARY: AddressSanitizer: heap-use-after-free /home/runner/work/incubator-kvrocks/incubator-kvrocks/src/redis_cmd.cc:1568:25 in Redis::CommandBPop::TryPopFromList()\r\n```\r\n\r\nSeems there are some ASan errors in TCL tests: https://github.com/apache/incubator-kvrocks/runs/7125505577?check_suite_focus=true

> ```\r\n> tests/tmp/server.20197.3/stderr:==21201==ERROR: AddressSanitizer: heap-use-after-free on address 0x608000014068 at pc 0x0000006a79ef bp 0x7f33e0b7e9f0 sp 0x7f33e0b7e9e8 \r\n> tests/tmp/server.20197.3/stderr:SUMMARY: AddressSanitizer: heap-use-after-free /home/runner/work/incubator-kvrocks/incubator-kvrocks/src/redis_cmd.cc:1568:25 in Redis::CommandBPop::TryPopFromList()\r\n> ```\r\n> \r\n> Seems there are some ASan errors in TCL tests: https://github.com/apache/incubator-kvrocks/runs/7125505577?check_suite_focus=true\r\n\r\nWeird since sometime these errors do not appear.

Thanks everyone, the ASan report has been addressed in #699, and I will try to solve it later. Merging...

Thanks for your hard work.\r\n\r\n> rm test case "cmsgpack can pack and unpack circular references". Becase luajit\

BTW, We can do the Luajit extension based on this repository. https://github.com/openresty/luajit2.\r\nThe repo add some new function  https://github.com/openresty/luajit2#new-lua-apis 

![image](https://user-images.githubusercontent.com/52393536/177028973-88de5154-1386-4ee8-aba9-5538fe78422d.png)\r\nThis is  mem leak on my ubuntu

Many thanks for @xiaobiaozhao hard try

> As I commented in discussion, Redis did some changes to lua codebases, not only lua extensions. You can check [here](https://github.com/redis/redis/commits/unstable/deps/lua/src) to see these commits. I haven\

> [here](https://github.com/redis/redis/commits/unstable/deps/lua/src) The part of the lua code is modified for Redis 7.0, mainly the "redis function". I don\

@tisonkun and @ShooterIT can take a look if you have time. For https://github.com/apache/incubator-kvrocks/pull/697#discussion_r910672053, I also prefer providing an option to let user fallback if they have any problem.\r\n\r\nGeneral implementation is good to me.

Maybe you need to add `*.hpp` to line 42 in lua.cmake to pass the "Ubuntu GCC without luajit" job.

> Maybe you need to add `*.hpp` to line 42 in lua.cmake to pass the "Ubuntu GCC without luajit" job.\r\n\r\ndone

There are some memory leaks reported by ASan which are steadily reproduced and need some investigation. \r\n\r\nThe rest is good for me. 👍 

> There are some memory leaks reported by ASan which are steadily reproduced and need some investigation.\r\n> \r\n> The rest is good for me. 👍\r\n\r\nThose memory leaks looks not related to this PR.\r\n

@tisonkun @PragmaTwice @ShooterIT I think we can merge this PR if no objections.

@tisonkun Agree, I also think this is a big change and we can release it on the next release.

Thanks all, merging...

Thanks for @xiaobiaozhao great contribution again.

cool, thanks

thanks everyone! 

Does this patch fix some issues existing now? BTW as the building logic grow complexity, we may rewrite it with Python or other more structural tools :)

> Does this patch fix some issues existing now? BTW as the building logic grow complexity, we may rewrite it with Python or other more structural tools :)\r\n\r\nNot yet, but I think it is a potential risk of not specifying the compiler for building dependencies.\r\n\r\nRewriting `build.sh` in python sounds great.

Thanks everyone, merging...

Push 62c2952b8a8792bedfb308ba278303085b81c629 to ensure that cache logic works.

@PragmaTwice thanks for your input. Another approach is still run more Build and Test tasks every day but separate docker job into another workflow.

Another thing is that since we build the nightly version of the docker image we can automatically publish it to some registries like the docker hub, e.g. some tags like `nightly`, `nightly-20220629`.\r\n\r\nAnd I also think building docker image in CI triggered by PRs is unnecessary (maybe an arm evironment is useful, but a docker environment is not necessary for build and test).

> Another thing is that since we build the nightly version of the docker image we can automatically publish it to some registries like the docker hub, e.g. some tags like `nightly`, `nightly-20220629`.\r\n> \r\n> And I also think building docker image in CI triggered by PRs is unnecessary (maybe an arm evironment is useful, but a docker environment is not necessary for build and test).\r\n\r\nGood point that we can push the `nightly` docker image, latest image need to wait for the new release is too slow for most users.

> LGTM. @PragmaTwice I think we can add a short section in README.md for consumers of this feature.\r\n\r\nThanks for your suggestion, done. @tisonkun 

We may need to find some queuing strategy to reduce update branch overhead :P

> We may need to find some queuing strategy to reduce update branch overhead :P\r\n\r\nI think we can also use ccache and actions/cache to reduce build time in CI by incremental build. \r\nAlthough this will make the workflow more complicated.

Thanks for your contribution! Merging...

 Can we merge this PR and #680 into one? I think they are almost the same.

> Can we merge this PR and #680 into one? I think they are almost the same.\r\n\r\ndone

move to #681

This patch can be blocked by #645 as `missingInclude` is tricky to address. See also https://github.com/apache/incubator-kvrocks/pull/645#issuecomment-1162042973.

Could you remove `--suppress=noCopyConstructor:src/server.cc --suppress=noOperatorEq:src/server.cc` in `./cppcheck.sh` to make CI pass? Sorry for discommodity.

Thanks for @zz-jason contribution, merging...

```c++\r\ntemplate<typename T, typename ...Args> auto UniqueConfig(Args &&... args) {\r\n  return std::unique_ptr<ConfigField>(new T(std::forward<Args>(args)...));\r\n}\r\n```\r\nYou can add this utility function to smplify the changes : )

> ```c++\r\n> template<typename T, typename ...Args> auto UniqueConfig(Args &&... args) {\r\n>   return std::unique_ptr<ConfigField>(new T(std::forward<Args>(args)...));\r\n> }\r\n> ```\r\n> \r\n> You can add this utility function to smplify the changes : )\r\n\r\nLooks like changing a constructor is clearest. I have done it.

Thanks @PragmaTwice ! ❤️👍

Thanks for you contribution! Merging...

I have use `emplace_back` place the vector [List-initialization](https://en.cppreference.com/w/cpp/language/list_initialization).\r\n\r\nBecause it will use `copy constructor`, it will call `FieldWrapper(const FieldWrapper &wrapper)`.\r\n\r\nSo, I use `emplace_back`

Oh, my IDE cleaned all space in workflow files too. Although they are not intended to be changed in this PR but seem cool?

Thanks all, merging.

> @tisonkun You\

A related problem: I think that kvrocks should not output anything on stdout if we set a log file (which is a regular file).\r\n\r\nCurrently kvrocks still outputs something to stdout when the log file is set, which is confusing (to have multiple output locations), e.g. we could output the version to log file or via `kvrocks -v`.\r\n\r\nOf course we can consider this issue later so in this PR stdout is still a possible output location.\r\n\r\n@git-hulk @tisonkun 

OK. Let me push a followup commit to switch to `LOG(INFO)` version.

> OK. Let me push a followup commit to switch to `LOG(INFO)` version.\r\n\r\nJust address an already-exist problem, so feel free to use stdout. \r\nI think we can consider this problem later, and it should not block this PR.

It seems logs are fragmented among files and we should find another way to determinately confirm server has been restarted.

@git-hulk @PragmaTwice I think this patch is ready to merge. You can give it a review now :)

I have tried four time but failed reproduce it.😂. (It may be related with OS)\r\n\r\nEnv:\r\n- mbp m1\r\n- clang version 13.0.1\r\n\r\ninvestigate this problem later (other PR), this PR just fix warning.\r\n\r\n

Thanks for your contribution! Merging...

Cool, can you add some test cases in [protocol.tcl](https://github.com/apache/incubator-kvrocks/blob/unstable/tests/tcl/tests/unit/protocol.tcl) to make sure it has already fixed?

Shall this patch fix #633?

> Shall this patch fix #633?\r\n\r\nyes

Thanks everyone, merging...

Hi @PragmaTwice is there a bug? what is its danger?

> Hi @PragmaTwice is there a bug? what is its danger?\r\n\r\nYeah, it is an undefined behavior (to compare iterators from different containers), so the compiler and the standard library implementation can literally do anything, e.g. either return true or false, or even spend $1 from your mastercard 🤣  althought maybe libstdc++ and libc++ will not do that.\r\nIn some standard library implementation the `end()` iterator of a hashmap is just a `nullptr`, so the compare work well, but I means that I think we should not rely on a specific implementation and an undefined behavior.

@ShooterIT I think we can disable the test cases which failed frequently before finding the root cause, how do you think?

@git-hulk if so, we should hold a pinned issue to investigate and move forward fixing them.

Thanks everyone. Merging...

Probably we could switch from cppcheck to clang tidy for static analysis 🤣 

Finally fix the cppcheck issue, update cppcheck to 1.90 (in ubuntu 20.04) and add some extra flags.\r\nI disabled `missingInclude` because it is a complex procedure to let cppcheck find all headers since we have not configured the repo by cmake and not generated a compile command database.

Clicked wrong button 🤣

And this PR is ready for review now~

Thanks everyone! Merging...

Thanks for your contribution again @fishery86

Required CI name changed. Perhaps We can update `.asf.yaml` first.

You can add a final step named "Required" first and check all previous steps succeeded. As in https://github.com/apache/skywalking/blob/84de39cc07440edd9b629afb384be61225663698/.github/workflows/skywalking.yaml#L609-L645

> You can add a final step named "Required" first and check all previous steps succeeded. As in https://github.com/apache/skywalking/blob/84de39cc07440edd9b629afb384be61225663698/.github/workflows/skywalking.yaml#L609-L645\r\n\r\nSeems just check "Build and test" is enough? It will fail if previous job like license or lint fails.

@PragmaTwice Yes. However, it should be a separated PR - we introduce such step first, update .asf.yaml required logic, and then rebase this PR on those changes.

> @PragmaTwice Yes. However, it should be a separated PR - we introduce such step first, update .asf.yaml required logic, and then rebase this PR on those changes.\r\n\r\nGot it. I will prepare a PR introducing the "required" job with .asf.yml changed.

Blocked by #636

Thanks everyone, merging this PR...

@PragmaTwice PTAL

@jackwener thanks for your contribution!

Thanks for @GoGim1 contribution.

Wow, thanks for your great work,

These removed jobs will be re-added while https://github.com/apache/incubator-kvrocks/issues/639 is solved.

Thanks everyone, merging...

LGTM, it seems, in cloud environments, we may encounter this error.

Thanks for your contribution!

Hi @PragmaTwice Currently daily CI is failed, please have a look,\r\none tips for debugging daily CI on github, you can remove the rule of daily https://github.com/apache/incubator-kvrocks/blob/unstable/.github/workflows/daily-ci.yaml#L21 in you branch, you can submit a PR after there is no error report.

Actually I think whether we should running a daily CI or just run it every PR - with Apache INFRA we may have enough resource to run CI now. At least we may try to trigger daily CI when related files touched (however, "related files" can be any source files and thus it becomes a CI runs for every PR).

Hi @tisonkun it will cost much time if we run also daily CI for every PR, i think it is not friendly for contributors.\r\n\r\n> At least we may try to trigger daily CI when related files touched (however, "related files" can be any source files and thus it becomes a CI runs for every PR).\r\n\r\nThis idea sounds good, could we support this feature?

At least we can add:\r\n\r\n```yml\r\nname: Daily CI\r\non:\r\n  pull_request:\r\n    paths:\r\n      - ".github/workflows/daily-ci.yaml"\r\n  schedule:\r\n    - cron: \

> Actually I think whether we should running a daily CI or just run it every PR - with Apache INFRA we may have enough resource to run CI now. At least we may try to trigger daily CI when related files touched (however, "related files" can be any source files and thus it becomes a CI runs for every PR).\r\n\r\nI think we could use `ccache` and `actions/cache` to accelerate the build procedure in CI, so that more jobs could be added to the workflows triggered by PRs.\r\n\r\n\r\n> Hi @PragmaTwice Currently daily CI is failed, please have a look, one tips for debugging daily CI on github, you can remove the rule of daily https://github.com/apache/incubator-kvrocks/blob/unstable/.github/workflows/daily-ci.yaml#L21 in you branch, you can submit a PR after there is no error report.\r\n\r\nSorry for that, seems there is no pre-installed ninja in the virtual environment, some `apt install` is needed.

> ```yaml\r\n> name: Daily CI\r\n> on:\r\n>   pull_request:\r\n>     paths:\r\n>       - ".github/workflows/daily-ci.yaml"\r\n>   schedule:\r\n>     - cron: \

> In redis repo, maintainers can trigger manually daily CI, maybe it also is a way for us.\r\n\r\nWe can also trigger the daily CI manually since `workflow_dispatch` is added to https://github.com/apache/incubator-kvrocks/blob/unstable/.github/workflows/daily-ci.yaml#L24.\r\n\r\nJust like this, click the `Run workflow` button from a specific workflow page:\r\n![image](https://user-images.githubusercontent.com/20042607/173491421-6be7bec2-f379-4c0f-beb4-5df0011f04f0.png)\r\n

found, cool, thanks @PragmaTwice 

Merging...\r\n\r\nFWIW a discussion (#615) cannot be resolved. You may create an issue from the discussion the next time.

> Merging...\r\n> \r\n> FWIW a discussion (#615) cannot be resolved. You may create an issue from the discussion the next time.\r\n\r\nMy mistake, will open an issue next time :)

@ShooterIT So quickly.

So sorry for this mistake. 😢 

@PragmaTwice Aha, no sorry, we all also missed this issue.

There are some weird problem in the default xcode toolchains (i.e. apple clang):\r\n```\r\n$ echo "#include <assert.h>" | /Applications/Xcode_13.2.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ -c -\r\nclang: warning: treating \

Blocked by https://github.com/KvrocksLabs/lua/pull/3.

@PragmaTwice Seems ${CMAKE_CXX_COMPILER} will use the apple-clang and $CXX will use clang++, clang++ works good after updating the Lua. So we can replace `${CMAKE_CXX_COMPILER}` with `${CXX}` in lua.cmake?

> It is a common usage to specify `CMAKE_CXX_COMPILER` before build e.g. `cmake .. -DCMAKE_CXX_COMPILER=/some/path/some-g++`, so I think using `CXX` may lead to different compilers in one build. I think we can use `try_compile` to workaround it: if it is in osx and `try_compile(a code snippet with <assert.h>)` fail, we will use `CXX`, otherwise we use `CMAKE_CXX_COMPILER` to keep C++ compiler unchanged. These logics are easy to implement in cmake : )\r\n\r\ndone :tada:

cool, thanks to @PragmaTwice great work!

@PragmaTwice as https://github.com/KvrocksLabs/lua/pull/3 merged, is it still necessary to introduce such workaround? IIUC now it should be happy to build with `CMAKE_CXX_COMPILER`.

I think I figure out this problem.\r\nCMake deals with apple clang well because it automatically adds `-isysroot ${CMAKE_OSX_SYSROOT}` to compile options.\r\nBut Makefiles in lua do not do that, so `assert.h` cannot be found while using apple `clang++`.\r\nWe can just add it to `LUA_CFLAGS`!\r\n@git-hulk @tisonkun 

Sorry for many time to solve it, I do not have an Mac osx device so I use github actions in a temp repo to debug the build process.

Thanks for @PragmaTwice great investigation and contribution again. 👍 

This CI failure seems was related to issue: https://github.com/google/snappy/pull/128, will investigate why it only happens in current PR(maybe the GCC version was updated).

> This CI failure seems was related to issue: [google/snappy#128](https://github.com/google/snappy/pull/128), will investigate why it only happens in current PR(maybe the GCC version was updated).\r\n\r\nTry add `BUILD_SHARED_LIBS=OFF` to https://github.com/apache/incubator-kvrocks/blob/unstable/cmake/glog.cmake#L32, or set a global variable `set(BUILD_SHARED_LIBS OFF CACHE ...)`.\r\nSome options are changed in glog v0.6.0, which may affect the global configurations.\r\nThis error occurred only when shared libraries of snappy are built.

cool, thanks @PragmaTwice.

Thanks @ShooterIT @tisonkun @PragmaTwice, will merge this PR.

Is there some background of this change, e.g., a linked issue?

Thanks for @tisonkun remind, I can add the related issue to explain the background.

I wait this PR long time😂, @git-hulk yes, i agree, we already have thought `blob_file_size` uses byte unit but config file describe wrong unit, we should change it into byte unit.

> Perhaps `tests/config_test.cc` L74 should be updated from `256` to `268435456` also?\r\n\r\nYep, I updated.

Hi @WyattJia as #612 says, we also want to change `max_bytes_for_level_base` into `byte` unit instead of `MiB`, so do you also want to change it?

@ShooterIT I think it should be covered by #610. 

oh, yes, thanks @tisonkun  

@WyattJia Thanks for your contribution! Merging...

@WyattJia please also check again\r\n```\r\n  options->max_bytes_for_level_base = config_->RocksDB.max_bytes_for_level_base * MiB;\r\n```\r\nneed to remove `* MiB`\r\n

> @WyattJia please also check again\r\n> \r\n> ```\r\n>   options->max_bytes_for_level_base = config_->RocksDB.max_bytes_for_level_base * MiB;\r\n> ```\r\n> \r\n> need to remove `* MiB`\r\n\r\n@ShooterIT @tisonkun I have updated changes about unit and error config checker.Please check and review. Very thankful.

Thanks @WyattJia, merged.

Thanks @PragmaTwice 

> @PragmaTwice could you share the ASan report before and after this change?\r\n\r\nThe ASan report before this is about 6MiB, I will soon find a way to upload. Seems reach the size limit in GitHub?

[kvrocks-unittest-asan.log](https://github.com/apache/incubator-kvrocks/files/8760302/kvrocks-unittest-asan.log)\r\n[kvrocks-unittest-asan-after.log](https://github.com/apache/incubator-kvrocks/files/8760306/kvrocks-unittest-asan-after.log)\r\n\r\nSeems just some error due to my poor network, not limit exceeding. Done @tisonkun.\r\n

@PragmaTwice looks great! It seems that this PR fixes all leaks currently reported. We should fail if there are new leaks in #599.

> @PragmaTwice looks great! It seems that this PR fixes all leaks currently reported. We should fail if there are new leaks in #599.\r\n\r\nSeems there are also some leaks in TCL tests, I do not know the current status, maybe @git-hulk know that.\r\nIn #599, ASan is enabled in `daily-ci.yaml`, dont know whether it should be added to `kvrocks.yaml`.\r\n\r\nASan will slow down the program execution about x2 times, and TSan about x10 times.\r\nRefer to https://clang.llvm.org/docs/AddressSanitizer.html and https://clang.llvm.org/docs/ThreadSanitizer.html.

`./unittest` aborted while executed with AddressSanitizer:\r\n```\r\nSUMMARY: AddressSanitizer: 11268648 byte(s) leaked in 67853 allocation(s).\r\n```

Thanks @PragmaTwice Cool, I want to this commit too long time.\r\n\r\n> `./unittest` aborted while executed with AddressSanitizer:\r\n> \r\n> ```\r\n> SUMMARY: AddressSanitizer: 11268648 byte(s) leaked in 67853 allocation(s).\r\n> ```\r\n\r\nDid TCL tests report errors?\r\n

But a weird thing is that the failed redis-server reported by tcl test has an empty stderr file.

seems like some of leaks from unittest is due to missing the dtor call (i.e. delete operator) after `Storage::Open`

Copy that, thanks @PragmaTwice.

@PragmaTwice since #605 merged, is this PR ready for review (merge)?

> @PragmaTwice since #605 merged, is this PR ready for review (merge)?\r\n\r\nLeaks in unit tests are fixed, but these are still several leaks in TCL tests.\r\nI will investigate it.

Blocked by #614.

#614 is resolved now 🎉 

@PragmaTwice shall we merge master, running CI with nightly changes first, or current status is already ready for review?

> @PragmaTwice shall we merge master, running CI with nightly changes first, or current status is already ready for review?\r\n\r\nYeah, I think this PR is ready for review now~

@ShooterIT so far, sanitizers run daily and there will be failure reports in "Actions" page if daily CI failed.

> Truly thank you @PragmaTwice I waited too long time for this commit.\r\n> \r\n> But i still want to make sure these sanitizers work well, can they report errors if we leak memory or data race access designedly\r\n\r\nYeah, I think these sanitizers work well. \r\nActually we have fixed two memory leaks found by the LeakSanitizer (#605 and #614).\r\nMaybe later we can add more [matrix strategies](https://docs.github.com/en/actions/using-jobs/using-a-matrix-for-your-jobs) to CI so that both gcc and clang sanitizers can be used in tests.

Thanks @torwig, we can use `popMulti` to implement the `pop` as well? since I think pop is the special condition for popMulti.

```\r\nrocksdb::Status List::Pop(const Slice &user_key, bool left, std::string *elem) {\r\n  std::vector<std::string> elems;\r\n  auto s = PopMulti(user_key, left, 1, &elems);\r\n  if (!s.ok()) return s;\r\n  *elem = elems[0];\r\n  return rocksdb::Status::OK();\r\n}\r\n```\r\n\r\nI think this idea is better. WDYT @git-hulk 

cool, thanks @torwig 

> ```\r\n> rocksdb::Status List::Pop(const Slice &user_key, bool left, std::string *elem) {\r\n>   std::vector<std::string> elems;\r\n>   auto s = PopMulti(user_key, left, 1, &elems);\r\n>   if (!s.ok()) return s;\r\n>   *elem = elems[0];\r\n>   return rocksdb::Status::OK();\r\n> }\r\n> ```\r\n> \r\n> I think this idea is better. WDYT @git-hulk\r\n\r\nYes, I think that retrieves multi elements in `pop` is a bit weird, it would be better to use `popMulti` to implement `pop`.

@ShooterIT Can you help to merge this PR?

I first try to use `find_library` to find `libstdc++.a`, but in some environment (like github actions), cmake cannot find it via `find_library` but `gcc -static-libstdc++` actually works.\r\nSo I change the method to `try_compile`, the command will try to compile source files with some special compile&link options to test whether these options exist and work well.

cool, thanks @PragmaTwice contribution.

FWIW we may not use GitHub Actions to do the release work. At least the artifacts should go to Apache distribution file server.

I cannot reproduce the test error of github actions on my local, could you check it for whether it is about the PR or not? Thanks! @git-hulk 

Try it here: https://godbolt.org/z/8r48Mvz3W : )\r\n\r\nI will improve these sanitizer options in another PR.

Changed build type to `RelWithDebInfo`, and also fixed the typo. @git-hulk @ShooterIT 

The biggest problem we need to solve is how to compile these compression algorithms. 

Do guys think this change makes sense for Kvrocks?

The unit test failed in https://github.com/apache/incubator-kvrocks/runs/7003199495?check_suite_focus=true for 1ee66beb15407846cce3c9952dffd64ded2a3b98.\r\nJust to record for more investigate, I will try to rerun the CI.\r\n

> Cool! Is there any test coverage for lz4 compression option? Or do you verify it locally?\r\n\r\nI tested it on my side, run with LZ4 is ok and check the symbols:\r\n\r\n```shell\r\n000000000072a890 t LZ4HC_compress_generic_dictCtx\r\n0000000000725ec0 t LZ4HC_compress_generic_noDictCtx.part.0\r\n00000000007216f0 t LZ4HC_compress_optimal\r\n0000000000730790 T LZ4_attach_HC_dictionary\r\n0000000000714790 T LZ4_attach_dictionary\r\n0000000000721590 T LZ4_compress\r\n000000000070b9a0 T LZ4_compressBound\r\n0000000000730be0 T LZ4_compressHC\r\n0000000000730ec0 T LZ4_compressHC2\r\n00000000007319f0 T LZ4_compressHC2_continue\r\n...\r\n```

Thanks for @xiaobiaozhao contribution again!

@PragmaTwice can u have a look at this fix?

Thanks @PragmaTwice 

Thanks @PragmaTwice 👍 

Thanks for your review! This patch is trivial. Merging...

Thanks @torwig.\r\n\r\nNo worry, I can add test cases before merging.

@git-hulk Thank you. I merged your commit into my branch.

OK, thanks. Will have a look soon.

Good unit test coverage, thanks @torwig 👍 

one problem i need to confirm is that this command may not adapt slot migration, we should handle it in `batch_extractor.cc`\r\n\r\ncc @ChrisZMF 

> one problem i need to confirm is that this command may not adapt slot migration, we should handle it in `batch_extractor.cc`\r\n> \r\n> cc @ChrisZMF\r\n\r\n@ShooterIT Do you think is it a good idea to separate commands between Kvrocks and Kvrocks2Redis? Or we can do this in the next PR. 

@PragmaTwice we may still keep some targets in Makefile such as lints to running checker.\r\n\r\nIn a separated pass we can consider whether adopt Makefile to do such job or port them in scripts or other approaches.\r\n\r\nNOTICE: CI failed because of `make lint` missing a Makefile.

> @PragmaTwice we may still keep some targets in Makefile such as lints to running checker.\r\n> \r\n> In a separated pass we can consider whether adopt Makefile to do such job or port them in scripts or other approaches.\r\n> \r\n> NOTICE: CI failed because of `make lint` missing a Makefile.\r\n\r\nI think `lint` is the only target which is out of the responsibility of cmake (not sure, becuase actually it can be written into cmake), and it is quite simple, only call two scripts, `cppcheck.sh` and `cpplint.sh`.\r\n\r\nHence I consider to remove the target to delete all Makefiles to make sure these handwritten makefiles will not conflict with cmake-generated makefiles (i.e. call `cmake .` directly on the root directory of the project, which will overwrite current makefiles).

Merging...

@PragmaTwice thanks for your contribution!

Good catch

Thanks for the timely review, I will put forward any ideas in time.

Closed as conflict and no activities. @Hexiaoqiao you may open another PR to finish the task based on the nightly default branch.

After this PR, the cmake build process will not depend on any files in the `externel` directory. However, in order to make Makefiles still work, I will restore the `externel` directory (which is currently deleted in this pr).

@PragmaTwice so, how can I build kvrocks with cmake now? You may add a section on README also.

I try to run with:\r\n\r\n```bash\r\nmkdir cmake-build-debug ; cd cmake-build-debug\r\ncmake -DCMAKE_BUILD_TYPE=Release ..\r\n```\r\n\r\nwhich failed and complained:\r\n\r\n```\r\nCMake Error at /usr/local/lib/cmake/GTest/GTestTargets.cmake:37 (message):\r\n  Some (but not all) targets in this export set were already defined.\r\n\r\n  Targets Defined: GTest::gtest;GTest::gtest_main\r\n\r\n  Targets not yet defined: GTest::gmock;GTest::gmock_main\r\n\r\nCall Stack (most recent call first):\r\n  /usr/local/lib/cmake/GTest/GTestConfig.cmake:32 (include)\r\n  /usr/local/Cellar/cmake/3.23.1/share/cmake/Modules/FindGTest.cmake:194 (find_package)\r\n  cmake-build-debug/_deps/snappy-src/CMakeLists.txt:40 (find_package)\r\n```\r\n\r\nIt seems that current script make conflict with system-wise library?

> It seems that current script make conflict with system-wise library?\r\n\r\nSnappy use `find_package(gtest)` mechanism to locate gtest libs (in https://github.com/google/snappy/blob/b02bfa754ebf27921d8da3bd2517eab445b84ff9/CMakeLists.txt#L40).\r\nThe newer version of Snappy do not use `find_package` any more (it use `add_subdirectory` and submodules now), but for the current version, we probably should add `FindGTest.cmake` to tell Snappy we have already build our own gtest.\r\nBTW, we actually defined `SNAPPY_BUILD_TESTS=OFF`, so gtest is useless for Snappy (but in the current Snappy version we depend on, they just do not check the option before `find_package`, haha).\r\n

I feel that updating snappy from 1.1.7 to 1.1.9 is another good solution to this problem, which removes the need for us to add `FindGTest.cmake`. What do you think is better to do? @git-hulk 

@git-hulk @ShooterIT as stated in https://github.com/apache/incubator-kvrocks/pull/564#issuecomment-1121972637, we may proceed this patch to introduce a `FetchContent` based cmake build system. And remove Makefile build system in a separated PR.\r\n\r\ncc @PragmaTwice do I get it right?

Overall looks good, will take a new pass again and have a try on my side tonight.

I can lower the required cmake version to 3.11 (if it is confirmed that this version contains all the features used), if that helps with compatibility. 

> Yeah, I also think it would be good to remove makefiles after this PR.\r\n\r\nOK, thanks\r\n\r\n> I can lower the required cmake version to 3.11 (if it is confirmed that this version contains all the features used), if that helps with compatibility.\r\n\r\nIf cmake version 3.11 can work well, i vote 3.11

@ShooterIT for choosing a newer version, @PragmaTwice explain in this https://github.com/apache/incubator-kvrocks/pull/564#discussion_r869138940.

CMake 3.16.0 is released at Nov 26, 2019\r\nCMake 3.11.0 is released at Dec 01, 2018\r\n\r\nno much difference 😂

I just tested the build process in cmake 3.11, and an error occurred because a feature [here](https://cmake.org/cmake/help/latest/command/install.html) in cmake 3.13 is used.\r\n> New in version 3.13: [install(TARGETS)](https://cmake.org/cmake/help/latest/command/install.html#install-targets) can install targets that were created in other directories. When using such cross-directory install rules, running make install (or similar) from a subdirectory will not guarantee that targets from other directories are up-to-date. You can use [target_link_libraries()](https://cmake.org/cmake/help/latest/command/target_link_libraries.html#command:target_link_libraries) or [add_dependencies()](https://cmake.org/cmake/help/latest/command/add_dependencies.html#command:add_dependencies) to ensure that such out-of-directory targets are built before the subdirectory-specific install rules are run.\r\n\r\nAnd cmake 3.13 works well. I will change the required cmake version from 3.16 to 3.13.

@PragmaTwice Cool, Looks good to me. Also tested on my side on MacOSX and Debian.

@ShooterIT The required cmake version has been updated to 3.13. (refer to https://github.com/apache/incubator-kvrocks/pull/564#issuecomment-1123562228)

Thanks @PragmaTwice great work again.

@git-hulk Thanks for your review. And sorry to overwrite your update since I followed mail list only but not take care the latest commit. Anyway the latest commit fix issue mentioned above. Thanks.

Committed to dev branch. Thanks @git-hulk @tisonkun for your reviews.

@foxdalas Thanks for your contribution. No worry about the failure of CI tests, will find the root cause and fix it later.

@[git-hulk](https://github.com/git-hulk) Thanks. Maybe do you need ci for docker with many platforms? 

@git-hulk I built current head on my laptop. You can pull image `foxdalas/kvrocks:latest` `amd64/arm64.`\r\nEverything works in AWS EKS. 

@git-hulk please approve buildx actions.  

 @tisonkun @Hexiaoqiao approve action please 

@tisonkun @Hexiaoqiao one more run please

@git-hulk all checks have been passed. Please check your environment everything forks on ubuntu 22.04 in docker. 

Cool, thanks. Will have a try later.

@foxdalas @git-hulk please note that docker workflow takes about 197 minutes to complete. We may not run such workflow for every PR every activities.

@tisonkun I can add cross-compiling to project or we can build docker image on tag push. \r\nWhat do you think about release action with docker build and push ? 

@foxdalas We have built and pushed the docker image: [release.yaml#L80](https://github.com/apache/incubator-kvrocks/blob/unstable/.github/workflows/release.yaml#L80). I think we can add the cross compiling on the daily CI instead of every PR, HDYT? 

@git-hulk @tisonkun review please 

did you run daily CR? works well? how long did it take? since currently we only run  `kvrocks.yaml` CI when submitting a PR.

> did you run daily CR? works well? how long did it take? since currently we only run `kvrocks.yaml` CI when submitting a PR.\r\n\r\n197 minutes to complete for daily ci. Everything works on Graviton 2 in AWS.

cc @Hexiaoqiao @git-hulk 

cc @ShooterIT 

Hold as most tcl files are copied and modified from Redis files. We should take care of BSD-3 Clause License requirements.

Thanks. +1. 

cool, thanks @adulau 

Thanks! Merging...

cc @git-hulk @Hexiaoqiao 

On other branch, could we use `rebase` merge button?\r\nI used `rebase` merge  when I release new version, such as #468

@ShooterIT Need to add the License Header for new files

In #530, all contributors agree to change license to Apache 2.0

Cool, thanks for your great contribution!

@torwig Thanks for your great contribution again, we are planning to donate Kvrocks to ASF recently, so this PR would be pending before that. We also had filed the issue to let every contributor know and agree. For more detail can see: #530 

Thanks @torwig LGTM.\r\nAre you familiar with TCL? please add test cases if yes, if no, i will do that in the next PR.\r\n\r\n

@torwig fine, i merged, cheers!

@ChrisZMF you may separate the fix typo part into a dedicated patch and resolve the conflict so that we can continue reviewing this patch :)

Closed as inactive. You may resubmit the patch base on the latest unstable branch.

CI also failed 🙃

It passed 🙃 @ShooterIT 

Hi @ColinChamber #478  wishes to put log to stdout instead of stderr.

@ColinChamber Can use the RC version

OK, i notice new version will be released at April 1🥳

[google-glog 0.6.0-rc2](https://github.com/google/glog/releases/tag/v0.6.0-rc2). I need to modify the Makefile and CMakeLists.txt @git-hulk 

why we need to modify those files? 

The way glog is built has changed.

> I need to modify the Makefile and CMakeLists.txt \r\n\r\nKvrocks Makefile and CMakeLists.txt 

@ColinChamber glog 0.6 is released, you can push this forward 🥳

@ColinChamber Do you have time to finish this feature? If no, I can move on.

Will close this PR since we handled it in the new PR #613, thanks for @ColinChamber efforts.

Cool, can u help to add more information about the design of this fix.

On the way.

### How Kvrocks expire keys\r\nKeys are expired in only way: compaction.\r\nRocksDB provides a way to delete or modify key/value pairs based on custom logic in background by using custom compaction filter. Kvrocks implement MetadataFilter and SubkeyFilter to delete expired metadata and subkey in compaction.\r\n\r\n**MetadataFilter**:  input record considered expired by judging timestamp in value will be marked deleted\r\n**SubkeyFilter**:  input record are marked deleted under four conditions. 1) meta key is not found 2)  meta key is expired 3) type unmatch with meta key 4) version unmatch with meta key\r\n\r\n### How expires are handled in the replicas\r\n Unlike redis, the replicas expire keys independently.\r\n\r\n### Challenges and opportunities in existing mechanisms\r\n**Problem1**: The expration operation of subkey is not atomic so that some subkey may be lost while clients change expiration time during the period of compaction.\r\n\r\n**Problem2**: The design that replicas can expire keys independently by using compaction fliter is concise, But consistency is sacrificed. Compared with master, slaves may lose some keys forever.\r\n\r\nAbove all, master may lose data and eventual consistency can not be guaranted between replicas.\r\n\r\nProblem1 will be solved if SubkeyFilter is not dependent upon expiration time of meta key.\r\n\r\nFor problem2, what is expected ? In order to obtain a correct behavior without sacrificing consistency, when a key expires, a DEL operation is synthesized in both the WAL(AOF in Redis) file and gains all the attached replicas nodes. This way the expiration process is centralized in the master instance, and there is no chance of consistency errors. That is what Redis did.\r\n\r\n### Getting it to work\r\nFor problem1, a slight modification to SubkeyFilter is necessary. \r\n**new SubkeyFilter**: input record are marked deleted under three conditions. 1) meta key is not found 2) type unmatch with meta key 3) version unmatch with meta key\r\n\r\nFor problem2,  critical modification of MetadataFilter is that, when timestamp of input record shows it is timed out, lookup the latest value of the key from DB and append a delete operation to DB if the latest value shows the key is timed out. To prevent expiration time is modified during query, the key should be locked before query and be unlocked after appending deletion.\r\n\r\n### Make it better\r\nSubkeyfilter can be more efficient, subkey under three condition above will be simply dropped instead of output until the bottom level. It also reduces disk write. Refer to FilterV2 API https://github.com/facebook/rocksdb/wiki/Compaction-Filter.\r\n\r\nOtherwise, expire-deletion speed can be limited by max-expire-delete-mb. \r\n\r\n@git-hulk 

@shangxiaoxiong Cool, thanks for your detail explain.

@patpatbear Thanks for your contribution, cheers!

> LGTM, for `CheckResponseOnce`, I think we should return status instead of boolean since we need to propagate the status to callers. But it should not refactor in this PR, just for reminding.\r\n\r\nGot it, it should be refactored in the future PR.

@ShooterIT do you still need this draft? Is there an estimate to continue this patch?

Hi @ShooterIT , do you still have time to continue this PR?

CC @caipengbo 

I also think we should keep MB unit. and fix not multiplying `MB` when setting rocksdb config.\r\n

I think I should append some documentation about how to install compression libraries.

Yes, agreed. We should make sure it works after installing depend libraries.

Thanks for your hard work. @WyattJia 

@iushas Indeed, the tool is not perfect yet, would you be interested in contributing a PR?

Cool, thanks for your contribution.

@WyattJia I have a look at the PR, overall was good to me. But we need to fix those lint issues first:\r\n```\r\nsrc/config.cc:152:  Lines should be <= 120 characters long  [whitespace/line_length] [2]\r\nsrc/config.cc:153:  Lines should be <= 120 characters long  [whitespace/line_length] [2]\r\nsrc/config.cc:427:  Line ends in whitespace.  Consider deleting these extra spaces.  [whitespace/end_of_line] [4]\r\nsrc/config.cc:428:  Lines should be <= 120 characters long  [whitespace/line_length] [2]\r\n```

@WyattJia Cool, really thanks for your contribution and patient.

@ShooterIT Please take a look again

@WyattJia have a look?

Merged, thanks @patpatbear 

opened new PR to unstable branch https://github.com/KvrocksLabs/kvrocks/pull/496, closing this one.

@ColinChamber Yeah, the key order was easy to change, what i think was to seek out all fragments and apply the operator on those fragments without depending on the key order.

Do you mean `GetString` and then do the operation? That seems not a suitable method that will use too much memory and is inefficient when operating on **multiple keys**.

@ColinChamber RocksDB write batch would also cache those fragments on memory before writing indeed, so it should be not different.

> RocksDB write batch would also cache those fragments on memory before writing indeed.\r\n\r\nThat is the memory consumed by destkey. \r\nSeeking out all  fragments will increase the memory consumed by srckey.\r\n\r\n>* BITOP AND destkey srckey1 srckey2 srckey3 ... srckeyN\r\n>* BITOP OR destkey srckey1 srckey2 srckey3 ... srckeyN\r\n>* BITOP XOR destkey srckey1 srckey2 srckey3 ... srckeyN\r\n>* BITOP NOT destkey srckey\r\n\r\n

Yeah, we can apply those fragments one by one, so the number of memory was same as current solution.

It will depend on the subkey order. If we want to seek, then iterate one by one.

I have no advice on this PR. @ShooterIT @ChrisZMF 

> \r\n\r\nLGTM @ShooterIT 

FYI @ChrisZMF 

FYI @ColinChamber 

I did not reproduce by executing some simple commands in Redis-CLI. You just provided code for a new feature. Please give the execution steps. @shangxiaoxiong 

The new multi-get api works well in most scenarios which include your mentioned tests by redis-cli. But in some corner cases, it works not as expected. Tests in flush.tcl of my PR [444](https://github.com/KvrocksLabs/kvrocks/pull/444) may be one of the corner cases. The test case shows that the key deleted in master can be read out in slave. It should be noted that the test case has specific context. There may be a problem with my implementation of the new feature but possibility is very small in my view. The stronger evidence is that the new feature [444](https://github.com/KvrocksLabs/kvrocks/pull/444) works well with old multi-get api. @ColinChamber 

@shangxiaoxiong Did you find the root cause in `MultiGet` API?

Thanks @shangxiaoxiong . Just give me a second to reproduce it in `flush.tcl`

> @shangxiaoxiong Did you find the root cause in `MultiGet` API?\r\n\r\nIndeed, `MultiGet` API doesn\

Cool

The new feature of my PR [444](https://github.com/KvrocksLabs/kvrocks/pull/444) is part of the corner case and you just miss it. @ColinChamber 

![image](https://user-images.githubusercontent.com/52746580/151016991-25dc0ece-837b-4edc-95d0-cd625f8e1110.png)\r\n

Thank you for your serious reply. Your analysis is rigorous. There are many factors affecting the results. If separate RocksDB test case about the new multi get api is provided, it is easier to explain this problem. I will try to do that. @ColinChamber 

I have read your code, and it seems to be ok. Thank you for this interesting case. I am looking forward to your separated RocksDB test case. @shangxiaoxiong 

@shangxiaoxiong @ColinChamber is there any conclusion?

I want to release a new ASAP

Maybe you can wait a moment. I will submit `bitop` tomorrow. @ShooterIT 

@shangxiaoxiong @ColinChamber have a look https://github.com/facebook/rocksdb/pull/9453\r\n\r\n

closed, as describe above. reopen again if there are problems i missed.

Merged, thanks @ColinChamber 

TCL test script failed:  tests/unit/pubsub.tcl  

Thanks @guoxiangCN, for the pubsub tcl test case, we are still investigating the root cause. Can see the issue: https://github.com/KvrocksLabs/kvrocks/issues/448, no worry about this case.

> We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\r\n\r\nYes, this is our goal❤️

thanks, merged!

Redis with the right version should be installed to ensure that tcl tests pass. I failed on my mac. @git-hulk \r\nThere may be no corresponding installation package in some env.

Older redis-cli was failed. 

@git-hulk First step, i want to support compile, currently, macOS leaks tool reports some errors and i have no idea to resolve, so i just remove `test tcl` in macOS CI, we can add it into daily CI and resolve it gradually. WDYT?

cool, many thanks for @ColinChamber doing this improvement.

> cool, many thanks for @ColinChamber doing this improvement.\r\n\r\nThanks a lot, nothing can be done for this improvement without @smartlee 

> # Background\n> In Kvrocks, `bitmap` and `string` are two completely different types. Although `setbit`, `getbit`, `bitcount`, and `bitpos` operations on the string type are supported. @smartlee once mentioned that when using `setbit` on the string type, master-slave replication will have a tremendous amount of write amplification because Kvrocks replication does not use command propagation like Redis. In addition, using type `bitmap` conserves storage space. So we recommend using type bitmap. But some of our colleagues want to get the whole string after `setbit`, so we implement this interface.\n> # Plan\n> We have implemented the **`bitop`** command. If this commit is approved, the "Redis bitops.tcl"  can be wholly converted to "Kvrocks bitmap.tcl" when submitting bitop.\n> # QA\n> * Why we need to swap the bit.\n> If you `setbit bit 0 1`, the value is stored as `0x01` in Kvocks but `0x80` in Redis. So  swapping bits is to get the same return value as Redis.\n\n@git-hulk 

Got it, thanks @ColinChamber 

@ColinChamber  anything updated? did you forget to push commits?

Thanks for your guidance, too.

merged, truly like this idea, so now in most regular cases, users should not feel \r\n\r\n> bitmap and string are two completely different types.

@calvinxiao cool, can also help to fix the size in the metadata should be 4 bytes instead of 8 bytes.

l found a bug through test and l am locating

@shangxiaoxiong solved?

Happy New Year! Best wishes to the Kvrocks community!

@ColinChamber Thanks for your contribution again~

@git-hulk Otherwise, we also should use `MultiGet` to implement `xxxMGET` commands such as `hmget`

> Thanks @calvinxiao , cool job. I add some comments, please have a look.\r\n> \r\n> @git-hulk We should add memory and thread sanitizer to CI ASAP.\r\n\r\nyeah

Rebased, tests should pass.

Allow to run the CI with jemalloc, but I think we should only run it manually labelled with `run ci without jemalloc` like the current PR. HDYT @ShooterIT 

@ShooterIT We take the job into the single file since we want to run it when the PR was labelled, and other jobs should be always running.

Good Catch!

thanks @fishery86 currently code also is fine to me.\r\n\r\nBut there are remaining jobs after merging\r\n\r\n- [ ] Test kvrocks2redis (include  converting sortedint to zset) , and add tests into CI jobs\r\n- [ ] Update kvrocks2redis readme.md for converting sortedint to zset implicitly\r\n\r\n could you do these jobs? @fishery86  or @caipengbo 

@ChrisZMF  Please resolve conflicts （related to kvrocks2redis）

Hi @ChrisZMF I have merge `CLUSTER SETSLOT` PR, please resolve the conflicts and implement what we have reached an agreement!\r\n 

I am really glad this PR is accepted. Thank you for the suggestions you given for this PR to make it better. It really makes a lot of sense to me. @ShooterIT @git-hulk 

resolved in #431

lgtm, cool job!\r\n\r\nAfter merging this PR, i think we should support EXAT PXAT options for SET and CAS, since these options are frequently-used and friendly, WDYT? @caipengbo @git-hulk 

> After merging this PR, i think we should support EXAT PXAT options for SET and CAS, since these options are frequently-used and friendly, WDYT? \r\n\r\nYeah, we should be as compliant with Redis commands as possible.

@caipengbo Could update our wiki https://github.com/KvrocksLabs/kvrocks/wiki/Support-Commands for these new commands

oh, let me update

Can deal with part of the #283 

Cool. I have some suggestions, maybe it would be better to be more similar to the interface of [Tairhash](https://help.aliyun.com/document_detail/145970.html). Many people have already used it, which already been provided to a [client](https://github.com/aliyun/alibabacloud-tairjedis-sdk).

Thank you very much. I will take your advice seriously. @ColinChamber 

Meta data is mostly a point-query scenario, subkey is often a range query. Can we use only row cache in meta and only block cache in subkey?

ping @caipengbo 

> should we also show "io_stalls.total_slowdown" and "io_stalls.total_stop"\r\n\r\n@ShooterIT I don\

fine, both are ok to me

Really good feature, some users also look forward to the blob store. https://github.com/KvrocksLabs/kvrocks/issues/366

By the way, I will solve #384 

Please also do some surveys on` memtable_prefix_bloom_size_ratio` and `memtable_whole_key_filtering`.\r\ni think for metadata CF, there are most point lookup, it is a good idea to enable memtable filter, for subkey CF, it may not, sine users may execute `lrange` `hgetall` `zrangeXX` commands that use range lookup.

Summary of this PR\r\n- Support blob db (key-value separation) which requires RocksDB version greater than 6.18\r\n- Enable partitioned index/filter for SST and hash index for data block\r\n- Enable whole key filter for memtable\r\n- Remove dynamic resize block and SST\r\n- Only apply compression to SST which level is more than 2\r\n\r\n@caipengbo do i miss something?

close this for repeated #393 

WOW, many thanks for @singgel contribution.

great idea!\r\ncould you add some tcl tests?

I wanted to add some TCL test cases, but found that in Rocksdb, the statistic `NUMBER_KEYS_READ` is always 0.

> I wanted to add some TCL test cases, but found that in Rocksdb, the statistic `NUMBER_KEYS_READ` is always 0.\r\n\r\nNUMBER_KEYS_READ would increase after getting a value from rocksdb, see the [db_impl.cc](https://github.com/facebook/rocksdb/blob/29102641ddf4632bddf34bbd35de9637ea526dee/db/db_impl/db_impl.cc#L1896) for more details.

> NUMBER_KEYS_READ would increase after getting a value from rocksdb, see the db_impl.cc for more details.\r\nYeah, I read the details, but test it still 0.

Because `MultiGet` is used in Kvrocks, its corresponding statistic is `NUMBER_MULTIGET_KEYS_READ`.\r\nI add it together with `NUMBER_KEYS_READ` to measure the read load on Rocksdb.\r\n\r\nIn addition, I add some tcl tests.\r\n

why did you close it? want to update something?

> why did you close it? want to update something?\r\n\r\nSorry for the wrong operation just now. Please reopen it

The word `logo` is spelt incorrectly in the title.

> The word `logo` is spelt incorrectly in the title.\r\n\r\nCan fix the message before merging

Thanks @nioshield merged

The reason about UT failed is that I forgot to call `PopulateCommands` before calling `Redis::IsCommandExists()`, causing `original_commands` to be empty.

like cleaning up

Tests will be added in the next PR.

cool,  really speedy.

> A lot of comments, maybe it makes no sense, please check i think we should add more comments for some special operations, otherwise, it is hard to understand.\r\n\r\nmost advice totally made sense and was great, I have fixed them but some comments. You can have a look again.

wow, really a good feature.

 Still 10,000 feet to achieve the whole functions of the Redis scripts, but at least the `eval` command works now. We would implement other scripting commands after this PR was merged.

currently, i wish to add LUA memory usage in INFO command, since lua eats too much memory but we have no idea to know, it is hard to find memory problem

> currently, i wish to add LUA memory usage in INFO command, since lua eats too much memory but we have no idea to know, it is hard to find memory problem\r\n\r\naha, has already added, you can have a look at the last commit again

So the replica would be failed after restarting the server if we rewrite the config file, right?

> So the replica would be failed after restarting the server if we rewrite the config file, right?\r\n\r\nYes, and other clients would see wrong master port, for sentinel, it will always try to fix replication config but fails\r\n

After this PR, maybe this card is not urgent, https://github.com/KvrocksLabs/kvrocks/projects/1#card-62672516

> After this PR, maybe this card is not urgent, https://github.com/KvrocksLabs/kvrocks/projects/1#card-62672516\r\n\r\nyes

We still need it, because it can check indent, tab width, max line length...\r\nAs you said, `#include <glog/logging.h>` instead of `#include "glog/logging.h"` make it hard to distinguish third party lib with c system lib, maybe it is the problem of kvrocks.

> We still need it, because it can check indent, tab width, max line length...\r\n> As you said, `#include <glog/logging.h>` instead of `#include "glog/logging.h"` make it hard to distinguish third party lib with c system lib, maybe it is the problem of kvrocks.\r\n\r\nYes, it\

@git-hulk @ShooterIT Do you guys remember how this atomicity was broken? Is there any example?\r\n\r\nseem the change casue this issue:\r\n```\r\n127.0.0.1:6379> flushall\r\nOK\r\n127.0.0.1:6379> msetnx k v1 k v2\r\n(integer) 1\r\n127.0.0.1:6379> get k\r\n"v2\r\n\r\n127.0.0.1:6666> flushall\r\nOK\r\n127.0.0.1:6666> msetnx k v1 k v2\r\n(integer) 0\r\n127.0.0.1:6666> get k\r\n"v1"\r\n```\r\n\r\nRedis allow we overriding the same key but kvrocks will fail in this case

Hi @enjoy-binbin \r\n\r\n> Is there any example?\r\n\r\nIt should be a bit hard to reproduce, but we did exist since the MSET would write key-value one by one, so it will be partially written the DB if Kvrocks exited in the middle way. The inconsistent behavior you mentioned should be also affected by this implementation.

we already checked whether the key exists before:\r\n```\r\n  for (StringPair pair : pairs) {\r\n    keys.emplace_back(pair.key);\r\n  }\r\n  if (Exists(keys, &exists).ok() && exists > 0) {\r\n    return rocksdb::Status::OK();\r\n  }\r\n```\r\n\r\nso the exist in for loop looks a bit redundant, in which case we need this exist (the for loop one, i.e. this PR diff)?\r\nand another question is do we want to fix this inconsistency?\r\n

> we already checked whether the key exists before:\r\n> \r\n> ```\r\n>   for (StringPair pair : pairs) {\r\n>     keys.emplace_back(pair.key);\r\n>   }\r\n>   if (Exists(keys, &exists).ok() && exists > 0) {\r\n>     return rocksdb::Status::OK();\r\n>   }\r\n> ```\r\n> \r\n> so the exist in for loop looks a bit redundant, in which case we need this exist (the for loop one, i.e. this PR diff)? and another question is do we want to fix this inconsistency?\r\n\r\nYes, we should use MGet and MSet at once instead of the loop writes.

you mean i remove the `for (StringPair pair : pairs) {` loop, and then just call a `MSet`?\r\nsomething like this:\r\n```diff\r\n-  std::string ns_key;\r\n-  for (StringPair pair : pairs) {\r\n-    AppendNamespacePrefix(pair.key, &ns_key);\r\n-    LockGuard guard(storage_->GetLockManager(), ns_key);\r\n-    if (Exists({pair.key}, &exists).ok() && exists == 1) {\r\n-      return rocksdb::Status::OK();\r\n-    }\r\n-    std::string bytes;\r\n-    Metadata metadata(kRedisString, false);\r\n-    metadata.expire = expire;\r\n-    metadata.Encode(&bytes);\r\n-    bytes.append(pair.value.data(), pair.value.size());\r\n-    auto batch = storage_->GetWriteBatchBase();\r\n-    WriteBatchLogData log_data(kRedisString);\r\n-    batch->PutLogData(log_data.Encode());\r\n-    batch->Put(metadata_cf_handle_, ns_key, bytes);\r\n-    auto s = storage_->Write(storage_->DefaultWriteOptions(), batch->GetWriteBatch());\r\n-    if (!s.ok()) return s;\r\n-  }\r\n+  rocksdb::Status s = MSet(pairs, ttl);\r\n+  if (!s.ok()) return s;\r\n+\r\n```

good catch @smartlee

@Mergifyio backport 1.3\r\n\r\n

so Redis updates the last interaction time after writing to the client socket buffer?

yep

Yes, there is no a easy way, maybe enable libevent write event and set appropriate water level

okay

cool, thanks for your contribution. Can u help to fix issues found by cpplint?\r\n\r\n```\r\nsrc/redis_db.cc:468:  Extra space for operator  ++;  [whitespace/operators] [4]\r\nsrc/redis_hash.cc:178:  Add #include <utility> for move  [build/include_what_you_use] [4]\r\n```

cpplint error https://github.com/KvrocksLabs/kvrocks/runs/3101400846#step:4:721 please add necessary header files. 

It is great not to link `unwind` if it is not existed, i also find this problem, cool job 💯 , from this point, i want to merge this commit into 2.0.2. But in this commit, you also changed some others. so should we open another pr to resolve `unwind` warnings?

thanks, merged

so i may ask you to add tcl test cases, the list must have not less than 20 items to `trim` some range in test cases

yeah, we should clear the buf before using it\r\n\r\n

@ChrisZMF  Can you help to fix the trim test case as well:\r\n\r\nhttps://github.com/KvrocksLabs/kvrocks/blob/11ea1f6b4862d6ab05bce9abfda2576900ea1b5d/tests/t_list_test.cc#L104

oh, you are right, one index is 8 bit, there is no possibility to have so many items in one list. i changed title and comments.

@ChrisZMF any thought for above comments

Really cool!! Have tested in ARM MacOSX?

I am rich man, I have newest MacBook Pro (Apple Silicon M1 inside) 😎

@git-hulk That is a good question, I still consider that, we must should know the nodes may keep old cluster topology if restarted. That may cause terrible things  if the IP and port are reused.

yup, use ip:port as the unique identifier would cause problems, the node id would be better if we also persist it.

Just as we discussed, i record one card https://github.com/bitleak/kvrocks/projects/1#card-62672516, we should do that ASAP @bitleak/kvrocks-contributors 

Thank you @git-hulk , I will modify those problems and update the contents based on latest source code.

Many thanks for @smartlee finding this issue

You may aslo need to fix `PUBLISH/PSUBSCRIBE after PUNSUBSCRIBE without arguments` test\r\n\r\n> `punsubscribe $rd1`

@shangxiaoxiong Maybe you should add a new rocksdb CF to identify cluster mode and check if it is safe to enable/disable cluster mode in config file. WDYT?

Good suggestion！l will do it right away. @ShooterIT 

Comparing with multi-db of Redis,  I prefer that multi-namespace is allowed whether slot-id-encoded is enabled or not but not allowed when cluster mode is enabled. 

I think maybe w can add a special column family named `server`, and special information about server, such as `slot-id-encoded`, `cluster-enabled`, in the future, cluster topology also could be persistent in this CF.

@shangxiaoxiong The remaining work\r\n- kvrocks2redis support `slot-id-encoded`, and you also should make sure all keys belonging to one slot are stored together.\r\n- `slot-id-encoded` information are stored in a special CF after we support cluster mode

> @shangxiaoxiong The remaining work\r\n> \r\n> * kvrocks2redis support `slot-id-encoded`, and you also should make sure all keys belonging to one slot are stored together.\r\n> * `slot-id-encoded` information are stored in a special CF after we support cluster mode\r\n\r\nStrictly speaking, make sure all keys belonging to one slot of **a namespace** are stored together.

@shangxiaoxiong Finally, merged, another thing i find is that we should tell users our cursor of scan is not a number instead of a string that has a special prefix `-`.

auto resize block_size will be added in another PR (rocksdb 6.18.0 add this feature https://github.com/facebook/rocksdb/pull/7936)

@Mergifyio backport 1.3

**Command `backport 1.3`: success**\n> **Backports have been created**\n> * [#295 Support the auto-resize-block-and-sst config directive (backport #289)](https://github.com/bitleak/kvrocks/pull/295) has been created for branch `1.3`\n

Very cool for finding out such a corner case in replication. I have a question that the master should transmit the TCP `fin` packet to the slave after closing the connection, and the slave would get an `EOF` if it reads the fd(write operation would get the broken pipe error), right?

> Very cool for finding out such a corner case in replication. I have a question that the master should transmit the TCP `fin` packet to the slave after closing the connection, and the slave would get an `EOF` if it reads the fd(write operation would get the broken pipe error), right?\r\n\r\nI think you are right !

This commit is a esay way to mitigate this problem. \r\nBut there also is a bad case: master tcp still is alive but master hang much time, replicas should disconnect with master, let us know that. I remember, for redis, replicas will disconnect if it is too long time without recieving master replication stream.

I think this PR would fix the issue when the master server went away suddenly before the kernel sent out the fin packet. EOF and Error should have checked now.

@git-hulk oh, thanks, i missed this, let me fix

It may affect the performance since we use the read lock when executing commands, right? We can have a simple benchmark before merging.

For me, it is ok, actually, I also want to remove it, but just one concern, old version maybe need this tool?

> For me, it is ok, actually, I also want to remove it, but just one concern, old version maybe need this tool?\r\n\r\n1.3 branch has this tool

@karelrooted And could you supplement some tcl tests

> @karelrooted And could you supplement some tcl tests\r\n\r\nadd a rename one command testcase,  the current code base will not allow a multi rename-command directive,  so rename-two-command  testcase is not included

Maybe this way should be default way in CI, or one job

cool~ @bitleak/kvrocks-core-team 

src/semisync_master.cc && src/semisync_master.h is needed in CMakeLists.txt target_sources 

@popunit truly thanks for your PR, but so big PR, it is hard to review, could you mind write your design and implementation details? 

> @popunit truly thanks for your PR, but so big PR, it is hard to review, could you mind write your design and implementation details?\r\n\r\nsorry, my fault to close just now. i reopen again. @ShooterIT ok, i have one but without details,  i need to take some time to complete. 

update pr

Really cool!!!

cool, i always want it 

https://github.com/bitleak/kvrocks/runs/2584412916?check_suite_focus=true#step:4:1795\r\nhi @git-hulk do you know why

it works fine after updating to this commit: https://github.com/git-hulk/kvrocks/actions/runs/842629626

WoW, I love this PR!

> So we can try on macOS\r\n\r\nmac os artifacts is not being builded in this PR, add on this PR or wait for next PR ? 

yeah, I would submit another PR to support home brew

@Mergifyio backport 1.3

**Command `backport 1.3`: success**\n> **Backports have been created**\n> * [#266 Fix uninitialized compiling warnings with localtime_r (backport #261)](https://github.com/bitleak/kvrocks/pull/266) has been created for branch `1.3`\n

cool, do you mean that got a stale manifest number when adding test cases?

Yes, CURRENT file points out a wrong MANIFEST file\r\n\r\n> W0512 16:00:19.617869 77352 storage.cc:388] [storage] Fail to open master checkpoint, error: IO error: No such file or directoryWhile opening a file for sequentially reading: ./tests/tmp/server.76661.3/db/MANIFEST-000008: No such file or directory

@Mergifyio backport 1.3

I changed the title since it is truly a bug not only typo

Result of this PR:\r\n\r\n1. No more warning from tsan when starting up the server\r\n2. No more warning from tsan when `make test`\r\n3. All tests passed.\r\n4. Only one warning left\r\n```\r\n==================\r\nWARNING: ThreadSanitizer: signal-unsafe call inside of a signal (pid=24508)\r\n    #0 operator new(unsigned long) <null> (libtsan.so.0+0x8c012)\r\n    #1 google::LogMessage::Init(char const*, int, int, void (google::LogMessage::*)()) <null> (kvrocks+0x2359cb)\r\n    #2 std::function<void ()>::operator()() const /usr/include/c++/9/bits/std_function.h:688 (kvrocks+0x23298d)\r\n    #3 signal_handler /home/eagle/github/my/kvrocks/src/main.cc:38 (kvrocks+0x23298d)\r\n    #4 <null> <null> (libtsan.so.0+0x2c4e3)\r\n    #5 std::thread::join() <null> (libstdc++.so.6+0xd6fe6)\r\n    #6 Server::Join() /home/eagle/github/my/kvrocks/src/server.cc:118 (kvrocks+0x1dd18f)\r\n    #7 main /home/eagle/github/my/kvrocks/src/main.cc:318 (kvrocks+0x94882)\r\n\r\nSUMMARY: ThreadSanitizer: signal-unsafe call inside of a signal (/lib/x86_64-linux-gnu/libtsan.so.0+0x8c012) in operator new(unsigned long)\r\n==================\r\n```

Good catch, many thanks to @calvinxiao. maybe we should integrate some sanitizers into CI to help us find out those issues.

@git-hulk  Looking at all the usage of `LockGuard`, I found `ZSet::RangeByScore` and `ZSet::Range` have the same conditional lock problem.

> @git-hulk Looking at all the usage of `LockGuard`, I found `ZSet::RangeByScore` and `ZSet::Range` have the same conditional lock problem.\r\n\r\nnice! also fixed that too

> > nice! also fixed that too\r\n> \r\n> I saw you fixed them in #256 , why not use `unique_ptr`?\r\n\r\nWe lock/unlock manually seems fine within few return branches 

Both were ok for me, I’ll glad to close  it if you fix those issues in this PR.

They are not only warnings of compiler but bugs

@Mergifyio backport 1.3

**Command `backport 1.3`: success**\n> **Backports have been created**\n> * [#258 Fix data race for accessing database in some commands (backport #253)](https://github.com/bitleak/kvrocks/pull/258) has been created for branch `1.3`\n

LGTM, not read. Could you add some tcl tests for that

@Mergifyio backport 1.3

**Command `backport 1.3`: success**\n> **Backports have been created**\n> * [#259 Fix condition race for ReclaimOldDBPtr (backport #246)](https://github.com/bitleak/kvrocks/pull/259) has been created for branch `1.3`\n

do we need a new way to implement pub/sub without rely on wal replication ?   the current solution write to db  cause  wal seq to increase (have some feedback from user that it sometimes cause a fullsync when a redis-sentinel failover is trigged, cause by "publish __sentinel:hello" message in new slave)

It seems fine that don’t write the WAL when the role was slave. But it would be better to propagate it without writing to WAL.

I feel ok for current solution. But i have a question, why we persist publish messages？

We want to use the WAL to help us propagating the publish command like others.

@Mergifyio backport 1.3

@Mergifyio backport 2.0

@Mergifyio backport 1.3

**Command `backport 1.3`: success**\n> **Backports have been created**\n> * [#237 Fix uninitialized Rocksdb.level0_stop_writes_trigger (backport #236)](https://github.com/bitleak/kvrocks/pull/237) has been created for branch `1.3`\n

@Mergifyio backport 1.3

**Command `backport 1.3`: failure**\n> **No backport have been created**\n> GitHub App like Mergify are not allowed to create pull request where `.github/workflows` is changed.\n

@Mergifyio refresh

**Command `refresh`: success**\n> **Pull request refreshed**\n> \n

@Mergifyio refresh

**Command `refresh`: success**\n> **Pull request refreshed**\n> \n

related rocksdb issue: https://github.com/facebook/rocksdb/issues/8217

@Mergifyio backport 1.3

Successly compile on Docker centos 8.2 ( gcc 8.3.1), Docker ubuntu 20.04 (gcc  9.3.0), Mac OS 10.15.7 (cmake)

cool, so now rocksdb is latest release

Many thanks to @karelrooted try hard to fix this hidden deep issue.

@karelrooted I think we need to port back to 1.3 branch

> @karelrooted I think we need to port back to 1.3 branch\r\n\r\n👌

 Awesome speed, many thanks to @karelrooted !

Amazing 😲 

@git-hulk Is it necessary to back port to 1.3 version? it looks like just a rare case, right?

close #198 

@git-hulk OK, i will optimize it later

awesome!!!

Hi @git-hulk i think you can merge firstly, and i will change my PR to resolve conflict.

LGTM 

centos 应该需要 epel 源才有 gtest, 不过不影响先合并

我本地为了安装cmake3，确实有安装了epel源

LGTM, thanks @clyang 

LGTM, nice~

cool,  thanks for your excellent work! @popunit\r\n\r\nI would merge this PR after fixing warnings reported by cpplint

i fixed the code style issue @git-hulk 

cc @karelrooted 

LGTM

Just curious, are you adding new commands which are not part of the official Redis core? 

@adulau yeah,  we would add some new useful commands which not part of the Redis core.

@smartlee 所有写入的地方需要生成版本，比如 Hash::IncrBy,  之前所有Metadata的实例化直接调用了generateVersion （即使是读的不需要的地方也合成了版本）\r\n

@karelrooted 所以是提升了读的性能？这个优化会提升多少呢

@smartlee 对, compact_filter里面会循环处理每个key，每个key实例化一次metadata，之前每次合成版本比较耗cpu时间，优化了后compact的时间会变短（实验数据 10m hash key (one field) 大概会快1/3）

@karelrooted 赞 谢谢

![image](https://user-images.githubusercontent.com/1262498/77626032-a4860e00-6f7f-11ea-8d54-dc63a1ff67d0.png)\r\n每10分钟有个CPU的波峰 这个CPU怎么可以降低？ @karelrooted 

@smartlee 你看看日志是不是因为 compaction 导致的，如果是的话可以通过 config 在线关闭 auto compaction 功能，依赖 compaction_cron 每天做一次 compaction 就好了。

@git-hulk  现象上看，是否关闭 auto compaction 好像没影响，看日志也是隔几分钟就compaction一次，而compaction会非常耗CPU，下面这个监控是关闭compaction的\r\n![image](https://user-images.githubusercontent.com/1262498/78759348-10ba3600-79b2-11ea-9ba0-4781fd81c8c0.png)\r\n

@smartlee 你是如何关闭的，最新的版本才开始支持 disable_auto_compactions 参数(在线开关)

@smartlee 你是如何关闭的，最新的版本才开始支持 disable_auto_compactions 参数(在线开关)

@git-hulk \r\n\x08使用了你们最新的版本 开始就在配置文件里面置为yes的\r\n![image](https://user-images.githubusercontent.com/1262498/78858280-8a0f6280-7a5e-11ea-8564-cb91913b2140.png)\r\n

@smartlee 没有问题，CPU usage 监控有区分 sys/usr 使用比例吗，看看是什么线程占用的？

LGTM

LGTM

LGTM

LGTM

LGTM