Discussions: [{'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000717, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE3', 'repository_id': 33884891, 'emoji': ':bulb:', 'name': 'Ideas', 'description': 'Share ideas for new features', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'ideas', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/33526', 'id': 5536137, 'node_id': 'MDEwOkRpc2N1c3Npb241NTM2MTM3', 'number': 33526, 'title': 'Add support for Microsoft Azure Blob Storage in Google Cloud Storage Transfer Service operators', 'user': {'login': 'mik-laj', 'id': 12058428, 'node_id': 'MDQ6VXNlcjEyMDU4NDI4', 'avatar_url': 'https://avatars.githubusercontent.com/u/12058428?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/mik-laj', 'html_url': 'https://github.com/mik-laj', 'followers_url': 'https://api.github.com/users/mik-laj/followers', 'following_url': 'https://api.github.com/users/mik-laj/following{/other_user}', 'gists_url': 'https://api.github.com/users/mik-laj/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/mik-laj/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/mik-laj/subscriptions', 'organizations_url': 'https://api.github.com/users/mik-laj/orgs', 'repos_url': 'https://api.github.com/users/mik-laj/repos', 'events_url': 'https://api.github.com/users/mik-laj/events{/privacy}', 'received_events_url': 'https://api.github.com/users/mik-laj/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1463392518, 'node_id': 'MDU6TGFiZWwxNDYzMzkyNTE4', 'url': 'https://api.github.com/repos/apache/airflow/labels/provider:google', 'name': 'provider:google', 'color': 'bfd4f2', 'default': False, 'description': 'Google (including GCP) related issues'}, {'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}, {'id': 1970155342, 'node_id': 'MDU6TGFiZWwxOTcwMTU1MzQy', 'url': 'https://api.github.com/repos/apache/airflow/labels/good%20first%20issue', 'name': 'good first issue', 'color': '6cdb53', 'default': True, 'description': ''}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 16, 'created_at': '2020-04-13T23:05:12Z', 'updated_at': '2023-08-19T10:23:47Z', 'author_association': 'MEMBER', 'active_lock_reason': None, 'body': "**Description**\r\n\r\nHello,\r\n\r\nWe should add support for Microsoft Azure Blob Storage in [CloudDataTransferServiceCreateJobOperator](https://github.com/apache/airflow/blob/42eef38/airflow/providers/google/cloud/operators/cloud_storage_transfer_service.py#L157). This support should allow pass credentials to Azure from the connection. A similar feature already exists for AWS.\r\nhttps://github.com/apache/airflow/blob/42eef38/airflow/providers/google/cloud/operators/cloud_storage_transfer_service.py#L48-L57\r\nThis issue is due to [the March 23 service update](https://cloud.google.com/storage-transfer/docs/release-notes#March_23_2020).\r\n\r\nTo complete this task you must complete the following steps:\r\n- [ ] Update the operator\r\n- [ ] Update the example DAG\r\n- [ ] Update the documentation\r\n- [ ] Run system tests (final check)\r\n\r\nIf you haven't used the GCP yet, after creating the account you will [get $300](https://cloud.google.com/free), which will allow you to get to know these services better.\r\n\r\nThe implementation of this task will allow a better understanding of GCP services, as well as learn methods of testing that is required by the community. If anyone is interested in this task, I am willing to provide all the necessary tips and information.\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A", 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/33526/reactions', 'total_count': 2, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 2, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/33526/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000715, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE1', 'repository_id': 33884891, 'emoji': ':hash:', 'name': 'General', 'description': "Chat that doesn't fit anywhere else", 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'general', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/24385', 'id': 4135327, 'node_id': 'MDEwOkRpc2N1c3Npb240MTM1MzI3', 'number': 24385, 'title': 'MLFlow Operator', 'user': {'login': 'felipemoz', 'id': 60521463, 'node_id': 'MDQ6VXNlcjYwNTIxNDYz', 'avatar_url': 'https://avatars.githubusercontent.com/u/60521463?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/felipemoz', 'html_url': 'https://github.com/felipemoz', 'followers_url': 'https://api.github.com/users/felipemoz/followers', 'following_url': 'https://api.github.com/users/felipemoz/following{/other_user}', 'gists_url': 'https://api.github.com/users/felipemoz/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/felipemoz/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/felipemoz/subscriptions', 'organizations_url': 'https://api.github.com/users/felipemoz/orgs', 'repos_url': 'https://api.github.com/users/felipemoz/repos', 'events_url': 'https://api.github.com/users/felipemoz/events{/privacy}', 'received_events_url': 'https://api.github.com/users/felipemoz/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1697155831, 'node_id': 'MDU6TGFiZWwxNjk3MTU1ODMx', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:providers', 'name': 'area:providers', 'color': 'd4c5f9', 'default': False, 'description': ''}, {'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}, {'id': 1970155342, 'node_id': 'MDU6TGFiZWwxOTcwMTU1MzQy', 'url': 'https://api.github.com/repos/apache/airflow/labels/good%20first%20issue', 'name': 'good first issue', 'color': '6cdb53', 'default': True, 'description': ''}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 17, 'created_at': '2020-04-24T15:10:49Z', 'updated_at': '2023-06-01T07:34:44Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '\r\n**Description**\r\nHelping MLOps team to tigger ML jobs after Airflow steps (new operator)\r\n\r\n**Use case / motivation**\r\nYou can trigger mlflow jobs like sagemaker operator\r\n', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/24385/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/24385/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000717, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE3', 'repository_id': 33884891, 'emoji': ':bulb:', 'name': 'Ideas', 'description': 'Share ideas for new features', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'ideas', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/30790', 'id': 5114578, 'node_id': 'MDEwOkRpc2N1c3Npb241MTE0NTc4', 'number': 30790, 'title': 'Integrate Google Analytics Reporting API from plugin', 'user': {'login': 'dinigo', 'id': 3038962, 'node_id': 'MDQ6VXNlcjMwMzg5NjI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3038962?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dinigo', 'html_url': 'https://github.com/dinigo', 'followers_url': 'https://api.github.com/users/dinigo/followers', 'following_url': 'https://api.github.com/users/dinigo/following{/other_user}', 'gists_url': 'https://api.github.com/users/dinigo/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dinigo/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dinigo/subscriptions', 'organizations_url': 'https://api.github.com/users/dinigo/orgs', 'repos_url': 'https://api.github.com/users/dinigo/repos', 'events_url': 'https://api.github.com/users/dinigo/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dinigo/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1463392518, 'node_id': 'MDU6TGFiZWwxNDYzMzkyNTE4', 'url': 'https://api.github.com/repos/apache/airflow/labels/provider:google', 'name': 'provider:google', 'color': 'bfd4f2', 'default': False, 'description': 'Google (including GCP) related issues'}, {'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}, {'id': 1970155342, 'node_id': 'MDU6TGFiZWwxOTcwMTU1MzQy', 'url': 'https://api.github.com/repos/apache/airflow/labels/good%20first%20issue', 'name': 'good first issue', 'color': '6cdb53', 'default': True, 'description': ''}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 4, 'created_at': '2020-04-27T12:19:00Z', 'updated_at': '2023-04-21T11:15:46Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'body': "Short: Integrate airflow [plugin](https://github.com/airflow-plugins/google_analytics_plugin) , notified in this [issue](https://github.com/airflow-plugins/google_analytics_plugin/issues/18) on their issue tracker.\r\n\r\nBefore the official (beta for now) operators for Google Analytics landed in the official airflow repo a number of custom plugins appeared in the community. This repo gathers a couple of them, quite spread.\r\n\r\nWho's responsibility should it be to merge this?\r\nCan we simply copy-paste the code? I think it would be better to keep the commit-tree but not sure how it applies to airflow\r\nIs Google working in this issue on it's own or does this belong to the airflow community?", 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/30790/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/30790/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000716, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE2', 'repository_id': 33884891, 'emoji': ':pray:', 'name': 'Q&A', 'description': 'Ask the community for help', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'q-a', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/18825', 'id': 3614173, 'node_id': 'MDEwOkRpc2N1c3Npb24zNjE0MTcz', 'number': 18825, 'title': 'JDBCOperator with SSH Tunnel option', 'user': {'login': 'jvaesteves', 'id': 32674762, 'node_id': 'MDQ6VXNlcjMyNjc0NzYy', 'avatar_url': 'https://avatars.githubusercontent.com/u/32674762?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jvaesteves', 'html_url': 'https://github.com/jvaesteves', 'followers_url': 'https://api.github.com/users/jvaesteves/followers', 'following_url': 'https://api.github.com/users/jvaesteves/following{/other_user}', 'gists_url': 'https://api.github.com/users/jvaesteves/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jvaesteves/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jvaesteves/subscriptions', 'organizations_url': 'https://api.github.com/users/jvaesteves/orgs', 'repos_url': 'https://api.github.com/users/jvaesteves/repos', 'events_url': 'https://api.github.com/users/jvaesteves/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jvaesteves/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 0, 'created_at': '2020-04-28T18:35:16Z', 'updated_at': '2022-09-02T13:01:33Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hello, I am using the latest version of Airflow (1.10.10), and I want to user JDBCOperator to connect to a Oracle database when get some keys for the next step, but to access the machine, I need first to SSH to a jumpbox, but the operator documentation does not give any information of how can I achieve it, or if its even possible. Can someone help me here?\r\n\r\n', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/18825/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/18825/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000715, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE1', 'repository_id': 33884891, 'emoji': ':hash:', 'name': 'General', 'description': "Chat that doesn't fit anywhere else", 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'general', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/21366', 'id': 3858382, 'node_id': 'MDEwOkRpc2N1c3Npb24zODU4Mzgy', 'number': 21366, 'title': 'subdag is not able to correctly render {{ next_ds }} jinja template when start_date uses timezone info.', 'user': {'login': 'RajkotiyaDivyesh', 'id': 59406870, 'node_id': 'MDQ6VXNlcjU5NDA2ODcw', 'avatar_url': 'https://avatars.githubusercontent.com/u/59406870?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/RajkotiyaDivyesh', 'html_url': 'https://github.com/RajkotiyaDivyesh', 'followers_url': 'https://api.github.com/users/RajkotiyaDivyesh/followers', 'following_url': 'https://api.github.com/users/RajkotiyaDivyesh/following{/other_user}', 'gists_url': 'https://api.github.com/users/RajkotiyaDivyesh/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/RajkotiyaDivyesh/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/RajkotiyaDivyesh/subscriptions', 'organizations_url': 'https://api.github.com/users/RajkotiyaDivyesh/orgs', 'repos_url': 'https://api.github.com/users/RajkotiyaDivyesh/repos', 'events_url': 'https://api.github.com/users/RajkotiyaDivyesh/events{/privacy}', 'received_events_url': 'https://api.github.com/users/RajkotiyaDivyesh/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 199168283, 'node_id': 'MDU6TGFiZWwxOTkxNjgyODM=', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:bug', 'name': 'kind:bug', 'color': 'fc2929', 'default': False, 'description': 'This is a clearly a bug'}, {'id': 2392693450, 'node_id': 'MDU6TGFiZWwyMzkyNjkzNDUw', 'url': 'https://api.github.com/repos/apache/airflow/labels/pending-response', 'name': 'pending-response', 'color': 'e99695', 'default': False, 'description': ''}, {'id': 2853238496, 'node_id': 'MDU6TGFiZWwyODUzMjM4NDk2', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:core-operators', 'name': 'area:core-operators', 'color': 'fef2c0', 'default': False, 'description': 'Operators, Sensors and hooks within Core Airflow'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 7, 'created_at': '2020-05-01T11:05:37Z', 'updated_at': '2022-07-19T09:59:33Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': "\r\n```\r\n# ==========================\r\n# dag_utc logs:\r\n#  outer_test >>  cur_date: 2020-04-29, next_date: 2020-04-30, tomorrow_date: 2020-04-30, execution_date: 2020-04-29T05:00:00+00:00\r\n#  inner_test >>  cur_date: 2020-04-29, next_date: 2020-04-30, tomorrow_date: 2020-04-30, execution_date: 2020-04-29T05:00:00+00:00\r\n#\r\n# dag_ist logs:\r\n#   outer_test >>  cur_date: 2020-04-29, next_date: 2020-04-30, tomorrow_date: 2020-04-30, execution_date: 2020-04-29T05:00:00+00:00\r\n#   inner_test >>  cur_date: 2020-04-29, next_date: 2020-04-29, tomorrow_date: 2020-04-30, execution_date: 2020-04-29T05:00:00+00:00\r\n# ==========================\r\nfrom datetime import datetime\r\nimport pendulum,logging\r\nfrom airflow import DAG\r\nfrom airflow.operators.python_operator import PythonOperator\r\nfrom airflow.operators.subdag_operator import SubDagOperator\r\ndef get_test_operator(task_id):\r\n    def fun(cdate, ndate, tdate, edate, **context):\r\n        logging.info('cur_date: {}, next_date: {}, tomorrow_date: {}, execution_date: {}'.format(cdate, ndate, tdate, edate))\r\n    return PythonOperator(task_id=task_id,\r\n        python_callable=fun, provide_context=True,\r\n        op_kwargs = {'cdate': '{{ds}}', 'ndate': '{{next_ds}}', 'tdate': '{{tomorrow_ds}}', 'edate': '{{execution_date}}'})\r\ndef test_subdag(parent_dag_name, start_date, schedule_interval):\r\n    with DAG('%s.%s' % (parent_dag_name, 'child_dag'), schedule_interval=schedule_interval, start_date=start_date) as dag:\r\n        task = get_test_operator('inner_test')\r\n        return dag\r\nwith DAG('dag_utc', schedule_interval='0 5 * * *', start_date=datetime(2020,4,29)) as dag:\r\n    task = get_test_operator('outer_test')\r\n    sub_task = SubDagOperator(subdag=test_subdag(dag.dag_id, dag.start_date, dag.schedule_interval),task_id = 'child_dag')\r\n    globals()['dag_utc'] = dag\r\nwith DAG('dag_ist', schedule_interval='30 10 * * *', start_date=datetime(2020,4,29, tzinfo=pendulum.timezone('Asia/Calcutta'))) as dag:\r\n    task = get_test_operator('outer_test')\r\n    sub_task = SubDagOperator(subdag=test_subdag(dag.dag_id, dag.start_date, dag.schedule_interval),task_id = 'child_dag')\r\n    globals()['dag_ist'] = dag\r\n```\r\n**Apache Airflow version**: **v1.10.9**\r\nhello airflow users,when i am trying to run this dags (dag_utc,dag_ist) somehow  for both dag in outer_test task '{{next_ds}}'  is  giving next execution date  but for  dag_ist(with 'Asia/Calcutta' timezone) in subdag(child_dag) '{{next_ds}}'  is giving current execution date for inner_test task.(here dag_ist execution_date is ( 2020-04-29T05:00:00+00:00) . can anyone explain is that a bug or  i am missing something?", 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/21366/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/21366/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000715, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE1', 'repository_id': 33884891, 'emoji': ':hash:', 'name': 'General', 'description': "Chat that doesn't fit anywhere else", 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'general', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/24557', 'id': 4158786, 'node_id': 'MDEwOkRpc2N1c3Npb240MTU4Nzg2', 'number': 24557, 'title': 'Kubernetes Pods fail silently when container runs out of memory', 'user': {'login': 'Sinsin1367', 'id': 30203001, 'node_id': 'MDQ6VXNlcjMwMjAzMDAx', 'avatar_url': 'https://avatars.githubusercontent.com/u/30203001?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Sinsin1367', 'html_url': 'https://github.com/Sinsin1367', 'followers_url': 'https://api.github.com/users/Sinsin1367/followers', 'following_url': 'https://api.github.com/users/Sinsin1367/following{/other_user}', 'gists_url': 'https://api.github.com/users/Sinsin1367/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Sinsin1367/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Sinsin1367/subscriptions', 'organizations_url': 'https://api.github.com/users/Sinsin1367/orgs', 'repos_url': 'https://api.github.com/users/Sinsin1367/repos', 'events_url': 'https://api.github.com/users/Sinsin1367/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Sinsin1367/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}, {'id': 2504183113, 'node_id': 'MDU6TGFiZWwyNTA0MTgzMTEz', 'url': 'https://api.github.com/repos/apache/airflow/labels/provider:cncf-kubernetes', 'name': 'provider:cncf-kubernetes', 'color': 'bfd4f2', 'default': False, 'description': 'Kubernetes provider related issues'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 5, 'created_at': '2020-05-05T19:58:14Z', 'updated_at': '2022-06-20T14:51:31Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'body': '**Description**\r\n\r\nIn Airflow when a container runs out of memory the pod fails and OOMKilled is not published as a pod event. So nothing would show up in the logs. The OOMKilled is displayed as container status. We need to log container statuses if the pod fails to show in the logs that the container ran out and needs more resources to succeed.\xa0\r\n\r\n**Use case / motivation**\r\n\r\n<!-- What do you want to happen?\r\n\r\nIt would be nice to indicate in logs that OOMKilled happened and on what container.\r\n\r\n-->\r\n\r\n**Related Issues**\r\n\r\nNone\r\n', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/24557/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/24557/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000716, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE2', 'repository_id': 33884891, 'emoji': ':pray:', 'name': 'Q&A', 'description': 'Ask the community for help', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'q-a', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/29388', 'id': 4827662, 'node_id': 'MDEwOkRpc2N1c3Npb240ODI3NjYy', 'number': 29388, 'title': 'Missing logs if the pod was killed by Kubernetes', 'user': {'login': 'RyanSiu1995', 'id': 24809843, 'node_id': 'MDQ6VXNlcjI0ODA5ODQz', 'avatar_url': 'https://avatars.githubusercontent.com/u/24809843?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/RyanSiu1995', 'html_url': 'https://github.com/RyanSiu1995', 'followers_url': 'https://api.github.com/users/RyanSiu1995/followers', 'following_url': 'https://api.github.com/users/RyanSiu1995/following{/other_user}', 'gists_url': 'https://api.github.com/users/RyanSiu1995/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/RyanSiu1995/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/RyanSiu1995/subscriptions', 'organizations_url': 'https://api.github.com/users/RyanSiu1995/orgs', 'repos_url': 'https://api.github.com/users/RyanSiu1995/repos', 'events_url': 'https://api.github.com/users/RyanSiu1995/events{/privacy}', 'received_events_url': 'https://api.github.com/users/RyanSiu1995/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 199168283, 'node_id': 'MDU6TGFiZWwxOTkxNjgyODM=', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:bug', 'name': 'kind:bug', 'color': 'fc2929', 'default': False, 'description': 'This is a clearly a bug'}, {'id': 2504183113, 'node_id': 'MDU6TGFiZWwyNTA0MTgzMTEz', 'url': 'https://api.github.com/repos/apache/airflow/labels/provider:cncf-kubernetes', 'name': 'provider:cncf-kubernetes', 'color': 'bfd4f2', 'default': False, 'description': 'Kubernetes provider related issues'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 7, 'created_at': '2020-05-08T07:28:25Z', 'updated_at': '2023-02-06T12:45:08Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'body': '**Apache Airflow version**: 1.10.10\r\n\r\n**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): \r\n```\r\n>\xa0kubectl version\r\nClient Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.7", GitCommit:"8fca2ec50a6133511b771a11559e24191b1aa2b4", GitTreeState:"clean", BuildDate:"2019-09-18T14:47:22Z", GoVersion:"go1.12.9", Compiler:"gc", Platform:"darwin/amd64"}\r\nServer Version: version.Info{Major:"1", Minor:"14+", GitVersion:"v1.14.10-gke.27", GitCommit:"145f9e21a4515947d6fb10819e5a336aff1b6959", GitTreeState:"clean", BuildDate:"2020-02-21T18:01:40Z", GoVersion:"go1.12.12b4", Compiler:"gc", Platform:"linux/amd64"}\r\n```\r\n\r\n**Environment**:\r\n- GKE\r\n- Linux\r\n- `Linux pod-name-597d575d8-xrmkn 4.14.138+ #1 SMP Tue Sep 3 02:58:08 PDT 2019 x86_64 GNU/Linu`\r\n- `pip install`\r\n\r\n**What happened**:\r\nWe used Kubernetes executor to execute our pipeline with a GCS remote log storage. We encountered the log was not found in the GCS bucket for some DAGs.\r\nIt shares the same error like this.\r\n```\r\n*** Unable to read remote log from gs://xxxx/zzz_test_on_hold/on_hold/2020-05-08T07:06:18.991443+00:00/1.log\r\n*** 404 GET https://storage.googleapis.com/download/storage/v1/b/xxxx/o/zzz_test_on_hold%2Fon_hold%2F2020-05-08T07%3A06%3A18.991443%2B00%3A00%2F1.log?alt=media: (\'Request failed with status code\', 404, \'Expected one of\', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)\r\n```\r\nAnd we have gone into the log on stdout and found that there was error like this.\r\n```\r\nPlease make sure that airflow[gcp] is installed and the GCS connection exists.\r\nCould not write logs to \x1bgs://xxxx/zzz_test_on_hold/on_hold/2020-05-08T07:06:18.991443+00:00/1.log: \'NoneType\' object has no attribute \'upload\'\r\n```\r\n\r\n**What you expected to happen**:\r\nI expected the logs can still upload to GCS if the pod was unexpectedly killed by Kubernetes.\r\n\r\n**How to reproduce it**:\r\nWrite a DAG with the following function.\r\n```python\r\ndef on_hold():\r\n    """On hold the process"""\r\n    while True:\r\n        logger.warn("this is on holding now...")\r\n        time.sleep(1000)\r\n```\r\nWhen the task pod is spawned, kill the pod with kubectl.\r\nThe log then will not be able to be uploaded.\r\n\r\n**Anything else we need to know**:\r\nNope', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/29388/reactions', 'total_count': 1, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 1}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/29388/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000715, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE1', 'repository_id': 33884891, 'emoji': ':hash:', 'name': 'General', 'description': "Chat that doesn't fit anywhere else", 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'general', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/29646', 'id': 4879233, 'node_id': 'MDEwOkRpc2N1c3Npb240ODc5MjMz', 'number': 29646, 'title': 'Provide option to time delta on upstream tasks in TimeDeltaSensor', 'user': {'login': 'klsnreddy', 'id': 706914, 'node_id': 'MDQ6VXNlcjcwNjkxNA==', 'avatar_url': 'https://avatars.githubusercontent.com/u/706914?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/klsnreddy', 'html_url': 'https://github.com/klsnreddy', 'followers_url': 'https://api.github.com/users/klsnreddy/followers', 'following_url': 'https://api.github.com/users/klsnreddy/following{/other_user}', 'gists_url': 'https://api.github.com/users/klsnreddy/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/klsnreddy/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/klsnreddy/subscriptions', 'organizations_url': 'https://api.github.com/users/klsnreddy/orgs', 'repos_url': 'https://api.github.com/users/klsnreddy/repos', 'events_url': 'https://api.github.com/users/klsnreddy/events{/privacy}', 'received_events_url': 'https://api.github.com/users/klsnreddy/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}, {'id': 1970155342, 'node_id': 'MDU6TGFiZWwxOTcwMTU1MzQy', 'url': 'https://api.github.com/repos/apache/airflow/labels/good%20first%20issue', 'name': 'good first issue', 'color': '6cdb53', 'default': True, 'description': ''}, {'id': 2853238496, 'node_id': 'MDU6TGFiZWwyODUzMjM4NDk2', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:core-operators', 'name': 'area:core-operators', 'color': 'fef2c0', 'default': False, 'description': 'Operators, Sensors and hooks within Core Airflow'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 2, 'created_at': '2020-05-15T15:32:28Z', 'updated_at': '2023-02-20T20:18:12Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'body': '**Description**\r\nProvide option to time delta on upstream tasks in TimeDeltaSensor. Currently it adds delta to the schedule end time, irrespective of where the TimeDeltaSensor is in DAG.\r\n\r\n**Use case / motivation**\r\nWe have a requirement of sending an email after 30 mins of previous task completion, but TimeDeltaSensor only waits till 30 mins after schedule end time, and by the the time we reach sensor if that delta + end time is less then current time, it will continue the execution with out waiting. Even this feature would be useful when working with eventual consistent system, where you want to wait for couple of minutes before proceeding further.', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/29646/reactions', 'total_count': 2, '+1': 2, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/29646/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000717, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE3', 'repository_id': 33884891, 'emoji': ':bulb:', 'name': 'Ideas', 'description': 'Share ideas for new features', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'ideas', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/21692', 'id': 3888685, 'node_id': 'MDEwOkRpc2N1c3Npb24zODg4Njg1', 'number': 21692, 'title': 'Get UI color of operators from airflow.cfg', 'user': {'login': 'JavierLTPromofarma', 'id': 62240502, 'node_id': 'MDQ6VXNlcjYyMjQwNTAy', 'avatar_url': 'https://avatars.githubusercontent.com/u/62240502?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/JavierLTPromofarma', 'html_url': 'https://github.com/JavierLTPromofarma', 'followers_url': 'https://api.github.com/users/JavierLTPromofarma/followers', 'following_url': 'https://api.github.com/users/JavierLTPromofarma/following{/other_user}', 'gists_url': 'https://api.github.com/users/JavierLTPromofarma/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/JavierLTPromofarma/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/JavierLTPromofarma/subscriptions', 'organizations_url': 'https://api.github.com/users/JavierLTPromofarma/orgs', 'repos_url': 'https://api.github.com/users/JavierLTPromofarma/repos', 'events_url': 'https://api.github.com/users/JavierLTPromofarma/events{/privacy}', 'received_events_url': 'https://api.github.com/users/JavierLTPromofarma/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 8, 'created_at': '2020-05-18T09:58:54Z', 'updated_at': '2023-05-05T00:29:51Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'body': "**Description**\r\n\r\nBeing able to get different ui colors than the default ones for operators from airflow.cfg file \r\n\r\n**Use case / motivation**\r\nI want to have more vivid colors for the operators, for being able to notice at a glance which operators are present. Nowadays the operators have different colors, but sometimes they are very similar and it's not that easy to differentiate them.\r\nCurrently, this change has to be done modifying the operator file in the installation folder of Airflow, which is inconvenient.\r\n\r\nIt would be nice to have something like this in airflow.cfg:\r\n[colors]\r\nS3ToRedshiftOperator = #blabla\r\nDummyOperator #nicecolor", 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/21692/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/21692/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000717, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE3', 'repository_id': 33884891, 'emoji': ':bulb:', 'name': 'Ideas', 'description': 'Share ideas for new features', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'ideas', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/44334', 'id': 7550588, 'node_id': 'MDEwOkRpc2N1c3Npb243NTUwNTg4', 'number': 44334, 'title': 'Google AdManager', 'user': {'login': 'Handtaker23', 'id': 30231904, 'node_id': 'MDQ6VXNlcjMwMjMxOTA0', 'avatar_url': 'https://avatars.githubusercontent.com/u/30231904?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Handtaker23', 'html_url': 'https://github.com/Handtaker23', 'followers_url': 'https://api.github.com/users/Handtaker23/followers', 'following_url': 'https://api.github.com/users/Handtaker23/following{/other_user}', 'gists_url': 'https://api.github.com/users/Handtaker23/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Handtaker23/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Handtaker23/subscriptions', 'organizations_url': 'https://api.github.com/users/Handtaker23/orgs', 'repos_url': 'https://api.github.com/users/Handtaker23/repos', 'events_url': 'https://api.github.com/users/Handtaker23/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Handtaker23/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1463392518, 'node_id': 'MDU6TGFiZWwxNDYzMzkyNTE4', 'url': 'https://api.github.com/repos/apache/airflow/labels/provider:google', 'name': 'provider:google', 'color': 'bfd4f2', 'default': False, 'description': 'Google (including GCP) related issues'}, {'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}, {'id': 1970155342, 'node_id': 'MDU6TGFiZWwxOTcwMTU1MzQy', 'url': 'https://api.github.com/repos/apache/airflow/labels/good%20first%20issue', 'name': 'good first issue', 'color': '6cdb53', 'default': True, 'description': ''}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 13, 'created_at': '2020-05-20T14:52:03Z', 'updated_at': '2024-11-25T06:29:27Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Description**\n\nI would be able to provide a hook and operator to load reports from Google AdManager. In this issue I would like to know if there is interest in this?!\n\n**Use case / motivation**\n\nDealing with ad platforms can be quite challenging because every platform has its own more or less well documented API. That is why I would like to provide an easy implementation to get reports from Google AdManager and to load the data into S3 or GCS.\n\n\n**Related Issues**\n\nNone that I know.', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/44334/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/44334/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000716, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE2', 'repository_id': 33884891, 'emoji': ':pray:', 'name': 'Q&A', 'description': 'Ask the community for help', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'q-a', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/18532', 'id': 3593188, 'node_id': 'MDEwOkRpc2N1c3Npb24zNTkzMTg4', 'number': 18532, 'title': 'dagrun_operator in Airflow Version 1.10.10 ERRORS _run_raw_task result = task_copy.execute(context=context)', 'user': {'login': 'shanit-saha', 'id': 42036720, 'node_id': 'MDQ6VXNlcjQyMDM2NzIw', 'avatar_url': 'https://avatars.githubusercontent.com/u/42036720?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/shanit-saha', 'html_url': 'https://github.com/shanit-saha', 'followers_url': 'https://api.github.com/users/shanit-saha/followers', 'following_url': 'https://api.github.com/users/shanit-saha/following{/other_user}', 'gists_url': 'https://api.github.com/users/shanit-saha/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/shanit-saha/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/shanit-saha/subscriptions', 'organizations_url': 'https://api.github.com/users/shanit-saha/orgs', 'repos_url': 'https://api.github.com/users/shanit-saha/repos', 'events_url': 'https://api.github.com/users/shanit-saha/events{/privacy}', 'received_events_url': 'https://api.github.com/users/shanit-saha/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 199168283, 'node_id': 'MDU6TGFiZWwxOTkxNjgyODM=', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:bug', 'name': 'kind:bug', 'color': 'fc2929', 'default': False, 'description': 'This is a clearly a bug'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 4, 'created_at': '2020-06-09T16:14:21Z', 'updated_at': '2022-06-21T09:11:05Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'On Migrating Airflow from V1.10.2 to V1.10.10 One of our DAG have a task which is of dagrun_operator type.\r\n\r\nCode snippet of the task looks something as below. Please assume that DAG `dag_process_pos` exists\r\n```\r\ntask_trigger_dag_positional = TriggerDagRunOperator(\r\n        trigger_dag_id="dag_process_pos",\r\n        python_callable=set_up_dag_run_preprocessing,\r\n        task_id="trigger_preprocess_dag",\r\n        on_failure_callback=log_failure,\r\n        execution_date=datetime.now(),\r\n        provide_context=False,\r\n        owner=\'airflow\') \r\n\r\ndef set_up_dag_run_preprocessing(context, dag_run_obj):\r\n        ti = context[\'ti\']\r\n        dag_name = context[\'ti\'].task.trigger_dag_id\r\n        dag_run = context[\'dag_run\']\r\n        trans_id = dag_run.conf[\'transaction_id\']\r\n        routing_info = ti.xcom_pull(task_ids="json_validation", key="route_info")\r\n        new_file_path = routing_info[\'file_location\']\r\n        new_file_name = os.path.basename(routing_info[\'new_file_name\'])\r\n        file_path = os.path.join(new_file_path, new_file_name)\r\n        batch_id = "123-AD-FF"\r\n        dag_run_obj.payload = {\'inputfilepath\': file_path,\r\n                               \'transaction_id\': trans_id,\r\n                               \'Id\': batch_id}\r\n```\r\nThe DAG runs all fine. In fact the python callable of the task mentioned until the last line. Then it errors out. \r\n```\r\n[2020-06-09 11:36:22,838] {taskinstance.py:1145} ERROR - No row was found for one()\r\nTraceback (most recent call last):\r\n  File "/usr/local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 983, in _run_raw_task\r\n    result = task_copy.execute(context=context)\r\n  File "/usr/local/lib/python3.6/site-packages/airflow/operators/dagrun_operator.py", line 95, in execute\r\n    replace_microseconds=False)\r\n  File "/usr/local/lib/python3.6/site-packages/airflow/api/common/experimental/trigger_dag.py", line 141, in trigger_dag\r\n    replace_microseconds=replace_microseconds,\r\n  File "/usr/local/lib/python3.6/site-packages/airflow/api/common/experimental/trigger_dag.py", line 98, in _trigger_dag\r\n    external_trigger=True,\r\n  File "/usr/local/lib/python3.6/site-packages/airflow/utils/db.py", line 74, in wrapper\r\n    return func(*args, **kwargs)\r\n  File "/usr/local/lib/python3.6/site-packages/airflow/models/dag.py", line 1471, in create_dagrun\r\n    run.refresh_from_db()\r\n  File "/usr/local/lib/python3.6/site-packages/airflow/utils/db.py", line 74, in wrapper\r\n    return func(*args, **kwargs)\r\n  File "/usr/local/lib/python3.6/site-packages/airflow/models/dagrun.py", line 109, in refresh_from_db\r\n    DR.run_id == self.run_id\r\n  File "/usr/local/lib64/python3.6/site-packages/sqlalchemy/orm/query.py", line 3446, in one\r\n    raise orm_exc.NoResultFound("No row was found for one()")\r\nsqlalchemy.orm.exc.NoResultFound: No row was found for one()\r\n```\r\nAfter which the `on_failure_callback` of that task is executed and all code of that callable runs perfectly ok as is expected. The query here is why did the dagrun_operator fail after the python callable.  \r\n\r\n\r\n**P.S** : The DAG that is being triggered by the `TriggerDagRunOperator` , in this case `dag_process_pos` starts with task of type`dummy_operator`. Also that the target dag that is invoked actually gets triggered. However the the Airflow task that triggers the DAG fails. [ _There is no Dag dependency in this case_ ]', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/18532/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/18532/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000717, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE3', 'repository_id': 33884891, 'emoji': ':bulb:', 'name': 'Ideas', 'description': 'Share ideas for new features', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'ideas', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/24195', 'id': 4120131, 'node_id': 'MDEwOkRpc2N1c3Npb240MTIwMTMx', 'number': 24195, 'title': 'To put a task ON HOLD', 'user': {'login': 'afsalabdup', 'id': 58460620, 'node_id': 'MDQ6VXNlcjU4NDYwNjIw', 'avatar_url': 'https://avatars.githubusercontent.com/u/58460620?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/afsalabdup', 'html_url': 'https://github.com/afsalabdup', 'followers_url': 'https://api.github.com/users/afsalabdup/followers', 'following_url': 'https://api.github.com/users/afsalabdup/following{/other_user}', 'gists_url': 'https://api.github.com/users/afsalabdup/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/afsalabdup/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/afsalabdup/subscriptions', 'organizations_url': 'https://api.github.com/users/afsalabdup/orgs', 'repos_url': 'https://api.github.com/users/afsalabdup/repos', 'events_url': 'https://api.github.com/users/afsalabdup/events{/privacy}', 'received_events_url': 'https://api.github.com/users/afsalabdup/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 6, 'created_at': '2020-06-15T06:53:01Z', 'updated_at': '2022-06-04T19:36:10Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Description: To put a future instance task ON HOLD.\r\n\r\nThere is a requirement where I want to put a task named \'task3\' on hold, so that once the DAG is triggered as per schedule, all upstream tasks would start running but when it reaches \'task3\', task3 should go "ON HOLD" state, so that downstream of task3 would not start running. If we can do that through CLI by providing future execution id then it would be great.\r\n\r\nCurrently I can mark a task to success through CLI by providing future execution id, but the issue is when I mark task3 to success, downstream tasks will start running immediately because downstream will check if its upstream task3 is success or not. I want to put task3 to ON HOLD so that it\'s downstream won\'t trigger by itself.\r\n\r\n', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/24195/reactions', 'total_count': 6, '+1': 6, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/24195/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000717, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE3', 'repository_id': 33884891, 'emoji': ':bulb:', 'name': 'Ideas', 'description': 'Share ideas for new features', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'ideas', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/21691', 'id': 3888681, 'node_id': 'MDEwOkRpc2N1c3Npb24zODg4Njgx', 'number': 21691, 'title': 'Improve selection options on task tries page', 'user': {'login': 'otienoanyango', 'id': 10798485, 'node_id': 'MDQ6VXNlcjEwNzk4NDg1', 'avatar_url': 'https://avatars.githubusercontent.com/u/10798485?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/otienoanyango', 'html_url': 'https://github.com/otienoanyango', 'followers_url': 'https://api.github.com/users/otienoanyango/followers', 'following_url': 'https://api.github.com/users/otienoanyango/following{/other_user}', 'gists_url': 'https://api.github.com/users/otienoanyango/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/otienoanyango/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/otienoanyango/subscriptions', 'organizations_url': 'https://api.github.com/users/otienoanyango/orgs', 'repos_url': 'https://api.github.com/users/otienoanyango/repos', 'events_url': 'https://api.github.com/users/otienoanyango/events{/privacy}', 'received_events_url': 'https://api.github.com/users/otienoanyango/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}, {'id': 2514952451, 'node_id': 'MDU6TGFiZWwyNTE0OTUyNDUx', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:UI', 'name': 'area:UI', 'color': '1d76db', 'default': False, 'description': 'Related to UI/UX. For Frontend Developers.'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 3, 'created_at': '2020-06-15T09:25:55Z', 'updated_at': '2022-08-31T06:58:06Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': "**Description**\r\n\r\nAdd a way of filtering the tasks you want graphed on the task tries UI page. At least an option to deselect all or select all would be quite welcome.\r\n\r\n**Use case / motivation**\r\n\r\nWhen there are many tasks, the task tries page does not allow filtering of what is shown in the graph which is useful when you want to only compare task duration for one or a few tasks. The current option is to deselect each of the tasks you don't want to see. Assuming 100 tasks, that's deselecting 95 if you're interested in only 5.\r\n\r\n**Related Issues**\r\nNot that I'm aware of\r\n\r\n", 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/21691/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/21691/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000716, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE2', 'repository_id': 33884891, 'emoji': ':pray:', 'name': 'Q&A', 'description': 'Ask the community for help', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'q-a', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/18270', 'id': 3574673, 'node_id': 'MDEwOkRpc2N1c3Npb24zNTc0Njcz', 'number': 18270, 'title': 'Delay until Docker Swarm recognizes finished service', 'user': {'login': 'CodingJonas', 'id': 15234763, 'node_id': 'MDQ6VXNlcjE1MjM0NzYz', 'avatar_url': 'https://avatars.githubusercontent.com/u/15234763?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/CodingJonas', 'html_url': 'https://github.com/CodingJonas', 'followers_url': 'https://api.github.com/users/CodingJonas/followers', 'following_url': 'https://api.github.com/users/CodingJonas/following{/other_user}', 'gists_url': 'https://api.github.com/users/CodingJonas/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/CodingJonas/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/CodingJonas/subscriptions', 'organizations_url': 'https://api.github.com/users/CodingJonas/orgs', 'repos_url': 'https://api.github.com/users/CodingJonas/repos', 'events_url': 'https://api.github.com/users/CodingJonas/events{/privacy}', 'received_events_url': 'https://api.github.com/users/CodingJonas/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 199168283, 'node_id': 'MDU6TGFiZWwxOTkxNjgyODM=', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:bug', 'name': 'kind:bug', 'color': 'fc2929', 'default': False, 'description': 'This is a clearly a bug'}, {'id': 1697155831, 'node_id': 'MDU6TGFiZWwxNjk3MTU1ODMx', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:providers', 'name': 'area:providers', 'color': 'd4c5f9', 'default': False, 'description': ''}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 16, 'created_at': '2020-06-15T14:05:58Z', 'updated_at': '2022-06-21T12:14:23Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'body': "**Apache Airflow version**: 2.0.0dev\r\n- **OS** (e.g. from /etc/os-release): Ubuntu 18.04\r\n- **Install tools**: pip\r\n\r\n**What happened**:\r\nWhen running a DockerSwarmOperator the service finishes, but Airflow will detect that finished service only 60 seconds after it has finished.\r\n\r\nThe issue lies in this line:\r\nhttps://github.com/apache/airflow/blob/832593a9fc80f4b56605f3cbf656375bd94cc136/airflow/providers/docker/operators/docker_swarm.py#L171\r\n```python\r\n    def _stream_logs_to_output(self):\r\n        logs = self.cli.service_logs(\r\n            self.service['ID'], follow=True, stdout=True, stderr=True, is_tty=self.tty\r\n        )\r\n        line = ''\r\n        while True:\r\n            try:\r\n                log = next(logs)\r\n```\r\nWhen removing this it works as expected. `next(logs)` is a blocking call, and for some reason the docker-py library, which is behind this call, does not recognize the finished service. After I think exactly 60 seconds this call crashes, which allows the operator to continue.\r\n\r\n**How to reproduce it**:\r\nAny DAG using the DockerSwarmOperator works, e.g.:\r\n```python\r\n    task1 = DockerSwarmOperator(\r\n        task_id='docker_swarm_validator',\r\n        image='alpine:3.11.5',\r\n        api_version='auto',\r\n        command='echo X',\r\n        tty=True,\r\n    )\r\n```\r\n\r\n**Anything else we need to know**:\r\n\r\nA workaround I found was to execute the logging in a separate process, while checking for the current status of the service in the main process. Once the service has finished, the logging process can simply be terminated.\r\nThe workaround would look something like this:\r\n```python\r\n        if self.enable_logging:\r\n            # Since this subprocess is daemonized, it will automatically be terminated once the main script finishes\r\n            p = Process(target=_stream_logs_to_output,\r\n                        args=(self.log,\r\n                              self.cli.service_logs(self.service['ID'], follow=True, stdout=True, stderr=True, is_tty=self.tty)\r\n                              ),\r\n                        daemon=True\r\n                        )\r\n            p.start()\r\n```\r\n_I removed the `_stream_logs_to_output` function from the class to better separate used ressources._", 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/18270/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/18270/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000715, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE1', 'repository_id': 33884891, 'emoji': ':hash:', 'name': 'General', 'description': "Chat that doesn't fit anywhere else", 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'general', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/24194', 'id': 4120130, 'node_id': 'MDEwOkRpc2N1c3Npb240MTIwMTMw', 'number': 24194, 'title': 'Maintenance Mode for Connections', 'user': {'login': 'darwinyip', 'id': 3499338, 'node_id': 'MDQ6VXNlcjM0OTkzMzg=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3499338?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/darwinyip', 'html_url': 'https://github.com/darwinyip', 'followers_url': 'https://api.github.com/users/darwinyip/followers', 'following_url': 'https://api.github.com/users/darwinyip/following{/other_user}', 'gists_url': 'https://api.github.com/users/darwinyip/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/darwinyip/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/darwinyip/subscriptions', 'organizations_url': 'https://api.github.com/users/darwinyip/orgs', 'repos_url': 'https://api.github.com/users/darwinyip/repos', 'events_url': 'https://api.github.com/users/darwinyip/events{/privacy}', 'received_events_url': 'https://api.github.com/users/darwinyip/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 14, 'created_at': '2020-07-10T22:50:04Z', 'updated_at': '2022-06-04T19:34:14Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'body': '**Description**\r\n\r\nBe able to set status of connection as "being in maintenance" so that it automatically pauses all DAGs that uses it, and un-pause them once maintenance mode is over.\r\n\r\n**Use case / motivation**\r\n\r\nThis feature will be useful when database is known to be down, so to avoid all related DAGs failing and retrying.\r\nAlso saves effort from needing someone to manually remember, pause, and un-pause DAGs when a known maintenance window is scheduled.\r\n', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/24194/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/24194/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000717, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE3', 'repository_id': 33884891, 'emoji': ':bulb:', 'name': 'Ideas', 'description': 'Share ideas for new features', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'ideas', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/21693', 'id': 3888749, 'node_id': 'MDEwOkRpc2N1c3Npb24zODg4NzQ5', 'number': 21693, 'title': 'Add option to prevent most recent run on DAG enable with catchup=False', 'user': {'login': 'tambulkar', 'id': 28329861, 'node_id': 'MDQ6VXNlcjI4MzI5ODYx', 'avatar_url': 'https://avatars.githubusercontent.com/u/28329861?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/tambulkar', 'html_url': 'https://github.com/tambulkar', 'followers_url': 'https://api.github.com/users/tambulkar/followers', 'following_url': 'https://api.github.com/users/tambulkar/following{/other_user}', 'gists_url': 'https://api.github.com/users/tambulkar/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/tambulkar/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/tambulkar/subscriptions', 'organizations_url': 'https://api.github.com/users/tambulkar/orgs', 'repos_url': 'https://api.github.com/users/tambulkar/repos', 'events_url': 'https://api.github.com/users/tambulkar/events{/privacy}', 'received_events_url': 'https://api.github.com/users/tambulkar/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 6, 'created_at': '2020-07-21T14:30:23Z', 'updated_at': '2024-04-16T07:51:52Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': "**Description**\r\n\r\nAs mentioned in [AIRFLOW-1156](https://github.com/apache/airflow/pull/8776), I want to add the option to not run the most recent scheduled DAG on enabling. \r\n\r\n**Use case / motivation**\r\n\r\nWe want to be enable the DAG for future runs, but don't want it to run the most recent task automatically, since there are tasks e.g. sending an email, that needs to be done manually on occasion. We do not want to repeat this task multiple times.\r\n\r\n**Related Issues**\r\n\r\nAIRFLOW-1156\r\n", 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/21693/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/21693/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000716, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE2', 'repository_id': 33884891, 'emoji': ':pray:', 'name': 'Q&A', 'description': 'Ask the community for help', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'q-a', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/24161', 'id': 4117779, 'node_id': 'MDEwOkRpc2N1c3Npb240MTE3Nzc5', 'number': 24161, 'title': 'Task Pod logs are not getting populated in Airflow UI', 'user': {'login': 'Siddharthk', 'id': 718854, 'node_id': 'MDQ6VXNlcjcxODg1NA==', 'avatar_url': 'https://avatars.githubusercontent.com/u/718854?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Siddharthk', 'html_url': 'https://github.com/Siddharthk', 'followers_url': 'https://api.github.com/users/Siddharthk/followers', 'following_url': 'https://api.github.com/users/Siddharthk/following{/other_user}', 'gists_url': 'https://api.github.com/users/Siddharthk/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Siddharthk/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Siddharthk/subscriptions', 'organizations_url': 'https://api.github.com/users/Siddharthk/orgs', 'repos_url': 'https://api.github.com/users/Siddharthk/repos', 'events_url': 'https://api.github.com/users/Siddharthk/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Siddharthk/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 199168283, 'node_id': 'MDU6TGFiZWwxOTkxNjgyODM=', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:bug', 'name': 'kind:bug', 'color': 'fc2929', 'default': False, 'description': 'This is a clearly a bug'}, {'id': 1769748000, 'node_id': 'MDU6TGFiZWwxNzY5NzQ4MDAw', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:logging', 'name': 'area:logging', 'color': 'ededed', 'default': False, 'description': ''}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 0, 'created_at': '2020-07-22T08:52:38Z', 'updated_at': '2022-06-03T11:40:39Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': "Apache Airflow version: airflow:1.10.10.1-alpha2-python3.6 \r\n\r\nKubernetes version (if you are using kubernetes) (use kubectl version): 1.16.8\r\n\r\nEnvironment:\r\n\r\nCloud provider or hardware configuration: AWS EKS\r\nOS (e.g. from /etc/os-release): Redhat\r\nInstall tools: Official Helm Chart\r\n\r\nWhat happened:\r\n\r\nSome Tasks are fetching logs from pod. Getting failures when reading logs from S3 bucket. \r\n\r\n```\r\nLog file does not exist: /opt/airflow/logs/mydag/mytask/2020-07-21T11:58:55.019748+00:00/2.log\r\n*** Fetching from: http://taskpod-49ccd964791a4740b199:8793/log/mydag/mytask/2020-07-21T11:58:55.019748+00:00/2.log\r\n*** Failed to fetch log file from worker. HTTPConnectionPool(host='taskpod-49ccd964791a4740b199', port=8793): Max retries exceeded with url: /log/mydag/mytask/2020-07-21T11:58:55.019748+00:00/2.log (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f486c9b8be0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',))\r\n```\r\n\r\nWhat you expected to happen:\r\n\r\nTasks should not fetch logs from pod. There should be no error in reading logs from S3 bucket. It should always try to read from S3 and not from pod.\r\nSome task are working fine:\r\n```\r\n*** Reading remote log from s3://mybucket/airflow/logs/mydag/mytask1/2020-07-21T11:58:55.019748+00:00/2.log.\r\n```\r\n\r\nAnything else we need to know:\r\n\r\nPlease note that this is happening intermittently. When running parallel tasks, I am able to read logs from S3 for some tasks while other give above error. Any help is appreciated.", 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/24161/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/24161/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000717, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE3', 'repository_id': 33884891, 'emoji': ':bulb:', 'name': 'Ideas', 'description': 'Share ideas for new features', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'ideas', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/29483', 'id': 4849260, 'node_id': 'MDEwOkRpc2N1c3Npb240ODQ5MjYw', 'number': 29483, 'title': 'Email noise reduction for failing tasks', 'user': {'login': 'metra', 'id': 199940, 'node_id': 'MDQ6VXNlcjE5OTk0MA==', 'avatar_url': 'https://avatars.githubusercontent.com/u/199940?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/metra', 'html_url': 'https://github.com/metra', 'followers_url': 'https://api.github.com/users/metra/followers', 'following_url': 'https://api.github.com/users/metra/following{/other_user}', 'gists_url': 'https://api.github.com/users/metra/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/metra/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/metra/subscriptions', 'organizations_url': 'https://api.github.com/users/metra/orgs', 'repos_url': 'https://api.github.com/users/metra/repos', 'events_url': 'https://api.github.com/users/metra/events{/privacy}', 'received_events_url': 'https://api.github.com/users/metra/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 2, 'created_at': '2020-07-22T20:53:47Z', 'updated_at': '2023-02-12T07:39:02Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': "**Description**\r\n\r\nFailing tasks can create a lot of email noise if you have both `email_on_failure` and `sla` enabled.\r\n\r\n**Use case / motivation**\r\n\r\nOutside of working hours, say 6pm through 10am, I can receive 16*2 email notifications. Every hour, one email for the failure and one email for the missed SLA.\r\n\r\nTwo ideas:\r\n\r\n- Sending both the SLA miss and the failure email feels redundant.  If a task is failing then it should be obvious that it's also missing its SLA. Perhaps skip the SLA email in this case?\r\n- The SLA miss emails are threaded because their subject lines are equivalent. However, the failure emails include the failure date and time, which breaks threading. Consider dropping the time and/or date from the failure email?\r\n\r\n**Related Issues**\r\n\r\nNone that I could find.\r\n", 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/29483/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/29483/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000715, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE1', 'repository_id': 33884891, 'emoji': ':hash:', 'name': 'General', 'description': "Chat that doesn't fit anywhere else", 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'general', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/24069', 'id': 4112443, 'node_id': 'MDEwOkRpc2N1c3Npb240MTEyNDQz', 'number': 24069, 'title': 'Duplicate tasks invoked for a single task_id when manually invoked task details modal.', 'user': {'login': 'andytrigg', 'id': 90088, 'node_id': 'MDQ6VXNlcjkwMDg4', 'avatar_url': 'https://avatars.githubusercontent.com/u/90088?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/andytrigg', 'html_url': 'https://github.com/andytrigg', 'followers_url': 'https://api.github.com/users/andytrigg/followers', 'following_url': 'https://api.github.com/users/andytrigg/following{/other_user}', 'gists_url': 'https://api.github.com/users/andytrigg/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/andytrigg/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/andytrigg/subscriptions', 'organizations_url': 'https://api.github.com/users/andytrigg/orgs', 'repos_url': 'https://api.github.com/users/andytrigg/repos', 'events_url': 'https://api.github.com/users/andytrigg/events{/privacy}', 'received_events_url': 'https://api.github.com/users/andytrigg/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 199168283, 'node_id': 'MDU6TGFiZWwxOTkxNjgyODM=', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:bug', 'name': 'kind:bug', 'color': 'fc2929', 'default': False, 'description': 'This is a clearly a bug'}, {'id': 1545560198, 'node_id': 'MDU6TGFiZWwxNTQ1NTYwMTk4', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:Scheduler', 'name': 'area:Scheduler', 'color': '6a3aaa', 'default': False, 'description': 'including HA (high availability) scheduler'}, {'id': 3023710536, 'node_id': 'MDU6TGFiZWwzMDIzNzEwNTM2', 'url': 'https://api.github.com/repos/apache/airflow/labels/affected_version:2.1', 'name': 'affected_version:2.1', 'color': 'F0F573', 'default': False, 'description': 'Issues Reported for 2.1'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 38, 'created_at': '2020-07-28T02:43:19Z', 'updated_at': '2022-06-01T10:01:40Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Apache Airflow version**:\r\n1.10.11\r\n\r\n**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): NA\r\n\r\n**Environment**:\r\n\r\n- **Cloud provider or hardware configuration**: AWS (EC2 instances)\r\n- **OS** (e.g. from /etc/os-release):\r\n```\r\nNAME="Amazon Linux"\r\nVERSION="2"\r\nID="amzn"\r\nID_LIKE="centos rhel fedora"\r\nVERSION_ID="2"\r\nPRETTY_NAME="Amazon Linux 2"\r\nANSI_COLOR="0;33"\r\nCPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"\r\nHOME_URL="https://amazonlinux.com/"\r\n```\r\n- **Kernel** (e.g. `uname -a`):\r\n`Linux airflow-scheduler-10-229-13-220 4.14.165-131.185.amzn2.x86_64 #1 SMP Wed Jan 15 14:19:56 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\n- **Install tools**:\r\n- **Others**:\r\n\r\n**What happened**:\r\n\r\nWhen manually invoke a task from the task details dialog, we see the task running for approximately 22 seconds before we see the following appear in the log...\r\n\r\n```\r\n[2020-07-28 01:25:14,726] {local_task_job.py:150} WARNING - Recorded pid 26940 does not match the current pid 26751\r\n[2020-07-28 01:25:14,728] {helpers.py:325} INFO - Sending Signals.SIGTERM to GPID 26757\r\n```\r\n\r\nThe task then is killed. We notice this is accompanied with a second failure shortly afterwards that correlates to the new pid that has been written to the `task_instance` table.\r\n\r\nIt is interesting to note that if the task is scheduled as part of a normal dag run, or by clearing state and allowing the schedular to schedule its execution then we do not experience any issue.\r\n\r\nWe have attempted to specify `task_concurrency` on our operators with no effect.\r\n\r\n**What you expected to happen**:\r\nWe expected a single process to be spawned for the manually executed task.\r\n\r\n**How to reproduce it**:\r\nManually invoke a task via the task details dialog where that task execution is going to be longer than the heart rate interval that has been set.\r\n\r\nThe heart rate checks the pid and sees a mismatch and so kills the task.\r\n\r\n\r\n**Anything else we need to know**:\r\n\r\nWe can produce this reliably if the task execution time is > than the heart rate interval.\r\n\r\n', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/24069/reactions', 'total_count': 5, '+1': 5, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/24069/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000716, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE2', 'repository_id': 33884891, 'emoji': ':pray:', 'name': 'Q&A', 'description': 'Ask the community for help', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'q-a', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/18325', 'id': 3579091, 'node_id': 'MDEwOkRpc2N1c3Npb24zNTc5MDkx', 'number': 18325, 'title': 'Not able to see logs | Log file does not exist: /opt/airflow/logs/hello_world/hello_world/2020-07-28T00:00:00+00:00/1.log', 'user': {'login': 'scalaenthu', 'id': 68823765, 'node_id': 'MDQ6VXNlcjY4ODIzNzY1', 'avatar_url': 'https://avatars.githubusercontent.com/u/68823765?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/scalaenthu', 'html_url': 'https://github.com/scalaenthu', 'followers_url': 'https://api.github.com/users/scalaenthu/followers', 'following_url': 'https://api.github.com/users/scalaenthu/following{/other_user}', 'gists_url': 'https://api.github.com/users/scalaenthu/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/scalaenthu/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/scalaenthu/subscriptions', 'organizations_url': 'https://api.github.com/users/scalaenthu/orgs', 'repos_url': 'https://api.github.com/users/scalaenthu/repos', 'events_url': 'https://api.github.com/users/scalaenthu/events{/privacy}', 'received_events_url': 'https://api.github.com/users/scalaenthu/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 199168283, 'node_id': 'MDU6TGFiZWwxOTkxNjgyODM=', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:bug', 'name': 'kind:bug', 'color': 'fc2929', 'default': False, 'description': 'This is a clearly a bug'}, {'id': 1769748000, 'node_id': 'MDU6TGFiZWwxNzY5NzQ4MDAw', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:logging', 'name': 'area:logging', 'color': 'ededed', 'default': False, 'description': ''}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 13, 'created_at': '2020-07-29T06:57:30Z', 'updated_at': '2023-07-21T10:28:15Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'I am using a compose file to run the airflow.\r\n\r\nMy DAG is running fine, the tasks are able to push the data to airflow as well, but i want to see the logs.\r\n\r\n**What happened:**\r\n\r\n **Log file does not exist: /opt/airflow/logs/hello_world/hello_world/2020-07-28T00:00:00+00:00/1.log\r\n*** Fetching from: http://fef16ec26222:8793/log/hello_world/hello_world/2020-07-28T00:00:00+00:00/1.log\r\n*** Failed to fetch log file from worker. HTTPConnectionPool(host=\'fef16ec26222\', port=8793): Max retries exceeded with url: /log/hello_world/hello_world/2020-07-28T00:00:00+00:00/1.log (Caused by NewConnectionError(\'<urllib3.connection.HTTPConnection object at 0x7f3406532e80>: Failed to establish a new connection: [Errno 111] Connection refused\',))**\r\n\r\nfef16ec26222 is the container id of my scheduler\r\nI suspected that it was a port issue 5793, i created a docker file and exposed the port 5793 also i tried putting everything in the bridge network.\r\n\r\n\r\nMy compose file is:\r\n\r\n\r\n```\r\nversion: "2.1"\r\nservices:\r\n  airflow_postgres:\r\n    image: postgres:12\r\n    environment:\r\n        - POSTGRES_USER=airflow\r\n        - POSTGRES_PASSWORD=airflow\r\n        - POSTGRES_DB=airflow\r\n    ports:\r\n        - "5432:5432"\r\n    networks:\r\n      - airflow-backend\r\n\r\n  meta_postgres:\r\n    image: postgres:12\r\n    environment:\r\n        - POSTGRES_USER=developer\r\n        - POSTGRES_PASSWORD=secret\r\n        - POSTGRES_DB=meta\r\n    ports:\r\n        - "5433:5432"\r\n    networks:\r\n      - airflow-backend\r\n\r\n  scheduler:\r\n    build: .\r\n    restart: always\r\n    depends_on:\r\n      - airflow_postgres\r\n      - meta_postgres\r\n      - webserver\r\n    env_file:\r\n      - .env\r\n    volumes:\r\n        - ./dags:/opt/airflow/dags\r\n    command: scheduler\r\n    healthcheck:\r\n        test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]\r\n        interval: 30s\r\n        timeout: 30s\r\n        retries: 3\r\n    networks:\r\n      - airflow-backend\r\n\r\n  webserver:\r\n    build: .\r\n    restart: always\r\n    depends_on:\r\n        - airflow_postgres\r\n        - meta_postgres\r\n    env_file:\r\n      - .env\r\n    volumes:\r\n        - ./dags:/opt/airflow/dags\r\n        - ./scripts:/opt/airflow/scripts\r\n    ports:\r\n        - "8080:8080"\r\n    entrypoint: ./scripts/airflow-entrypoint.sh\r\n    healthcheck:\r\n        test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]\r\n        interval: 30s\r\n        timeout: 30s\r\n        retries: 3\r\n    networks:\r\n      - airflow-backend\r\nnetworks:\r\n  airflow-backend:\r\n    driver: bridge\r\n\r\n\r\n```\r\n\r\n\r\nThe env is:\r\n\r\n\r\n```\r\nAIRFLOW_HOME=/opt/airflow\r\nAIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://airflow:airflow@airflow_postgres:5432/airflow\r\nAIRFLOW__CORE__FERNET_KEY=*************\r\nAIRFLOW_CONN_METADATA_DB=postgres://developer:secret@meta_postgres:5432/meta\r\nAIRFLOW__VAR__METADATA_DB_SCHEMA=meta\r\n\r\n```\r\n\r\nDockerfile:\r\n\r\n```\r\nFROM apache/airflow\r\nENV AIRFLOW_HOME=/opt/airflow\r\nEXPOSE 5555 8793\r\n```\r\n**How to reproduce it:**\r\nUse the docker file, env and compose file. Can be tested on any DAG.\r\n\r\n**What you expected to happen:**\r\nAble to see logs on webserver UI\r\n\r\nI am using a sequencial operator (by default), I do not want to go with celery right now. \r\n', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/18325/reactions', 'total_count': 1, '+1': 1, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/18325/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000717, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE3', 'repository_id': 33884891, 'emoji': ':bulb:', 'name': 'Ideas', 'description': 'Share ideas for new features', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'ideas', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/24158', 'id': 4117668, 'node_id': 'MDEwOkRpc2N1c3Npb240MTE3NjY4', 'number': 24158, 'title': 'SSH Hook Private Key Decryption', 'user': {'login': 'davido912', 'id': 48397009, 'node_id': 'MDQ6VXNlcjQ4Mzk3MDA5', 'avatar_url': 'https://avatars.githubusercontent.com/u/48397009?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/davido912', 'html_url': 'https://github.com/davido912', 'followers_url': 'https://api.github.com/users/davido912/followers', 'following_url': 'https://api.github.com/users/davido912/following{/other_user}', 'gists_url': 'https://api.github.com/users/davido912/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/davido912/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/davido912/subscriptions', 'organizations_url': 'https://api.github.com/users/davido912/orgs', 'repos_url': 'https://api.github.com/users/davido912/repos', 'events_url': 'https://api.github.com/users/davido912/events{/privacy}', 'received_events_url': 'https://api.github.com/users/davido912/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1697155831, 'node_id': 'MDU6TGFiZWwxNjk3MTU1ODMx', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:providers', 'name': 'area:providers', 'color': 'd4c5f9', 'default': False, 'description': ''}, {'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 0, 'created_at': '2020-07-29T17:10:56Z', 'updated_at': '2022-06-03T10:39:07Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'body': "Relating to the SSH Hook stored in airflow/providers/ssh/hooks/ssh.py. \r\nAt the moment, the hook offers two ways of authenticating the connection. The first, reading the private key from file and if it is decrypted, the password will be passed to the connect function (the core paramiko one) and would be used as a passphrase there (since paramiko's connect states that if passphrase is none, and password is not none - use password to decrypt). \r\n\r\nThe second, ingesting a string containing the private key, casting it to be a file object with StringIO. The private key is taken from the extra JSON. However, this method does not offer a way of decrypting the key with a passphrase. This results in connections failing in cases of decrypted private key and defaults to using only a private key path, which is unfortunate in cases of wanting to leverage secrets for example as a backend that stores the private key in a URI. \r\n\r\nI was thinking about a way to solve this, however, not quite sure if putting the connection password for passphrase would be the way to go. And not sure if security wise for the users, putting the passphrase in the extra json is secure by itself. \r\n\r\nI'm putting this here up for discussion, because it basically renders one way of using this hook as completely irrelevant. ", 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/24158/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/24158/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000717, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE3', 'repository_id': 33884891, 'emoji': ':bulb:', 'name': 'Ideas', 'description': 'Share ideas for new features', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'ideas', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/18831', 'id': 3614847, 'node_id': 'MDEwOkRpc2N1c3Npb24zNjE0ODQ3', 'number': 18831, 'title': 'Teradata Tools and Utilities hooks and operators', 'user': {'login': 'flolas', 'id': 6232349, 'node_id': 'MDQ6VXNlcjYyMzIzNDk=', 'avatar_url': 'https://avatars.githubusercontent.com/u/6232349?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/flolas', 'html_url': 'https://github.com/flolas', 'followers_url': 'https://api.github.com/users/flolas/followers', 'following_url': 'https://api.github.com/users/flolas/following{/other_user}', 'gists_url': 'https://api.github.com/users/flolas/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/flolas/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/flolas/subscriptions', 'organizations_url': 'https://api.github.com/users/flolas/orgs', 'repos_url': 'https://api.github.com/users/flolas/repos', 'events_url': 'https://api.github.com/users/flolas/events{/privacy}', 'received_events_url': 'https://api.github.com/users/flolas/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1697155831, 'node_id': 'MDU6TGFiZWwxNjk3MTU1ODMx', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:providers', 'name': 'area:providers', 'color': 'd4c5f9', 'default': False, 'description': ''}, {'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 2, 'created_at': '2020-07-31T06:09:41Z', 'updated_at': '2022-07-26T06:48:21Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'body': "**Description**\r\nI'm working in operators and hooks for Teradata Database and I want to share with the Airflow Community. There are several tools(C tools) for loading/extract data in Teradata called TTU (https://downloads.teradata.com/download/tools/teradata-tools-and-utilities-linux-installation-package-0)\r\n\r\nProposed operators\r\n* **FastloadOperator**: Wrapper for FastLoad Teradata Utility for loading tables to Teradata\r\n* **FastexportOperator**: Wrapper for Fastexport Teradata Utility for download tables from teradata to CSV\r\n* **BteqOperator**: Wrapper for BTEQ Teradata Utility for BTEQ SQL language\r\n\r\nProposed hooks\r\n* **TtuHook**: Contains all the wrappers with SO processes for executing commands.\r\n\r\n**Use case / motivation**\r\n\r\nShare and collaborate with people using Teradata and Airflow.\r\n\r\n**Questions**\r\n**(1)** Having a dep with Teradata binaries, what considerations should i have to know when contributing with a PR? \r\n**(2)** I have a first version working in Airflow 1.10; would be better to port as 2.0 hook/op?\r\n\r\nThanks!\r\n", 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/18831/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/18831/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000716, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE2', 'repository_id': 33884891, 'emoji': ':pray:', 'name': 'Q&A', 'description': 'Ask the community for help', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'q-a', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/20719', 'id': 3783532, 'node_id': 'MDEwOkRpc2N1c3Npb24zNzgzNTMy', 'number': 20719, 'title': 'Reschedule not working as expected with http_sensor', 'user': {'login': 'paramesh', 'id': 347814, 'node_id': 'MDQ6VXNlcjM0NzgxNA==', 'avatar_url': 'https://avatars.githubusercontent.com/u/347814?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/paramesh', 'html_url': 'https://github.com/paramesh', 'followers_url': 'https://api.github.com/users/paramesh/followers', 'following_url': 'https://api.github.com/users/paramesh/following{/other_user}', 'gists_url': 'https://api.github.com/users/paramesh/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/paramesh/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/paramesh/subscriptions', 'organizations_url': 'https://api.github.com/users/paramesh/orgs', 'repos_url': 'https://api.github.com/users/paramesh/repos', 'events_url': 'https://api.github.com/users/paramesh/events{/privacy}', 'received_events_url': 'https://api.github.com/users/paramesh/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 199168283, 'node_id': 'MDU6TGFiZWwxOTkxNjgyODM=', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:bug', 'name': 'kind:bug', 'color': 'fc2929', 'default': False, 'description': 'This is a clearly a bug'}, {'id': 1697155831, 'node_id': 'MDU6TGFiZWwxNjk3MTU1ODMx', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:providers', 'name': 'area:providers', 'color': 'd4c5f9', 'default': False, 'description': ''}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 1, 'created_at': '2020-07-31T19:37:11Z', 'updated_at': '2022-07-18T13:30:28Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Apache Airflow version:** 1.10.10\r\n\r\n**Environment:**\r\nLinux RHEL , Python 3.6 with Celery worker and Mysql/Redis.\r\n\r\n**What happened:**\r\nI am facing issues when trying to use Reschedule with the http_sensor.\r\n\r\nThe following works fine without issues.\r\n\r\n ```\r\ntask_http_sensor_check_1 = HttpSensor(\r\n task_id=\'http_sensor_check_1\',\r\n http_conn_id=\'stub_default\',\r\n endpoint=\'/api/v1/products\',\r\n request_params={},\r\n response_check=lambda response: "new185" in response.text,\r\n poke_interval=5,\r\n dag=dag)\r\n \r\n```\r\nIf I just add reschedule to this, I see this just run into a continuous retry loop.\r\n\r\n```\r\ntask_http_sensor_check_1 = HttpSensor(\r\n    task_id=\'http_sensor_check_1\',\r\n    http_conn_id=\'stub_default\',\r\n    endpoint=\'/api/v1/products\',\r\n    request_params={},\r\n    response_check=lambda response: "new185" in response.text,\r\n    poke_interval=5,\r\n    **mode=\'reschedule\',**\r\n    dag=dag)\r\n```\r\n \r\nThe above one just runs into an indefinite loop even after the actual lambda response_check is successful and following is the log I am seeing\r\n\r\n```\r\nINFO - Task is not able to be run\r\n[2020-07-31 18:40:33,549] {taskinstance.py:664} INFO - Dependencies not met for <TaskInstance: simplelineardagnovar185.http_sensor_check_1 2020-07-30T00:42:00+00:00 [up_for_retry]>, dependency \'Not In Retry Period\' FAILED: Task is not ready for retry yet but will be retried automatically. Current date is 2020-07-31T18:40:33.548968+00:00 and task will be retried at 2020-07-31T18:41:02.308501+00:00.\r\n```\r\n\r\nOn the Scheduler side I am seeing the following\r\n`2020-07-31 18:40:32,287 ERROR - Executor reports task instance <TaskInstance: simplelineardagnovar185.http_sensor_check_1 2020-07-30 00:42:00+00:00 [queued]> finished (success) although the task says its queued. Was the task killed externally?`\r\n\r\n\r\n**What you expected to happen:**\r\nThe task should be rescheduled and pass when the condition is met.\r\n\r\n**How to reproduce it:**\r\nUse http sensor with "mode=reschedule" to reproduce the issue.\r\n\r\n**Anything else we need to know:**\r\n\r\nPFB the complete DAG\r\n\r\nMy Dag is quite simple.\r\n\r\n```\r\ndefault_args = {\r\n \'owner\': \'Paramesh\',\r\n \'depends_on_past\': True,\r\n \'start_date\': datetime(2020, 7, 5),\r\n \'email\': \'params85@gmail.com\',\r\n \'email_on_failure\': False,\r\n \'email_on_retry\': False,\r\n \'retries\': 10,\r\n \'retry_delay\': timedelta(seconds=30),\r\n}\r\n\r\n# Catch-up is set to false as we don\'t want any backfill.\r\ndag = DAG(\'simplelineardagnovar185\', schedule_interval=\'*/7 * * * *\', default_args=default_args, catchup=False)\r\n\r\nPOST_PRODUCTS = SimpleHttpOperator(\r\n task_id=\'POST_PRODUCTS\',\r\n method=\'POST\',\r\n http_conn_id=\'stub_default\',\r\n endpoint=\'/api/v1/products\',\r\n data=json.dumps(\r\n {"description": "new185", "name": "product3", "price": "500.0", "sleep": "300000"}),\r\n headers={"Content-Type": "application/json"},\r\n xcom_push=True,\r\n dag=dag)\r\n\r\ntask_http_sensor_check_1 = HttpSensor(\r\n task_id=\'http_sensor_check_1\',\r\n http_conn_id=\'stub_default\',\r\n endpoint=\'/api/v1/products\',\r\n request_params={},\r\n response_check=lambda response: "new185" in response.text,\r\n mode=\'reschedule\',\r\n poke_interval=5,\r\n dag=dag)\r\n\r\nDELETE_PRODUCTS = SimpleHttpOperator(\r\n task_id=\'DELETE_PRODUCTS\',\r\n method=\'DELETE\',\r\n http_conn_id=\'stub_default\',\r\n endpoint=\'/api/v1/products/desc/new185\',\r\n headers={"Content-Type": "application/json"},\r\n dag=dag)\r\n\r\nPOST_PRODUCTS >> task_http_sensor_check_1 >> DELETE_PRODUCTS\r\n```\r\nThe POST_PRODUCTS will post an async request and that will takes ~5 min to get committed to DB and the http_sensor_check just do the get to identify whether the POST is successful.', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/20719/reactions', 'total_count': 3, '+1': 3, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/20719/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000716, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE2', 'repository_id': 33884891, 'emoji': ':pray:', 'name': 'Q&A', 'description': 'Ask the community for help', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'q-a', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/18324', 'id': 3579075, 'node_id': 'MDEwOkRpc2N1c3Npb24zNTc5MDc1', 'number': 18324, 'title': 'TaskHandlerWithCustomFormatter adds prefix twice', 'user': {'login': 'bellhea', 'id': 47355054, 'node_id': 'MDQ6VXNlcjQ3MzU1MDU0', 'avatar_url': 'https://avatars.githubusercontent.com/u/47355054?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/bellhea', 'html_url': 'https://github.com/bellhea', 'followers_url': 'https://api.github.com/users/bellhea/followers', 'following_url': 'https://api.github.com/users/bellhea/following{/other_user}', 'gists_url': 'https://api.github.com/users/bellhea/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/bellhea/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/bellhea/subscriptions', 'organizations_url': 'https://api.github.com/users/bellhea/orgs', 'repos_url': 'https://api.github.com/users/bellhea/repos', 'events_url': 'https://api.github.com/users/bellhea/events{/privacy}', 'received_events_url': 'https://api.github.com/users/bellhea/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 199168283, 'node_id': 'MDU6TGFiZWwxOTkxNjgyODM=', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:bug', 'name': 'kind:bug', 'color': 'fc2929', 'default': False, 'description': 'This is a clearly a bug'}, {'id': 1769748000, 'node_id': 'MDU6TGFiZWwxNzY5NzQ4MDAw', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:logging', 'name': 'area:logging', 'color': 'ededed', 'default': False, 'description': ''}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 0, 'created_at': '2020-08-06T16:05:07Z', 'updated_at': '2022-06-22T19:01:04Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '<!--\r\n\r\nWelcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.\r\nDon\'t worry if they\'re not all applicable; just try to include what you can :-)\r\n\r\nIf you need to include code snippets or logs, please put them in fenced code\r\nblocks.  If they\'re super-long, please use the details tag like\r\n<details><summary>super-long log</summary> lots of stuff </details>\r\n\r\nPlease delete these comment blocks before submitting the issue.\r\n\r\n-->\r\n\r\n<!--\r\n\r\nIMPORTANT!!!\r\n\r\nPLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE\r\nNEXT TO "SUBMIT NEW ISSUE" BUTTON!!!\r\n\r\nPLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!\r\n\r\nPlease complete the next sections or the issue will be closed.\r\nThis questions are the first thing we need to know to understand the context.\r\n\r\n-->\r\n\r\n**Apache Airflow version**: 1.10.11\r\n\r\n\r\n**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): 1.10.0\r\n\r\n**Environment**:\r\n\r\n- **Cloud provider or hardware configuration**:\r\n- **OS** (e.g. from /etc/os-release): rhel 7.7\r\n- **Kernel** (e.g. `uname -a`):\r\n- **Install tools**:\r\n- **Others**:\r\n\r\n**What happened**:\r\nI\'m trying to use the class [TaskHandlerWithCustomFormatter](https://github.com/apache/airflow/blob/master/airflow/utils/log/task_handler_with_custom_formatter.py).\r\nSince I am on version 1.10.11, this class is not yet available so I\'ve copied the class and using it in my custom log_config.py. This is how I\'ve used it:\r\n```\r\nimport logging\r\nfrom logging import StreamHandler\r\nfrom airflow.utils.helpers import parse_template_string\r\n\r\n\r\nclass TaskHandlerWithCustomFormatter(StreamHandler):\r\n    """\r\n    Custom implementation of StreamHandler, a class which writes logging records for Airflow\r\n    """\r\n    def __init__(self, stream):\r\n        super().__init__()\r\n        self.prefix_jinja_template = None\r\n\r\n    def set_context(self, ti):\r\n        """\r\n        Accept the run-time context (i.e. the current task) and configure the formatter accordingly.\r\n        :param ti:\r\n        :return:\r\n        """\r\n        if ti.raw:\r\n            return\r\n        prefix = "TASK - {{ti.dag_id}}-{{ti.task_id}}-{{ds}}-{{try_number}}"\r\n\r\n        rendered_prefix = ""\r\n        if prefix:\r\n            _, self.prefix_jinja_template = parse_template_string(prefix)\r\n            rendered_prefix = self._render_prefix(ti)\r\n        formatter = logging.Formatter(rendered_prefix + ":" + self.formatter._fmt)  # pylint: disable=W0212\r\n        self.setFormatter(formatter)\r\n        self.setLevel(self.level)\r\n\r\n    def _render_prefix(self, ti):\r\n        if self.prefix_jinja_template:\r\n            jinja_context = ti.get_template_context()\r\n            return self.prefix_jinja_template.render(**jinja_context)\r\n        logging.warning("\'task_log_prefix_template\' is in invalid format, ignoring the variable value")\r\n        return ""\r\n```\r\n\r\n```\r\n\'handlers\': {\r\n    \'console\': {\r\n        \'class\': \'logging.StreamHandler\',\r\n        \'formatter\': \'airflow\',\r\n        \'stream\': \'ext://sys.stdout\'\r\n    },\r\n    \'task_handler\': {\r\n        \'class\': \'task_handler_with_custom_formatter.TaskHandlerWithCustomFormatter\',\r\n        \'formatter\': \'airflow\',\r\n        \'stream\': \'ext://sys.stdout\'\r\n    },\r\n...\r\n\'loggers\': {\r\n    \'airflow.processor\': {\r\n        \'handlers\': [\'processor\', \'console\'],\r\n        \'level\': LOG_LEVEL,\r\n        \'propagate\': False,\r\n    },\r\n    \'airflow.task\': {\r\n        \'handlers\': [\'task_handler\'],\r\n        \'level\': LOG_LEVEL,\r\n        \'propagate\': False,\r\n    },\r\n```\r\n\r\nI can see the prefix as I have defined in the logs, however the prefix is printed twice before the actual log text, e.g.:\r\n\r\n`dag_id-task_id-2016-01-01-1:dag_id-task_id-2016-01-01-1:<LOG TEXT>`\r\n\r\nAny ideas what could be wrong here?\r\n\r\n<!-- (please include exact error messages if you can) -->\r\n\r\n\r\n**What you expected to happen**:\r\nThe prefix to only appear once\r\n<!-- What do you think went wrong? -->\r\n\r\n**How to reproduce it**:\r\nCode as above\r\n<!---\r\n\r\nAs minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.\r\n\r\nIf you are using kubernetes, please attempt to recreate the issue using minikube or kind.\r\n\r\n## Install minikube/kind\r\n\r\n- Minikube https://minikube.sigs.k8s.io/docs/start/\r\n- Kind https://kind.sigs.k8s.io/docs/user/quick-start/\r\n\r\nIf this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action\r\n\r\nYou can include images using the .md sytle of\r\n![alt text](http://url/to/img.png)\r\n\r\nTo record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.\r\n\r\n--->\r\n\r\n\r\n**Anything else we need to know**:\r\n\r\n<!--\r\n\r\nHow often does this problem occur? Once? Every time etc?\r\n\r\nAny relevant logs to include? Put them here in side a detail tag:\r\n<details><summary>x.log</summary> lots of stuff </details>\r\n\r\n-->\r\n', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/18324/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/18324/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000716, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE2', 'repository_id': 33884891, 'emoji': ':pray:', 'name': 'Q&A', 'description': 'Ask the community for help', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'q-a', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/24560', 'id': 4159201, 'node_id': 'MDEwOkRpc2N1c3Npb240MTU5MjAx', 'number': 24560, 'title': 'http hook uses schema field from Connection object instead of conn_type', 'user': {'login': 'kubatyszko', 'id': 826814, 'node_id': 'MDQ6VXNlcjgyNjgxNA==', 'avatar_url': 'https://avatars.githubusercontent.com/u/826814?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/kubatyszko', 'html_url': 'https://github.com/kubatyszko', 'followers_url': 'https://api.github.com/users/kubatyszko/followers', 'following_url': 'https://api.github.com/users/kubatyszko/following{/other_user}', 'gists_url': 'https://api.github.com/users/kubatyszko/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/kubatyszko/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/kubatyszko/subscriptions', 'organizations_url': 'https://api.github.com/users/kubatyszko/orgs', 'repos_url': 'https://api.github.com/users/kubatyszko/repos', 'events_url': 'https://api.github.com/users/kubatyszko/events{/privacy}', 'received_events_url': 'https://api.github.com/users/kubatyszko/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 199168283, 'node_id': 'MDU6TGFiZWwxOTkxNjgyODM=', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:bug', 'name': 'kind:bug', 'color': 'fc2929', 'default': False, 'description': 'This is a clearly a bug'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 2, 'created_at': '2020-08-09T00:27:53Z', 'updated_at': '2022-06-20T20:29:22Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'body': '**Apache Airflow version**: MASTER\r\n\r\n\r\n**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): 1.16\r\n\r\n**Environment**:\r\n\r\n- **Cloud provider or hardware configuration**: AWS\r\n- **OS** (e.g. from /etc/os-release): Amazon Linux\r\n- **Kernel** (e.g. `uname -a`):\r\n- **Install tools**:\r\n- **Others**:\r\n\r\n**What happened**:\r\n\r\nHTTP hook defaults to http connection schema, and I could never get it to accurately use the https that I had set in the connection URI.\r\nThis may only happen using secrets backend, since the connections set in airflow UI have their fields mapped directly.\r\n\r\n**What you expected to happen**: https://host.com should yield connection schema to be https\r\n\r\n**How to reproduce it**:\r\nPull airflow from master, create a sample DAG using http hook, with connection being set to https, with the connection string pulled from some secrets manager (in my case I used AWS secrets manager)\r\n\r\n\r\n\r\n**Anything else we need to know**:\r\nThis issue seems to be limited to using secrets backend for connection information, since connections configured via airflow UI have their "schema" field mapped directly.\r\n\r\nMore information\r\n\r\n```\r\n        if self.http_conn_id:\r\n            conn = self.get_connection(self.http_conn_id)\r\n            if conn.host and "://" in conn.host:\r\n                self.base_url = conn.host\r\n            else:\r\n                # schema defaults to HTTP\r\n****                schema = conn.schema if conn.schema else "http"****\r\n****                schema = conn.conn_type if conn.conn_type else "http"****\r\n                host = conn.host if conn.host else ""\r\n                self.base_url = schema + "://" + host\r\n\r\n```\r\ncode snippet from airflow.models.connection with highlights:\r\n\r\n```\r\n    def _parse_from_uri(self, uri: str):\r\n        uri_parts = urlparse(uri)\r\n        conn_type = uri_parts.scheme\r\n        if conn_type == \'postgresql\':\r\n            conn_type = \'postgres\'\r\n        elif \'-\' in conn_type:\r\n            conn_type = conn_type.replace(\'-\', \'_\')\r\n****        self.conn_type = conn_type ****\r\n****        self.host = _parse_netloc_to_hostname(uri_parts) ****\r\n****        quoted_schema = uri_parts.path[1:]\r\n****        self.schema = unquote(quoted_schema) if quoted_schema else quoted_schema ****\r\n        self.login = unquote(uri_parts.username) \\\r\n```\r\n\r\nQuick verification of my approach:\r\n\r\n```\r\nurlparse("https://host.com:443")\r\nParseResult(scheme=\'https\', netloc=\'host.com:443\', path=\'\', params=\'\', query=\'\', fragment=\'\')\r\n>>>urlparse("https://host.com:443/r").path[1:].\r\n\'r\'\r\n```', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/24560/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/24560/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000716, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE2', 'repository_id': 33884891, 'emoji': ':pray:', 'name': 'Q&A', 'description': 'Ask the community for help', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'q-a', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/18836', 'id': 3615023, 'node_id': 'MDEwOkRpc2N1c3Npb24zNjE1MDIz', 'number': 18836, 'title': 'initdb crashes when using non-empty sql_alchemy_schema in configuration', 'user': {'login': 'vitaly-krugl', 'id': 1441066, 'node_id': 'MDQ6VXNlcjE0NDEwNjY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1441066?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/vitaly-krugl', 'html_url': 'https://github.com/vitaly-krugl', 'followers_url': 'https://api.github.com/users/vitaly-krugl/followers', 'following_url': 'https://api.github.com/users/vitaly-krugl/following{/other_user}', 'gists_url': 'https://api.github.com/users/vitaly-krugl/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/vitaly-krugl/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/vitaly-krugl/subscriptions', 'organizations_url': 'https://api.github.com/users/vitaly-krugl/orgs', 'repos_url': 'https://api.github.com/users/vitaly-krugl/repos', 'events_url': 'https://api.github.com/users/vitaly-krugl/events{/privacy}', 'received_events_url': 'https://api.github.com/users/vitaly-krugl/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 199168283, 'node_id': 'MDU6TGFiZWwxOTkxNjgyODM=', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:bug', 'name': 'kind:bug', 'color': 'fc2929', 'default': False, 'description': 'This is a clearly a bug'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 1, 'created_at': '2020-08-13T02:37:08Z', 'updated_at': '2022-10-27T13:04:34Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Apache Airflow version**: 1.10.11\r\n\r\n\r\n**Environment**:\r\n\r\n- **OS** (e.g. from /etc/os-release): Debian GNU/Linux 9\r\n- **Kernel** (e.g. `uname -a`): Linux b15edd925a5a 4.19.76-linuxkit #1 SMP Tue May 26 11:42:35 UTC 2020 x86_64 GNU/Linux\r\n\r\n**What happened**:\r\n\r\nI specified `sql_alchemy_schema = af` in the `[core]` section of airflow.cfg and then ran `airflow initdb`.\r\n\r\nIt crashed with the following exception:\r\n\r\n```\r\nempower_wf_postgres | 2020-08-13 01:53:41.062 UTC [73] ERROR:  relation "af.slot_pool" does not exist at character 185\r\nempower_wf_postgres | 2020-08-13 01:53:41.062 UTC [73] STATEMENT:  SELECT af.slot_pool.id AS af_slot_pool_id, af.slot_pool.pool AS af_slot_pool_pool, af.slot_pool.slots AS af_slot_pool_slots, af.slot_pool.description AS af_slot_pool_description \r\nempower_wf_postgres | \tFROM af.slot_pool \r\nempower_wf_postgres | \tWHERE af.slot_pool.pool = \'default_pool\' \r\nempower_wf_postgres | \t LIMIT 1\r\nempower_wf_app | Traceback (most recent call last):\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context\r\nempower_wf_app |     self.dialect.do_execute(\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 593, in do_execute\r\nempower_wf_app |     cursor.execute(statement, parameters)\r\nempower_wf_app | psycopg2.errors.UndefinedTable: relation "af.slot_pool" does not exist\r\nempower_wf_app | LINE 2: FROM af.slot_pool \r\nempower_wf_app |              ^\r\nempower_wf_app | \r\nempower_wf_app | \r\nempower_wf_app | The above exception was the direct cause of the following exception:\r\nempower_wf_app | \r\nempower_wf_app | Traceback (most recent call last):\r\nempower_wf_app |   File "/a/bin/airflow", line 37, in <module>\r\nempower_wf_app |     args.func(args)\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/airflow/bin/cli.py", line 1329, in initdb\r\nempower_wf_app |     db.initdb(settings.RBAC)\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/airflow/utils/db.py", line 323, in initdb\r\nempower_wf_app |     upgradedb()\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/airflow/utils/db.py", line 387, in upgradedb\r\nempower_wf_app |     add_default_pool_if_not_exists()\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/airflow/utils/db.py", line 74, in wrapper\r\nempower_wf_app |     return func(*args, **kwargs)\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/airflow/utils/db.py", line 90, in add_default_pool_if_not_exists\r\nempower_wf_app |     if not Pool.get_pool(Pool.DEFAULT_POOL_NAME, session=session):\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/airflow/utils/db.py", line 70, in wrapper\r\nempower_wf_app |     return func(*args, **kwargs)\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/airflow/models/pool.py", line 45, in get_pool\r\nempower_wf_app |     return session.query(Pool).filter(Pool.pool == pool_name).first()\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 3397, in first\r\nempower_wf_app |     ret = list(self[0:1])\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 3171, in __getitem__\r\nempower_wf_app |     return list(res)\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 3503, in __iter__\r\nempower_wf_app |     return self._execute_and_instances(context)\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 3528, in _execute_and_instances\r\nempower_wf_app |     result = conn.execute(querycontext.statement, self._params)\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1014, in execute\r\nempower_wf_app |     return meth(self, multiparams, params)\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection\r\nempower_wf_app |     return connection._execute_clauseelement(self, multiparams, params)\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1127, in _execute_clauseelement\r\nempower_wf_app |     ret = self._execute_context(\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context\r\nempower_wf_app |     self._handle_dbapi_exception(\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception\r\nempower_wf_app |     util.raise_(\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 178, in raise_\r\nempower_wf_app |     raise exception\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context\r\nempower_wf_app |     self.dialect.do_execute(\r\nempower_wf_app |   File "/tmp/empower-wf-venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 593, in do_execute\r\nempower_wf_app |     cursor.execute(statement, parameters)\r\nempower_wf_app | sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "af.slot_pool" does not exist\r\nempower_wf_app | LINE 2: FROM af.slot_pool \r\nempower_wf_app |              ^\r\nempower_wf_app | \r\nempower_wf_app | [SQL: SELECT af.slot_pool.id AS af_slot_pool_id, af.slot_pool.pool AS af_slot_pool_pool, af.slot_pool.slots AS af_slot_pool_slots, af.slot_pool.description AS af_slot_pool_description \r\nempower_wf_app | FROM af.slot_pool \r\nempower_wf_app | WHERE af.slot_pool.pool = %(pool_1)s \r\nempower_wf_app |  LIMIT %(param_1)s]\r\nempower_wf_app | [parameters: {\'pool_1\': \'default_pool\', \'param_1\': 1}]\r\nempower_wf_app | (Background on this error at: http://sqlalche.me/e/13/f405)\r\n```\r\n\r\n**What you expected to happen**:\r\n\r\nI expected `airflow initdb` to succeed. \r\n\r\n**How to reproduce it**:\r\n\r\nSpecify `sql_alchemy_schema = af` in the `[core]` section of airflow.cfg and then run `airflow initdb` against postgresql database. I am using "postgres:10.11" docker image.\r\n\r\n\r\n**Anything else we need to know**:\r\n\r\nAccording to this stackoverflow - https://stackoverflow.com/questions/58991074/airflow-initdb-slot-pool-does-not-exists - setting sql_alchemy_schema used to work in Airflow 1.10.2, and then started failing in Airflow 1.10.6 or earlier.', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/18836/reactions', 'total_count': 1, '+1': 1, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/18836/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000715, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE1', 'repository_id': 33884891, 'emoji': ':hash:', 'name': 'General', 'description': "Chat that doesn't fit anywhere else", 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'general', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/28089', 'id': 4624996, 'node_id': 'MDEwOkRpc2N1c3Npb240NjI0OTk2', 'number': 28089, 'title': 'Add native Terraform integration', 'user': {'login': 'jaketf', 'id': 11599048, 'node_id': 'MDQ6VXNlcjExNTk5MDQ4', 'avatar_url': 'https://avatars.githubusercontent.com/u/11599048?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jaketf', 'html_url': 'https://github.com/jaketf', 'followers_url': 'https://api.github.com/users/jaketf/followers', 'following_url': 'https://api.github.com/users/jaketf/following{/other_user}', 'gists_url': 'https://api.github.com/users/jaketf/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jaketf/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jaketf/subscriptions', 'organizations_url': 'https://api.github.com/users/jaketf/orgs', 'repos_url': 'https://api.github.com/users/jaketf/repos', 'events_url': 'https://api.github.com/users/jaketf/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jaketf/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 17, 'created_at': '2020-08-21T17:31:21Z', 'updated_at': '2024-10-30T03:26:30Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'body': '\r\n**Description**\r\n\r\nCreate a terraform integration for apache airflow.\r\n\r\n**Use case / motivation**\r\n\r\nUse terraform to manage ephemeral infrastructure used in airflow DAGs taking advantage of it\'s "drift" detection features and wide array of existing integrations. For teams who use terraform this could replace tasks like create / delete dataproc cluster operator. This could be really interesting for automating nightly large scale e2e integration tests of your terraform and data pipelines code bases (terraform apply >> run data pipelines >> terraform destory.\r\n\r\n\r\n**Related Issues**\r\n\r\nInspired by [this discussion](https://github.com/apache/airflow/pull/9593#issuecomment-667906041)  in #9593\r\n\r\ncc: @brandonjbjelland @potiuk \r\n\r\nBrandon and I will discuss a design for this next week.', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/28089/reactions', 'total_count': 4, '+1': 4, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/28089/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000716, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE2', 'repository_id': 33884891, 'emoji': ':pray:', 'name': 'Q&A', 'description': 'Ask the community for help', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'q-a', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/21690', 'id': 3888638, 'node_id': 'MDEwOkRpc2N1c3Npb24zODg4NjM4', 'number': 21690, 'title': 'Airflow shows "Log Url" localhost:8080, although I am using airflow behind nginx proxy on hostname:443 port', 'user': {'login': 'raulk89', 'id': 25365468, 'node_id': 'MDQ6VXNlcjI1MzY1NDY4', 'avatar_url': 'https://avatars.githubusercontent.com/u/25365468?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/raulk89', 'html_url': 'https://github.com/raulk89', 'followers_url': 'https://api.github.com/users/raulk89/followers', 'following_url': 'https://api.github.com/users/raulk89/following{/other_user}', 'gists_url': 'https://api.github.com/users/raulk89/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/raulk89/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/raulk89/subscriptions', 'organizations_url': 'https://api.github.com/users/raulk89/orgs', 'repos_url': 'https://api.github.com/users/raulk89/repos', 'events_url': 'https://api.github.com/users/raulk89/events{/privacy}', 'received_events_url': 'https://api.github.com/users/raulk89/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 199168283, 'node_id': 'MDU6TGFiZWwxOTkxNjgyODM=', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:bug', 'name': 'kind:bug', 'color': 'fc2929', 'default': False, 'description': 'This is a clearly a bug'}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 8, 'created_at': '2020-08-25T21:49:50Z', 'updated_at': '2023-07-24T14:45:11Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Apache Airflow version**: 1.10.11, 1.10.12\r\n\r\n**Environment**: Virtual machine\r\n\r\n- **OS** (e.g. from /etc/os-release): CentOS Linux release 7.8.2003 (Core)\r\n- **Kernel** (e.g. `uname -a`): 3.10.0-1127.18.2.el7.x86_64\r\n- **Install tools**: yum, pip3\r\n- **Others**:\r\n\r\n**What happened**: After executing DAG, I clicked on "running tasks", then opens up this page, where there is "Log Url" link. But this one does not open, since I am using it behind nginx proxy.\r\n\r\n![image](https://user-images.githubusercontent.com/25365468/91230793-d4a32a00-e734-11ea-9b1d-3b0d0a80561f.png)\r\n\r\n\r\n**What you expected to happen**:  The "Log Url" should be "https://hostname:443/..."\r\n\r\n**How to reproduce it**:\r\n\r\n**Just make sure that your ariflow webserver runs on http://localhost:8080, then**\r\n`# yum install nginx`\r\n\r\n**Add virtualhost conf and add the following:**\r\n_You can probably get by with just using port 80 also, then you do not need ssl and certs_\r\n_And then your application opens up http://hostname:80_\r\n\r\nvi /etc/nginx/conf.d/airflow_webserver_proxy.conf\r\n```\r\nserver {\r\n listen 80;\r\n server_name hostname.domain.com;\r\n return 301 https://$host$request_uri;\r\n}\r\n\r\nserver {\r\n listen 443 ssl http2;\r\n server_name hostname.domain.com;\r\n\r\n location / {\r\n  proxy_pass http://localhost:8080;\r\n  proxy_set_header Host $host;\r\n }\r\n\r\n ssl_certificate /etc/pki/tls/certs/<<location to .cer file>>;\r\n ssl_certificate_key /etc/pki/tls/private/<<location to .key file>>;\r\n\r\n ssl_session_timeout 60m;\r\n ssl_dhparam /etc/ssl/certs/<<self signed cert>>;\r\n\r\n ssl_protocols TLSv1.2;\r\n add_header Strict-Transport-Security "max-age=31536000;" always;\r\n\r\n}\r\n```\r\n`# systemctl start nginx`\r\n\r\nAnd then open up web browser, start a random DAG, click on "running task" and then there should be this "Log Url"\r\n\r\n**Anything else we need to know**: It happens always.', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/21690/reactions', 'total_count': 2, '+1': 2, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/21690/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000717, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE3', 'repository_id': 33884891, 'emoji': ':bulb:', 'name': 'Ideas', 'description': 'Share ideas for new features', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'ideas', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/23989', 'id': 4104965, 'node_id': 'MDEwOkRpc2N1c3Npb240MTA0OTY1', 'number': 23989, 'title': 'Allow xcom_pull to return multiple results if task_ids is None', 'user': {'login': 'paavanb', 'id': 1556995, 'node_id': 'MDQ6VXNlcjE1NTY5OTU=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1556995?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/paavanb', 'html_url': 'https://github.com/paavanb', 'followers_url': 'https://api.github.com/users/paavanb/followers', 'following_url': 'https://api.github.com/users/paavanb/following{/other_user}', 'gists_url': 'https://api.github.com/users/paavanb/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/paavanb/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/paavanb/subscriptions', 'organizations_url': 'https://api.github.com/users/paavanb/orgs', 'repos_url': 'https://api.github.com/users/paavanb/repos', 'events_url': 'https://api.github.com/users/paavanb/events{/privacy}', 'received_events_url': 'https://api.github.com/users/paavanb/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1923278889, 'node_id': 'MDU6TGFiZWwxOTIzMjc4ODg5', 'url': 'https://api.github.com/repos/apache/airflow/labels/kind:feature', 'name': 'kind:feature', 'color': '5319e7', 'default': False, 'description': 'Feature Requests'}, {'id': 2500918862, 'node_id': 'MDU6TGFiZWwyNTAwOTE4ODYy', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:core', 'name': 'area:core', 'color': 'd4c5f9', 'default': False, 'description': ''}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 3, 'created_at': '2020-08-25T22:53:22Z', 'updated_at': '2022-06-01T11:06:50Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Description**\r\n\r\n`TaskInstance.xcom_pull` only returns multiple xcom values if an array of task ids are passed in. This seems to make it impossible to get all XComs for a given key without also knowing exactly which `task_ids` set them, even though the documentation suggests that task ids are not necessary and that supplying `None` "removes the filter" ([comment here](https://github.com/apache/airflow/blob/1.10.12/airflow/models/taskinstance.py#L1540)). Is this intentional behavior?\r\n\r\nFortunately, I can work around this by writing my own function to call `XCom.get_many` directly, but it\'s not ideal.\r\n\r\n**Use case / motivation**\r\n\r\nSuppose there is a DAG in which tasks can declare artifacts that they output; these are all pushed as XComs with the same key (e.g., `"output-artifact"`). Future tasks attempting to use these artifacts can check for DAG consistency by pulling all XComs with that key and ensuring that a desired artifact was output by some previous task in the current DAG. But since these tasks are reusable/defined elsewhere, they don\'t know the `task_ids` of all preceding tasks. If `task_ids` is set to `None`, it seems that `xcom_pull` should return all XComs that match the parameters provided, but that\'s not true today.\r\n\r\nAppreciate any feedback!\r\n\r\n**Related Issues**\r\n\r\nhttps://github.com/apache/airflow/pull/6461\r\nhttps://issues.apache.org/jira/browse/AIRFLOW-5804', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/23989/reactions', 'total_count': 2, '+1': 2, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/23989/timeline'}, {'repository_url': 'https://api.github.com/repos/apache/airflow', 'category': {'id': 32000716, 'node_id': 'MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAwNzE2', 'repository_id': 33884891, 'emoji': ':pray:', 'name': 'Q&A', 'description': 'Ask the community for help', 'created_at': '2020-10-29T12:10:12.000Z', 'updated_at': '2020-10-29T12:10:12.000Z', 'slug': 'q-a', 'is_answerable': True}, 'answer_html_url': None, 'answer_chosen_at': None, 'answer_chosen_by': None, 'html_url': 'https://github.com/apache/airflow/discussions/33538', 'id': 5537153, 'node_id': 'MDEwOkRpc2N1c3Npb241NTM3MTUz', 'number': 33538, 'title': 'Unable to Stop / Kill the currently running Airflow DAG programmatically.', 'user': {'login': 'jsaradhy', 'id': 50027714, 'node_id': 'MDQ6VXNlcjUwMDI3NzE0', 'avatar_url': 'https://avatars.githubusercontent.com/u/50027714?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jsaradhy', 'html_url': 'https://github.com/jsaradhy', 'followers_url': 'https://api.github.com/users/jsaradhy/followers', 'following_url': 'https://api.github.com/users/jsaradhy/following{/other_user}', 'gists_url': 'https://api.github.com/users/jsaradhy/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jsaradhy/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jsaradhy/subscriptions', 'organizations_url': 'https://api.github.com/users/jsaradhy/orgs', 'repos_url': 'https://api.github.com/users/jsaradhy/repos', 'events_url': 'https://api.github.com/users/jsaradhy/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jsaradhy/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 1545560198, 'node_id': 'MDU6TGFiZWwxNTQ1NTYwMTk4', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:Scheduler', 'name': 'area:Scheduler', 'color': '6a3aaa', 'default': False, 'description': 'including HA (high availability) scheduler'}, {'id': 1595825223, 'node_id': 'MDU6TGFiZWwxNTk1ODI1MjIz', 'url': 'https://api.github.com/repos/apache/airflow/labels/invalid', 'name': 'invalid', 'color': 'ce3150', 'default': True, 'description': ''}, {'id': 1937475224, 'node_id': 'MDU6TGFiZWwxOTM3NDc1MjI0', 'url': 'https://api.github.com/repos/apache/airflow/labels/area:API', 'name': 'area:API', 'color': 'efe07c', 'default': False, 'description': "Airflow's REST/HTTP API"}], 'state': 'open', 'state_reason': None, 'locked': False, 'comments': 4, 'created_at': '2020-09-03T01:18:52Z', 'updated_at': '2023-08-19T20:23:47Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'I have an usecase, we are implementing the automation with Gitlab, AWS S3 & Codepipeline, Code & Airflow. In a nutshell when there is a new version of the dag is available in the DAG BAG, i need to pro-grammatically stop the currently running Airflow DAG.\r\n\r\nGurus, could you please shed some light on this : Programmatically how can i stop/ kill the currently running DAG in Airflow ? ', 'reactions': {'url': 'https://api.github.com/repos/apache/airflow/discussions/33538/reactions', 'total_count': 1, '+1': 1, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/airflow/discussions/33538/timeline'}]
