cc @potiuk as no reviewers were assigned and you were kind enough to review and merge https://github.com/apache/airflow/pull/47774 üôè 

Thanks for the steps. I will add that too.

![screenshot_2025-03-16_at_8.38.03___pm.png](https://github.com/user-attachments/assets/ff21ab9a-9f4a-459d-8c24-01e1a6ec728d)\n\nScreen grab of the filter

Hmm.. Still `something is not yes` as my friend used to say....

> Hmm.. Still `something is not yes` as my friend used to say....\r\n\r\nhuh looking

> > Hmm.. Still `something is not yes` as my friend used to say....\r\n> \r\n> huh looking\r\n\r\nFound it, fixing!

Nice, regexp is playing hide & seek :)

Failures are unrelated to the fix, merging.

cool seems v2 will be happy now :) 

> cool seems v2 will be happy now :)\r\n\r\nNo, unfortunately some further fixes needed....

Just lucky I had an EmptyOp in one of my DAGs :)

> Just lucky I had an EmptyOp in one of my DAGs :)\r\n\r\nWe have SmoothOperator on production to ensure that everything works smoothly üòâ

Workflow testing require, will be able to do it only after merge.

Closing and reopening to run k8s tests

2 failing tests should be fixed by https://github.com/apache/airflow/pull/47822

Now\r\n<img width="882" alt="Screenshot 2025-03-15 at 8 33 43\u202fAM" src="https://github.com/user-attachments/assets/e8b74571-c328-49ee-8aa2-3cf97526a5c7" />\r\nvs Before\r\n<img width="1141" alt="Screenshot 2025-03-15 at 8 34 17\u202fAM" src="https://github.com/user-attachments/assets/59f2ba9f-84f8-4552-a597-bf46bedd58c9" />\r\n\r\nSimilar changes in the other details pages as well.\r\n

@bugraoz93 Oh, committer status is not live in Github :-O\r\n![image](https://github.com/user-attachments/assets/6dc98939-f0b4-44bc-a5d5-15c681ed1363)\r\n

Merging. Static checks look better, will raise another PR for other fails.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Wow! Impressive metric! More files moved than code changed :-D \r\n![image](https://github.com/user-attachments/assets/c56b8818-0b09-4a1d-8fc3-dc6df2681f21)\r\n

> Wow! Impressive metric! More files moved than code changed :-D\r\n\r\nIndeed :)

>  Wow! Impressive metric! More files moved than code changed :-D\r\n\r\nThis changed now :) .. During making the PR green I used the opportunity since I was changing anyway the `package preparation` steps - that I **finally** will fix the long standing "what is a package" problem.\r\n\r\nIn the current incarnation of the change I made a consistent change to name what we produce "distributions" not "packages" - because "package" is really ambiguous.\r\n\r\nSee https://packaging.python.org/en/latest/discussions/distribution-package-vs-import-package/\r\n\r\nSo we will be:\r\n* preparing distribution (in a give distribution format - wheel, or sdist)\r\n* releasing distribution\r\n* verifying distribution\r\n* working on distribution \r\n\r\nWe will have `provider distributions` rather than `provider packages`. This is mostly changing `breeze` DISTRIBUTION in our code and documentation links etc. \r\n\r\nI think this is the right time of doing it especially that we are going to have those  DISTRIBUTIONS:\r\n\r\n* `apache-airflow`\r\n* `apache-airlfow-core`\r\n* `apache-airlfow-task-sdk`\r\n* `apache-airflow-providers-*` -> 90+ of those\r\n\r\nAnd most of those will have Python `airflow` package inside.\r\n

Little more context: if you have things misconfigured, you can land in a endless redirect loop.

> We are experiencing issue with SimpleAuthManager.\r\n\r\nFAB as well\r\n

Edited, thanks for the investigation \r\n

Strange we have pre-commit checking those - maybe we miss a check or something got broken there - I will check it later 

One nit - could not find the place in code... - can you please add some spacing between line number and content and make the line number right aligned - like it is rendered in the code panel?

@bbovenzi I have made the changes to ensure the line number is not copied and also updated the scroll logic.\r\n\r\n> Do we want to make sure the log line number isn\

It looks like `try_number` in URL is updated on selecting a try from the dropdown but copying the URL with try_number in query param and loading it in new tab still opens the latest try for the task instance. I will open an issue for that. \r\n\r\nMerging the changes for feedback. Thanks @bbovenzi and @jscheffl .

Thanks for the PR @aarochuk! Can you add a screenshot or 2 to the description as well?

If you look around the codebase, we already have a clipboard component that we should reuse here. [For example](https://github.com/apache/airflow/blob/main/airflow/ui/src/components/RenderedJsonField.tsx)

Nice K8S tests look much more stable.

It will be heck easier after #47798

Nice Small change :)

I checked the same PR regarding the upgrade from v17 to v18 (https://github.com/apache/airflow/pull/40158) and didn¬¥t see any changes related to the documentation. So, I guess not. 

> Hi there :) Yes, this is a valid change, so approved üëç\r\n\r\nGreat\r\n

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Thanks @Kayzwer! Congrats on your first commit üéâ 

One of the k9S tests failed, but this was a dffferent reason - looked like regular "failure" of the infrastructure.  Merging.

Cool. I love to see such changes merged when I am catching up after a day or two of head down on refactoring stuff :D

@potiuk thank you! I have a big place in my heart for developers getting started :) \r\nAnd breeze is the tool that gets them going.

I think we are missing some occurences in:\r\n- airflow/ui/dev/index.html|7 col 66| <link rel="icon" type="image/png" href="http://localhost:5173/public/pin_32.png" />\r\n- clients/python/test_python_client.py|69 col 32| host="http://localhost:8080/public",\r\n- docker_tests/test_docker_compose_quick_start.py|94 col 80| method: str, path: str, base_url: str = f"http://{DOCKER_COMPOSE_HOST_PORT}/public", **kwargs\r\n- airflow/api_fastapi/app.py|76 col 65| description="Airflow API. All endpoints located under ``/public`` can be used safely, are stable and backward compatible. "\r\n- airflow/api_fastapi/auth/managers/simple/ui/dev/index.html|6 col 66| <link rel="icon" type="image/png" href="http://localhost:5174/public/pin_32.png" />\r\n

I am fixing the conflicts and reopening.

> I think we are missing some occurences in:\r\n> \r\n> * airflow/ui/dev/index.html|7 col 66|\r\n> * clients/python/test_python_client.py|69 col 32| host="http://localhost:8080/public",\r\n> * docker_tests/test_docker_compose_quick_start.py|94 col 80| method: str, path: str, base_url: str = f"http://{DOCKER_COMPOSE_HOST_PORT}/public", **kwargs\r\n> * airflow/api_fastapi/app.py|76 col 65| description="Airflow API. All endpoints located under `/public` can be used safely, are stable and backward compatible. "\r\n> * airflow/api_fastapi/auth/managers/simple/ui/dev/index.html|6 col 66|\r\n\r\nMade the required changes and resolved conflicts.

Nice!

As I am re-looking ... why actually is there a play AND a stop button? Should it be "paused" and "stop" and "play" only while it is paused?

> As I am re-looking ... why actually is there a play AND a stop button? Should it be "paused" and "stop" and "play" only while it is paused?\r\n\r\nOh actually this backfill was paused, hence the play button. I guess you can still choose to cancel the backfill entirely while its paused.

cc: @dstandish let me know if this makes sense to you

@jscheffl Actually, I decided to move it all to a single Menu with submenus (see the new screenshot). I think this is much cleaner until we do AIP-68+

> @jscheffl Actually, I decided to move it all to a single Menu with submenus (see the new screenshot). I think this is much cleaner until we do AIP-68+\r\n\r\nFor me top-level was "nicer" but can accept the change as well.

> @jscheffl Actually, I decided to move it all to a single Menu with submenus (see the new screenshot). I think this is much cleaner until we do AIP-68+\r\n\r\nHave you considered loading any plugin into an iframe like FAB? (Or will this implicitly raise security problems?)

> > @jscheffl Actually, I decided to move it all to a single Menu with submenus (see the new screenshot). I think this is much cleaner until we do AIP-68+\r\n> \r\n> Have you considered loading any plugin into an iframe like FAB? (Or will this implicitly raise security problems?\r\n\r\nMenu items were already external links\r\nBut AppBuilderViews will probably be iframes\r\n\r\nAnd I"ll let others chime in about top-level or nested menu items. I could see where hiding links away is also a bad UX

<PersonalOpinion>I like better nested because it scales better. If there are many plugins installed in the environment, it might degrade the UI if we add them as top level in the menu.</PersonalOpinion>

Did you check that the login form renders properly? http://localhost:28080/auth/login.\r\n\r\nAlso, plugins are available here http://localhost:28080/pluginsv2. We either want to embed them as well in a iframe (I think it would be great) or leave the header for them, otherwise the experience will be quite bad for users.

Parallel fix #47744

Snap!

Nope . Not good. We want to skip 25.2.0 really :)

#47744 

cc: @jedcunningham @vincbeck 

Error unrelated to the PR. Merging

@pierrejeambrun , should we mark the k8s failing test as xfail until @jason810496 raises a PR?

Rebased to account for fixed static check in main

hi @potiuk, Is there anything I need to do now? Are the K8s tests related to this open ticket? [#47518](https://github.com/apache/airflow/issues/47518)

> hi @potiuk, Is there anything I need to do now? Are the K8s tests related to this open ticket? [#47518](https://github.com/apache/airflow/issues/47518)\r\n\r\nWe had a stability issue in main but this should be fixed now. Rebased to make sure.

cc: @vincbeck 

Yes we need to do the same mechanism as login :/ I can create the methods in the auth managers if you want then, you can use it in this PR

That would be great, thanks

https://github.com/apache/airflow/pull/47729

Unrelated failure, merging.

Actually I think this timetable was not backported at all?

> Actually I think this timetable was not backported at all?\r\n\r\nHow can it be?\r\nThe bug report was on 2.10.5 

Ah OK. That make sense.\r\nSo this fix is for Airflow 3

Oh god!!!

> Oh god!!!\r\n\r\nWe are still not getting branch protection back so ... yeah :(

@potiuk  My intention was to allow `get_uri()` in the Connection model to directly return a SQLAlchemy-compatible URI. However, I now see that my approach was incorrect. \r\n\r\nIf using `sqlalchemy_url` is the proper way, would it be better to update the get_uri() in `models/connection.py` docstring instead? \r\n\r\nI can modify the PR accordingly.

> Looks good. Maybe we are missing `cancel_backfill` and `update_xcom_entry` too ?\r\n\r\n`cancel_backfill` already has logging. We need to add `update_xcom_entry`.

Static checks need fixing.

We are dropping SLA in Airflow 3 - it will be replaced by a different feature https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=323488182 - I doubt we want to invest any time and effort into improving and optimizing the old feature.

Closing because I did this rebase incorrectly. See https://github.com/apache/airflow/pull/47705

Yes, I\

Amazon lower-bound tests passed. Merging.

Nice thanks

Failures are unrelated. Merging.

Any other opinion @potiuk, @ashb ?

> Any other opinion @potiuk, @ashb ?\r\n\r\nYes. But It would be great to have a simple way to set "normal" / FAB configuraiton for Breeze. If only to test it.

> > Any other opinion @potiuk, @ashb ?\r\n> \r\n> Yes. But It would be great to have a simple way to set "normal" / FAB configuraiton for Breeze. If only to test it.\r\n\r\nYou mean to add a `--auth-manager` parameter to `breeze start-airflow`?

yep. and maybe a way to have simple auth manager somewhat configurable (hard-coded configuraiton that can be used with different users ?)

> yep. and maybe a way to have simple auth manager somewhat configurable (hard-coded configuraiton that can be used with different users ?)\r\n\r\nI could create by default 2 users:\r\n- admin\r\n- viewer\r\n\r\nOr even more?\r\n\r\nI can also generate friendly passwords to avoid having to look at generated passwords (only for breeze of course)

Or we can keep it in a repo and simply pass an env variable to point to it?

The latter likely better\r\n

Oh but we already have a config `simple_auth_manager_passwords_file`. We can use that

One transient failure in pulling docker iamges. Going to merge this now a bit early as many jobs have passed already

Errors are unrelated to this PR. Merging.

Yeah, I think we could update the bar chart design so it matches everything better

Duplicate of #47679

Dup of #47676, not itself :)

TaskSDK tests are failing in task sdk. Fix is here https://github.com/apache/airflow/pull/47679

Merging. Failure is unrelated.

wtf is going on with those kub tests

Failures are unrelated to the changes made. Overriding and merging

There is a [discussion](https://apache-airflow.slack.com/archives/C06K9Q5G2UA/p1741788219567559) on Slack and `state` might get re-added to RunTimeTaskInstance. <s>Will keep this as draft for now, until it clarifies.</s> We should proceed with this PR regardless.

> Yeah works for now as a stop gap solution\r\n\r\nAgreed, this is not a substitute for a better solution.

@ashb @amoghrajesh is this mergeable? The failing CI check is related to the amazon provider not being able to compile the xmlsec wheel, which is not related to my change.

Yes, that is unrelated, merging it.

I think fix already merged https://github.com/apache/airflow/pull/47667 closing this one

same reason as https://github.com/apache/airflow/pull/47658#issuecomment-2718085359

The compat failure seems unrelated to my change, merging it

We should catch when the FAB provider isn\

> We should catch when the FAB provider isn\

Also, I am not saying this is wrong but do we want to expose it as API? We cannot pass it as config? Using an API would mean latency, so these additional links would be added to the menu after a small delay

https://github.com/apache/airflow/pull/47688

> #47688\r\n\r\nThanks ! I will rebase this PR after the fix being merged.

merged https://github.com/apache/airflow/pull/47688 and rebase this branch from the main branch

Same reason as https://github.com/apache/airflow/pull/47658#issuecomment-2718085359\r\n

One provider test failing, but I‚Äôm seeing the same in other PRs too. Merging this since it seems entirely unrelated.

After discussion permissions for execution API are not required yet and will not work in the same way when we do. For now we will just need to ensure that the client is authenticated with something like `requires_authenticated` that just validate the token integrity. I think that falls into @ashb current work around auth + execution api: https://github.com/apache/airflow/pull/46981 (and follow up PRs)\r\n\r\n\r\nI think we should close this PR and related ones on the execution api. 

Unrelated failure?

Lot of failures but none seem related, merging this to unblock one level of fixes 

cc: @eladkal @potiuk @amoghrajesh 

Is this ready @dstandish ? I just rebased it to fix CI failures that were fixed in main

> Is this ready @dstandish ? I just rebased it to fix CI failures that were fixed in main\r\n\r\nthink so. added tests.

I see dag run timeout as PT1H in the dag details screenshot. I remember a separate component in Airflow 2 to render timeout which can be a timedelta value as well. Does that still apply here?

Opened https://github.com/apache/airflow/issues/47660 for timedelta serialization since it seems pydantic renders it to ISO 8601 duration format which is not very intuitive. 

@ramitkataria is this a duplicate of #47593?

CI failures are unrelated

Can we add a test for this?

@prabhusneha can we please add some unit tests and also test with the dag that was originally provided in the bug report?

Added unit tests

Fixing the failing CI checks

Merging for openai errors - others are still to be fixed.

Please wait a little bit before merging that.\r\n\r\nBecause it will cause conflicts on many different other PRs that we are about to merge, while this might be easier to just update this one ?

> Please wait a little bit before merging that.\r\n> \r\n> Because it will cause conflicts on many different other PRs that we are about to merge, while this might be easier to just update this one ?\r\n\r\nWill do :)

Which PRs exactly? Just so I can monitor them and know when to merge it

Great thanks. I‚Äôm on my phone at the moment so it‚Äôs hard to link that but basically PRs for ‚ÄúAIP-84 Auth‚Äù. Multiple of them use those functions. (Dag tags, rawwar‚Äôs one too, and some others are coming).\r\n\r\nAll of them should be linked to the meta ‚ÄúAIP-84 Permission‚Äù issue where it‚Äôs easier to monitor.

I think most of the related PRs have been merged.  We can proceed here !

Companion PR on ruff: https://github.com/astral-sh/ruff/pull/16647

CI failure is unrelated and will be fixed by https://github.com/apache/airflow/pull/47643

I dismissed the alerts from CodeQL as false positive.\r\n\r\nFor the compat tests - git provider should only be for airflow 3 so you should also add git to the list of excluded providers in src/airflow_breeze/global_constants.py (PROVIDERS_COMPATIBILITY_TESTS_MATRIX)

> I dismissed the alerts from CodeQL as false positive.\r\n> \r\n> For the compat tests - git provider should only be for airflow 3 so you should also add git to the list of excluded providers in src/airflow_breeze/global_constants.py (PROVIDERS_COMPATIBILITY_TESTS_MATRIX)\r\n\r\nThanks! Almost got stuck at that! @dstandish is also fixing false positive at https://github.com/apache/airflow/pull/47685

Good catch! Is this true for dag runs too or only TIs?\r\n\r\nWe should also add a check to https://github.com/apache/airflow/blob/main/airflow/ui/src/pages/TaskInstances/TaskInstances.tsx#L140, https://github.com/apache/airflow/blob/main/airflow/ui/src/pages/TaskInstance/Details.tsx#L135,\r\nand https://github.com/apache/airflow/blob/main/airflow/ui/src/pages/MappedTaskInstance/Header.tsx#L49

> Is this true for dag runs too or only TIs?\r\n\r\nI _think_ just TIs.

> We should also add a check to https://github.com/apache/airflow/blob/main/airflow/ui/src/pages/TaskInstances/TaskInstances.tsx#L140, https://github.com/apache/airflow/blob/main/airflow/ui/src/pages/TaskInstance/Details.tsx#L135, and https://github.com/apache/airflow/blob/main/airflow/ui/src/pages/MappedTaskInstance/Header.tsx#L49\r\n\r\nDone! Sorry, I should have looked around a bit more :)

another fix is coming https://github.com/apache/airflow/pull/47667/files

I just rebased the PR too to eliminate issues with `default_view` : https://github.com/apache/airflow/pull/47616

Yeah I think the most problematic part here is where we load example dags into DagBag for tests. Tests just send things in that DagBag directly into the scheduler, but the scheduler expects the DAG class in `airflow.models` instead.\r\n\r\nThere are a couple of other failures but they are all pretty trivial like the one you mentioned.\r\n\r\nRelatedly, we currently do not have example DAGs that use the SDK. This would have been surfaced sooner if we added at least one.

>Relatedly, we currently do not have example DAGs that use the SDK. This would have been surfaced sooner if we added at least one.\r\n\r\nYeah we have some here https://github.com/apache/airflow/tree/main/task-sdk/tests/task_sdk/dags but we have not changed all the example_dags in the https://github.com/apache/airflow/tree/main/airflow/example_dags folder

close it in favor of https://github.com/apache/airflow/pull/47659

We probably need to do the same for variables too, but that can be handled seperately.

Fix to failing lower-bind checks in amazon xmlsec in https://github.com/apache/airflow/pull/47696

Failures are unrelated.

This resolves all front-end errors on the login page except one:\r\n```\r\nUncaught TypeError: moment(...).tz(...).format is not a function\r\n    at r (main.ec1d38d994d72bb083cd.js:2:826)\r\n    at u (main.ec1d38d994d72bb083cd.js:2:2623)\r\n    at HTMLDocument.<anonymous> (main.ec1d38d994d72bb083cd.js:2:4808)\r\n    at e (jquery-latest.js:2:30038)\r\n    at jquery-latest.js:2:30340\r\n```\r\n\r\nLooking into it but that can fixed in a separate PR\r\n

>Could you please include a test case for including a . in the label and see if it is without . and with -?\r\n\r\nAnd please make sure to add test case with `prefix_group_id = False` (with and without dots in the task_id)

Hi, \r\n\r\nI‚Äôve added screenshots to demonstrate the test cases requested‚Äîboth with and without a dot in the label, and with prefix_group_id set to True and False. I hope this satisfies the requirements. \r\nPlease let me know if anything else is needed!\r\n\r\n1. ) task_ids with \

CI failure unrelated and fixed in #47656, #47696\r\n

Rebased.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Thanks! I agree with fab.\r\n\r\nOn common messaging, it is ready on my side but maybe @vikramkoka wants to update the documentation?

We need to add the not-ready providers to be build and prepared for PROD image - even if they are not ready (currently they are not). This is why docker-compose and k8s tests fail because they are installing fab provider 1.5.3 as the "not-ready" fab provider is excluded 

Fixup is coming

The fixup I added **should help**

> We need to add the not-ready providers to be build and prepared for PROD image - even if they are not ready (currently they are not). This is why docker-compose and k8s tests fail because they are installing fab provider 1.5.3 as the "not-ready" fab provider is excluded\r\n\r\nIntresting. So why it wasn\

Rebased to latest version 

Test failure is not related to the PR.\r\nmerging

Lets wait for the Ci though

Thanks @phanikumv and @jedcunningham for the review.

Let me just try this locally, one moment please :) 

@pierrejeambrun Please. It seemed to work find for me in dev and prod modes

I am having trouble making it work with a URL prefix, any hint for me ?\r\n\r\nBasically setting the base_url to `http://localhost:28080/d12345/` (you might need to provide AIRFLOW__API__BASE_URL env variable to breeze too). The front-end should be on `http://localhost:28080/d12345/` but there seem to be some routing problem I have a blank page.\r\n\r\nIf I manually go to the dag page I end up with this:\r\n![Screenshot 2025-03-10 at 17 26 30](https://github.com/user-attachments/assets/9c1dcaf0-9ae2-4446-813f-29749495095b)\r\n\r\nLet me know if you managed to reproduced or if you want me to take a look. That might just be on my side though.\r\n

Well, I just tried on main and things do not seem to work anymore either..... :( but the errors are different

Wrong branch pushed. NOt ready for review yet

Note: failed test also failing on main - AWS failures in CI are un-related to the PR.

Yep. Rebase would be great.

@eladkal @potiuk I have prepared a fix for the import issue mentioned here https://github.com/apache/airflow/pull/47056

LGTM. 

@eladkal ?

When we add logout. We may want to use an axios interceptor too.

> cc: @jedcunningham @vincbeck\r\n> \r\n> What type of permissions do we want on dag versions ? Do we want a new entity ? Or dag related dag access / code is enough ?\r\n\r\nI think we should add a new entity in `DagAccessEntity`

merging

We already have PR for this https://github.com/apache/airflow/pull/47523

I‚Äôm going with the other PR since that one alreay has multiple approvals.

The title is duplicate with #47542. \r\nCould you update the title and include related issues in the description like `closes:#..` and `relates:#..` @aditya0yadav? If there is no issue, that is also fine but a small title separation would be amazing.

Seems like this is PR is the test of this https://github.com/apache/airflow/pull/47542? whilst the`/confirm` endpoint is deleted  :thinking:

Nice!

> common.messaging is draft - I think we need to mark this as not ready?\r\n\r\nWe could also make it 0.1, I do not think that there is harm in releasing it early.

> We could also make it 0.1, I do not think that there is harm in releasing it early.\r\n\r\nI reserved it in `PyPI` BTW.

> We could also move the ConfigMap patching step into a Breeze k8s utility that runs before shell and test commands. Open to discussion‚Äîany advice would be greatly appreciated!\r\n\r\nThat one is better I think, as theorethically we could run the tests outside of the `breeze k8s` utiltity. I always prefer to run setup - if only possible - in a pytest fixture, as this makes tests far more portable in general. We had a lot, a lot of cases where we had some environmental setup in outside scripts or even breeze image and that was a cause for many problems where someone attempted to run tests in not-exactly-the-way-it-was-supposed-to-be causing lots of confusion and time lost for diagnosing the issues.

BTW. Very nice and out-of-the-box idea ;) . I would have never come up with such an aproach, but it is really nice :)

Also it has a very nice side-effect ..... We are ACTUALLY testing rolling restart of API server this way :) 

> > We could also move the ConfigMap patching step into a Breeze k8s utility that runs before shell and test commands. Open to discussion‚Äîany advice would be greatly appreciated!\r\n> \r\n> That one is better I think, as theorethically we could run the tests outside of the `breeze k8s` utiltity. I always prefer to run setup - if only possible - in a pytest fixture, as this makes tests far more portable in general. We had a lot, a lot of cases where we had some environmental setup in outside scripts or even breeze image and that was a cause for many problems where someone attempted to run tests in not-exactly-the-way-it-was-supposed-to-be causing lots of confusion and time lost for diagnosing the issues.\r\n\r\nThanks for confirming, @potiuk I‚Äôll move it to Breeze in the next PR‚Äîalready noted in my backlog.  \r\n\r\n> BTW. Very nice and out-of-the-box idea ;) . I would have never come up with such an approach, but it is really nice :)  \r\n\r\nReally appreciate that! Glad you liked the idea‚Äîyou just made my day! 

Cool! Thanks for the fix!

CI failures seem unrelated. Rebasing

Okay, two tests from k8s are seems related. I will check this out to be sure, but implementation should be ready for review.

Nice!

Not needed after discussions on #47538. These will go to the standalone package anyway so no reason to split more in the core.

Thank you for reviewing this PR! I followed the contribution guidelines and added Idrica to INTHEWILD.md. Let me know if any changes are needed. üöÄ

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

For CI authenticated request on the K8S tests, you can take a look at https://github.com/apache/airflow/pull/47433 that encountered the same problem I believe.

just resolve the conflict and rebase from the latest main. we should have the required methods now

I think the openai issue is fixed in main. let me rebase and see how it works

@jason810496 how can this be flaky ? I thought the session was authenticated \r\nhttps://github.com/apache/airflow/actions/runs/13817174257/job/38655368903?pr=47532\r\n\r\nAlso `forbidden` means that the token is provided but not correct. (maybe the signature key is not the same at generation time and at verification time and need to be set in the config). => we do not explicitly set it in the config. Therefore we generate one and `set` it everytime which might end up causing different keys to be used...

all green and all comments resolved. merging

That would be great @jason810496 maybe that will help stabilizing, ty 

> @jason810496 how can this be flaky ? I thought the session was authenticated https://github.com/apache/airflow/actions/runs/13817174257/job/38655368903?pr=47532\r\n> \r\n> Also `forbidden` means that the token is provided but not correct. (maybe the signature key is not the same at generation time and at verification time and need to be set in the config). => we do not explicitly set it in the config. Therefore we generate one and `set` it everytime which might end up causing different keys to be used...\r\n\r\nhttps://github.com/apache/airflow/pull/47739 most likely

@ninsbl reported on the issue that is possibly a bug rather than just a doc update. Is that still the case?

For me the change is okay. To be on the safe side... one improvement idea: Can you add the code snippet rather as a new example DAG (e.g. name it `example_custom_weight.py`)? Then you can change the RST to include the code snippet and when loading Airflow with example DAGs you can see how it is working.\r\n\r\nIn the past such example DAG was not added because plugins/extensions were not loaded by default, but this has changed since then. So adding an example DAG might be a good option nowadays.

sure @jscheffl , i already update the code to add example dags. Also for the test on the dag and documentation using breeze there is no issue regarding this custom weight rule

thanks for the review @jscheffl , i have changed the code

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Nice. Thanks, @azharizz !

https://github.com/apache/airflow/issues/47511 reports the warning also on 2.10

This is ready for review.

I think we can also remove `common.compat`  - it is only needed when some providers depends on newer, not released version of the provider and I think all providers have been released recently

Thanks everyone for the comments. I am closing the PR. This can be revisited if there are requests from users in Airflow 3.x .

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> Nice, lets see if the CI agrees.\r\n\r\nIt does not :)

```python\r\n   "navbar_color": (re.compile(r"(?i)\\A#007A87\\z"), "#fff", "2.1"),\r\n    File "/usr/local/lib/python3.9/re.py", line 252, in compile\r\n      return _compile(pattern, flags)\r\n    File "/usr/local/lib/python3.9/re.py", line 304, in _compile\r\n      p = sre_compile.compile(pattern, flags)\r\n    File "/usr/local/lib/python3.9/sre_compile.py", line 788, in compile\r\n      p = sre_parse.parse(p, flags)\r\n    File "/usr/local/lib/python3.9/sre_parse.py", line 955, in parse\r\n      p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\r\n    File "/usr/local/lib/python3.9/sre_parse.py", line 444, in _parse_sub\r\n      itemsappend(_parse(source, state, verbose, nested + 1,\r\n    File "/usr/local/lib/python3.9/sre_parse.py", line 526, in _parse\r\n      code = _escape(source, this, state)\r\n    File "/usr/local/lib/python3.9/sre_parse.py", line 427, in _escape\r\n      raise source.error("bad escape %s" % escape, len(escape))\r\n  re.error: bad escape \\z at position 13\r\n```\r\n\r\nüò± 

Just a breeze test to fix

Sorry, making a bit of hash of this PR. ü§ûüèª 

sdfdsakf;jdsfkldsajfdsjk :table-flip:

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

got it. Thanks @pierrejeambrun . let me mark it as draft first 

https://github.com/apache/airflow/pull/47562 this will fix the issue and needs to be merged first. Tested with this PR change and mine, things are working as expected.

https://github.com/apache/airflow/pull/47562 merged! rebased

Static check need fixing (maybe you should run precommit locally to avoid this, especially when the API CI is full test -> very long)

hmmm... weird. changed nothing since the last CI green.

nothing can be fixed by running pre-commit locally ü§î let me push it again and see how it works

As this is approved and CI is green, gonna merge it

all green üôå

I added an additional commit to remove db calls in `iter_dag_dependencies` as well. This is called during DAG serialization. The result is stored in the SerializedDag and sent to the UI later. Removing the calls makes the UI not quite right, but @Lee-W is already working on changing the API around this.

Let me know when this is ready for review.

cc: @vincbeck 

Hello @valentinDruzhinin !\r\nCould you please provide a screenshot that this [example_bigquery_tables.py](https://github.com/apache/airflow/pull/47479/files#diff-b4438e6b9a5d880dd10348db43cf9756c4b8a26ac8a0f7bb4d46cc4342663327) system test passed successfully on your Airflow environment? 

Hey @vincbeck, I‚Äôd love to get your thoughts on this.\r\n\r\nI‚Äôve tested the connection setup, and this PR mainly focuses on documentation. Adding a test file could help validate the setup better.\r\n\r\nWhile working on example_exasol_to_s3.py, I noticed exasol_to_s3 could be refactored for clearer separation of concerns, making testing easier. Thinking of including this in the PR‚Äîdoes that sound good to you?\r\n\r\nI‚Äôll adjust the docs as needed once we decide. Looking forward to your thoughts! üöÄ

> Hey @vincbeck, I‚Äôd love to get your thoughts on this.\r\n> \r\n> I‚Äôve tested the connection setup, and this PR mainly focuses on documentation. Adding a test file could help validate the setup better.\r\n> \r\n> While working on example_exasol_to_s3.py, I noticed exasol_to_s3 could be refactored for clearer separation of concerns, making testing easier. Thinking of including this in the PR‚Äîdoes that sound good to you?\r\n> \r\n> I‚Äôll adjust the docs as needed once we decide. Looking forward to your thoughts! üöÄ\r\n\r\nThat sounds perfect to me :)

hanks for your feedback, @vincbeck! üòä\r\n\r\nGlad to hear you‚Äôre on board with the changes. I‚Äôll add the test file and refactor exasol_to_s3 for better separation of concerns and easier testing.\r\n\r\nI‚Äôll also update the docs accordingly‚Äîlet me know if you have any suggestions! üöÄ

Hey @vincbeck, thanks for your input!\r\n\r\nJust wanted to confirm‚Äîif this kind of introduction (how to test the Exasol connection without needing a credit card) is what you were expecting. If so, do you have any thoughts on where it would be best placed?\r\n\r\nLet me know what you think! üöÄ

Closing based on the discussion in #43264.

I noticed we still had some old recommendation in quick-start - uv + localvenv is the new recommended way.

As discussed https://github.com/apache/airflow/pull/46502 was accidentally removing the other tests in Feb üòì ..

We should bring back all tests.

There are also KPO tests for example.

cc: @jason810496 -> Maybe you can bring it back, apparently I missed that #46502 only run k8S test.

As https://github.com/apache/airflow/pull/46502 was made to remove flakiness... hopefully this does not add it back again :-O

üëÄ 

I accidentally committed the modified test case (I only ran the KubernetesExecutor locally for faster iteration during development).\n\nI believe this won‚Äôt reintroduce flakiness, as I simply replaced time.sleep with kubectl rollout status --watch.

Green (besides sadly I needed to add XFail for the LocalExecutor tests which are caused by not able to make LocalExecutor work in KinD --> #47518 )

Errors are not related to this PR

Uuups, sorry. Wanted to approve but was clicking the wrong button :-O

@bbovenzi  I guess this change caused some issue with log grouping where span tags causes all lines in a log group to appear in single line unlike the Text tag which makes it appear in separate lines. \r\n\r\nThis seems to render the grouped log lines as expected.\r\n\r\n```\r\ngit checkout 97d1645~1 src/\r\nUpdated 2 paths from 9f0b4399d\r\n```

(I need more emojis - :highfive:)

cc: @kacpermuda 

Shoutout to @tirkarthi for the easy reproduction steps.

Thank @pierrejeambrun! I forgot that the server is actually using `FabAuthManager`, and should set token in headers instead of api_key üòì

> Just a question, it looks like before `filter_permitted_dag_ids` and `get_permitted_dag_ids` would return a dag id if we had read or write permission on it.\r\n> \r\n> Now this is not the case anymore, we absolutely need the `read`. Are we sure that existing installation always set "GET" access when they grant "PUT" access to a user, could that cause disruption ? For instance certain legacy endpoint would not specify the method, defaulting to `GET` or `PUT`.\r\n\r\nNo, if you pass method="PUT", you do not need read access, only the edit access. `GET` is the default value for `method` but it does not mean you must have it

Maybe I got it wrong but that doesn\

> Maybe I got it wrong but that doesn\

Makes sense !

This is still not fully complete - some tests are failing but it proposes how we can move `task_sdk` to standalone `task-sdk` distribution. 

Looks good, green and straightforward :)\r\n

I moved the make_client stuff to `task_sdk/__init_,py`

I will need to manually update current constraint - so this one will not get green

> Nice, we need some tests too.\r\n\r\nYes, currently working on that, will update the PR once it is done.\r\nThankyou

Hi @pierrejeambrun , I have added few tests, COuld you please review it and let me know if any additional things to be required.\r\nThankyou

We need unit tests updated for that one.

> We need unit tests updated for that one.\r\n\r\nOnly fix the failing ones or also add a new one?

> > We need unit tests updated for that one.\r\n> \r\n> Only fix the failing ones or also add a new one?\r\n\r\nChoose what you think is best

@potiuk unit tests should work now.

You can resolve the conflict by accepting your changes then run `pre-commit run update-er-diagram --all-files` after and commit the result

https://github.com/apache/airflow/pull/47458

Finally fixed the Kubernetes tests locally!  \r\n\r\nSome notes:  \r\n\r\n1. The API server still uses FAB Auth Manager, so when retrieving a JWT, the same session must be used to handle the CSRF token session issue. Otherwise, it results in:  \r\n   ```\r\n   <html lang=en>\r\n   <title>400 Bad Request</title>\r\n   <h1>Bad Request</h1>\r\n   <p>The CSRF session token is missing.</p>\r\n   ```\r\n\r\n2. The current `redirect_url` from FAB is still `http://localhost:8080/?token=...`, but `KUBERNETES_HOST_PORT` is not `localhost:8080`. \r\n   ~This can be fixed in a future PR.~\r\n   Update: Fixed in https://github.com/apache/airflow/pull/47544

No error with [Kubernetes tests / K8S System:KubernetesExecutor-3.9-v1.29.12-true](https://github.com/apache/airflow/actions/runs/13738401436/job/38425329184?pr=47433#logs)\r\nbut [Kubernetes tests / K8S System:KubernetesExecutor-3.9-v1.29.12-false](https://github.com/apache/airflow/actions/runs/13738401436/job/38425329252?pr=47433#logs) always fail due to timeout.\r\n\r\n```\r\n=========================== short test summary info ============================\r\nFAILED kubernetes_tests/test_kubernetes_executor.py::TestKubernetesExecutor::test_integration_run_dag_with_scheduler_failure - Failed: Timeout >300.0s\r\n=================== 1 failed, 1 passed in 376.40s (0:06:16) ====================\r\n```\r\n\r\nUpdate:\r\nI just looked into the Kubernetes test with the `-false` suffix, which indicates that `useStandardNaming=false`. I will try deploying with `useStandardNaming=false` and observe the differences when running the Kubernetes test against a cluster with this setting.

The Kubernetes test has finally been fixed! üéâ However, the CI failed due to the flakiness of CeleryExecutor and LocalExecutor.

looks like flaky ones. let me retrigger after CI finishes this round

Retired a few times. Still fails. @jason810496 do you remember why it passed this morning ü§î (but it came with conflict, we still need to reran then ü•≤)

> Retired a few times. Still fails. @jason810496 do you remember why it passed this morning ü§î (but it came with conflict, we still need to reran then ü•≤)\r\n\r\nAt that time:  \r\n- `dag_reserialize` was called before the test.  \r\n- However, with `dag_reserialize` enabled:  \r\n  - The KubernetesExecutor seemed more stable in tests.  \r\n  - The CeleryExecutor and LocalExecutor tended to time out during tests.  \r\n\r\nRegardless, I will first fix `docker_tests/test_docker_compose_quick_start.py`. When I push the fix, I‚Äôll also run the Kubernetes tests again.  \r\n\r\nIf the Kubernetes tests still fail, we could try:  \r\n- Adding `dag_reserialize` back.  \r\n- Increasing the timeout threshold for CeleryExecutor and LocalExecutor.  

> Adding dag_reserialize back.\r\n\r\nWhy is this needed? Because the dag is not yet serialized? If so, should we add wait in the test instead?

> > Adding dag_reserialize back.\r\n> \r\n> Why is this needed? Because the dag is not yet serialized?\r\n\r\nYes, explicitly running `airflow dags reserialize` can resolve the issue described in https://github.com/apache/airflow/pull/47433#issuecomment-2708289346 (where triggering a new `dagRun` results in a 404, indicating that `DagModel.is_active` is `False`).  \r\n\r\n> If so, should we add a wait in the test instead?  \r\n\r\nI believe explicitly running `reserialize` is more precise than waiting for a fixed duration, which could introduce more flakiness in the future.\r\n

`CeleryExecutor` failed ü§î but one k8s seems to work fine. Looks like something could be fixed by rerun

We already have each executor successfully ran in k8s test, but we are not lucky enough to have them **all** green in same try.\n\nDo we still need to keep retry the test until we get all green in one try ? Most of them are failed with timeout. 

> Yep, what I meant was maybe we can extend the timeout time. but would like to confirm whether it was set that way for a reason\r\n\r\nSure. will increase it to 500 to ensure that the test failure is solely due to the timeout threshold.\r\nI will push if this try still fail.

Base on the following traceback, I think we need to add retry mechanism for `_get_jwt_token` part, or maybe having HTTP Adapter for 401, 403 status code.\r\n\r\nhttps://github.com/apache/airflow/actions/runs/13768914292/job/38504602702\r\n

New CI fail:[Prepare breeze & CI image:3.9](https://github.com/apache/airflow/actions/runs/13779187121/job/38534589200?pr=47433) probably related to rebase ? \r\n\r\ncc @Lee-W 

Yep, seems to be something starts to happen this morning and can be fixed by retriggering

Happy to take any other suggestion and will apply, i am not very good with front end stuff :) 

> Nice.\r\n> \r\n> To successfully close the issue, we need to do the same for other AuthManagers (FabAuthManager, etc...).\r\n> \r\n> Also we need to remove the front-end code that handles the token from the URL as it will not be needed anymore. (in `/airflow/airflow/ui/src/layouts/BaseLayout.tsx`)\r\n\r\ncool will update it :)

I have tried to use headers, but no luck, ended up doing with html render, If this is fine please let me know.

> What about other auth managers? How do they set the token?\r\n> \r\n> Did we consider setting the token in a cookie that the "main" app (_not_ the UI for the specific auth manager) then deletes once it\

Have removed now search params token extraction as it not required i think anymore.

> What about other auth managers? How do they set the token?\r\n> \r\n> Did we consider setting the token in a cookie that the "main" app (_not_ the UI for the specific auth manager) then deletes once it\

> > What about other auth managers? How do they set the token?\r\n> > Did we consider setting the token in a cookie that the "main" app (_not_ the UI for the specific auth manager) then deletes once it\

> The auth manager sets the token in cookies, and the main app retrieves it using document.cookie and stores it in localStorage. Then, it sets the expiration date for the token cookie. Does this align the expectation ?\r\n\r\nIdeally the auth_manager will return the token to the main app so that _it_ can set the cookie. If that is not possible then we at least need to document\xa0the requirement that "if you write a custom auth manager, it must set cookie X"

Hi @gopidesupavan. Thanks for your reactivity and all these modifications. This is quite important so I think that\

> > The auth manager sets the token in cookies, and the main app retrieves it using document.cookie and stores it in localStorage. Then, it sets the expiration date for the token cookie. Does this align the expectation ?\r\n> \r\n> Ideally the auth_manager will return the token to the main app so that _it_ can set the cookie. If that is not possible then we at least need to document\xa0the requirement that "if you write a custom auth manager, it must set cookie X"\r\n\r\nSure, have added document.\r\n\r\n> Hi @gopidesupavan. Thanks for your reactivity and all these modifications. This is quite important so I think that\

I am afraid you are right, we cannot redirect and send along headers. I think we will have to use cookies

Is there any reason each auth manager has its own login page? \r\n\r\nI have thought about another idea, if we have one login page that runs on main app then, we could use `/auth/token/` endpoint to retrieve the token and can be stored easily in the localStorage. WDYT?

> Is there any reason each auth manager has its own login page?\r\n> \r\n> I have thought about another idea, if we have one login page that runs on main app then, we could use `/auth/token/` endpoint to retrieve the token and can be stored easily in the localStorage. WDYT?\r\n\r\nYes each auth manager needs to implements its own login mechanism. In the future we might want to implement KeyCloak auth manager. Most likely KeyCloak will have its own login view and we will redirect to that page. You can also imagine some auth manager (most likely private) with no login form, only relying on information from the request (such as LDAP group etc).

> > Is there any reason each auth manager has its own login page?\r\n> > I have thought about another idea, if we have one login page that runs on main app then, we could use `/auth/token/` endpoint to retrieve the token and can be stored easily in the localStorage. WDYT?\r\n> \r\n> Yes each auth manager needs to implements its own login mechanism. In the future we might want to implement KeyCloak auth manager. Most likely KeyCloak will have its own login view and we will redirect to that page. You can also imagine some auth manager (most likely private) with no login form, only relying on information from the request (such as LDAP group etc).\r\n\r\nCool thanks got it.

> We are close :) Thanks again for the numerous iterations!\r\n\r\nYeah, there were some k8s and docker tests failures, hope this will resolve now.

The docker and k8s share some code related to jwt, will get that refactor in different pr.

I think this change fine now. except the tests failures related to k8s are unstable, even in CI its failing. will try to look.

> I think this change fine now. except the tests failures related to k8s are unstable, even in CI its failing. will try to look.\r\n\r\nThose errors are unrelated to this PR, you can ignore them

> > I think this change fine now. except the tests failures related to k8s are unstable, even in CI its failing. will try to look.\r\n> \r\n> Those errors are unrelated to this PR, you can ignore them\r\n\r\nOkay cool, will wait for anyone to look, as this changes also include in front end.

Merging now its finally green :) 

Really nice job! üëè 

Nice indeed 

Yep. looks good @o-nikolas \r\n

Damn

Moving fast. breaking things

I may miss context. the intention is to remove the K8S spec tab from the UI?

> I may miss context. the intention is to remove the K8S spec tab from the UI?\r\n\r\nThe PR description has all the details/history, but the tl;dr is yes. The `/rendered-k8s` endpoint is actually already removed. This PR just removes some of the remaining config bits left behind.

Any further thoughts you want to add before we merge @eladkal?

Nit: could we say (easily) which DAG it will trigger in the modal?

> Nit: could we say (easily) which DAG it will trigger in the modal?\r\n\r\nUpdated:\r\n\r\n<img width="894" alt="Screenshot 2025-03-06 at 11 13 12\u202fAM" src="https://github.com/user-attachments/assets/b731eaa9-da4b-4455-a3f7-9a309c033975" />\r\n\r\nI also added a check. If there are multiple upstream dags, disable the materialize option.\r\n

I think this widget can be added to the assets list page similar to dags list page having trigger button. A rough patch of this that I can take up in another PR if okay or ok with this PR too.\r\n\r\n![image](https://github.com/user-attachments/assets/6d2cba52-b1ba-4bbb-b601-472746e3f3e2)\r\n\r\n\r\n```patch\r\ndiff --git a/airflow/ui/src/pages/AssetsList/AssetsList.tsx b/airflow/ui/src/pages/AssetsList/AssetsList.tsx\r\nindex a8892bf250..84020b17aa 100644\r\n--- a/airflow/ui/src/pages/AssetsList/AssetsList.tsx\r\n+++ b/airflow/ui/src/pages/AssetsList/AssetsList.tsx\r\n@@ -28,6 +28,7 @@ import { useTableURLState } from "src/components/DataTable/useTableUrlState";\r\n import { ErrorAlert } from "src/components/ErrorAlert";\r\n import { SearchBar } from "src/components/SearchBar";\r\n import { SearchParamsKeys } from "src/constants/searchParams";\r\n+import { CreateAssetEvent } from "src/pages/Asset/CreateAssetEvent";\r\n import { pluralize } from "src/utils";\r\n \r\n import { DependencyPopover } from "./DependencyPopover";\r\n@@ -67,6 +68,12 @@ const columns: Array<ColumnDef<AssetResponse>> = [\r\n     enableSorting: false,\r\n     header: () => "Producing Tasks",\r\n   },\r\n+  {\r\n+    id: "trigger",\r\n+    cell: ({ row: { original } }: AssetRow) => <CreateAssetEvent asset={original} withText={false} />,\r\n+    enableSorting: false,\r\n+    header: "",\r\n+  },\r\n ];\r\n \r\n const NAME_PATTERN_PARAM = SearchParamsKeys.NAME_PATTERN;\r\n```

> I think this widget can be added to the assets list page similar to dags list page having trigger button. A rough patch of this that I can take up in another PR if okay or ok with this PR too.\r\n\r\nI wanted to save that for another PR.\r\n

Nice!

@potiuk tagging you since you pushed the image for the last version bump to pgbouncer_exporter in https://github.com/apache/airflow/pull/40303

Yep. I am just on it :)\r\n\r\n\r\n

ok. Pushed the image. `apache/airflow:airflow-pgbouncer-exporter-2025.03.05-0.18.0`  - can you please modfy helm-chart and change the defaults?

> ok. Pushed the image. `apache/airflow:airflow-pgbouncer-exporter-2025.03.05-0.18.0` - can you please modfy helm-chart and change the defaults?\r\n\r\nTo clarify - do you mean as part of this PR or a separate PR?

As part of this PR is best :)

added missing EOL

With some comments left and link to the issue you opened -> then I can  build and push the image - initially it will fail when you make the change but then I can push the image and restart failing jobs to pick up the new image

I think we never released chart with 1.24

We have not released chart for a looooong time.

> I think we never released chart with 1.24\r\n\r\nAh good point, I should have checked the release notes!

Built and pushed, approved workflows

@potiuk Who would be the best person to ping for a review on this PR?

Me :) 

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> Me :)\r\n\r\nThank you so much for your help and attention!

We only use this banner once. I think you can combined the two components, Banner and BackfillBanner, and not worry about arrays and keys.

Removed the usage of useEffect, it seems to work now. 

Nice work. Thanks for being so patient through my reviews!

Nice :)

However, this one is not good for now - because the `generate-openapi-spec` is a `breeze` tests - it runs inside the CI image (not sure if that is needed BTW) - and we have all the `breeze` tests at the end - also because they are skipped when breeze image is not built - to speed up static checks - so we should rather move the other test after generate test.

I will do it in a moment

https://github.com/apache/airflow/pull/47461 as follow up

@insomnes I am just going to rebase this one, once green, we can merge it

@enchant3dmango Continuing the conversation from [this discussion](https://github.com/apache/airflow/discussions/47166#discussioncomment-12359461), I tried running the `main` branch from your fork (https://github.com/enchant3dmango/airflow/tree/main) to check things out and ran into what seem like an unrelated issue so am not able to verify things\r\n\r\nUltimately, it\

cc @jason810496

I\

We actually need to keep the config. APIs provided by the FAB auth manager (such as users, roles, ...) need the auth backends. These APIs have not been migrated to fastapi and are still using Flask. Therefore, they need the auth backends. I updated the config description to reflect that.

However, we probably should move the config to FAB provider then?\r\n\r\nEdit: I moved it.

Test failures are not related to this PR, finally all tests are passing! Reviews are welcome :)

Error unrelated to this PR. Merging

Cool! Much cleaner now!

Closing, I directly merged https://github.com/apache/airflow/pull/47394 by mistake. Which hold changes for this PR as well.

I just noticed that `DagModel.asset_expression` is also missing an `asset_id`. We should update that too if possible

Closing this as discussed with @jedcunningham as per https://github.com/apache/airflow/pull/47371#issuecomment-2723792484

<img width="1918" alt="Screenshot 2025-03-04 at 4 41 03\u202fPM" src="https://github.com/user-attachments/assets/1f8ac715-7367-490f-a7ae-8a9a35f8fac7" />\r\n\r\nUpdated to make selected edges more prominent

> Very cool!\r\n> \r\n> There is just a small rendering glitch in `asset_alias_example_alias_consumer` for the box of the alias being generated:\r\n\r\nFixed!

> A few nits\r\n\r\nThank you, Pierre, for your review! I will address your comments shortly.

Since we are pulling postgres charts here we should adopt a similar usage for openshift as well \r\n\r\n  compatibility:\r\n    ## Compatibility adaptations for Openshift\r\n    ##\r\n    openshift:\r\n      ## @param global.compatibility.openshift.adaptSecurityContext Adapt the securityContext sections of the deployment to make them compatible with Openshift restricted-v2 SCC: remove runAsUser, runAsGroup and fsGroup and let the platform use their allowed default IDs. Possible values: auto (apply if the detected running cluster is Openshift), force (perform the adaptation always), disabled (do not perform adaptation)\r\n      ##\r\n      adaptSecurityContext: auto\r\n

Looks like a lot of tests are not happy because they rely on the first run being created against start_date. Let‚Äôs just add `catchup_by_default = true` in `airflow/config_templates/unit_tests.cfg` so tests still set catchup by default.

@abhishekbhakat That would be awesome, thank you! :) 

@TJaniF  raised another to fix remaining issues.\r\n

Ok I think this is ready, the tests that are failing recently I think are unrelated to this PR (K8s tests) üëÄ \r\n\r\nAlso added the config change to `airflow config lint` @sunank200 :)

Also need to fix static checks. Kubernetes test failures are unrelated I believe.

mypy error unrelated.

BTW. I do not think it is used anywhere in the new UI yet, but worth fixing anyway.

Thanks

> Thanks\r\n\r\nAll CI are green üéâ

@jason810496 , Thank you! \r\n\r\n

Cool!

Failure unrelated

Hi @potiuk !\r\nCan you please help us here? We have some import errors, not sure why exactly üò∫ 

Rebased it - to check - we had some teething issue with `devel-common` - maybe related.

Let the CI fixing war commence! ‚öîÔ∏è

Reopening to run tests

Dup of #47319, but probably not worth waiting on Vincent to wake up.

Oh I didnt see that one, it would be nice if we could post on Airflow CI CD channel for such fixes to avoid dupes.\r\n

I opened it in the meantime as well - and turned it into follow - up https://github.com/apache/airflow/pull/47338 has better description for root cause and link to the `mongomock` issue that is the root cause of the problem.

Approved that on @jedcunningham 

fyi @pierrejeambrun 

For posterity, this is undoing a bit of #47308, which started breaking things like the docker-compose tests.

Merging. Failures are unrelated.

Thanks, I missed that part sorry

If you really want to keep `@property` you might be able to return a coroutine\r\n\r\n```python\r\nclass AwsGenericHook:\r\n    @property\r\n    def async_conn(self) -> coroutine[client]:\r\n        return sync_to_async(self._get_async_conn)()\r\n```\r\n\r\nbut you will still need to await it\r\n\r\n```python\r\nasync def run(self):\r\n    client = await self.hook.async_conn\r\n```\r\n\r\nand thus it will be a breaking change for `async_conn`.

Does this look okay to merge @jedcunningham?\r\n\r\nAlso CC @jscheffl and @amoghrajesh who reviewed the other hybrid executor AF3.0 changes, you may be interested in this one as well. 

Oof that was a bit of a mission to land.

> Oof that was a bit of a mission to land.\r\n\r\nWohooo!

#protm

> Oof that was a bit of a mission to land.\r\n\r\nIndeed

https://github.com/apache/airflow/pull/47336 has been merged, @vincbeck closing this PR in favour of that

Unrelated errors already handled in main

Failures are unrelated. Merging.

No bad! But from coloring I assume other stock icons have some features to support dark mode. In dark mode (zoomed) it is hard to see, looks like this:\r\n![image](https://github.com/user-attachments/assets/a2bddac3-cfa5-4771-92c4-23dd9c4f2ce7)\r\n

Up

@potiuk are you able to approve workflows?

> @potiuk are you able to approve workflows?\r\n\r\nSorry, one more time. Hit update instead of merge and it has to rerun tests.\r\n\r\nI believe those workflows should work for me after making this initial commit?

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Merging. Failures are unrelated.

Errors unrelated to this PR

Can you please explain what are you trying to achieve here - and generally, if you are not attempting to make a change of some sort, avoid doing that in public Airflow repo?  Such PRs - even drafts are sending notifications to maintainer and they are introducing noise. With GitHub if you want to "play" with things - you can make PRs to your own repo, you do not have to make PRs to airflow repol.

Close it, as TP has already rewritten it

@ephraimbuddy This fixes the crash, but is the behaviour in the scheduler correct?  

There are some errors in the code - causing the build to fail , also it would be great to get a unit test showing the case that would have led to wrong path handling before.

Thanks for doing a PR that quick. I am a bit unsure if it solves the issue though. Do I understand it right that using "/" or "\\" is determined by the OS of the client now?\r\n\r\nIn my case / case of the bug, my client is Ubuntu 24 and the remote / samba system is a Windows-based file share. So checking the client OS would not help in this case.

suppressed by https://github.com/apache/airflow/pull/47598

Can you please check this out @Lee-W? Thanks!

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

@Lee-W Can you please check this? Thank you!

@Lee-W 

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Can you please check @Lee-W? Thank you!

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> Reviewed code, looks good! Just pipeline needs to turn green :-D\r\n\r\nOn it :)

While fixing the remaining issues, I have found that it will be easier, when I incorporate all "devel" dependencies into "tests-common" distribution - this way I could (@ashb  -> I think you will be super-happy) of ALL `devel-` and bundle dependencies. This one will still need to get fixes to the failing builds but, It already looks pretty cool, with a lot of complexity removed from `hatch_builtd.py` and all the references to old devl* and bundle extras removed.\r\n\r\nThis might be followed by similar `doc` distribution removal (doc is pretty special beast here).

That actualy makes me think if we should not name it `devel-common`  rather than `tests-common` ?

See here https://github.com/apache/airflow/pull/47281/files#diff-3f4511e8ff90f1a9605b62a41cc871c86e08a8818d4fe5415f5031eeffeb9365R1

> That actualy makes me think if we should not name it `devel-common` rather than `tests-common` ?\r\n\r\n+1!

Nice to see hatch_build.py file modifications :) 

Slightly moving the deps  found two dependencies that had bad "minimum" and failed the lowest direct resolution tests :)

Renamed the project to `devel-common`

All right, I had to do a bit of custom installation of devel dependencies for the "compatibility" tests - because we are basically removing sources for airflow and task_sdk from those builds and only use the "devel-common" and "tests" and the rest is installed from packages, but I think I finally nailed it :)

wooohoo!

Woohooo :) 

Huge :open_mouth: :tada: 

I will raise a PR soon for removing `session.query()`  from `airflow-core` and `task-sdk` as we discussed in #45714

> I will raise a PR soon for removing `session.query()` from `airflow-core` and `task-sdk` as we discussed in #45714\r\n\r\nYou can do it in this PR. This PR should raise and point to where there are usages of session.query

Compatibility should be added for past versions of airflow.

Cool !

Just need conflicts resolution 

just rebase with the latest main, we should be able to use `ReadableDagsFilterDep` now

No worries. Just want to ensure we don‚Äôt work on the same thing. Will comment before I start working on it as well üôÇ

Hi @Lee-W I start working on it.

Thanks @Lee-W  and @pierrejeambrun  for the review, I just resolved all comments and refactored the logic in router and the test as well.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Yep. This is cool now :)

And it works as expected:\r\n\r\n<img width="399" alt="Screenshot 2025-03-02 at 08 34 48" src="https://github.com/user-attachments/assets/2f8262d4-0ac5-43c7-8c1d-c06e42535c60" />\r\n

@eladkal can you re-review? Or shall I ask somebody else?

cc @bugraoz93 as promised earlier :)

Yep ill fix it later today!

@potiuk could you take a look again?

Triggered by #47247

Merging as other failures are fixed in parallel PRs

Merging to un-block other PRs. Leftover static check problems were fixed in #47259

Will also run a "canary" one from main to see if it works there as well.

The "canary" one here https://github.com/apache/airflow/pull/47258 

> Have you done some manual testing in real DAGs that show this is working as you expect?\n\nYes It Works, in fact Im running this in production. It follows the same aproach of BashOperator but instead of using only one exit code, this passes a list of exist code 

Hey @bugraoz93, thank you. I will update those backticks in a follow up PR. I wanted to do soemthing else here :)

No worries, entirely valid!

Regenerated using: \r\n\r\n```\r\nbreeze release-management prepare-provider-documentation --reapply-templates-only common.messaging\r\n```

Ach... Two merges crossed. But there is more to it I noticed:\r\n\r\nhttps://github.com/apache/airflow/pull/47253\r\n\r\n

I also looked at the way how upgrade checks are run and I noticed they were not **really** run even in canary runs .. testing it now.

Yeah. Our auto-upgrade checks were not really run for some time as result of a CI job refactoring :). But not a long time :)

Merging - the errors were because it was "canary" build run from a fork and it tried to push things without permissions.

Nice :)

Thanks for the fix - I was also noticing it is broken when I implemented #47250 \r\n\r\nNot being an expert in UI stuff, I assume this really fixes it.

@vincbeck , Since we are now exposing AssetAlias, both AWS auth manager and FAB need to implement `is_authorized_asset_alias`. Can I add them separately in a different PR or should I covert them here?

"Get alias" and "Get aliases" just show Id, name and group. No details regarding an asset. So, I think there\

> @vincbeck , Since we are now exposing AssetAlias, both AWS auth manager and FAB need to implement `is_authorized_asset_alias`. Can I add them separately in a different PR or should I covert them here?\r\n\r\nYou have to, otherwise mypy will not be happy 

failed tests do not seem to be related to the changes in this PR.\r\n\r\n@vincbeck , can you please retrigger just the failed boto test?

> failed tests do not seem to be related to the changes in this PR.\r\n> \r\n> @vincbeck , can you please retrigger just the failed boto test?\r\n\r\n#47512 is the fix

> "Get alias" and "Get aliases" just show Id, name and group. No details regarding an asset. So, I think there\

> > "Get alias" and "Get aliases" just show Id, name and group. No details regarding an asset. So, I think there\

If I understand correctly that is good from an asset_alias endpoints point of views (TP + Wei) as well as from the AuthManager permissions (vincent).\r\n\r\nThe only thing missing is to add AssetAlias permission check on Asset endpoints ? (I think we can do that in a followup PR, this one focus on asset aliases and it seems to be completed ?)

@eladkal could you please review this PR 

@o-nikolas Could you please review this PR?

Test the version apache-airflow-providers-apache-druid==4.1.0rc1 and its working fine 

> Test the version apache-airflow-providers-apache-druid==4.1.0rc1 and its working fine\r\n\r\nThanks for letting us know ! Updated it in the "Status" issue :) 

Needs https://github.com/apache/airflow/pull/47243 to pass.

Rebased to include node update.

Fixed static check :)

NICE :) 

Better support for airflow 3 generated configuration / jwt_secrets persisted between breeze runs\r\n

Nice!

Off-topic to the PR but I guess it will be useful to have the logs API accept level parameter so that logs related to the level are filtered in the backend to send only the relevant parts to frontend. This will reduce help with bandwidth and parsing the log in the frontend.

Found a minor nit while debugging another issue

CI seems unrelated

nice! thanks!

I\

2 is good

> Sure, if it fixes the issue, lets limit it and wait for the tests\r\n\r\nIn this case we have no choice, because we are using packages from PyPI, not the locally modified pyproject.toml - so we have to "workaround" it - in the past we had similar limits for other packages that were problematic.

Cool!

Constraints failed on 2.8.1 kafka build - already solved in main 

> @vatsrahul1001 could we please add some tests under helm_tests?\r\n\r\n@amoghrajesh updated the existing test

Rebased but some __init__.py files should be added (pre-commit will do it automatically).

it\

I have a fix for [Tests / CI image checks / Test Python API client (pull_request)](https://github.com/apache/airflow/actions/runs/13699881538/job/38311352233?pr=47208) basically that is not specifically related to this PR. The test is just broken at the moment. (passing on main but not testing anything)

https://github.com/apache/airflow/pull/47460\r\n

### Backport successfully created: v3-0-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v3-0-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/47205"><img src="https://img.shields.io/badge/PR-47205-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

> It updated all the provides because of schema change in template file?\r\n\r\nI manually run `breeze release-management prepare-provider-documentation --reapply-templates-only --answer yes` to apply them. This is doing the same as what release manager would do but without bumping version numbers and regenerating commit list and changelog entries, just applying templates and regenerating what is automaticlaly generated.

a small question , `uv sync` remove the "necessary" packages to run unit test out of breeze \

> a small question , uv sync remove the "necessary" packages to run unit test out of breeze \

thanks @potiuk :+1: 

Oh wow.... Someone ACTUALLY reads those :) ‚ù§Ô∏è 

As we introduce provider module-level CLI commands, `ProviderManager` is now initialized within `cli_parser`. Should I remove the `airflow providers lazy-loaded` command, or modify as that `ProviderManager` is only initialized once within `cli_parser`?  \r\n\r\ncc @potiuk

Nice. But can you please install pre-commit and `commit --amend` ? This will auto-generate the rest of changes resulting from it? BTW. I will update the contributing docs to make it clearer :)

> Nice. But can you please install pre-commit and `commit --amend` ? This will auto-generate the rest of changes resulting from it? BTW. I will update the contributing docs to make it clearer :)\r\n\r\ndone @potiuk 

Static checks are still failing. Could you please run the following and commit?\r\n`pre-commit run --al-files` \r\nYou can run `pre-commit install` this will make the hooks run in each commit.

`breeze static-checks --only-my-changes` should do the same.

not sure why static is failing. It says Generate the FastAPI API spec...............................................Failed.\r\n\r\nBut is passes in local\r\n![image](https://github.com/user-attachments/assets/af39614d-a268-45b5-9475-35b201156e8f)\r\n\r\n

> not sure why static is failing. It says Generate the FastAPI API spec...............................................Failed.\r\n\r\n\r\nLikely because of this:\r\n\r\n```diff\r\n> fastapi==0.115.9\r\n---\r\n> fastapi==0.115.10\r\n```\r\n\r\nThere is a new fastapi generated and it apparently generates new  output - you will see the same thing in canary builds https://github.com/apache/airflow/actions/runs/13602495440\r\n\r\nYour PR modifies dependencies, so (unlike most other PRs) we will attempt to resolve new dependencies using whatever has been released since the last time we "froze" dependencies in constraints.\r\n\r\nYou can always see the list of updated dependencies in "summary" of the checks (just scroll down below the graph with the CI jobs and you will see what has been upgraded in your PR.\r\n\r\nI will separately fix it in `main` - so once this is merged, you should be able to rebase and this one should be green then.

Fastapi has been pinned temporarily already in main in https://github.com/apache/airflow/pull/47233 . Rebasing to account for it.

Another badly merged main failure fixed :)

FINALLY :) 

yeah finally its done!!

I am also unsure about the propagation of these changes to `GKEStartPodTrigger`. 

CI is unrelated. The fix is coming in #47259. You may need to rebase later once more

Up

@jason810496 Thank you for the review! Added test to check default split_statements in run method. Not sure if any other tests are needed?

Hi, this is my first "contribution", even if it is just a suggestion. I\

To save few clicks from manually select each directory üòÑ \r\n\r\nWe can update this file as we restructure so that it will be easy to fixup all in one go

We could also commit equivalent configuration from vscode if it is portable and separate from all other vscode configuration.

The only problem I see that in breeze we might want to have another venv:\r\n\r\n```\r\n    <orderEntry type="jdk" jdkName="Python 3.12 (breeze)" jdkType="Python SDK" />\r\n    <orderEntry type="sourceFolder" forTests="false" />\r\n```\r\n\r\n(BTW. It\

BTW. I am not super strong on the committing, copying the files will mostly work as well, and that "jdk" is potentially blocking, but might be worth seeing if that will work :).\r\n\r\nIt woudl be awesome if contributor just opens the `airflow` folder as project in PyCharm/IntelliJ and has everything set-up.  

> BTW. I am not super strong on the committing, copying the files will mostly work as well, and that "jdk" is potentially blocking, but might be worth seeing if that will work :).\r\n> \r\n> It woudl be awesome if contributor just opens the `airflow` folder as project in PyCharm/IntelliJ and has everything set-up.\r\n\r\n\r\nFor JdkName it actually auto populated when i created virtual environment, so when we change interpreter paths in Pycharm settings then that will update the airflow.iml file. i have tested changing multiple paths in pycharm interpreter its updating the airflow.impl file\r\n\r\nah not sure how to setup when airflow folder opens :) 

I have another idea, can add python script, it generates airflow.iml, module.xml files and updates automatically something like this cli.\r\n\r\npython setup_ide.py --venv_name {}\r\n\r\nWe can update this script whenever changes happens :) ?\r\n\r\nSo users has to run first time and when any new changes happens.

> I have another idea, can add python script, it generates airflow.iml, module.xml files and updates automatically something like this cli.\r\n\r\n\r\nThat seems like a nice middle ground

Better fix in https://github.com/apache/airflow/pull/47202

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Merging. Static check failure is unrelated.

GREEN

Booom. -7.500 lines :) 

The smaller brother.\r\n\r\n![image](https://github.com/user-attachments/assets/886fc031-8c61-4f83-88d0-87572d45d9e9)\r\n

> Booom. -7.500 lines :)\r\n\r\nWas 12k LoC before when the opanapi spec YAML was also deleted, so once we switch the client to the new API spec this is another opportunity to clean :-D

cc: @dabla 

I hope last commit will fix all the failing tests as well ü§û \r\n

Similar to https://github.com/apache/airflow/pull/46968 the asset event widget in dashboard could link to pages like asset detail, event etc. I can take this up in another PR once this is merged.

Ah looks like I need to pin the version in the precommit script 

> Ah looks like I need to pin the version in the precommit script\r\n\r\nAlready fixed and merged... Merging.\r\n

Yeah. There is a bit more to it - I tried to do it yesterday, but I realized that this needs a bit more careful removals and in some cases some replacements :) 

@vincbeck Can you also move this PR to the apache/main repo such that I can push some fixes on it?\r\nOtherwise you need to make it all yourself from MWAA team :-D

> @vincbeck Can you also move this PR to the apache/main repo such that I can push some fixes on it? Otherwise you need to make it all yourself from MWAA team :-D\r\n\r\nAs long as I can not push, this diff fixes breeze unit tests:\r\n```\r\ndiff --git a/dev/breeze/tests/test_selective_checks.py b/dev/breeze/tests/test_selective_checks.py\r\nindex fe79b1bb1b..45e1d27548 100644\r\n--- a/dev/breeze/tests/test_selective_checks.py\r\n+++ b/dev/breeze/tests/test_selective_checks.py\r\n@@ -149,7 +149,7 @@ def assert_outputs_are_printed(expected_outputs: dict[str, str], stderr: str):\r\n             pytest.param(\r\n                 ("airflow/api/file.py",),\r\n                 {\r\n-                    "selected-providers-list-as-string": "amazon common.compat fab",\r\n+                    "selected-providers-list-as-string": "amazon common.compat databricks fab",\r\n                     "all-python-versions": "[\

> @vincbeck Can you also move this PR to the apache/main repo such that I can push some fixes on it? Otherwise you need to make it all yourself from MWAA team :-D\r\n\r\nGood idea :) 

~~Not sure how to do that tho?~~

> ~Not sure how to do that tho?~\r\n\r\nHere we are: https://github.com/apache/airflow/pull/47171\r\nYou need to `git branch delete-api_connexion` and `git push --set-upstream upstream delete-api_connexion`

`db_instance_stopped` is a custom waiter defined in https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/waiters/rds.json#L4. You can find more information custom waiters [here](https://github.com/apache/airflow/tree/main/providers/amazon/src/airflow/providers/amazon/aws/waiters). However, I do not understand why it cannot find the waiter `db_instance_stopped`

nice :)

Cool.

@mobuchowski -> would you like to take a look ?

Looks good to me üôÇ 

### Backport failed to create: v2-10-test. View the failure log <a href=\

This is provider only PR. No need to backport to 2.10 branch

Need - likely -manual cherry-picking @kacpermuda 

ü§¶ right,

I believe we moved adding dependency to provider info and pyproject.toml files, so IMHO It would be better to add this dependency to pyproject.toml https://github.com/apache/airflow/blob/main/providers/common/sql/pyproject.toml#L57 \r\n\r\nThen the pre-commit will add that to readme and provider info file https://github.com/apache/airflow/blob/main/providers/common/sql/src/airflow/providers/common/sql/get_provider_info.py

> I believe we moved adding dependency to provider info and pyproject.toml files, so IMHO It would be better to add this dependency to pyproject.toml https://github.com/apache/airflow/blob/main/providers/common/sql/pyproject.toml#L57\r\n\r\nYep. This is the new (much more standard) way how we should add dependencies. In fact I am a little puzzled why this PR did not fail with validation of the provider.yaml, because ... it should ... We do not exapect dependencies field in provider.yaml any more. Let me see why it did not fail.

OK. I know ... Missing case for selective checks\r\n

The https://github.com/apache/airflow/pull/47148 improves selective checks to cover that case.

Awesome to see how quickly this issue has been resolved üëç 

The static-checks error is likely from `main`

The static check failure handled in https://github.com/apache/airflow/pull/47172

Also opened https://github.com/yandex-cloud/python-sdk/issues/138

And the buggy yandexcloud versions are yanked now. We can remove the exclusion

Also tested with the DAG from https://github.com/apache/airflow/issues/46976.\r\n\r\nDAG:\r\n```\r\nfrom __future__ import annotations\r\n\r\nimport datetime\r\nfrom pathlib import Path\r\n\r\nfrom airflow.decorators import task\r\nfrom airflow.models.dag import DAG\r\nfrom airflow.sdk import Param\r\nfrom airflow.utils.trigger_rule import TriggerRule\r\n\r\n# [START params_trigger]\r\nwith DAG(\r\n    dag_id=Path(__file__).stem,\r\n    dag_display_name="Params Trigger UI",\r\n    description=__doc__.partition(".")[0],\r\n    doc_md=__doc__,\r\n    schedule=None,\r\n    start_date=datetime.datetime(2022, 3, 4),\r\n    catchup=False,\r\n    tags=["example", "params"],\r\n    params={\r\n        "names": Param(\r\n            ["Linda", "Martha", "Thomas"],\r\n            type="array",\r\n            description="Define the list of names for which greetings should be generated in the logs."\r\n            " Please have one name per line.",\r\n            title="Names to greet",\r\n        ),\r\n        "english": Param(True, type="boolean", title="English"),\r\n        "german": Param(True, type="boolean", title="German (Formal)"),\r\n        "french": Param(True, type="boolean", title="French"),\r\n    },\r\n) as dag:\r\n\r\n    @task(task_id="get_names", task_display_name="Get names")\r\n    def get_names(**kwargs) -> list[str]:\r\n        params = kwargs["params"]\r\n        if "names" not in params:\r\n            print("Uuups, no names given, was no UI used to trigger?")\r\n            return []\r\n        return params["names"]\r\n\r\n    @task.branch(task_id="select_languages", task_display_name="Select languages")\r\n    def select_languages(**kwargs) -> list[str]:\r\n        params = kwargs["params"]\r\n        selected_languages = []\r\n        for lang in ["english", "german", "french"]:\r\n            if params[lang]:\r\n                selected_languages.append(f"generate_{lang}_greeting")\r\n        return selected_languages\r\n\r\n    @task(task_id="generate_english_greeting", task_display_name="Generate English greeting")\r\n    def generate_english_greeting(name: str) -> str:\r\n        return f"Hello {name}!"\r\n\r\n    @task(task_id="generate_german_greeting", task_display_name="Erzeuge Deutsche Begr√º√üung")\r\n    def generate_german_greeting(name: str) -> str:\r\n        return f"Sehr geehrter Herr/Frau {name}."\r\n\r\n    @task(task_id="generate_french_greeting", task_display_name="Produire un message d\

I think I managed to handle task groups as well.\r\n\r\nExample DAG:\r\n```\r\nfrom airflow import DAG\r\nfrom airflow.decorators import task, task_group\r\nfrom datetime import datetime\r\n\r\n# Track the results for verification (only for testing purposes)\r\nresults = {}\r\n\r\n# Expected values for reference\r\nexpected_values = {\r\n    ("tg.t1", 0): ["a", "b"],\r\n    ("tg.t1", 1): [4],\r\n    ("tg.t1", 2): ["z"],\r\n    ("tg.t2", 0): ["a", "b"],\r\n    ("tg.t2", 1): [4],\r\n    ("tg.t2", 2): ["z"],\r\n    ("t3", None): [["a", "b"], [4], ["z"]],\r\n}\r\n\r\n# Define the DAG\r\nwith DAG(\r\n    dag_id="tg_dag",\r\n    start_date=datetime(2025, 1, 1),\r\n    schedule=None,\r\n    catchup=False,\r\n) as dag:\r\n    @task\r\n    def t(value, ti=None):\r\n        # Store results for verification\r\n        global results\r\n        results[(ti.task_id, ti.map_index)] = value\r\n\r\n        print("Value is", value)\r\n\r\n        return value\r\n\r\n    @task\r\n    def t(value, ti=None):\r\n        # Store results for verification\r\n        global results\r\n        results[(ti.task_id, ti.map_index)] = value\r\n\r\n        print("Value is", value)\r\n\r\n        return value\r\n\r\n    @task_group\r\n    def tg(va):\r\n        # Each expanded group has one t1 and t2 each.\r\n        t1 = t.override(task_id="t1")(va)\r\n        t2 = t.override(task_id="t2")(t1)\r\n        return t2\r\n\r\n    t2 = tg.expand(va=[["a", "b"], [4], ["z"]])\r\n\r\n    t3 = t.override(task_id="t3")(t2)\r\n\r\n```\r\n\r\n\r\nGraph vieW:\r\n<img width="861" alt="image" src="https://github.com/user-attachments/assets/bdb0834c-120a-4f0b-b1d1-71d3b6961458" />\r\n\r\nRun:\r\n<img width="1213" alt="image" src="https://github.com/user-attachments/assets/bbce9e21-0d89-4866-8d03-29ebfdb624ff" />\r\n\r\n\r\nXCOM table:\r\n```\r\n8,tg.t1,0,return_value,tg_dag,manual__2025-02-27T15:00:50.382954+00:00_BGTtj5oT,"""[\\""a\\"", \\""b\\""]"""\r\n8,tg.t2,0,return_value,tg_dag,manual__2025-02-27T15:00:50.382954+00:00_BGTtj5oT,"""[\\""a\\"", \\""b\\""]"""\r\n8,tg.t1,1,return_value,tg_dag,manual__2025-02-27T15:00:50.382954+00:00_BGTtj5oT,"""[4]"""\r\n8,tg.t2,1,return_value,tg_dag,manual__2025-02-27T15:00:50.382954+00:00_BGTtj5oT,"""[4]"""\r\n8,tg.t2,2,return_value,tg_dag,manual__2025-02-27T15:00:50.382954+00:00_BGTtj5oT,"""[\\""z\\""]"""\r\n8,tg.t1,2,return_value,tg_dag,manual__2025-02-27T15:00:50.382954+00:00_BGTtj5oT,"""[\\""z\\""]"""\r\n\r\n```\r\n\r\n\r\n\r\n\r\n

Cool! Thanks for fixing!

cc: @vincbeck

Why do we need both? What is the different between `PUT` and `PATCH`? Maybe using HTTP terms in auth manager was not that much of a good idea actually but here we just want to express action, what kind of action the user is trying to do?\r\n- Create (POST)\r\n- Edit (PUT)\r\n- Delete (DELETE)\r\n- Read (GET)\r\n\r\nIf you want to add PATCH just because you have PATCH routes defined in the fastapi application I think this is wrong. We should then have a layer that translate fastapi method to auth manager action and convert PATCH to PUT in that layer

> Why do we need both? What is the difference between PUT and PATCH?\r\n\r\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Methods\r\n\r\nThey both are intended for different types of updates. \r\n

I understand this. What I am saying is, on the auth manager side, we do not need to have both, we do not need to say: "I want to give user X permissions for PUT but not for PATCH". Do we? On the auth manager side we should limit the number of actions and I dont think it is needed to add that new action. I dont think we need this granularity?

We are not adding an action. We are mapping the PATCH HTTP method to the same existing `can_edit` action. So basically once the mapping is used to get the FAB action (that already exists) from this PATCH method, everything is untouched from the fab auth manager perspective.\r\n\r\nDo you have an alternative in mind ?\r\n\r\n> We should then have a layer that translate fastapi method to auth manager action and convert PATCH to PUT in that layer\r\n\r\nThis is exactly what is happening `get_fab_action_from_method_map` will map both HTTP methods (PUT PATCH) to the same FAB action.\r\n\r\nMaybe I missed something.

No, you are adding a new method to `ResourceMethod` which configures which are the action available in auth managers (I am not talking about FAB auth manager specifically here but auth manager in general). By adding this method then you can do things like `auth_manager.is_authorized_configuration(method=ResourceMethod.PUT)` and `auth_manager.is_authorized_configuration(method=ResourceMethod.PATCH)` You need to understand that FAB auth manager is not the only auth manager. \r\n\r\nYou are right `get_fab_action_from_method_map` translate `ResourceMethod` to FAB action but again here the issue is, we should not add a new action to the list of actions in `ResourceMethod` which are shared across all auth managers.\r\n\r\nI think we are on the same page but I think this implementation is wrong. What we want to do is to translate HTTP method `PATCH` to the resource method `ResourceMethod.PUT`. 

Ohhh, yes you are talking about `ResourceMethod = Literal["GET", "POST", "PUT", "PATCH", "DELETE", "MENU"]`. \r\n\r\nI was focusing on `"PATCH": ACTION_CAN_EDIT,`, I understand, yes the ResourceMethod change is not good.\r\n\r\nThen I guess we can do the mapping on the fastapi side. So that `PATCH` will map to `ResourceMethod.PUT`.\r\n\r\nOr don\

Waiting for @vincbeck input but I think we will most likely close this one.

@pierrejeambrun , We now have an Asset Alias. Can we consider this as an Asset Access Entity? Similar to how Dag runs is a Dag Access Entity.\r\n\r\nOr should these be considered a separate entity and create AssetAliasDetail? As alias will have its own id and use invokes API with `/assets/aliases/{asset_alias_id},`. We will not get any info about the asset_id here.

hi @potiuk could you also take a look at this one?

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

I agree. I also found it confusing to have [serialization](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/serializers.html) and [dag file processing](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dagfile-processing.html) in `Authoring and Scheduling`. I see `Authoring and Scheduling` a section of the doc for DAG authors that help them understanding how to write and schedule DAGs, I am not sure these docs fit will in this section? Maybe "core concepts"?

Thanks! 

> I still see a failing call being emitted to the `-1` map index.\r\n\r\n\r\nFixed!

Merging as it passed the failing tests in #46942

Merging it quickly - to see the effect in next canary :)\r\n

@vincbeck\r\n\r\nI can do it, but could you suggest what to check? [The current test](https://github.com/apache/airflow/blob/main/providers/amazon/tests/unit/amazon/aws/operators/test_bedrock.py#L335) is pretty simple

> @vincbeck\r\n> \r\n> I can do it, but could you suggest what to check? [The current test](https://github.com/apache/airflow/blob/main/providers/amazon/tests/unit/amazon/aws/operators/test_bedrock.py#L335) is pretty simple\r\n\r\nIndeed :) It is pretty simple. Ideally we want to test the `execute` function. In an ideal world we would like to test different branches such as when `wait_for_completion` is `True`, when `wait_for_completion` is `False`, same for `deferrable` but given this is not your code change, at least one test covering the case when `wait_for_completion` is `True` and `deferrable` is `False` would suffice. You can check that `get_waiter` has been called once with the given parameters. If you look at other tests you can find aspiration, this is pretty standard

Needs rebase and reaolve conflicts 

> Needs rebase and reaolve conflicts\r\n\r\ndone, thanks

@eladkal @romsharon98 Hi, tests failed locally so i fixed a small issue with renaming, could you please re-run tests the workflow? it completes successfully locally now

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

There is also https://github.com/apache/airflow/pull/47088 \r\nwhich PR do we track on this change?

@eladkal I will close https://github.com/apache/airflow/pull/47088. its not relevant for the time being.

Merged. There was a strange "mongo" test failure with lower-binding - but I think that is a new one and we will fix it later.

I also corrected it for "upper" bound :)

@eladkal thanks, youre correct, lets avoid the problem of 1.1.6. Making it >=1.1.7

drill integration test is failing

Yeah. I am tracking it on the slack channel

Closing as its not relevant for the time being.

marking as changelog skip. this is only to unblock CI

@potiuk this is ready for another look. Needed to make a number of changes to the k8s tests...

Good call. Pushed.

>  having a clear procedure that can be easily recommended to new users to help debug connection woes and test things like networking configurations.\r\n\r\nWhy not just use the operator they actually want to use?\r\n\r\nI am not convinced about this feature

@eladkal my thoughts:\r\n- establishing/testing connections is something I see Airflow users struggle with constantly, and it can often be pretty frustrating/obscure what is or isn\

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> Thanks a lot! Looks good!\r\n> \r\n> We have one more here in the AWS provider auth_manager, it may fail since the configuration has changed.\r\n> \r\n> https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/auth_manager/aws_auth_manager.py#L87\r\n\r\nGood catch! :)

Thanks, my grepping failed me!

Uh, non-db test error unrelated. We need to backport 946a62a to v2-10-test

~~Need #47087 first to fix CI.~~ Merged; branch updated.

conflicts :(

Merge conflicts have been resolved!

Closing this PR since I messed up with updating the branch, will fix this and reopen the PR.

Generated contents of the .whl distributions: \r\n\r\n```\r\nArchive:  dist/apache_airflow_providers_apache_beam-6.0.2-py3-none-any.whl\r\n    testing: airflow/providers/apache/beam/LICENSE   OK\r\n    testing: airflow/providers/apache/beam/README.md   OK\r\n    testing: airflow/providers/apache/beam/__init__.py   OK\r\n    testing: airflow/providers/apache/beam/get_provider_info.py   OK\r\n    testing: airflow/providers/apache/beam/hooks/__init__.py   OK\r\n    testing: airflow/providers/apache/beam/hooks/beam.py   OK\r\n    testing: airflow/providers/apache/beam/operators/__init__.py   OK\r\n    testing: airflow/providers/apache/beam/operators/beam.py   OK\r\n    testing: airflow/providers/apache/beam/triggers/__init__.py   OK\r\n    testing: airflow/providers/apache/beam/triggers/beam.py   OK\r\n    testing: apache_airflow_providers_apache_beam-6.0.2.dist-info/entry_points.txt   OK\r\n    testing: apache_airflow_providers_apache_beam-6.0.2.dist-info/WHEEL   OK\r\n    testing: apache_airflow_providers_apache_beam-6.0.2.dist-info/METADATA   OK\r\n    testing: apache_airflow_providers_apache_beam-6.0.2.dist-info/RECORD   OK\r\n```\r\n\r\nBoth `airflow.providers` and `apache.providers.apache` are still implicit packages after building the packages - even if they are legacy packages in the sources.\r\n

> Aaaah, cool. Finally after re-structuring the syntax highlighting in VSCode works again!\r\n\r\nIndeed looks like :)

Random failure. Merging :) 

Hi @pierrejeambrun,  \r\n\r\nHere is a draft for the first entity that introduces authentication and permissions. I want to ensure consistency across other entities and would appreciate your advice on the following changes:  \r\n\r\n- **`tests/api_fastapi/conftest.py`**  \r\n  - Added a bearer token with an admin role to the original `test_client` fixture.  \r\n  - Renamed the original `test_client` to `unauthenticated_test_client` (since adding the `requires_access_*` dependencies to routers means we should now respect the authorization header).  \r\n\r\n- **`tests/api_fastapi/core_api/routes/public/test_dags.py`**  \r\n  - Is adding `_should_response_401` test cases for each router sufficient?  \r\n  - Or should we cover more scenarios, such as Vertical Privilege Escalation?  \r\n\r\nLooking forward to your thoughts!\r\ncc @rawwar 

After discussing with @rawwar offline, we determined that the previous test failure with `403` status code was caused by JWT and test cases using `time_machine`.  \r\n\r\n> The test cases with `time_machine` are decorated with `@time_machine.travel(timezone.utcnow(), tick=False)`. However, when the method is executed, the `test_client` fixture is recreated, causing the token to be perceived as being from the future.  \r\n\r\nAfter further discussion, by setting `iat` and `nbf` to a much earlier time (when Airflow was created) and `exp` to 24 hours in the future resolves the issue.  \r\n\r\n> - **`iat`**: Issued at  \r\n> - **`nbf`**: Not before  \r\n> - **`exp`**: Expiration time  \r\n\r\ncc @pierrejeambrun \r\n\r\n

\r\n\r\nThe rest of CI failures are caused by some side effect of auth_manage with FastAPI app, but I still can\

I found this piece of code in the dag_maker:\r\n```\r\n            if AIRFLOW_V_3_0_PLUS:\r\n                from airflow.models.dagbundle import DagBundleModel\r\n\r\n                if (\r\n                    self.session.query(DagBundleModel).filter(DagBundleModel.name == self.bundle_name).count()\r\n                    == 0\r\n                ):\r\n                    self.session.add(DagBundleModel(name=self.bundle_name))\r\n                    self.session.commit()\r\n\r\n            return self\r\n```

> LGTM just a few nits (non blocking)\r\n\r\nThanks @pierrejeambrun , just resolved and rebased.

Nice!

Nice!

Hi, @jedcunningham @hussein-awala \r\nIf you have a time, would you like to review it?\r\nThank you. 

Hey @ramitkataria, it is not easy for us to track each provider and each usage when such custom behaviour is implemented in inheriting hooks. From your justification, seems like you know the cause of the error, could you try and potentially fix it too?\r\n\r\n- One thing I would do is, log the response `self.conn` or related errors to see what is going wrong for starters.

:scream_cat: 

The skeleton is here now. Please check when you have time Vincent. \r\nIf it looks good, I will move on with integrating into FastAPI.  Otherwise (preferably), I can do it in the next PR maybe, it would be cleaner and easier to review. 

I will check the tests

> A doc in FAB provider would super useful to explain to users how to use this API in order to get a token. This will be the only way for users to call Rest API so it will be definitely super useful!\r\n\r\nThanks for the quick reviews! I will address them soon. I agree, it would be really useful, I will also include documentation. This will be kind of a breaking change in the API usage. Should we include `newsfragments` too? That would be also useful to point users to check the documentation.\r\n\r\nI am surprised that the unit tests are fine in my local and not in the CI. I need to check the environment and dependencies and maybe image rebuild. 

> > A doc in FAB provider would super useful to explain to users how to use this API in order to get a token. This will be the only way for users to call Rest API so it will be definitely super useful!\r\n> \r\n> Thanks for the quick reviews! I will address them soon. I agree, it would be really useful, I will also include documentation. This will be kind of a breaking change in the API usage. Should we include `newsfragments` too? That would be also useful to point users to check the documentation.\r\n> \r\n> I am surprised that the unit tests are fine in my local and not in the CI. I need to check the environment and dependencies and maybe image rebuild.\r\n\r\nNewsfragment is a good idea :)

I will fix the static check and test soon. It seems it still needs a couple of touches 

Thanks for the review and all the comments, Vincent! :)\r\n

The errors are unrelated to this PR. Merging

I also am not sure if that will work -> kaxil will be the committer in this case, but I would merge it just to try :). I am sure he would not mind co-authoring such typo :).

Seems that there are more changes in here - not only widgets update?

Some static checks failing:\r\n```\r\nTraceback (most recent call last):\r\n  File "/home/runner/work/airflow/airflow/task_sdk/dev/generate_models.py", line 39, in <module>\r\n    from common_precommit_utils import (\r\n  File "/home/runner/work/airflow/airflow/scripts/ci/pre_commit/common_precommit_utils.py", line 29, in <module>\r\n    from rich.console import Console\r\nModuleNotFoundError: No module named \

This is superseded by #47065.

Some mypy checks

Alrighty, getting back on this one üëçüèΩ

Finally got a green run on this one!

Damn this will need a rebase now!

Just rebased it, will wait for the CI and merge :) 

Removing restriction https://github.com/apache/airflow/pull/47005

> Can we just create the kms key as part of setup class?\r\n\r\nNo dont think it works, because it should be under mock_aws fixture so creating key under fixture(inside test) would be better, otherwise we may sometimes end up with weird errors like credentials :)

Merging now, failures tests not related to this.

Yeah. Moto has a history of becoming suddenly a bit more "demanding" and expecting more "real" parameters - that sometimes break our tests and signals that they are not "real enough".

Thanks @Lee-W for the review. Just resolved and rebased it.

hi @potiuk, could you take a look at this one?

Nice!

Can you however please fix the static checks and docs building errors?

Will do

hi @potiuk could you run the ci again, it passed locally

Rebased and tests are running.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

@nevcohen \r\nDone with implementing the requested changes, let me know if further adjustments are needed :)

Cool!

Nice :)

@sunank200 can you review this please.? 

I see that the PR is still in draft, let me know when you need another review.

> I see that the PR is still in draft, let me know when you need another review.\r\n\r\nForgot to undraft it, should be good now to review :)

Wondering if gremlin should be a subdirectory in the Apache folder, similar to Hive, iceberg

> > Wondering if gremlin should be a subdirectory in the Apache folder, similar to Hive, iceberg\r\n> \r\n> I think it\

Created another PR after major conflicts with latest changes from www deletions\r\nnew PR #47446 

Random error. Merging.

> Great idea! I\

> 1. Have "hotkey" as a parameter that can be passed to `SearchBar` defaulting to `Ctrl+k`.\r\n> 2. Make "hotkey" to be enabled/disabled as a boolean and not to add hotkey when the task instances page is rendered inside a dagrun by looking for dagId and runId in URL params. To have hotkey only enabled for tasks on global task instances page.\r\n\r\nLet\

I have implemented the second option to disable hotkeys when the task search appears in task instances list inside a dagrun page. On global task instances page the the hotkey will be enabled. Rebased with latest main and fixed conflicts. Thanks.

Can you update the title and description of this PR to say its for Trigger Dag Run? I was worried we were making it nullable everywhere for a second.

> Can you update the title and description of this PR to say its for Trigger Dag Run? I was worried we were making it nullable everywhere for a second.\r\n\r\nChanged it. I should have been more clear in my title.

PTAL @vincbeck and @pierrejeambrun . I do worry about the upgrade path, but the issue of generating the jwt_signing key in memory was causing me issues in my work on updating the JWT signer to support public/private key in another PR.\r\n\r\nWhat do you think?

there it goes @vincbeck https://github.com/apache/airflow/pull/47228

Unitl you run `breeze down` - all the configuration and database data is kept which means that you can easily:\r\n\r\n* close and restart breeze without loosin fernet key and jwt_secrets \r\n* also easily clean them up by running `breeze down` 

Okay, just saw this PR... because came-down attempting to debug why EdgeWorker in breeze fails since a few days.\r\nIn breeze if the secrets are not set (they are not per default) then every process call generates a new token. If you start breeze and run `airflow config list|grep secret_key` you see that there is with every call a new secret being generated.\r\n\r\nWhile I assume this is a good hardening, I think in breeze we need to fix this. Will raise a PR in a moment.\r\n\r\nOtherwise, did we consider back-porting to 2.10 branch? If it is security relevant we should bring it there as well?

### Backport failed to create: v2-10-test. View the failure log <a href=\

Nice :) 

Moving to #47602

Which version has a problem?

> Which version has a problem?\r\n\r\nThe CI run failed due to an issue with Cloud Build. In CI, the latest dependencies are used, and tests are run. This PR addresses https://github.com/apache/airflow/pull/46947 and has already been merged with changes. However, the PR build is still downloading an outdated version of Cloud Build. Forcing the use of the latest version should resolve the issue.\r\n\r\nWDYT?

Nope.

Feel free to suggest, we can have separate groups if needed, example: a separate group PR for build-system-requires dependencies and normal dependencies from toml file.

Nice! 

Now we can do it finally :) 

However all those deps will fail because of generated dependencies 

But I plan to remove them now - I think we should be able to avoid generating the .Jason file in pre-commit - it should be now much easier to rest them. Directly from pyprohect.tomls 

> But I plan to remove them now - I think we should be able to avoid generating the .Jason file in pre-commit - it should be now much easier to rest them. Directly from pyprohect.tomls\r\n\r\ncool :)

Alright we hit another problem :), dependaboat timedout to update dependencies, i think its because of to many folders toml files listing and taking time to resolve dependencies\r\nhttps://github.com/apache/airflow/actions/runs/13461120736/job/37616457933\r\n\r\nShould we split providers path in to multiple groups that we can speed up the process. unfortunately we dont have any config exposed to control dependaboat timeout.\r\n\r\nmy proposal would be like this, we group 3 or 2 providers\r\n\r\n```\r\n  - package-ecosystem: pip\r\n    directories:\r\n      - /providers/airbyte/\r\n      - /providers/alibaba/\r\n      - /providers/amazon/\r\n    schedule:\r\n      interval: daily\r\n    groups:\r\n      providers[airbyte,alibaba,amazon]-dependencies:\r\n        patterns:\r\n          - "*"\r\n          \r\n  - package-ecosystem: pip\r\n    directories:\r\n      - /providers/apache/\r\n      - /providers/apprise/\r\n      - /providers/arangodb/\r\n    schedule:\r\n      interval: daily\r\n    groups:\r\n      provider[apache,apprise,arangodb]-dependencies:\r\n        patterns:\r\n          - "*"\r\n```

merging now it solves on main

CC: @jedcunningham @jscheffl @amoghrajesh\r\n\r\nPR to mark the static executors as deprecated in Airflow core and mark multi-exec as stable. Re: https://lists.apache.org/thread/qmdf5dx71lzcvf4fd7qyc9gq0z0j2jqr

Getting the tests to ignore the deprecation warning at the right time has proven very annoying/tricky.\r\n\r\nThe last stubborn one is related to one instances of the warning not being thrown, but it seems that some underlying failure is actually happening, but I\

FYI I assume the failed CI run goes away on re-base... or as it is on v2-10-test you might need to cherry-pick https://github.com/apache/airflow/pull/46980\r\n\r\nNote I just realize that the PR target is v2-10-stable, I _think_ this should rather be directed to v2-10-test!

@jscheffl I added the backport-to-v2-10-test label to your PR to have it cherry picked back

Now we removed www, FAB is not automatically set up for tests. This is causing problems in FAB provider tests.

I did too @jscheffl -> fixed some more

> I did too @jscheffl -> fixed some more\r\n\r\nI was unsure and wated to contact you.... but you fixed what I suspected and wondered why this was the impact.

I was attempting to make some fixes to the pytests failing, but I am missing an idea how the previous mocked `app` based on appbuilder+Flask can be replicated over. I fear either we need to re-write a lot of tests or we need to add the dependency to FAB provider... which is also not helpful.\r\nA common solution would be great but I am lagging an idea.

> I was attempting to make some fixes to the pytests failing, but I am missing an idea how the previous mocked app based on appbuilder+Flask can be replicated over. I fear either we need to re-write a lot of tests or we need to add the dependency to FAB provider... which is also not helpful.\r\nA common solution would be great but I am lagging an idea.\r\n\r\nI think it\

@pierrejeambrun @bbovenzi take a look at https://github.com/apache/airflow/pull/46942/commits/6a01eaab83cbbd1a92c4e8abdf41c08e81982d04 - I didn\

> @pierrejeambrun @bbovenzi take a look at https://github.com/apache/airflow/commit/6a01eaab83cbbd1a92c4e8abdf41c08e81982d04 - I didn\

Looking promising !

Some tests such as `providers/google/tests/unit/google/common/auth_backend/test_google_openid.py` and `providers/databricks/tests/unit/databricks/plugins/test_databricks_workflow.py` rely heavily on Flask. Not sure what to do with them?\r\n

> Some tests such as `providers/google/tests/unit/google/common/auth_backend/test_google_openid.py` and `providers/databricks/tests/unit/databricks/plugins/test_databricks_workflow.py` rely heavily on Flask. Not sure what to do with them?\r\n\r\nI think they should be switched to fastapi_app - they are for authentiction or some "api" calls.\r\n\r\n\r\n

> > Some tests such as `providers/google/tests/unit/google/common/auth_backend/test_google_openid.py` and `providers/databricks/tests/unit/databricks/plugins/test_databricks_workflow.py` rely heavily on Flask. Not sure what to do with them?\r\n> \r\n> I think they should be switched to fastapi_app - they are for authentiction or some "api" calls.\r\n\r\nI think it is more complicated than that for Databrick. They provide a plugin built on top of Airflow 2 UI, they expect the grid view to exist.

> > > Some tests such as `providers/google/tests/unit/google/common/auth_backend/test_google_openid.py` and `providers/databricks/tests/unit/databricks/plugins/test_databricks_workflow.py` rely heavily on Flask. Not sure what to do with them?\r\n> > \r\n> > \r\n> > I think they should be switched to fastapi_app - they are for authentiction or some "api" calls.\r\n> \r\n> I think it is more complicated than that for Databrick. They provide a plugin built on top of Airflow 2 UI, they expect the grid view to exist.\r\n\r\nI dont think there is another solution other than deleting the plugin and its associated test? 

> > @pierrejeambrun @bbovenzi take a look at [6a01eaa](https://github.com/apache/airflow/commit/6a01eaab83cbbd1a92c4e8abdf41c08e81982d04) - I didn\

I pulled out the change to fix the tmux/`start-airlow` command https://github.com/apache/airflow/pull/47060 so that it runs the right thing based on the airflow version in use (i.e. 2.x still gets webserver, 3.x only gets fastapi)

I pulled out removal of the link to the old UI as well: #47078

Are we planning to update the Python API Client to use the new fastapi server?

> Are we planning to update the Python API Client to use the new fastapi server?\r\n\r\nWe have to - but as far as I know client is generated using openapi spec, which we have already, so it should be just using that openapi spec?

All SQLLite tests are failing as well (whereas the other DBs succeed) because the database is locked. If anyone has any idea why and/or can look into it, that would be great :)\r\n\r\n```\r\nERROR tests/api_fastapi/core_api/routes/public/test_dag_run.py::TestTriggerDagRun::test_should_respond_200[None-None-None-None] - sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked\r\n[SQL: DELETE FROM connection]\r\n(Background on this error at: https://sqlalche.me/e/14/e3q8)\r\n```

> All SQLLite tests are failing as well (whereas the other DBs succeed) because the database is locked. If anyone has any idea why and/or can look into it, that would be great :)\r\n\r\n\r\nLet me rebase first. I will have a look at some of those today. Now when we have this one as candidate for PR of the month, we need to merge it quicky :)

Rebase is in progress - I solved all the conflicts, but I think quite a bit of scrutiny will be needed as I might have solved some things wrongly - we\

Pushed a rebase with conflicts (hopefully) resolved.

API client tests should be fixed\r\n

Hey @mobuchowski, I am making this change https://github.com/apache/airflow/pull/46961, I dont think it should impact, but can you check to be sure?

@amoghrajesh those look orthogonal to me üôÇ 

> Very cool.\r\n> \r\n> One nit and... I started with Firefox on Ubuntu and the grid somehow does not populate. Is this a bug? ![image](https://private-user-images.githubusercontent.com/95105677/415361003-8436b1ba-95a6-40a8-af45-836d1f030811.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDAwODQ2MzEsIm5iZiI6MTc0MDA4NDMzMSwicGF0aCI6Ii85NTEwNTY3Ny80MTUzNjEwMDMtODQzNmIxYmEtOTVhNi00MGE4LWFmNDUtODM2ZDFmMDMwODExLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjIwVDIwNDUzMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY2MzM5YTQyNjE5MTY2YzhhZDc3YTkyMzllZDc4MDE4ZTNkNDEwMGY4OGMzODgwNjE1YzRkYzI5ODZiMzdlMjEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.cj1-jkhnA3c1ki7M_QJMPjOJFNDDos7uL9m3jOYP0hE)\r\n\r\nAre there any errors in the console? And if you look in the network tab for `http://localhost:29091/ui/grid/{dag_id}?offset=0&limit=14&order_by=-run_after`, does it exist and is there a response?

> Are there any errors in the console? And if you look in the network tab for `http://localhost:29091/ui/grid/{dag_id}?offset=0&limit=14&order_by=-run_after`, does it exist and is there a response?\r\n\r\nAh, you are right... HTTP 500:\r\n![image](https://github.com/user-attachments/assets/a2f781a1-0439-4857-a4b2-9d9dc1005165)\r\n\r\n![image](https://github.com/user-attachments/assets/f2cc489a-b9b1-4f2a-afa8-4aa28587fd71)\r\n\r\n![image](https://github.com/user-attachments/assets/098b9a10-1cfc-427d-b283-45dba7f9832e)\r\n

> > Are there any errors in the console? And if you look in the network tab for `http://localhost:29091/ui/grid/{dag_id}?offset=0&limit=14&order_by=-run_after`, does it exist and is there a response?\r\n\r\nA different DAG is working better:\r\n![image](https://github.com/user-attachments/assets/26508b7f-630b-44b1-995c-c2b33a9c0361)\r\n

> Ah, you are right... HTTP 500: ![image](https://private-user-images.githubusercontent.com/95105677/415363640-\r\n\r\nI tried the same dag and could replicate. But this is a bug on main, not due to changes to this branch. 

Thanks @bbovenzi .

> The presence of titles dag, dag run, etc makes the breadcrumb little misaligned at first look personally. The Airflow 2 UI doesn\

> > The presence of titles dag, dag run, etc makes the breadcrumb little misaligned at first look personally. The Airflow 2 UI doesn\

updated dependaboat config change here with groups: https://github.com/apache/airflow/pull/46948

### Guide for specific error code:\r\n```bash\r\n$ airflow error-guide --error-code AERR100\r\n\r\nThe error you\

### Backport failed to create: v2-10-test. View the failure log <a href=\

Cool!

Love the titles !

## What Changed\r\n- Added validation to ensure `key` in `XCom.set()` is not an empty string.\r\n- Raises `ValueError` if `key` is empty, preventing invalid XCom entries.\r\n\r\n## Why This Change?\r\n- Empty `key` can lead to difficult-to-debug issues when reading XComs.\r\n- Improves input validation for more robust XCom behavior.\r\n\r\n## Related Issue\r\nFixes #46925\r\n\r\n## Notes\r\n- This is a non-breaking change; it only validates input to improve reliability.

@ashb I‚Äôve applied the suggested change. Let me know if anything else is needed. Thanks!

@ashb , @amoghrajesh  ‚Äì Based on #46417 , we were leaning towards the conclusion of not allowing `None` as a key. However, @uranusjr  brought up a valid point, and the docstring explicitly states that users can pass `None` to remove the filter:\r\n\r\n:param key: A key for the XComs. If provided, only XComs with matching keys will be returned. Pass *None* (default) to remove the filter.\r\n\r\nI find it useful to allow `None` so that users can retrieve all XComs when needed. What are your thoughts on this?

Let‚Äôs change the type annotation of `get_value` and `get_one` to remove None too.

> Let‚Äôs change the type annotation of `get_value` and `get_one` to remove None too.\r\n\r\n@uranusjr,  The docstring explicitly states that passing None to key removes the filter and returns all XComs:\r\n\r\n"Pass None (default) to remove the filter."\r\n\r\nThis clearly indicates that None is a valid and intentional use case. If we disallow None, we risk breaking scenarios where fetching all XComs without filtering by key is necessary. Since we rely on this behavior, we should ensure that None remains allowed, right?

> The docstring explicitly states that passing None to key removes the filter and returns all XComs:\r\n> \r\n> "Pass None (default) to remove the filter."\r\n\r\nIt does not make sense to remove the key filter from those functions. They get one single XCom value. Without the filter, you‚Äôd get an arbitrary value associated to the task, which has no practical use case and only produces a potential way to introduce bugs. The docstrings are _technically_ correct, but we should remove them.

@uranusjr - can we merge this PR, or are any other changes needed?

I made a small change to how `get_many` checks non-emptiness. Looks good to me.

@uranusjr, I fixed the issue with unused imports and ran the pre-commit checks locally. However, some tests did not run due to a Docker error. I believe the issue is fixed now. Please review and approve again.

Created the PR so that the K8s system test can run. \r\n\r\nWill update the desc etc

The failures arent related to the PR

Can you please fix conflicts and add a unit test ?

> there are a couple of print statements as well in the `backfill_command.py` that you might consider to clean up as part of this PR?\r\n\r\nThese print statements are necessary as this is a CLI command, and they provide essential output for users. Removing them could impact usability.

> > there are a couple of print statements as well in the `backfill_command.py` that you might consider to clean up as part of this PR?\r\n> \r\n> These print statements are necessary as this is a CLI command, and they provide essential output for users. Removing them could impact usability.\r\n\r\nWhat I meant is not removing, but using `console.print` may be? I saw that in other `cli` commands in the codebase. 

> > > there are a couple of print statements as well in the `backfill_command.py` that you might consider to clean up as part of this PR?\r\n> > \r\n> > \r\n> > These print statements are necessary as this is a CLI command, and they provide essential output for users. Removing them could impact usability.\r\n> \r\n> What I meant is not removing, but using `console.print` may be? I saw that in other `cli` commands in the codebase.\r\n\r\nAgreed, I have implemented this as you suggested @phanikumv.

> Nice.\r\n> \r\n> I can follow up with the front-end change required for #46715\r\n\r\nThanks Pierre! We probably have a few places in the UI that do `taskInstance.rendered_map_index ?? taskInstance.map_index` that we can now simplify.

Re-ran the tests and the failing test passed (it was a timing test, and those tend to be flakey). Just one comment!\r\n

Great cleaning! Should we include this part in this PR or another one? \r\nhttps://github.com/apache/airflow/blob/c505b240bce6ae50c6010260a3e673a8f22d2a8c/providers/fab/src/airflow/providers/fab/auth_manager/fab_auth_manager.py#L216\r\n\r\n**Edit:**\r\nThere is one more :)\r\nhttps://github.com/apache/airflow/blob/c505b240bce6ae50c6010260a3e673a8f22d2a8c/providers/fab/src/airflow/providers/fab/auth_manager/fab_auth_manager.py#L152

My 2c, a follow up for those would be better.

Great, make sense! Was just wondering your ideas. Thanks! :)

Should I separate the test refactoring from this PR?

The taskflow tutorial docs change `docker` -> `cncf.kubernetes` already applied in\r\nhttps://github.com/apache/airflow/pull/47387  

I think that this should be adjusted to use TaskSDK and not depend on direct DB access, so made it a draft for now, before I would understand the proper way of doing it.

This is a good start. \r\nI have a bunch of additional content for this which I will add on top of this PR. 

@potiuk @amoghrajesh @sunank200 @eladkal \r\n\r\nSince I see that most of you are involved in decision making and recent contributions to the google-provider, and this is my first time contributing to airflow, I would love to hear your view/opinion on this feature.

> Rather than just putting this URL in the log message, please look at the "OperatorExtraLinks" concept -- you can make this show up as a button in the Airflow UI.\n> \n> (They way I would suggest wiring that up is to have the operator push the url to XCom, and the OperatorLink can then pull the URL out of xcom)\n\nThanks, wasn\

Another idea I thought during testing would be in case of failed latest dagrun get to the dagrun page with task instances filtered by `state=failed&state=upstream_failed&state=up_for_retry` in the url param. But it felt like too much magic since there could be other reasons of failure with no failed taskinstance like dagrun is marked manually as failed and differs from successful dagrun.

@dependabot rebase

Closing in favour of #46938 

@dependabot rebase

Closing in favour of #46938 

Might be a bit weird to have a run link to a TI. We can always adjust later.

> Might be a bit weird to have a run link to a TI. We can always adjust later.\r\n\r\nYes, but this is on the Task details (across all runs) page. I think in that context it works.\r\n\r\n<img width="857" alt="Screenshot 2025-02-19 at 11 53 22\u202fAM" src="https://github.com/user-attachments/assets/0741b720-fab5-4b79-93fb-589323770dac" />\r\n

Need to fix the `config` endpoint that is reached, it‚Äôis not the correct one.

hi @eladkal not sure why i am not able to see an option to assign reviewers. Can you please review this?

Looks good! Thanks for the fixes! 

hello @eladkal can you help in merging the pr?

Just a small tip, pinging people does not make things faster. Whenever someone or Elad have free time to check this, they will :)

One suggestion is to join our community Slack workspace and attract more attention by using the `#contributors` channel instead of pinging them individually. \r\nhttps://airflow.apache.org/community/

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Definitely not. But we should start making this change once we start moving providers to use task sdk imports. That way providers will not have a airflow.models import

Looks good. Maybe wait for Ash confirmation.

I honestly have no idea

I think I‚Äôm going to ParamsDict them first. We can always add it back if someone needs it, but it‚Äôs a lot more difficult to remove it afterwards. I think it‚Äôs only useful for typehints anyway? In those cases it‚Äôs fine to just hint the variable as `Mapping` since ParamsDict does not actually provide any additional structural information.

This will fix the first issue, but what should `Past` or `Future` buttons in the UI do when logical date = None?

We should probably either disable them or make them do nothing. There‚Äôs no corresponding Python function for them too.

> Do we need to add a test case for `validate_context`?\r\n\r\nThere is already a test for trigger_dag_run which calls `validate_context`

Amazing cleaning! Looks great. 

-600 ... yay!

:drum: :drum: :drum: :drum: :drum: :drum: :drum: 

Let us all make smores by the fire üî• üç´ 

74 kLoC deleted. WOW :-D

Links to Airflow 2 UI should be updated . Opened https://github.com/apache/airflow/issues/46902

PR moved over to https://github.com/apache/airflow/pull/46942

@potiuk \r\nHi there! Can you please review changes from this PR? Thanks :)

> The interface for "CommandType" has been changed to accept either a sequence string or workloads.ExecuteTask which will be useful for KubernetesExecutor.\r\n\r\nWe shouldn\

> > The interface for "CommandType" has been changed to accept either a sequence string or workloads.ExecuteTask which will be useful for KubernetesExecutor.\r\n> \r\n> We shouldn\

Waiting for a green CI now!

I think i fixed the static check now, lets see how it goes!

Thanks for the approvals, @ashb and @o-nikolas! Merging this one

Nice!

> Looks good! Thanks! Agree on the unit test.\r\n\r\nadded

I gave this a spin with the following DAG, and it would sometimes still retrieve just part of the xcom file.\r\n\r\n```python\r\nfrom airflow import DAG\r\nfrom airflow.operators.python import PythonOperator\r\nfrom airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\r\n\r\ndef consume_xcom(**context):\r\n    ti = context[\

This PR is super-seeded by https://github.com/apache/airflow/pull/47568

Looks like it fixes it. Merging

I have one more question. I noticed that `arg_subdir` is an unused argument in the `dag_state` cli command since the required fields are `dag_id` and either `logical_date` or `run_id` (which are unique for each DAG run).  \r\n\r\nDo you think we should modify the function to support `dag-bundle`, allowing it to display all DAG runs for the specified `dag_bundle`, or should I simply remove `arg_subdir`?

Paging @bugraoz93! For `dags report`, was there a plan for getting the parsing duration time once that command is moved to be remote? I don\

> Paging @bugraoz93! For `dags report`, was there a plan for getting the parsing duration time once that command is moved to be remote? I don\

Some tests are still failing. Could you please check those?

> Looks good! Could you please rebase your branch?\r\n\r\nüëçüèª 

> Could you please add unit tests?\n\nFor sure, I am going to add them. Sorry for double-checking! \n\nI am going to work on the changes needed in FAB next for this integration. I might again need your review there :)

> > Could you please add unit tests?\r\n> \r\n> For sure, I am going to add them. Sorry for double-checking!\r\n> \r\n> I am going to work on the changes needed in FAB next for this integration. I might again need your review there :)\r\n\r\nFor sure! Happy to review :) Feel free to add me as reviewer/tag me

> > > Could you please add unit tests?\r\n> > \r\n> > \r\n> > For sure, I am going to add them. Sorry for double-checking!\r\n> > I am going to work on the changes needed in FAB next for this integration. I might again need your review there :)\r\n> \r\n> For sure! Happy to review :) Feel free to add me as reviewer/tag me\r\n\r\nThanks a lot! I will do that :) I added unit tests and moved some parts to `conftest`.  

Sorry I did not add comments as part of one review, they arrived in my mind after my first review :)

I addressed them all. Whenever you have time :)

> Nice! Thanks for addressing all the comments \n\nThanks for your time! :)

> I think you should not replace the old ways. Maybe you can add new ways and say create "NativeTrinoOperator" to do things differently. There is a good reason why we have the common sql abstraction and we want all databases to have more or less the same behviour and this is the way we want to promote it with our users: this allows them to easily switch between different datbases - mostly by changing connections.\r\n> \r\n> Happy to review it when it is changed to be "addition".\r\n\r\nThere was a dedicated TrinoOperator, which was deprecated in favor of SQLExecuteQueryOperator (Issue #25259).\r\n\r\nHowever, SQLExecuteQueryOperator does not fully support Trino‚Äôs client protocol. Trino requires the client to fetch results before finalizing query execution. Without this, Trino cancels the query with a USER_CANCELED error, even when data modifications have been applied. This leads to unreliable data modification handling.\r\n\r\nI implemented TrinoOperator as a subclass of SQLExecuteQueryOperator since the required change is minimal, it ensures results are fetched. This approach is consistent with how other databases with unique behaviors are handled in Airflow. For example, TeradataOperator and SnowflakeOperator also inherit from SQLExecuteQueryOperator while implementing their own specifics.

I think there is a much simpler solution.  The condition [here](https://github.com/apache/airflow/blob/main/providers%2Fcommon%2Fsql%2Fsrc%2Fairflow%2Fproviders%2Fcommon%2Fsql%2Foperators%2Fsql.py#L305) needs to be corrected, to pass the handler, even if the `do_xcom_push` is Flase.

Yes. I would rather avoid creating specific operators if it can be done using built-in common.sql operators. Note there are many more operators in common.sql that provide much richer functionality and you would have to subclass them alll to have similar usage - also things like GenericTransfer work by jus providing two connection_ids that create hooks and they are not using "specific" operators - so making surethe "generic" interface handles also Trino in the way that is consistent with others is way better approach.\r\n\r\nSnowflake and Teradata operators should not be there, really I think it\

Currently set this PR as DRAFT.\r\nOpened a specific PR for adding configuration to fetch results within the SQLExecuteQueryOperator, #46997\r\n\r\n@potiuk FYI

@potiuk I closed [this MR](https://github.com/apache/airflow/pull/46773) and opened this with the tests.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

This is failing it\

> This is failing it\

quick update on the current progress. core tests should already be fixed. some provider ones remaining.\r\n\r\n* providers/amazon/tests/unit/amazon/aws/log/test_cloudwatch_task_handler.py\r\n* providers/amazon/tests/unit/amazon/aws/log/test_s3_task_handler.py\r\n* providers/google/tests/unit/google/cloud/log/test_gcs_task_handler.py\r\n* providers/elasticsearch/tests/unit/elasticsearch/log/test_es_task_handler.py

just add compat code to providers. The tests should pass now... I think...

fixed the remaining ones!

all green!

regular tests fail now.

> regular tests fail now.\r\n\r\n- I fixed the name of test parameters to adhere to Airflow Connection naming.\r\n- I enhanced the code to treat login and password if they are either None or empty strings.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

cc @jason810496 also @Lee-W @jedcunningham -> we should be careful with adding new pre-commits.  This small addition in pre-commit-config had almost all performance problems I could think of with pre-commits :D

cc: @jason810496  --> You likely did not know that :) but:\r\n\r\n* pre-commit by default runs checks in parallel when you run it on group of files - it will split them in chunks and run as many processes as you have processors (this can be disabled by require_serial)\r\n* also by default pre-commit will pass the list of files to pre-commit - those files that were locally changed in the PR and match the "files" specification if it is there. We **really** don\

Yeah. We have *VERY* complex and *VERY* comprehensive test harness and it has a few "you need to read a few pages few times and get burned once or twice to start to understand how things work" 

@potiuk  Thanks for reminding us! Yep, I should have thought it through. ü§¶\u200d‚ôÇÔ∏è

> @potiuk Thanks for reminding us! Yep, I should have thought it through. ü§¶\u200d‚ôÇÔ∏è\r\n\r\nI realise that only a handful of people including me live and breathe the CI and dev env of Airflow, so I will keep on reminding and explaining - no worries :)

> It seems "::group::" is the documented approach though code also supports "##group##" . Do we need to support this?\r\n\r\nThe syntax with `::` is the Github style. The variant with `##` is the way how ADO (MS AzureDevOps Pipelines) handle log grouping. As I was contributing this initially I wanted to make it compatible to both. If this is major complexity we can re-discuss. Else I\

Thanks @jscheffl  for the review. Glad to have the points since you implemented it in Airflow 2.\r\n\r\n> The syntax with :: is the Github style. The variant with ## is the way how ADO (MS AzureDevOps Pipelines) handle log grouping. As I was contributing this initially I wanted to make it compatible to both. If this is major complexity we can re-discuss. Else I\

I think we need to make sure this is compatible with the log handler changes here: https://github.com/apache/airflow/pull/46827

Thanks, whichever gets merged first I can try to fix the compatibility issues. I guess color highlighting based on keywords is also easier since I just need to add `color` attribute to `Text` based on keyword present in the line which can be done in a follow up PR before release.

I rebased and with structlog rendering I guess I might need to start from scratch on the implementation :( `renderStructuredLog` handles `logMessage` which could be both `string` and `StructuredLogMessage` . Is there a way to see from response headers or content if the log response is a task-sdk log and a normal log to switch between implementations?

> I rebased and with structlog rendering I guess I might need to start from scratch on the implementation :( `renderStructuredLog` handles `logMessage` which could be both `string` and `StructuredLogMessage` . Is there a way to see from response headers or content if the log response is a task-sdk log and a normal log to switch between implementations?\r\n\r\nI was afraid that change was going to be hard to rebase against.\r\n\r\n@Lee-W Do you know the answer to this question?\r\n\r\nI think regardless we need to check the logMessage string or `logMessage.event` string if it includes ::group::

> Is there a way to see from response headers or content if the log response is a task-sdk log and a normal log to switch between implementations?\r\n\r\nIn the latest change, we make `StructuredLogMessage` starts like this\r\n\r\n```python\r\n           StructuredLogMessage(event="::group::Log message source details", sources=source_list),  # type: ignore[call-arg]\r\n            StructuredLogMessage(event="::endgroup::"),\r\n```\r\n\r\nThis logic is implemented in `airflow/utils/log/file_task_handler.py` so I\

@Lee-W  Sorry, by normal log I meant the Airflow 2.x logs which were mostly just string.

Strangely `pnpm test` fails but `pnpm test src/pages/TaskInstance/Logs/Logs.test.tsx` passes locally. Is there something I am missing while running tests in parallel?

The default timeout for `waitFor` is 1000ms (1 second) which might be causing random failures. Increased it to 10s and it seems to be fixed.\r\n\r\nhttps://testing-library.com/docs/dom-testing-library/api-async/#waitfor

I think the screenshot you have there looks good. \r\n\r\nNice catch on the timeout.

Although it looks like sometimes we just repeat the log group message\r\n\r\n<img width="843" alt="Screenshot 2025-03-03 at 9 43 59\u202fAM" src="https://github.com/user-attachments/assets/4d602478-ff74-4c72-a094-5be398f4f80d" />\r\n

Task sdk logs render the event which has group name along with any other key=value pair as single string so while parsing this after the concatenation looks like the actual log group name. The concatenation and grouping are done in different functions. Not sure how to fix it.

Rebased  - there were some issues with "check-default-configuratoin" static check that should be already fixed in main

I assume the referenced bug thicket is not the right one, it refers to a PR

I have modified

Hello, if you have any questions about the process, I can modify it.

If the return code is 1, then the spark submit subprocess exited abnormally.\r\n![image](https://github.com/user-attachments/assets/35ed0e20-44c0-4a9d-b5a8-3a79753e21ec)\r\nFirst, I am using Kubernetes Executor, spark client mode. This is the worker log. The exit code is displayed in the log, but it does not mean that the spark program has really exited due to an error.

The cluster mode is similar. The sparksubmit process is a child process, and the oom exit code may appear occasionally.

> If the return code is 1, then the spark submit subprocess exited abnormally.\n> ![image](https://github.com/user-attachments/assets/35ed0e20-44c0-4a9d-b5a8-3a79753e21ec)\n> First, I am using Kubernetes Executor, spark client mode. This is the worker log. The exit code is displayed in the log, but it does not mean that the spark program has really exited due to an error.\n\nYou run the spark driver on the worker itself?

I did some tests here(spark on kubernetes).\r\ncluster mode:we get that the returncode is 0 but the spark k8s driver exit code is 1.\r\n```\r\nimport subprocess\r\n\r\nif __name__ == \

> I did some tests here(spark on kubernetes).\n> cluster mode:we get that the returncode is 0 but the spark k8s driver exit code is 1.\n> ```\n> import subprocess\n> \n> if __name__ == \

I have changed

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Thanks everyone for the assistance!

Congrats @bin-lian!

nice one :)

> I see that the PR is in Draft, from a first quick read it looks good.\r\n> \r\n> Is there anything else you would like to add to the PR before I do an in depth review ?\r\n\r\nWe are good. I will raise another PR for rest of endpoint

Nice thanks.

Great thanks @insomnes!

> But since tests/www will be deleted soon together with the old UI, I woudl not worry too much about it.\r\n\r\nThat\

@alecsg77 @romsharon98 @eladkal Could you have a llok at this PR (and possibly also #46779 which takes a bit of a different approach to fix the same issue). Needs to be part of helm release 1.16.0

> Found one nit and some error in docs (uff). Otherwise - looking good.\r\n\r\nApplied nit fixes, also hopefuly the constraint/provider generation with suffix as well

FYI: also cc: @eladkal -> while fixing provider doc generation in the PR (was failing when generating fab) I also re-enabled (that was commented out with a TODO for me to re-enable) linting of the generated .rst files and found out that for FAB with 1.5.2 the generated documentation is not valid (repeated links in commits.rst). Also I forward-ported FAB changes from 1.5.3

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Thank you @Vincent550102! Looks good :)

Nice

Nice!

@potiuk \r\n\r\ni wanna ask something \r\nwhen i was working on docs \r\ni see there are already docs of airflow config lint but why is it not seen in a documentation

> Nice!\r\n\r\nThanks

Closes #46756

Sorry, but you start to use some weird `tests_common` package in your imports which is not well defined as test dependency or else. You better to define it as some subpackadge of airflow package or create `airflow.testing package` - the best idea to share common test utils, or to use pytest fixtures, but never ever import something from tests into tests.

> Sorry, but you start to use some weird tests_common package in your imports which is not well defined as test dependency or else. You better to define it as some subpackadge of airflow package or create airflow.testing package - the best idea to share common test utils, or to use pytest fixtures, but never ever import something from tests into tests.\r\n\r\nNo. We do not "start" using it - we already use it for a long time. And the plan is that `tests_common` will be turned as dependent package in pyproject.toml files of the providers, so it won\

BTW. @jscheffl => I had to move the `load_*` files to tests_common, because we used "microsoft" provider test code from "hive" one. And the standard way to handle that is to move the code to "tests_common". \r\n\r\nAs explained to @kxepal -> one of the next steps in the cleanup will be to make "tests_common" a shared distribution that will be used in our workspace by all providers and airflow code. I am going to do it next as apparently it causes confusion now.

> > Sorry, but you start to use some weird tests_common package in your imports which is not well defined as test dependency or else. You better to define it as some subpackadge of airflow package or create airflow.testing package - the best idea to share common test utils, or to use pytest fixtures, but never ever import something from tests into tests.\r\n> \r\n> No. We do not "start" using it - we already use it for a long time. And the plan is that `tests_common` will be turned as dependent package in pyproject.toml files of the providers, so it won\

> –û–∫, sorry for that. Still, better to extract it into separate project with a better name to avoid any confusion (:\r\n> I would like to use some test fixtures from airflow in my test cases, but have to copy-paste them just because of lacking of package to depend on it.\r\n\r\nSee the email which I just sent to devlist - explaining what is the next step in cleaning up our structure: https://lists.apache.org/thread/jonodvjnycv01qcsnlqczpvrjhfsx04g  - chime in if needed.

> I would like to use some test fixtures from airflow in my test cases, but have to copy-paste them just because of lacking of package to depend on it.\r\n\r\nBTW. It\

> @potiuk thank you, but no block is needed just because of me. I\

And BTW - when we finish the packaging story, we will have a bit more than 100 distributions in Airflow. More separation, dependencies and splitting is coming. That discussion is yet ahead of us, but we already have a vision written down.

Finally green :). It required a few more "touches" :D

> Was OK\

And merged :)

Please rebase the PR

> Please rebase the PR\r\n\r\npulled the latest main changes :) thanks\r\n

@bugraoz93 @ephraimbuddy I squashed commits, please take a look :)\r\n

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Congrats @valentinDruzhinin 

Congratulations, @valentinDruzhinin!

@ephraimbuddy @bugraoz93 thank you guys :)

nice :) 

Still when the params contain "\

Ah yes. good catch!

@potiuk shlex is the way to go. I tried with double and single quotes, works as I expected.

@amoghrajesh  The injection still exists. I just send a PR.\r\n\r\nhttps://github.com/apache/airflow/pull/46809

Background from Slack:\r\n\r\n> Yes breeze down is the way to go. The issue is that while we have ‚Äúin-progress‚Äù work or when you switch branches  - some of the migrations might or might not be valid, and your database might or might not be upgradeable/healthy. Breeze down cleans up the volumes used by the DB and generally make the DB reinitialize from scratch next time\r\n\r\n> It‚Äôs rather difficult to find out when you should clean-up the database (for example this might happen when you just rebase to latest version of airlfow or go back to previous version because your old PR was based on old version of airflow - but maybe someone could come with a proposal how to communicate it better in case this happens

There is a caveat for tests with `--run-in-parallel`: the notification would be printed separately for every parallel run.

Fantastic. Thanks @insomnes ! \r\n\r\nI tested it by modifying `backend-mysql.yaml` a bit and making health check fai:\r\n\r\n<img width="873" alt="Screenshot 2025-02-15 at 13 15 12" src="https://github.com/user-attachments/assets/d8c0faa3-1dfe-4924-87eb-e3ee7c0e8c15" />\r\n\r\n`SELECT 1EEEEE` fails, and decreasing the times for check make it fails fast :)\r\n\r\nResult:\r\n\r\n<img width="848" alt="Screenshot 2025-02-15 at 13 14 10" src="https://github.com/user-attachments/assets/39a46804-03c0-4aeb-a180-2950b0ac2028" />\r\n\r\n\r\n

Please do not use real airlfow repository to experiment with AI or even if it is not AI  generated this PR makes no sense. You are tapping into time of maintainers who have to look at your PR and decide that it is spam @Maryam3107.\r\n\r\nI reported you to GitHub and they might close your account if you continue doing so.

Needs rebase :)

Looks good to me! (Although needs rebase)\r\nVerified, on UI in Dataproc everything works as expected both in deff and non-deff modes :)\r\n\r\n@potiuk Can you please approve and merge, if it looks good to you? \r\n\r\nThanks :)

Thanks, updated.

anything remaining @eladkal :)? 

@eladkal can you please take a look again? I cannot merge this PR without your approval. Thanks.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

We need to create the label `log exception`. Maybe this will be needed even for this one because the test contains the same code pieces that the new Selective Check is catching.

> We need to create the label `log exception`. Maybe this will be needed even for this one because the test contains the same code pieces that the new Selective Check is catching.\r\n\r\nCreated. I think this time it did not fail? 

> > We need to create the label `log exception`. Maybe this will be needed even for this one because the test contains the same code pieces that the new Selective Check is catching.\r\n> \r\n> Created. I think this time it did not fail?\r\n\r\nI will watch out for PRs to observe the behaviour :crossed_fingers: Thanks for the review :) 

@alecsg77 I see you also already provided a PR. I also just yet added a PR #46811 \r\nI took the different approach to replace the comma so the label is valid but still shows the multiple executors.\r\n\r\n

There are not many differences since in both we are using strict index to read the data and splitting the string. Both are too similar approaches using different variables. \r\nCC: @jedcunningham could you please check this when you have time? There are two solutions in our hands now. I think it would be great to get your take on this

Makes sense to me to include for a 2.X bug fix. Care to port this to v2-10 over once it merges?

Need to close and re-open the PR to have the build green...

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/46790"><img src="https://img.shields.io/badge/PR-46790-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

closing in favor of https://github.com/apache/airflow/pull/47126

I am always suspicious of changes that change the code but have no "test" modified., This usually means the test coverage was not enough (if tests pass) or that the tests will fail. Both cases scream for tests adding/modification. Can you please add tests to cover it @dantonbertuol ?

<img width="1385" alt="Screenshot 2025-02-14 at 10 22 06\u202fPM" src="https://github.com/user-attachments/assets/39080bfe-5949-4e75-8e46-6d7112e6be9f" />\r\nWIP UI, still need to add a few more columns

<img width="1385" alt="Screenshot 2025-02-23 at 11 22 01\u202fAM" src="https://github.com/user-attachments/assets/448683c1-f85d-4365-895c-dcdacfc53f0b" />\r\nThis is the current UI

Nice. I see you took the dataset_dependencies endpoint. I think we should expand it back to the overall dag dependencies graph to include sensors or triggers. I think all we have to do is change the name of the endpoint, move it to `dependencies.py` file, and then remove the if statement that only includes dags, assets and asset aliases.

I apologize for the delay. Due to some hardware issues, I have lost all my data and am now raising a new PR. \r\n\r\nCould you please review PR #47106 when you have some free time?\r\n\r\n

Hello @Prab-27,\r\n\r\nI needed that to do the front-end and did the PR yesterday to be able to move forward. https://github.com/apache/airflow/pull/47066\r\n\r\nWe have a deadline this week to be able to have that in the beta release.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

@potiuk This is failing static tests -- `Remove trailing whitespace at end of line...................................`

> @potiuk This is failing static tests -- `Remove trailing whitespace at end of line...................................`\r\n\r\nI have run the tests and all passed.\r\nWhat issue are you saying. \r\n@ashb ?\r\n

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

### Backport failed to create: v2-10-test. View the failure log <a href=\

backported to v2-10-test

@potiuk is working on #46608 which should address this issue. So I am not sure if this one is a duplicate or can be a short term solution until  #46608 is merged?

@potiuk, how do I get a reviewer or who can I reach out to review this PR? I notice that the `google` provider has no CODEOWNER.

Hi! I see that you have added unit test, but can you please provide here a screenshot of successfully executed system test for this logic? Thanks!

No need in this case - we trust you :)

@potiuk and @VladaZakharova, sorry for the delay. \r\n\r\n<img width="662" alt="image" src="https://github.com/user-attachments/assets/4ce98cf8-dfb5-472b-8d1e-fea748e465eb" />\r\n

Rebased to see if #46608 fixes the failures :)

It looks like it does :)

Oops

So only affected alpha releases for now

Optionally we could mark it as stable in 2.11 if we wanted to give people more time to migrate. Thoughts?\r\n

I think we better have lazy consensus vote on this one.\r\nAlso we need to decide what to do with CeleryKubernetesExecutor and the others od that kind. They qere not deprecated because Hybrid was marked experimintal.\r\n\r\nPersonally I would prefer to wait with marking it stable. There were reports on this feature not working with helm chart and so far the fix was not released (waiting for chart 1.16)

Now that #46944 is merged, and Jed is planning to release a new Helm Chart soon, we should be able to merge this one as well.\r\n\r\nThis small PR simply maintains consistency with the above mentioned change in 2.10.X\r\n\r\nCC: @jedcunningham @jscheffl @amoghrajesh 

Sorry, I made an unforgivable mistake with git.

> Sorry, I made an unforgivable mistake with git.\r\n\r\nGit is not forgetting - so you can find still all in the history.

Yes, and sometimes, it is not even forgiving as well.üòÑ\r\nI did not freely choose to close it; it was closed automatically.\r\nHowever, this PR has received too many unsolicited reviewers, so I will create another one that includes the test fix.

I am still not sure if this is the intended implementation or not, but it is limited by the current execution API restriction of 1 XCom result in response. I didn\

If this PR is not in the right direction I am ok with full rework.  I just need to understand what was planned for this feature then. 

Hello @insomnes, apologies for the delayed review. Taking a look at it.

@amoghrajesh thanks for the review! No worries about the delay!\r\nI appreciate your time and I‚Äôm glad I could contribute to Task SDK development.

Up. If this PR should wait for something else or other solution should be chosen, than it would be nice to have some update about it.

Where are you seeing an error?\r\n\r\nNot having to pass `={true}` for a boolean param is a common practice

All good. Once in a while I have to manually restart the eslint server if it gets out of sync.\r\n\r\n<img width="240" alt="Screenshot 2025-02-13 at 9 50 58\u202fPM" src="https://github.com/user-attachments/assets/29578e43-8148-419c-a219-3d699d78cb22" />\r\n

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Only failures are related to #46608

Do not merge until #46722 is (GH should automatically re-target that to main when that happens)

Yeah, i think i have to fix the execution API tests

Oh some task sdk tests will fail now, will take a look!

>Impact on custom XCOM backends.\r\nFlow:\r\n\r\n>TLDR: Custom XCOM backends store the "path" to the xcom object remotely stored in the DB as a JSON type, but store the data at that "path" without serialisation (as JSON objects). So the interface to interact with this backend is in JSON objects, which is what the new proposed flow above does precisely.\r\n\r\n\r\nNot always, example `XComObjectStorageBackend` only stores value in a remote object if it is greater than a certain threshold, for others it serializes the objects into string using `json.dumps`\r\n\r\nhttps://github.com/apache/airflow/blob/8f63b828ace09b1095229f22b0c6d1f0f85ea81a/providers/common/io/src/airflow/providers/common/io/xcom/backend.py#L116-L118\r\n\r\nhttps://github.com/apache/airflow/blob/8f63b828ace09b1095229f22b0c6d1f0f85ea81a/providers/common/io/src/airflow/providers/common/io/xcom/backend.py#L126-L133

> > Impact on custom XCOM backends.\r\n> > Flow:\r\n> \r\n> > TLDR: Custom XCOM backends store the "path" to the xcom object remotely stored in the DB as a JSON type, but store the data at that "path" without serialisation (as JSON objects). So the interface to interact with this backend is in JSON objects, which is what the new proposed flow above does precisely.\r\n> \r\n> Not always, example `XComObjectStorageBackend` only stores value in a remote object if it is greater than a certain threshold, for others it serializes the objects into string using `json.dumps`\r\n> \r\n> https://github.com/apache/airflow/blob/8f63b828ace09b1095229f22b0c6d1f0f85ea81a/providers/common/io/src/airflow/providers/common/io/xcom/backend.py#L116-L118\r\n> \r\n> https://github.com/apache/airflow/blob/8f63b828ace09b1095229f22b0c6d1f0f85ea81a/providers/common/io/src/airflow/providers/common/io/xcom/backend.py#L126-L133\r\n\r\nThanks! So, yeah, thats true. We still are keeping the same behaviour even with this change, for smaller xcom values, we store in DB in a serialised manner, and for larger data, we store the path in a serialised manner. So when the get xcom is called for an xcom whcih went to object store, the data would be returned as a json object, correct? And so would it for smaller xcom values too. Is that right or i misunderstood?\r\n

Yeah, why is this needed? We can already manually trigger a dag run or manually create an asset event through the API.

It‚Äôs in the vision of the data awareness proposal. The motivation is to be able to treat assets as the central concept like DAGs. When you want an asset to be refreshed, you can operate on the asset directly, instead of needing to go through the thought process of finding what DAG contains a task that refreshes the asset, and triggers that DAG instead (DAG is the central concept in this thought process).\r\n\r\nYeah we definitely don‚Äôt really _need_ this, and we probably don‚Äôt need to do anything in the UI for 3.0. This is just a new thing that promotes the asset centric idea.

Seems it does :)

Thx @potiuk 

Thanks @jedcunningham for fixing this. I have also noticed this issue yesterday.

left some nits

Coud you please add a unit test for it and explain a bit more what is the problem you are solving in description of your commit? 

(just to explain - while it links to the issue, keeping the cause explanation in the commit, will make it easier to find the cause and provide a changelog).

@potiuk I have updated the PR description to add the details of issue, and also added a test (I am not sure if this test will suffice).. please let me know :)

@potiuk Updated, please check

Fixed already in #46700

Cool @vincbeck > closing.

Good catch. I thought I had that in there but maybe it got lost in rebasing.

Implementation wise, here is my thinking. I am starting by `MessageQueueTrigger`.\r\n\r\nGiven `msg_queue`, `MessageQueueTrigger` needs to figure which hook it will use to poll/pop a message from the queue. Example: `if msg_queue.starts_with("https://sqs."): hook = SqsHook(...)`. Then we can use the hook to retrieve the message. The hook will contain the logic for each provider (AWS, Google, Kafka, ...). This means, this new provider will have a dependency with all these providers. Do you think this is an issue? Did you have something else in mind?

> Implementation wise, here is my thinking. I am starting by `MessageQueueTrigger`.\r\n> \r\n> Given `msg_queue`, `MessageQueueTrigger` needs to figure which hook it will use to poll/pop a message from the queue. Example: `if msg_queue.starts_with("https://sqs."): hook = SqsHook(...)`. Then we can use the hook to retrieve the message. The hook will contain the logic for each provider (AWS, Google, Kafka, ...). This means, this new provider will have a dependency with all these providers. Do you think this is an issue? Did you have something else in mind?\r\n\r\nYou are right Vincent. I did think about the "Composition vs. Inheritance" approach tradeoff. \r\n\r\nThe composition style interface as defined here is easier for the DAG author, but more maintenance for us. \r\nI talked about this with Ash and Jed as well and because of the underlying plumbing already present in Airflow for finding connections, et al, this seemed reasonable as an approach in order to make the end-user experience better. \r\n\r\n

I updated the PR. I focused only on the trigger side. Please let me know if this is what you had in mind in terms of implementation regarding the trigger. I really see it as a proxy of the provider triggers. I could not test it because the new provider is not recognized but once that solved I should be able to test it.

Generally @vincbeck -> look at everything below `[dependency-groups]` in the root `pyproject.toml` - the provider should be added in all those places.

Thank you :D

I am not sure what is going on üëÄ `check-providers-subpackages-init-file-exist` is failing because it detects missing `__init__` file in `/home/runner/work/airflow/airflow/providers/common/messaging/src/airflow/__init__.py` but this is the old provider directory. It no longer exist.\r\n\r\n```\r\nAdded missing __init__.py file: /home/runner/work/airflow/airflow/providers/common/messaging/src/airflow/__init__.py\r\nAdded missing __init__.py file: /home/runner/work/airflow/airflow/providers/common/messaging/src/airflow/providers/__init__.py\r\nAdded missing __init__.py file: /home/runner/work/airflow/airflow/providers/common/messaging/src/airflow/providers/common/__init__.py\r\nAdded missing path extension to __init__.py file /home/runner/work/airflow/airflow/providers/common/messaging/src/airflow/__init__.py\r\n```

> I dont understand what Sphinx is complaining about: `/opt/airflow/docs/apache-airflow-providers-common-messaging/_api/airflow/providers/common/messaging/providers/base_provider/index.rst: WARNING: document isn\

I think it would be great to somehow explain that "toctree" better :) 

Likely some documentation about base_provider should be added here https://github.com/apache/airflow/blob/f4fd6fd5ae45cd20924149aa0201d2da08a63112/providers/common/messaging/docs/providers.rst\r\n\r\n> You need to extend base_provider  ble ble ble....

> Likely some documentation about base_provider should be added here https://github.com/apache/airflow/blob/f4fd6fd5ae45cd20924149aa0201d2da08a63112/providers/common/messaging/docs/providers.rst\r\n> \r\n> > You need to extend base_provider  ble ble ble....\r\n\r\nIt is already there: https://github.com/apache/airflow/pull/46694/files#diff-f54feaaca8fd8ecfad946ef2cc5b389e082660ba53d305843bab44f5a014d582R36

> It is already there: https://github.com/apache/airflow/pull/46694/files#diff-f54feaaca8fd8ecfad946ef2cc5b389e082660ba53d305843bab44f5a014d582R36\r\n\r\nSo if the index to the "__init__.py" is not linked (and does not need to be linked) from anywhere - it should be excluded in "docs/conf.py" explicitly for provider package builds. 

And yes - I reverse engineered it having similar issues. Likely it should be done bettter, so we do not have to do it manually.

All green :) I also tested it manually and triggered few DAGs using `MessageQueueTrigger` and it works nicely. I really like the user experience, users do not have to worry about the actual implementation of the queue provider

NICE! 

Failing tests are unrelated (and likely fixed in main soon or already :).

Closing in favor of https://github.com/apache/airflow/pull/46691 that has been merged.

@ashb nope, it comes like this:\r\n```\r\n    {\r\n        "timestamp": "2025-02-12 13:54:16.024020",\r\n        "ti": "RuntimeTaskInstance(id=UUID(\

Failures are unrelated to the PR and a fix is available in #46700

An example: https://github.com/apache/airflow/pull/46613\r\n\r\nI want to add a working example in the feature newsfragment

I\

 > I\

> I\

I also like the idea from Wei but seems like its an overkill?

> does a significant newsfragments only cover "breaking" changes or is it OK to have a significant change that is not a "breaking change"?\r\n\r\nI intentionally called it "significant" and not "breaking" for a reason :) but that is a subtle nuance, so fair question. (edit: just to be very clear, yes it\

> adding only the first line to the release note. Rest of them are for ones that cares about the details.\r\n\r\nInteresting idea. Long term the only place these live is in the release notes themselves - the newsfragment files are deleted once its ends up in a release. So are you suggesting the additional lines are added as rst comments? Not visible in the rendered release notes, but visible if you want to look deeper? Or were you thinking those files lived on post release?

> I intentionally called it "significant" and not "breaking" for a reason :) but that is a subtle nuance, so fair question. (edit: just to be very clear, yes it\

Good ideas @Lee-W and @jedcunningham.\r\n\r\nFor the time being, I think I am gonna proceed with changing the newsfrag in my PR to a significant and close this one in that sense. But we should definitely follow up and reconsider the idea of adding rst comments. Sounds ok?

Closing this one in favour of the previous comment.

I need to re-examing logs to make sure they still get routed to the right place, and fix all the tests

No, all good. It will fail in #46613 because I do:\r\n```\r\nti.task.blahblah\r\n```\r\n\r\nAnd task is only defined on runtime TI

conflicts.

> conflicts.\r\n\r\nfixed :)

You need to resolve conflicts now

Hi @rawwar !\r\nCould you please review this PR again and resolve the remaining conversations? Thanks!

Hi @eladkal @potiuk !\r\nI believe we can go ahead and merge this PR

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Merging, need that to check if we need an additional fix or not. (we encounter an issue with the trigger form)

good start :)

@potiuk Do we need to `cleanup_python_generated_files()` as part of `breeze doctor`?

One small comment only. Possibly we should add "confirm" method call for every step? The way it is implemented now, It always performs all cleanup, but maybe it\

> One small comment only. Possibly we should add "confirm" method call for every step? The way it is implemented now, It always performs all cleanup, but maybe it\

I see the failures, will check them out shortly üòÆ\u200düí®

Awesome! I am just noticing this PR. Thanks for tagging me @kunaljubce 

> I see the failures, will check them out shortly üòÆ\u200düí®\r\n\r\nSeems like the failures are related to the `apache-kafka==2.8.1` issue, should be resolved now that PR #47204 is merged. Rebased this feature branch with `main` and rerunning the tests.

NICE!

Do these views replace the list task instances and dagruns page in Airflow 2 under browse menu that used fab?

Just noticed that the filter for dag_run should not have all the states, and the badge is truncated. (not related to this PR)\r\n![Screenshot 2025-02-12 at 11 33 40](https://github.com/user-attachments/assets/90ef68c9-c22b-4b8d-ad91-f74904823475)\r\n![Screenshot 2025-02-12 at 11 33 54](https://github.com/user-attachments/assets/21dae927-284b-4974-a6ef-df07d7b1163d)

@pierrejeambrun Thanks, looks like a typo on copy paste from my end regarding import statement in `src/pages/Runs/Runs.tsx` . Following is the fix and I can create a PR for the same.\r\n\r\nFix\r\n\r\n```js\r\n// src/pages/Dag/Runs/Runs.tsx\r\nimport { dagRunStateOptions as stateOptions } from "src/constants/stateOptions";\r\n```

> Do these views replace the list task instances and dagruns page in Airflow 2 under browse menu that used fab?\r\n\r\nExactly.\r\n\r\nAnd I just fixed the dag run state and Task Instance naming.

I see the first element in the tooltip always highlighted on open in Firefox 132 on Ubuntu 20.04 .\r\n\r\n![Screenshot from 2025-02-12 09-59-43](https://github.com/user-attachments/assets/aa3d5bcf-9f7f-45ac-8c86-e5037f80043f)\r\n

Had to turn off autoFocus from chakra. (Why would they have that `true` by default??)

Nice. Good one @amoghrajesh. Actually our compatibility tests detected nicely the issue - https://github.com/apache/airflow/pull/43243 added uuid6 as dependency of Airlfow3, and Openlineage started to use it in  https://github.com/apache/airflow/pull/45294, so adding dependency to openlineage is what is needed.

Thanks! Yeah the compat tests saved us here!

thanks @amoghrajesh for the fix üôÇ 

https://github.com/apache/airflow/pull/47464 should fix it

@potiuk This is only one suggestion of handling it. If you had something else in mind as part of of the providers restructure cleanup , we can close this PR.

interesting. If there are other problems with running system tests -  let me know please @vincbeck -> we will liely have a few more problems and I would like to make sure they are mainstream

> interesting. If there are other problems with running system tests - let me know please @vincbeck -> we will liely have a few more problems and I would like to make sure they are mainstream\r\n\r\nFor sure üëç 

conflicts and failing tests

Hi there!\r\nI have some questions regarding the changes here. I  was testing them and when deleting pod in deferrable mode I got this error:\r\n```\r\n[2025-02-18T09:59:22.793+0000] {pod.py:187} INFO - Checking pod \

I believe the approach I took in https://github.com/apache/airflow/pull/46608. that cleans-up a lot of common and duplicated provider code after we moved all providers is better.\r\n\r\nIt should fix the deprecations_ignore.yml problem. After removing the "old" way providers were added and basically removing the entire "providers" package we could do the deprecations_ignore differently -  namely instead of adding deprecations_ignore purely for the provider in question, we are adding all deprecations_ignore found in all providers, ALWAYS in pytest_plugin. \r\n\r\nThis way:\r\n\r\na) we can keep deprecations_ignore in corresponding providers\r\nb) no matter which provider is run, the ignores are added for all of them\r\nc) we do not have to copy around the code that reads the deprecations_ignore\r\n\r\nI have to still make the PR green - there are still few things failing there, and also add some more docs and templating/pre-commits to make sure that "integration" and "system" tests in provider have the right __init__.py files and few more things to remove, but this evening the tests in CI already look very promising -  I have most integration tests running and the deprecation warnings from google provider seem to be handled well - via the above mechanism.\r\n\r\ncc: @ashb \r\n\r\n> (Also wow that\

Thank you for clarification!\n\nThe main reason for this PR was to unblock https://github.com/apache/airflow/pull/46612 and fix scheduled CI pipelines. Now it is merged, so there is no reason to do it "hacky" way.\n\nI will close this one. \n\nMaybe it\

The PR is merged now - also see the discussion i started on the devlist https://lists.apache.org/thread/xm6th19xw2v1zo8k8kjomsbzz51njxgv -> I think, the description of deprecations_ignore behaviour is better in contributing docs - somewhere in https://github.com/apache/airflow/tree/main/contributing-docs/testing - especially that we have no longer "providers/tests" folder :D

Fails due to\r\n```\r\nproviders/amazon/tests/provider_tests/amazon/aws/hooks/test_emr.py:180: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nproviders/amazon/src/airflow/providers/amazon/aws/hooks/emr.py:136: in create_job_flow\r\n    response = self.get_conn().run_job_flow(**config)\r\n/usr/local/lib/python3.12/site-packages/botocore/client.py:569: in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n/usr/local/lib/python3.12/site-packages/botocore/client.py:1005: in _make_api_call\r\n    http, parsed_response = self._make_request(\r\n/usr/local/lib/python3.12/site-packages/botocore/client.py:1029: in _make_request\r\n    return self._endpoint.make_request(operation_model, request_dict)\r\n/usr/local/lib/python3.12/site-packages/botocore/endpoint.py:119: in make_request\r\n    return self._send_request(request_dict, operation_model)\r\n/usr/local/lib/python3.12/site-packages/botocore/endpoint.py:196: in _send_request\r\n    request = self.create_request(request_dict, operation_model)\r\n/usr/local/lib/python3.12/site-packages/botocore/endpoint.py:132: in create_request\r\n    self._event_emitter.emit(\r\n/usr/local/lib/python3.12/site-packages/botocore/hooks.py:412: in emit\r\n    return self._emitter.emit(aliased_event_name, **kwargs)\r\n/usr/local/lib/python3.12/site-packages/botocore/hooks.py:256: in emit\r\n    return self._emit(event_name, kwargs)\r\n/usr/local/lib/python3.12/site-packages/botocore/hooks.py:239: in _emit\r\n    response = handler(**kwargs)\r\n/usr/local/lib/python3.12/site-packages/botocore/signers.py:106: in handler\r\n    return self.sign(operation_name, request)\r\n/usr/local/lib/python3.12/site-packages/botocore/signers.py:198: in sign\r\n    auth.add_auth(request)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <botocore.auth.SigV4Auth object at 0x7f858a34a030>\r\nrequest = <botocore.awsrequest.AWSRequest object at 0x7f8569b580e0>\r\n\r\n    def add_auth(self, request):\r\n        if self.credentials is None:\r\n            raise NoCredentialsError()\r\n>       datetime_now = datetime.datetime.utcnow()\r\nE       DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\r\n```\r\n\r\nI believe this in unrelated and has a fix?

> I believe this in unrelated and has a fix?\r\n\r\nYes, this is also the same problem on main. LGTM!

@pankajkoti Would you mind reviewing this PR?

> @pankajkoti Would you mind reviewing this PR?\r\n\r\nyes happy to. Would be nice to review once consensus is reached based on @MikhailKulikov-db and your discussions on the feedback.

![image](https://github.com/user-attachments/assets/aaa5588c-3951-40fd-b7a8-565139f4f37f) I think GitHub is having a moment

Test failing for AF2.9 and some MYQL and SQLITE tests. I will look into these

I‚Äôm going to merge this now. The spelling issue is currently being discussion on the mailing list, and can be mass-replaced once we decide how to automatically detect them.

@pierrejeambrun I have refactored them to a common component `DurationChart` . Most of the part is same for task instance and dag run except the queued duration calculation which needs `queued_when` and `queued_at` . I am passing `kind` to denote the type and also using it in switch case to use the correct attribute for queued duration. I am new to JS/React refactoring. Please let me know if there is a better way. Another approach I thought of was making the component as a base class and each class `TaskInstanceDurationChart` and `DagRunDurationChart` inherits from the base class to implement queued logic duration calculation as a function for the respective class.

Thanks @bbovenzi and @pierrejeambrun .

Cool :)

Error unrelated to this PR

Nice

cc: @tirkarthi 

> Two minor issues in test set up, otherwise this should be ready.\r\n\r\n@uranusjr these should be fixed now.

Should something like this pass: \r\n```\r\n    @pytest.mark.usefixtures("clear_all_logger_handlers")\r\n    def test_extra_operator_links_logs_error_for_non_registered_extra_links(self):\r\n        """\r\n        Assert OperatorLinks not registered via Plugins and if it is not an inbuilt Operator Link,\r\n        it can still deserialize the DAG (does not error) but just logs an error.\r\n\r\n        We test NOT using caplog as this is flaky, we check that the task after deserialize\r\n        is missing the extra links.\r\n        """\r\n\r\n        class TaskStateLink(BaseOperatorLink):\r\n            """OperatorLink not registered via Plugins nor a built-in OperatorLink"""\r\n\r\n            name = "My Link"\r\n\r\n            def get_link(self, operator, *, ti_key):\r\n                return "https://www.google.com"\r\n\r\n        class MyOperator(BaseOperator):\r\n            """Just a EmptyOperator using above defined Extra Operator Link"""\r\n\r\n            operator_extra_links = [TaskStateLink()]\r\n\r\n            def execute(self, context: Context):\r\n                pass\r\n\r\n        with DAG(dag_id="simple_dag", schedule=None, start_date=datetime(2019, 8, 1)) as dag:\r\n            MyOperator(task_id="blah")\r\n\r\n        serialized_dag = SerializedDAG.to_dict(dag)\r\n\r\n        sdag = SerializedDAG.from_dict(serialized_dag)\r\n        assert sdag.task_dict["blah"].operator_extra_links == []\r\n```\r\n\r\nI do not really understand what:\r\n```\r\n        Assert OperatorLinks not registered via Plugins and if it is not an inbuilt Operator Link,\r\n        it can still deserialize the DAG (does not error) but just logs an error.\r\n\r\n        We test NOT using caplog as this is flaky, we check that the task after deserialize\r\n        is missing the extra links.\r\n```\r\n\r\nmeans.

Update:\r\n\r\n- Migrated over all the functions using plugins manager from sdk/abstractoperator over to the serialisation module.\\\r\n- Handled all nits\r\n- Fixed all tests (hopefully)\r\n\r\nWhats left:\r\n- Adding examples to the newsfragment (for which I need to extend it to support over 1 line)\r\n- Test an e2e scenario again

A lot of core tests are failing, working on fixing them

Last few tests to go, almost there!

Yeah finally!! ü•á\r\n\r\n@ashb could you take a look again at this one?

Added some additional information about the design et al in the last self review.

If I understood it correctly changes from here are required first:\r\n\r\nhttps://github.com/apache/airflow/pull/46608\r\n\r\nTo address other providers deprecation problems in full-tests 

I cannot understand the exact reason for the failure of the check, my initial thought was about `deprecations_ignore.yml` but it seems unrelated to the `AriflowProviderDeprecationsWarning` in `google` tests.

I can only say that the problems look the same as they were for\r\n- https://github.com/apache/airflow/pull/46600\r\n- https://github.com/apache/airflow/pull/46599

After investigation, I am sure that the problem with google deprecation warning failures is outside of this PR scope, should be fine after https://github.com/apache/airflow/pull/46638 or another PR that would fix the problem 

There are still few errors due to cross-provider dependencies with fab provider / azure paths not fixed - but generally it should be ready for review (I will fix the errors tomorrow).

Would it be possible to split this PR? This is really a "beast" to review.

> Would it be possible to split this PR? This is really a "beast" to review.\r\n\r\nUnfortunately ... I am afraid it\

For those who will be reviewing this one - I recommend `git diff` -> UI of Github is very, very slow with that huge of a change.

Rebased and applied the nits. 

BTW.I plan to have a separate discussion on the "final" layout of the provider test code so I plan to merge it rather quickly and then write up a discussion thread where I would want people to chime in and make some final decisions. I am afraid if I won\

Merged !

Just a quick update for @amoghrajesh . This PR has resolved my previous zip file comment in your PR.

I was already working on those as part of a bigger cleanup :) but we can have it merged first :)

> I was already working on those :) but we can have it merged first :)\r\n\r\nLet me know... how far are you? Am I adding extra work to re-base? Are you close-by? :-D 

> > > I was already working on those :) but we can have it merged first :)\r\n> > \r\n> > \r\n> > Let me know... how far are you? Am I adding extra work to re-base? Are you close-by? :-D\r\n> \r\n> Rebaseing is fine - I will rebase mine when ready..\r\n> \r\n> How far I am ? Depends how much wine with my friend we\

> Be careful! Don\

![image](https://github.com/user-attachments/assets/369a26ce-a27d-490a-8f39-4edd47491b2f)\r\n

Alcohol-Free merged.

@pierrejeambrun, should I consider including a path_prefix or just using the base_url as we do in legacy?\r\n\r\nI\

Thanks @shubhamraj-git for pushing some changes!\r\nI think now as the component is extracted this leaves the option to move back the JavaScript into the component w/o linter errors. Let me reverse this...

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/46607"><img src="https://img.shields.io/badge/PR-46607-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

Nice!

"Hi, @vargacypher is there anything I could help you with since I\

Nice. I love how this focuses on making all contributors more productive. It also helps even seasoned maintainers on how to help those who have certain problems and have issues with some edge cases! \r\n\r\nThanks a lot for that!

> Seems the upper margin and the lower margin of each elements are not same, a bit weird, can we have it in sync?\r\n\r\nGood catch. I just checked the overall height and the label... but there seems to be a bit of un-aligned margin onthe filed in the right column... will update...

> > Seems the upper margin and the lower margin of each elements are not same, a bit weird, can we have it in sync?\r\n> \r\n> Good catch. I just checked the overall height and the label... but there seems to be a bit of un-aligned margin onthe filed in the right column... will update...\r\n\r\nOkay, fixed the layout glitch. I first thought it is a margin problem and then needed to hunt-down the root a bit. The problem was a condition that the helper text was _not_ conditionally be rendered but always also as an empty element. That space was causing an un-alignment.\r\nTherefore I needed to adjust the param spec interface as the description is provided by API as `null` and due to eslint all was assumed to be `undefined`. But it is not the same. Needed some `eslint-ignore` to correctly use and itnerpret a `null` value from description as this is always passed with null even if empty.\r\nSide effect is that the condition for markdown actually was broken, now the markdown is happyly and correctly displayed.

cc: @o-nikolas -> could not add fixup in your PR so created a new one (but you are author still) :) 

Yeah. I took it from your PR initially and added some fixes :) 

> Yeah. I took it from your PR initially and added some fixes :)\r\n\r\nSad that it merged as your commit, because I spent a lot of time on it :sweat_smile:,  but thanks again for taking it across the line!

:tada: 

> Sad that it merged as your commit, because I spent a lot of time on it üòÖ, but thanks again for taking it across the line!\r\n\r\nWell. Not really. I am very careful about those of not to remove the original author\r\n\r\nWe are both co-authors of that merged commit:\r\n\r\n<img width="470" alt="Screenshot 2025-02-11 at 00 00 48" src="https://github.com/user-attachments/assets/afe8da74-d2fe-4fbd-9516-3fb32fc508d3" />\r\n

Thanks Shahar, I just squashed commits into 1 and will address other comments. Aiming to get this in beta3 ü§û \r\n\r\n>- Right now the ShortCircuitOperator should be working ok - there might be issues with ignore_downstream_trigger_rules , or at least the tests that check it.\r\n> - There is an issue with the branch operator because it tries to access the DB directly (see my recent comment on the PR). It should be hopefully relatively easy to solve.

**Known Issue**:\r\n\r\nBranching in Mapped Task Groups does not work. So the issue mentioned in https://github.com/apache/airflow/issues/30883 is back. We will handle that in a separate PR.

Added one more known issue to the PR description and added some screenshots

We should be finally good to go on this!!\n\nWaiting for the CI ü§û 

CI passed: https://github.com/apache/airflow/actions/runs/13866617491/job/38807327529?pr=46584\r\n\r\nThe static checks with providers was fixed in the last commit

@shahar1 and I will fix any teething issues if something fails -- but we wanted to get it in since it is already too big and has been out for a while.

Nice! thank you!

I also added the `with_conn` wrapper to avoid repeating `with self.conn() as conn`.\r\n\r\nHi @dabla , I think this might be related to your recent PRs. Please feel free to take a look.

@Dawnpool really good catch, as indeed this will speed up the store_directory and retrieve_directory functions as only one connection will be created, but personally, and that\

Nice, but can you please add some unit tests testing the behaviour (identity of the client).

> Nice, but can you please add some unit tests testing the behaviour (identity of the client).\r\n\r\nGood point @potiuk.  You could indeed add some assertions within the existing tests to make sure connection gets opened/closed only once during the whole operation.

@potiuk @dabla \r\nThanks for your opinions. I added test code for it. Please check.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

insane PR mate

@potiuk You might want to take a look at this one

> @potiuk You might want to take a look at this one\r\n\r\nOh absolutely .. I knew we will have to review and update those (and possibly fix some things that we missed) - it was just sometimes a bit too **complex** to get it all right with new and old provider structures.\r\n\r\nI\

Hey @jedcunninghamin, should we pass the bundle names as a list of strings?

Nice. Some mypy checks but it looks good :)

Quick fix @kacpermuda :)

Sure

No need to be a single point of failure üôÇ \r\nPR looks good to me.

> LGTM but I think it would be worth adding a unit test to cover the flow?\r\n\r\nTest case added thanks.

cc: @shubhamraj-git 

obsoleted by https://github.com/apache/airflow/pull/46959

Followup:\r\n\r\nShould we add a `pre-commit` check to ensure the default configuration doesn‚Äôt raise issues with `airflow config lint`?\r\ncc @Lee-W 

> Followup:\r\n> \r\n> Should we add a `pre-commit` check to ensure the default configuration doesn‚Äôt raise issues with `airflow config lint`? cc @Lee-W\r\n\r\nsounds like a great idea!

Thanks for addressing this. Will leave this open for 1 or 2 days before merging. 

ah... forget to merge it . will do once CI is green again

> Wohoo! Get rid of caplog!\r\n\r\nIndeed :D ... ~Caplog~ 

One down :D Thanks! I have started listing all caplog usage and will create a high-level plan this weekend to see how we can easily approach it. I will create an engagement again on the approach. The first layer is to include a selective check to make maintainers aware, which created the issue already #46445

> Nice and concise now that all provider code is under one directory :)\r\n\r\nVery much so.. One of the benefits of this move :) 

Nice!

Nice one @insomnes !

Seems `apache.impala` provider was a bit orphaned when it comes to moving it :)

OK. there is a small caveat with "latest botocore" - Let me add a fixup - I know how to fix it\r\n

When I try to reproduce this error locally with `breeze testing core-tests --run-in-parallel`, it returns errors but seemingly unrelated to `yandex` provider changes - \r\n\r\n```shell\r\n=========================== short test summary info ============================\r\nFAILED tests/always/test_example_dags.py::test_should_be_importable[providers/google/tests/system/google/cloud/ml_engine/example_mlengine.py] - AssertionError: import_errors={\

pushed likely fix

Thanks @potiuk. For my understanding, can you please explain a little about the botocore issue?

> Thanks @potiuk. For my understanding, can you please explain a little about the botocore issue?\r\n\r\nYeah. The thing is that with botocore tests we are upgrading to latest (i.e. newest) possible botocore - basically by removing aiobotocore that holds it back. The `aiobotocore` is non-aws, 3rd-party async botocore extension and the problem with it is that it lags behind the main botocore - which is released almost daily\r\n\r\nSee: https://pypi.org/project/botocore/#history\r\nvs. https://pypi.org/project/aiobotocore/#history\r\n\r\nbotocore - released today, aiobotocore released 22 Jan and it has `Requires-Dist: botocore<1.36.4,>=1.36.0` -> 1.36.0 of botocore was released on 15th of Jan.\r\n\r\nSo those tests are removing aiobotocore and try to run all the tests with latest botocore. And yandex also uses a aiobotocore I think.\r\n\r\nI see my fix did not work, so I have to fix it differently

Ok. I pushed simplified version. I think in this case we do not have to run `pip check` - because this is what caused the failures. When we simply upgrade botocore to latest version it will cause conflicting dependencies with other package(s) - - but this should be no big issue , as long as all the tests pass.

:crossed_fingers: 

> Are you sure you want to delete all the RST when moving the provider?\r\n\r\nAAAAAH... Merged tooo fast... good eye Jens.

Ah no --... yandex docs were copied previously (accidentally) as part of another provider (TERADATA) :) . All good :) 

> Ah no --... yandex docs were copied previously (accidentally) as part of another provider (TERADATA) :) . All good :)\r\n\r\nDid not know but good you confirmed.... and no Aaaah :-D

By ‚Äúsoon‚Äù it‚Äôs after 3.0 right? I feel we should still keep the old view work the same as the new to not confuse users too much.

COOOOL

@ferruzzi  Thanks for the feedback. I tested it:\r\n![image](https://github.com/user-attachments/assets/c462d18a-331b-4a15-b8c6-7bc592104bc3)\r\n\r\nHere you see that after  switching to my changes the metric includes all DAGs and not only for one.\r\nI will also try to check the counter issue because I ran into some strange behavior. \r\n\r\nDo you have a link to the issue thread were the issues are described which are mentioned by you?

> Do you have a link to the issue thread were the issues are described which are mentioned by you?\r\n\r\nYou found the one that I was thinking about, bu if you [search open Issues for "otel"](https://github.com/apache/airflow/issues?q=is%3Aissue%20state%3Aopen%20otel) there are around 20 or so, if you are interested in helping.    `sla_miss` is removed in 3.0 so that one might not be worth fixing now, and the one about duplicate config options needs to be discussed, but there are some that might be interesting if you are inclined.

It‚Äôs not mentioned in the linked issue, but in other issues there‚Äôs a rule saying if logical date is not available, data interval should not be either. It‚Äôs probably not a bad idea to do them both in one PR.

suppressed by https://github.com/apache/airflow/pull/46460/

A number of tests/checks to fix though :) 

Hello @potiuk @hussein-awala  Could you have another round of review?

 I am afraid the tests are failing. Rebased it to check but look like problems to be solved.

Thanks @bbovenzi , I suppose a very similar overview page will be present for each task too with recent failed task instances, task duration chart etc, as per the design.\r\n\r\nhttp://localhost:8000/webapp/dags/tutorial_taskflow_api/tasks/load

> Thanks @bbovenzi , I suppose a very similar overview page will be present for each task too with recent failed task instances, task duration chart etc, as per the design.\r\n> \r\n> http://localhost:8000/webapp/dags/tutorial_taskflow_api/tasks/load\r\n\r\nYes, eventually we want to do a lot more with these dashboard views on the homepage and on a Dag or Task.\r\n\r\nWhat were you trying to link here?

The link was to a sample task details page which now only has task instances and events tab. My understanding from the designs is that there will be a overview page for a task similar to dag page. The task duration chart was useful with median plot and it will be good to have it somewhere in Airflow 3 too.

Yeah. Some problems are expected - especially for such huge providers as amazon. This is one of the reasons we did no move all providers at once, because rather than havin 10 people solving those things individually, we would have to have 1 person solving all 100 providers issues at once, we are basically crowd-sourcing problem solving.\r\n\r\nBut I see my role here as help to guide people (and I did that in other providers) how to solve those difficult issues. That\

I think you need to rebase and resolve conflicts @o-nikolas - in order to trigger tests. \r\n\r\nBTW. The mypy `satisfying` (I prefer that name ;) change looks good.

That looks way better now :) -> only small/easy things seems to be remaining :)

Closing in favour of https://github.com/apache/airflow/pull/46590 -> that was started from this one 

Still, 3 tests are failing :( Also, some parts missed from my local mypy. Checking now

Fixed the mypy errors and one more test. Two to go if my sleep will allow me. Otherwise, I will ping the channel with latest state :)

@bugraoz93 merged my PR after fixing the tests. Can you close this one?\n\nThanks for help!!

You should add unit test to that change

since provider has been moved. I will restart from a proper branch

Okay got this one through!!

NICE! COOL @amoghrajesh !

Hi @pierrejeambrun,  \r\n\r\nDo I need to configure `DagBundlesManager` explicitly to set `("dag_processor", "dag_bundle_config_list")`?  \r\nI\

Yes bundles need to be configured.\r\n\r\nYou can take a look at the `make_dag_with_multiple_versions` which does that. (You need a configuration + git connection for it to work in your test)

> Looks good, some tests need fixing.\r\n\r\nThe CI failure is due to `sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked`. We may need to retrigger the SQLite test ~

> The CI failure is due to sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked. We may need to retrigger the SQLite test.\r\n\r\nOk, I just restarted the failed step.

This should fix your CI issue https://github.com/apache/airflow/pull/47016

Rebased, conflict solved.

Thanks for the review @potiuk !! The build failed with:\r\n\r\n```\r\nThe image /mnt/ci-image-save-linux_amd64-3.9.tar does not exist.\r\n```\r\n\r\nduring the "Prepare breeze & CI Image: 3.9" step of [Lowest direct dependency providers tests / All-core:LowestDeps-Postgres:13:3.9](https://github.com/apache/airflow/actions/runs/13162285492/job/36745165887#logs).\r\n\r\nI don\

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Can we have some frontend magic to help user fill the fields since they are related to each other most of the time? Say\r\n\r\n* If data interval fields are empty, fill them with logical date if it‚Äôs filled\r\n* If logical date field is empty, fill it with data interval end or start if available (in this order)

> Can we have some frontend magic to help user fill the fields since they are related to each other most of the time? Say\r\n\r\n> If data interval fields are empty, fill them with logical date if it‚Äôs filled\r\n> If logical date field is empty, fill it with data interval end or start if available (in this order)\r\n\r\nIt appears that we want to completely get rid of the `dataIntervalStart` and `dataIntervalEnd` in the trigger form. Updated accordingly in my last commit.\r\n\r\ncc: @cmarteepants 

Thats a very good idea!

Merging. The "generate constraints" alredy used the new retry and mypy checks passed.

Created [PR ](https://github.com/apache/airflow/issues/46045) to move WinRM provider to new structure

Need some static fixes

Yeah I assme once you rebase and run `pre-commit run -a` then all should be fixed.

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/46468"><img src="https://img.shields.io/badge/PR-46468-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

Looks good imo, but tests are failing now

I think: the problem is on the CI side with k8s cluster tests and not in the code, will wait a bit and retry by rebasing the branch. My last attempt failed to set the environment, and previous failures were about failing jobs. 

Oh, now there is conflict after the provider move to a new structure.  Will  check it a bit later today

Yeah... Sorry :) . It was finaly green, so I merged it. But I think in this case git will properly fix stuff when you rebase -  automatically 

still conflicts to solve :(

Holy moly I am so sorry for this mess, and I am really sorry for the ping for the whole repo I will close this one and apply my changes to the new structure, that would be easier. I am sorry for bothering you all 

No worries :)

blocked by https://github.com/apache/airflow/pull/46295 and https://github.com/apache/airflow/pull/46398

The dag and task can be correctly triggered and finished. However, the UI can not find the correct dag. Not sure whether it\

> the UI can not find the correct dag\r\n\r\nCan you elaborate? I don‚Äôt have enough context to notice what‚Äôs wrong in the screenshot.

> > the UI can not find the correct dag\r\n> \r\n> Can you elaborate? I don‚Äôt have enough context to notice what‚Äôs wrong in the screenshot.\r\n\r\nSure, the second dag `asset_producer_2` has some success tasks but no dags on the UI.

@uranusjr I just addressed the comments and created related issues. waiting for the CI to run

@uranusjr The CI is green now üôå

Looks good to me aside from the one question above

But right now we have zip files\r\n`tests/dags/test_zip.zip` I think par to of the task is to drop what we have now and replace it with creation of the zip on the fly.

> But right now we have zip files `tests/dags/test_zip.zip` I think par to of the task is to drop what we have now and replace it with creation of the zip on the fly.\r\n\r\nYes. But I believe we do not need\xa0pre-commit for that - .gitignore should be enough. And even if we want to have .zip check the precommit should be just `false` command  if `.*.zip` files match - there is no need to have python for that. 

_and_ if we want to forbid zip files the error message or at least the check in pre-commit should say why.

Closing this as https://github.com/apache/airflow/pull/46436 has the same changes

I don‚Äôt even think multiple cron expressions can express a bi-weekly schedule though? To be able to identify bi-weekly, you‚Äôll need some way to count weeks, which cron simply does not allow you to.\r\n\r\nThe linked issue does provide a useful example though:\r\n\r\n> every 10 min between 16:30 to 18:10\r\n\r\nI‚Äôll create some documentation using that instead.

If this one goes through, will raise an issue against retry http.

Closing and reopening to run all tests

Created a bug against retryhttp: https://github.com/austind/retryhttp/issues/20

nice!

Oh, just saw this late - THANKS for fixing!

> Dont forget also to forward port the changelog and commit.rst to main branch\r\n\r\nYep. Thanks for reminding :)

~~@hardeybisey  is already working on this - https://github.com/apache/airflow/issues/46045#issuecomment-2626683707~~\r\n\r\nJust noticed the comment - https://github.com/apache/airflow/pull/46132#issuecomment-2634622838

One of the docs is failing due to a path showing old system tests. `apache-airflow/tutorial/taskflow.rst`

@jason810496 , I noticed that adding\r\n\r\n```\r\nMISSING_EXAMPLES_FOR_CLASSES = {\r\n        "src.airflow.providers.cncf.kubernetes.operators.job.KubernetesDeleteJobOperator"\r\n    }\r\n```\r\n\r\nto TestCncfProviderProjectStructure and that\

Ok, so, the issue is that PROVIDER is set to `cncf`  in https://github.com/rawwar/airflow/blob/4821da5cc3fe58e1f64d49967b52026adfa89dbe/tests/always/test_project_structure.py#L288\r\n\r\nbecause of that, its unable to find the example dags

> Ok, so, the issue is that PROVIDER is set to `cncf` in https://github.com/rawwar/airflow/blob/4821da5cc3fe58e1f64d49967b52026adfa89dbe/tests/always/test_project_structure.py#L288\r\n> \r\n> because of that, its unable to find the example dags\r\n\r\nI just replaced the `PROVIDER = "cncf"` with `PROVIDER = "cncf/kubernetes"` and ran the test, but it still fail.\r\nhttps://github.com/rawwar/airflow/blob/4821da5cc3fe58e1f64d49967b52026adfa89dbe/tests/always/test_project_structure.py#L571\r\n\r\nUPDATED:\r\nThe test fail due to:\r\n```\r\n{\

So, the actual issue for the example test is at https://github.com/rawwar/airflow/blob/4821da5cc3fe58e1f64d49967b52026adfa89dbe/tests/always/test_project_structure.py#L272\r\n\r\nHere we just split and consider from 2nd index. since, its `cncf.kubernetes.src`  in this case, its failing. This needs to be fixed. Maybe later or in this PR\r\n\r\n\r\nOn to K8s failing tests

@jason810496 , raised a separate pr - https://github.com/apache/airflow/pull/46454 . that might handle it. Locally, tests passed . looking into k8s tests failure

it looks like https://github.com/apache/airflow/pull/46454 also fixes K8s tests. I just ran them locally and they passed

> it looks like https://github.com/apache/airflow/pull/46454 also fixes K8s tests. I just ran them locally and they passed\n\nIf then, I will revert the last commit and wait for #46454 be merged. Big thanks to @rawwar !

So, the only other fix required here for the example tests is to make providers = "cncf/kubernetes". \r\n\r\nBut, k8s tests are still failing. I just raised a [dummy pr](https://github.com/apache/airflow/actions/runs/13157601342/job/36720083829?pr=46461) to see the test results and they failed. Locally, They are successful. 

@jason810496 -> the k8s tests here are failing because you need to modify `scripts/ci/kubernetes/k8s_requirements.txt` -> You need to add this:\r\n\r\n```\r\n-e ./providers/cncf/kubernetes\r\n```\r\n\r\n

The import error has finally been resolved! Big thanks to @potiuk! \r\n\r\nThe only remaining CI failures:  \r\n- **Additional PROD image tests**  \r\n  - **Docker Compose quick start with PROD image**  \r\n    - `upstream_failed` in `docker_tests/test_docker_compose_quick_start.py::test_trigger_dag_and_wait_for_result`  \r\n- **K8s tests**  \r\n  - `upstream_failed` or `failed` for both `CeleryExecutor` and `LocalExecutor`.  \r\n\r\nNot sure if rerunning the tests would resolve these errors. \r\n\r\n

> Not sure if rerunning the tests would resolve these errors\r\n\r\nThose are tests failing in main now - due to some `task_sdk`  errors and there are other PRs that should already fix them https://github.com/apache/airflow/pull/46492 - merged after your change, so rebasing should fix them. Let me try it.

Ah... You need to rebase :( there are some changes merged in the meantime. Sorry :( \r\n\r\nBut we are REALLY close.

> But we are REALLY close.\r\n\r\nHope this one will be the last one to be green  üöÄ üü¢

Ah.. For some reason you have all teh `_api` files generated added to that PR @jason810496 -> you should remove them!

> Ah.. For some reason you have all teh `_api` files generated added to that PR @jason810496 -> you should remove them!\r\n\r\nDo you mean all the docs prefix with `_api` should be removed ?

Yep. I have not seen you are around - just pushed that fixup.\r\n\r\nYeah. The `_api` docs are generated automatically when docs are being built and they should be `.gitignored` in general - but they were added in this PR.\r\n

Wooho.

> Wooho.\r\n\r\nGreatly appreciate it, thanks @potiuk! üôåüéâ\r\n

Random failure, recorded in https://github.com/apache/airflow/issues/46432  - merging.

Great!

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

@alexott  ?

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Old signature:\r\n\r\n```python\r\nclass MyOperatorLink(BaseOperatorLink):\r\n    def get_link(self, operator, dttm):\r\n        ...\r\n```\r\n\r\nNew signature:\r\n\r\n```python\r\nclass MyOperatorLink(BaseOperatorLink):\r\n    def get_link(self, operator, *, ti_key):\r\n        ...\r\n```\r\n\r\nNote that in the old signature, `dttm` is used by Airflow as a _positional_ argument, so the argument name in the function declaration may not be `dttm`. In the new signature, however, the argument name must be `ti_key` (although it may be non-keyword-only; `self, operator, ti_key` should be considered new signature). So the deprecated signature detection rule should be **exactly three positional arguments, the third one not named `ti_key`**.\r\n\r\nThe migration tool should search files for subclasses of `BaseOperatorLink`, and check their `get_link` declaration.\r\n\r\nIdeally, the tool should check all entries defined in the `operator_extra_links` list of a plugin, even if it does not subclass `BaseOperatorLink`, because Airflow does not actually require you to subclass, as long as the signature match at runtime. But this can be difficult to do, so I think it‚Äôs OK to ignore this. We always subclass `BaseOperatorLink` in documentation, and the non-subclass implementation works only by accident.\r\n\r\nRelevant documentation on operator extra link: https://airflow.apache.org/docs/apache-airflow/stable/howto/define-extra-link.html

Delaying this a bit per discussion with @ashb. The task SDK work may need to change this.

Got a go-ahead. We can progress now.

I think it is not good to reduce the grace time to fail in case of HTTP 500. Most often K8s infra recovery or a DB restart takes longer than 3 minutes.\r\nFrom the productive use of Edge Worker we can say that the most stable situation we have is by the settingss we made currently as default. I fear reducing this will lower the quality of service.\r\n\r\nI propose to handle functional problems in a different error code to skip retry.\r\n\r\nIf you reduce it as here in the PR, please make it configurable at least such that we can increase it back to 10 attempts, else I fear it will lower the stability when moving to Task SDK.

For this Example: Please do not raise 500 if the XCom is not valid JSON.\r\n400 Bad Request would be correct in this case.

> I think it is not good to reduce the grace time to fail in case of HTTP 500. Most often K8s infra recovery or a DB restart takes longer than 3 minutes. From the productive use of Edge Worker we can say that the most stable situation we have is by the settingss we made currently as default. I fear reducing this will lower the quality of service.\r\n> \r\n> I propose to handle functional problems in a different error code to skip retry.\r\n> \r\n> If you reduce it as here in the PR, please make it configurable at least such that we can increase it back to 10 attempts, else I fear it will lower the stability when moving to Task SDK.\r\n\r\nThanks for your reply, @jscheffl. Was hoping to get review from you.\r\n\r\nWe do have an option to configure the number of retries to 10 or any chosen number via the env variable. The issue is that the conf is not yet ported over to the sdk. But the intention is that it can be overriden.\r\n\r\n

> I propose to handle functional problems in a different error code to skip retry.\r\n\r\nCan do. I too think 500 is not good for xcom invalid json case. Let me see what I can do.\r\n\r\nI think 400 is more ideal.

@jscheffl I gave it some thought and this is what I think:\r\n\r\nThe part of code that returns 500:\r\n```\r\n    result = query.with_entities(BaseXCom.value).first()\r\n\r\n    if result is None:\r\n        raise HTTPException(\r\n            status_code=status.HTTP_404_NOT_FOUND,\r\n            detail={\r\n                "reason": "not_found",\r\n                "message": f"XCom with key \

> Here, `result` is also something that comes from the server. So, the issue is actually is at the server end and not the client. 400 would be ideal if there\

@jscheffl thanks for that. I will do one thing, I will let this PR only handle the logging level improvements. And for the part of moving that xcom API to 4xx, and handling that differenty, will do in a follow up, soon.

@jscheffl i pushed a fix here, could you take a look?

> @jscheffl i pushed a fix here, could you take a look?\r\n\r\nI _think_ this would work.. but would it not be cleaner and rather following the purpose to add a custom `before_sleep` methos as described in https://tenacity.readthedocs.io/en/latest/api.html#before-sleep-functions ?\r\n\r\nSeel also https://github.com/jd/tenacity/blob/main/tenacity/before_sleep.py\r\n\r\nUPDATE: The tutorial in https://www.dataleadsfuture.com/conquer-retries-in-python-using-tenacity-an-end-to-end-tutorial/ and the last chapter also seems to be making what you want to achieve?

Indeed. Looks good. The fix to main is coming https://github.com/apache/airflow/pull/46492

Coo, super seeded by #46428

Ah have not seen it :) .But yeh - similar concepts :)

I am doing changes in https://github.com/apache/airflow/pull/46616 as logic has changed now.

@jscheffl it will be consumed via UI, to search like we use to do for other pages.

Just random failure - merging

Anyone? @ashb @Lee-W ?

> Almost good, just one question on removed run_id logic.\r\n\r\nReplied [here](https://github.com/apache/airflow/pull/46390#discussion_r1948694375) 

@uranusjr I was testing this and found that the DAG run is being inserted into the `dagrun` table with a None logical date, but it remains in the queued state. 

> @uranusjr I was testing this and found that the DAG run is being inserted into the `dagrun` table with a None logical date, but it remains in the queued state.\r\n\r\nThis would be fixed bt @Lee-W  in https://github.com/apache/airflow/pull/46460

> > @uranusjr I was testing this and found that the DAG run is being inserted into the `dagrun` table with a None logical date, but it remains in the queued state.\r\n> \r\n> This would be fixed bt @wei in #46460\r\n\r\nah... my ID is `Lee-W` 

cc: @ashb 

Some docs issues stil :) 

I pushed a small doc fixup to fix remaining example references.

### Backport successfully created: providers-fab/v1-5\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>providers-fab/v1-5</td>\n                    <td><a href="https://github.com/apache/airflow/pull/46392"><img src="https://img.shields.io/badge/PR-46392-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

Static checks are failing https://github.com/apache/airflow/actions/runs/13154423653/job/36708789726?pr=46380#step:6:218

> Static checks are failing https://github.com/apache/airflow/actions/runs/13154423653/job/36708789726?pr=46380#step:6:218\r\n\r\nshould be fixed now

After looking up issues, PRs, tests, and code I am pretty sure this file was missed. \r\n\r\nShould I add `significant.rst` about it (delete is backward incompatible)?

I am not sure if we can remove it now.\r\nShould it wait for next major release? cc @romsharon98 @jedcunningham WDYT?

@romsharon98 @jedcunningham UP with the Elad\

@eladkal / @Lee-W would this require a newsfragment?\r\n\r\nSecretsMasker was always a public interface and some users would have some custom operators would probably have used it for their redaction reasons / anything or that sort. That would mean that they might have to rewrite these operators atleast for the imports. 

> @eladkal / @Lee-W would this require a newsfragment?\r\n> \r\n> SecretsMasker was always a public interface and some users would have some custom operators would probably have used it for their redaction reasons / anything or that sort. That would mean that they might have to rewrite these operators atleast for the imports.\r\n\r\nYes, it would be categorized as `Behavior change` based on previous discussion

@Lee-W I added a newsfragment for this, does it look OK only with respect to that?

Thanks for the reviews, merging it

> What about updating the documentation? Like in `pod_override` [here](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/kubernetes_executor.html#pod-override).\r\n\r\nThe documentation is actually quite good there! I did add a mention related to the non default `pod_template_file` being used to render the K8s Pod Spec though. Thanks for the suggestion!

I applied pre-commit

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Sorry about that, was unblocking some builds

Actually - not sure if that is a good idea - those df are cluttering the output quite a bit.

> Can you also add for PythonOperator?\r\n\r\nIt seems PythonOperator is not changed in #42252?

> > Can you also add for PythonOperator?\r\n> \r\n> It seems PythonOperator is not changed in #42252?\r\n\r\nAh right, read the file name again, needs to be added, just not in this PR newsfragment

You need to rebase after moving provider to new structure.

Linked issue was resolved by #46561. IMHO this PR can be closed.

Mhm, not sure but smells like image constraint generation failure is un-related to this PR... at least should be...

Docs build problems seem to be un-related, merging. Dcos will need another fix

I think it is valueable to run Draft PRs also through CI. Then as a contributor you can have early feedback. Before opening a PR up toreviewers and generating notifications. What else should a WIP/Draft PR be for?\r\nOr do we need to fear that we have too high CI workload and need to reduce infra capacity? Then still a reduced run would be of value in my view.

You can always add `[skip ci]` in your PR title or message if you want to skip CI in draft PR https://github.blog/changelog/2021-02-08-github-actions-skip-pull-request-and-push-workflows-with-skip-ci/ 

But you also need to run pre-commits :)

It seems/smells like this PR broke main. Docs build fails now in https://github.com/apache/airflow/actions/runs/13109792195/job/36571965268\r\n\r\nCan this be fixed short term or shall we revert this PR and re-fix it?

Hi, thanks for reverting. Let me check what happened later.

@nailo2c can you resubmit the PR and we will look into what was the issue?

Ah cool. Will you raise PR for the other peoviders listed in the issue that were not yet handled?

Hey @bugraoz93!\r\n\r\nJust merged the fix for it: https://github.com/apache/airflow/pull/46350

Closing this in favor of #46350.

cool :)

Please do not create noise in Airflow by creating educational PRs. You can work on your private repo.\r\n\r\nCan you please explain - did someone tell you in some instructions how to create the PRs and forgot to mention that you shoudl not create an issue in Airflow repo? We received similar PR from another person just recently but it did not happen before. Can you please explain where the idea of creating PR in airflow repo came from? 

Hi Jarek,\r\n\r\nApologies for the confusion. I mistakenly created a PR in the Apache\r\nAirflow repository while learning about Airflow and GitHub integration.\r\n\r\nI will make sure to work in my own private repository and avoid making\r\nunnecessary PRs in the official repo.\r\n\r\nThanks for letting me know!\r\n\r\nRegards,\r\n\r\nDinesh\r\n\r\n\r\nOn Sat, 1 Feb 2025 at 20:18, Jarek Potiuk ***@***.***> wrote:\r\n\r\n> Please do not create noise in Airflow by creating educational PRs. You can\r\n> work on your private repo.\r\n>\r\n> Can you please explain - did someone tell you in some instructions how to\r\n> create the PRs and forgot to mention that you shoudl not create an issue in\r\n> Airflow repo? We received similar PR from another person just recently but\r\n> it did not happen before. Can you please explain where the idea of creating\r\n> PR in airflow repo came from?\r\n>\r\n> ‚Äî\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/pull/46349#issuecomment-2628981301>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BIE2KJZZURESW26GJLBBW6T2NTNFDAVCNFSM6AAAAABWJLHUYWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMMRYHE4DCMZQGE>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n

Still needs some fixing up, throws a bunch of errors on sending the requests. Pushing now to get some feedback on the UI changes.

<img width="1470" alt="Screenshot 2025-02-08 at 2 53 56\u202fPM" src="https://github.com/user-attachments/assets/5ea8e428-b5d6-4755-8b3d-1b91550a48cd" />\r\nThis is what the UI looks like right now.

> <img alt="Screenshot 2025-02-08 at 2 53 56\u202fPM" width="1470" src="https://private-user-images.githubusercontent.com/24430013/411215118-5ea8e428-b5d6-4755-8b3d-1b91550a48cd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzkyMTQzODEsIm5iZiI6MTczOTIxNDA4MSwicGF0aCI6Ii8yNDQzMDAxMy80MTEyMTUxMTgtNWVhOGU0MjgtYjVkNi00NzU1LThiM2QtMWI5MTU1MGE0OGNkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMTAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjEwVDE5MDEyMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWEzNWY2Yzk2MzIzYWEwY2Q0ODUxNzViZDFlNjFiNjdmZTQ3MTI3NGJlOGJkNjYzY2IzODgyMDhjYTI0M2U5ZTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.gR0zb7iw_a3bi_1IW425IxXM9ClxeEdA7_KLSSMR_FU"> This is what the UI looks like right now.\r\n\r\n\r\nLooking better. A few notes:\r\n\r\n- "Data Interval Start/End" should simply be "Date Range"\r\n- "Run Type" should be "Reprocess Behavior"\r\n- Let\

<img width="1470" alt="Screenshot 2025-02-12 at 10 18 56\u202fPM" src="https://github.com/user-attachments/assets/b02e8079-2cf4-416e-bdf8-9f4a81606723" />\r\nLatest UI

Thanks for all the feedback!

Google issue with docs - already fixed. Merging.

Please don\

Sorry sir for this mistake I have changed the code in repo.\r\n\r\n________________________________\r\nFrom: Jarek Potiuk ***@***.***>\r\nSent: Saturday, February 1, 2025 2:29 PM\r\nTo: apache/airflow ***@***.***>\r\nCc: Gyaneshwar ***@***.***>; Author ***@***.***>\r\nSubject: Re: [apache/airflow] add a dummy changes (PR #46346)\r\n\r\n\r\nPlease don\

I reviewed and merged the PRs that could be merged on that one @rawwar, and warned the authors of the other PRs that they will have to rebase their PRs. I think this one might show you what needs to be fixed manually after the move (unit tests, mypy and some imports of the common code, so feel free to take it on, redo the move on latest main and I suggest we try to merge it quickly :)

If you need some help - ping me on slack - I am at FOSDEM now, but I can take a quick look usually and help to make decisions what to do if you are blocked 

> If you need some help - ping me on slack - I am at FOSDEM now, but I can take a quick look usually and help to make decisions what to do if you are blocked\r\n\r\nThanks. I will reach out on slack if I need help. As of now, resolving several mypy issues locally.

discussion threads on slack - https://apache-airflow.slack.com/archives/C08AA9EJFB5/p1738464696036019

@o-nikolas any further comments? If not, happy to merge

NICE!

Started PR to move remaining Microsoft [providers to new provider structure](https://github.com/apache/airflow/issues/46045)

I think you have not used the script to move the providers @dabla? Looks like all those are manually moved :) (we have  the script in `/dev/moving_providers/` that does most of the necessary modifications and creates necessary files :)

> I think you have not used the script to move the providers @dabla? Looks like all those are manually moved :) (we have the script in `/dev/moving_providers/` that does most of the necessary modifications and creates necessary files :)\r\n\r\nOk thx Jarek indeed I did manually. Will revert current commit and use script instead :)

Looks like code loading resource file in tests needs to be fixed.

> Looks like code loading resource file in tests needs to be fixed.\r\n\r\nYes indeed but the conftest was shared across all Microsoft providers so think!it will have to be split now

> > Looks like code loading resource file in tests needs to be fixed.\r\n> \r\n> Yes indeed but the conftest was shared across all Microsoft providers so think!it will have to be split now\r\n\r\nYeah. There are two options:\r\n\r\n* if common code is small -> copy it. Tests do not have to be DRY\r\n* If the code is substantial enough to justify keeping it DRY - we have `tests_common` project which is added as a plugin to all providers and common code is there.

Close. Just some static checks missing. I can propose to run `pre-commit run -a` if you have a larger change-set.

> Close. Just some static checks missing. I can propose to run `pre-commit run -a` if you have a larger change-set.\r\n\r\nStatic checks are fixed, all other tests also pass so I think this one is good now and can be merged.

Thx @jscheffl 

Need to close and re-open to re-build

And another time :-(

I am afraid some fixes are needed.

@potiuk yaa, tests needs to be fixed. Bit struggling to make it right.

Note to release manager. This is a bug fix of https://github.com/apache/airflow/issues/43757

@RNHTTR @potiuk  Can you please review my changes.

Soem static checks are failing

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> From `ISSUE_TRIAGE_PROCESS.rst` doc:\r\n> \r\n> > Committers can invite people to become members of the triage team if they see that the users are already helping and responding to issues and when they see the users are involved regularly. But you can also ask to become a member of the team (on devlist) if you can show that you have done that and when you want to have more ways to help others.\r\n> \r\n> I saw he commented on 3 Github issues and created one PR, is he active on Slack or the dev list?\r\n\r\nNot yet, but I think he will be (although feel free to speak for yourself @gyli üôÇ)

All contributions are welcome!

üëç

Cool !

Ah. This is because apache-beam 2.55.0 has grpcio.tools as "build requirements" not "requirements". In a modern build packaging - when a frontend is building package (because it does not have corresponding binary wheel) it will perform the build of that package from .sdist in "isolated build environment" -> where it downloads and sets up  temporary venv with the dependencies specified in the "build requirements" of that package. \r\n\r\nFrom the error looks like apache-beam 2.55.0 has grpcio-tools 1.53.0 set as their `build requirements`. 

So yeah - we should limit `apache.beam` not `grpc-tools` in this case.

Nice!

Some docs issues :)

Needs breeze unit tests fix,

‚ù§Ô∏è 

I missed one test in the previous commit, just fixed it

Do I wait for tests or just merge it now do you think @amoghrajesh ?

Lets wait for the tests just to be sure

Good catch üêü !!

For context on why we are removing this:\r\n\r\n1. It never actually made it in to a release (it was tagged with 2.10.0 milestone but somehow got missed in that release)\r\n2. It relies on AIP-44 and the pydantic models it creates to work, and those have been removed in main/Airflow 3\r\n3. With the split out of the new standard provider it will be easier to add this back in quicker, but trying to support it now will delay Airflow 3 work as we have a long list of _critical_ features related ti AIP-72/Task Execution interface, and unfortunately we have to cut something.\r\n\r\nSorry to revert this yet again :( 

@ephraimbuddy are we good to send this one in?

Makes sense. Thanks for the reasoning @ashb . Should we also add an issue for 3.1 to re-add it ?

> Makes sense. Thanks for the reasoning @ashb . Should we also add an issue for 3.1 to re-add it ?\r\n\r\nThanks @potiuk, created https://github.com/apache/airflow/issues/46361 for tracking sake.

Could someone please take a look at this? @ashb @XD-DENG @dstandish @hussein-awala 

Ups, due to other merges some new conflicts. I assume it just needs a bit of rebase, then good!

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Need to look at failures.

I added a couple of changes to fix tests in ways I think make more sense. This looks good to me except the parts involving backfill; deferring those to Daniel.

Let‚Äôs take out the changes in backfill.py from this PR so we can merge the model changes first. Everything else sort of depends on them. This means backfilling would break if you hit a duplicate logical date, but I think it‚Äôs fine as long as we fix it before the final release.\r\n\r\nI‚Äôll do that tomorrow if nobody beats me to it.

@uranusjr @dstandish I have removed backfill related changes from this PR. We should be good to merge this after CI is green

> @uranusjr @dstandish I have removed backfill related changes from this PR. We should be good to merge this after CI is green\r\n\r\nok cool

It does WHAT? ü§Ø 

> It does WHAT? ü§Ø\r\n\r\nYep. ü§Ø \r\n

Wow, great catch!

> Wow, great catch!\r\n\r\nüêü 

Applied `canary` label to test latest boto tests.

Merged - failing tests are addressed in https://github.com/apache/airflow/pull/46291\r\n

Example failure: \r\n\r\nhttps://github.com/apache/airflow/actions/runs/13047585952/job/36404137972#step:6:5204\r\n\r\nFails when tries to build 1.24.4 on 3.12\r\n\r\n```python\r\nResolved 178 packages in 4.30s\r\n    √ó Failed to build `numpy==1.24.4`\r\n    ‚îú‚îÄ‚ñ∂ The build backend returned an error\r\n    ‚ï∞‚îÄ‚ñ∂ Call to `setuptools.build_meta:__legacy__.build_wheel` failed (exit\r\n        status: 1)\r\n  \r\n        [stderr]\r\n        Traceback (most recent call last):\r\n          File "<string>", line 8, in <module>\r\n          File\r\n        "/root/.cache/uv/builds-v0/.tmppPFSGG/lib/python3.12/site-packages/setuptools/__init__.py",\r\n        line 10, in <module>\r\n            import distutils.core\r\n        ModuleNotFoundError: No module named \

Merging to get next canary builds to run it\r\n

Cool! Yeah, did not notice this but also in example DAGs numpy < 1.26 failed with Python 3.12. Good that this is also fixed here!

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/47068"><img src="https://img.shields.io/badge/PR-47068-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

@mobuchowski FYI

Oh, a tad bit late!

CI failure due to an unrelated check.

nice!

Look great! Thanks!\nShould we aim to maybe scan and plan to remove caplog pieces other than cases where it is strictly necessary? Or do we need a lazy consensus to decide removing caplog pieces? Also, including a CI check to prevent to be included more caplog test without mocking if decisionis more towards to this approach? \n

> Should we aim to maybe scan and plan to remove caplog pieces other than cases where it is strictly necessary? \r\n\r\nYes\r\n\r\n> Or do we need a lazy consensus to decide removing caplog pieces? \r\n\r\nMight be worth doing to get people aware/engaged.\r\n\r\n> Also, including a CI check to prevent to be included more caplog test without mocking if decisionis more towards to this approach?\r\n\r\nAbsolutely.\r\n\r\nGo ahead with that :)

> > Should we aim to maybe scan and plan to remove caplog pieces other than cases where it is strictly necessary?\r\n> \r\n> Yes\r\n> \r\n> > Or do we need a lazy consensus to decide removing caplog pieces?\r\n> \r\n> Might be worth doing to get people aware/engaged.\r\n> \r\n> > Also, including a CI check to prevent to be included more caplog test without mocking if decisionis more towards to this approach?\r\n> \r\n> Absolutely.\r\n> \r\n> Go ahead with that :)\r\n\r\nAmazing! I will make the engagement and will create follow-ups :) This was one of the things I always wanted to support and wished to make it happen :partying_face: 

> Amazing! I will make the engagement and will create follow-ups :) This was one of the things I always wanted to support and wished to make it happen ü•≥\r\n\r\nüëÄ üëÄ üëÄ üëÄ üëÄ 

More explanation:\r\n\r\nThe difference between metadata of 1.78.0 vs. 1.79.0\r\n\r\n<img width="2039" alt="Screenshot 2025-01-30 at 01 39 30" src="https://github.com/user-attachments/assets/3ba660ca-c014-4642-83d7-948da5f4489f" />\r\n\r\nWe\

That works - no failures in static checks - other errors fxed in upcoming PRs.

Cool!

Conflicts again - but this time I think ü§û all of those flaky tests shoudl be handled.

> Conflicts again - but this time I think ü§û all of those flaky tests shoudl be handled.\r\n\r\n Thanks for checking and syncing! :crossed_fingers: 

> CI Error seems to be in Google Operator (from main) and not from this change. I think we can merge.\r\n\r\nYes, I am going to check if I can fix that in another PR. First will check if anyone is already working on a fix. Thanks for your reviews!

It seems like no one is working. I will make the changes.

The mentioned bug in the spark k8s operator test is related to mocking and new changes. \r\n\r\nPreviously, tests were failed because the new KPO `execute()` validates the name set for a pod on an operator by calling `_set_name` (because of templating changes, we need to have context applied to templates before name validation).\r\nThus the tests in k8s spark operator with `mock_create_job_name` could not succeed. MagickMock is not a str instance, so validation failed.\r\n\r\nThis led me to understand that calls to `assert op.name.startswith("...")` are broken in these tests, otherwise, they would fail too (this is checked after operator execution). My investigation showed that changing prefix to anything like `name.startswith("ABRA-CADABRA")` didn\

LGTM!

Some CI failures, static checks complaining about extra "====":\r\n```\r\nindex d4a2491..001620b 100644\r\n--- a/providers/celery/README.rst\r\n+++ b/providers/celery/README.rst\r\n@@ -51,14 +51,14 @@ The package supports the following python versions: 3.9,3.10,3.11,3.12\r\n Requirements\r\n ------------\r\n \r\n-==================  ==============================\r\n+==================  ==================\r\n PIP package         Version required\r\n-==================  ==============================\r\n+==================  ==================\r\n ``apache-airflow``  ``>=2.9.0``\r\n ``celery[redis]``   ``>=5.4.0,<6``\r\n ``flower``          ``>=1.0.0``\r\n ``google-re2``      ``>=1.0``\r\n-==================  ==============================\r\n+==================  ==================\r\n```

> no such table: dag_run\r\n\r\nThat seems to be the issue @jens, likely this is the task in still trying to use DB as not everything has been ported over yet?

> > no such table: dag_run\r\n> \r\n> That seems to be the issue @jens, likely this is the task in still trying to use DB as not everything has been ported over yet?\r\n\r\nYeah, am not sure. In Edge the DB Connection gets dropped, maybe this is the side effect if still defined in ENV for celery. In 2.10 all was clean, maybe some refactoring re-introduced this or the cleanup of AIP-44 leftovers. So no blame to this PR :-D just noticed... LGTM! ...ups was already :-D

Needs rebase for (finally!) green main

VERY VERY VERY MUCH AGREE

Thanks @jscheffl for the bold approach ! 

Closing in favor of #46029 - this is the "better" fix.

Ah NICE.... I missed that one @jscheffl ü§¶ 

Amazing idea! caplog has more cons than its pros. I think it is one of the most unstable test types. 

I am updating "zombie task" terminology to "task instance heartbeat timeout" in the Airflow configurations, source code, and documentation after receiving approval in [this dev list discussion](https://lists.apache.org/list?dev@airflow.apache.org:lte=1M:Updating%20%22zombie%20task%22%20terminology%20to%20%22task%20heartbeat%20timeout%22).\r\n\r\nThree tests from the Edge Executor keep failing on this PR with the below error message even though I have changed the `scheduler_zombie_task_threshold` config to `task_instance_heartbeat_timeout` in airflow/config_templates/config.yml.\r\n```\r\nraise AirflowConfigException(f"section/key [{section}/{key}] not found in config")\r\nE   airflow.exceptions.AirflowConfigException: section/key [scheduler/task_instance_heartbeat_timeout] not found in config\r\n```\r\n- I ran all tests for the Edge provider on my local branch with `breeze testing providers-tests --test-type "Providers[edge]"` and did not see any failures.\r\n![image](https://github.com/user-attachments/assets/e26c3f20-5a6e-4574-8773-958b95e7135c)\r\n- I built and ran the image from my local branch, then opened a Python shell and ran the code that is failing on the PR tests, but did not observe any errors. This confirms the config `task_instance_heartbeat_timeout` exists in my branch.\r\n```\r\n(base) karenbraganza@Karens-MacBook-Pro airflow % docker run -it ghcr.io/apache/airflow/main/prod/python3.9 bash\r\n\r\nairflow@6a0d443df236:/opt/airflow$ python3\r\nPython 3.9.21 (main, Feb 25 2025, 10:00:15) \r\n[GCC 12.2.0] on linux\r\nType "help", "copyright", "credits" or "license" for more information.\r\n>>> from airflow.configuration import conf\r\n>>> from datetime import timedelta\r\n>>> \r\n>>> timedelta(seconds=conf.getint("scheduler", "task_instance_heartbeat_timeout") + 1)\r\ndatetime.timedelta(seconds=301)\r\n```\r\n- I have synced my remote branch, local branch, and main airflow branch multiple times, but the test keeps failing on this PR.\r\n\r\nCould someone please help me understand why these tests are failing on my remote branch even though they are passing on my local branch, and my local and remote branches are synced?\r\n\r\n

Yeah the challenge is that the Edge Prover package is also tested against Airflow 2.10 - and obviously the new config parameter is missing there.\r\n\r\nSo in https://github.com/apache/airflow/pull/46257/files#diff-a985248c583a066f2433c82948e4b473b415d22a9469dc457932f768c10992dbR198 as well as alongside tests you need to adjust the logic to fetch the config key based on the airflow version.\r\nImport `from airflow.providers.edge.version_compat import AIRFLOW_V_3_0_PLUS` and then you can `if AIRFLOW_V_3_0_PLUS: ... else: ...`

Closing in favour of https://github.com/apache/airflow/pull/46274

tests need to be fixed/

> tests need to be fixed/\r\n\r\nYeah, working on the tests right now. Should be done in the next couple of days!

> > tests need to be fixed/\r\n> \r\n> Yeah, working on the tests right now. Should be done in the next couple of days!\r\n\r\nCool. If you need any help - let me know :) 

Compat tests finally passed ü§Ø 

> Compat tests finally passed ü§Ø\r\n\r\nWoooooooo! 

Ah yeah.`pytst_plugin` is missing. Let me push a fixup real quick :)

Yeah I am also checking that one Jarek

> Ah yeah.`pytst_plugin` is missing. Let me push a fixup real quick :)\r\n\r\nThanks!

Ah.. I forgot we need to set `pytest_plugins` before importing `airflow` :) 

Seems even more things - with imports...

Green! Merging :)

> I think we should add a test for this in future PRs.\r\n\r\nAdding a test in a separate PR

> Add test for AIP-83 amendment - update backfill #46605\r\n\r\n@prabhusneha I think Rahul has added test here: [46605](https://github.com/apache/airflow/issues/46605). Please check.

cc: @VladaZakharova - is that ok with you ?

I merge it so that we can make provider move - but we can always remove / update if needed.

Can you also create an issue for that so we do not forget to take care of that later ? 

Seems next canary is also failing as the PR is open: https://github.com/apache/airflow/actions/runs/13038931877/job/36376833483

I think we can merge if failures are unrelated to unblock

Merging. All errors in CI checks are residual leftovers of other problems, this PR still improves the broken main state.

hi @potiuk !\r\nCan you please check this PR?

lgtm but looks like some db tests are unhappy\r\n

Hi, looks good!\n\nI would love to understand why it makes a difference if it runs every 15 minutes or every hour (for example)? \n\nHow does this affect the worker performance?

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Do we know when this started breaking?

Not very sure how to filter the PR checks to find that but I recall from today itself, atleast on my PR.\r\nIAC, the change is harmless and is setting root logger specifically.

Merging this to unblock CI!

Thanks for your review @eladkal \r\nJust added an exception raise and test code for it.  Please let me know if you have any more concerns.

Looks cool. Thanks @morooshka for all the valuable comments !

Few tests are failing looking into it

Failing test cases will require change in backfill reprocess behaviour, I have create [PR](https://github.com/apache/airflow/pull/46295) which will fix these failing tests

What‚Äôs the order we should merge the two PRs? Tests are not passing on this one, but the other PR needs this to be merged first.\r\n\r\n--\r\n\r\nEdit: Talked to Rahul, this should be made greeen and go in first.

> What‚Äôs the order we should merge the two PRs? Tests are not passing on this one, but the other PR needs this to be merged first.\r\n> \r\n> --\r\n> \r\n> Edit: Talked to Rahul, this should be made greeen and go in first.\r\n\r\n@uranusjr Fixed failing tests now only failing tests are now related to backfill `test_reprocess_behavior` which are fixed in https://github.com/apache/airflow/pull/46295 .\r\n\r\n\r\n

> @uranusjr Fixed failing tests now only failing tests are now related to backfill `test_reprocess_behavior` which are fixed in #46295 .\r\n\r\n\r\n\r\n> test is still failing ü§î Please re-request for review once fixed. Thanks!\r\n\r\n@Lee-W as mentioned [here](https://github.com/apache/airflow/pull/46232#issuecomment-2630802325), failing tests are now related to backfill test_reprocess_behavior which are fixed in https://github.com/apache/airflow/pull/46295 .

Discussed with @Lee-W as https://github.com/apache/airflow/pull/46295 is based on this PR and have changes with CI green. Its better to close this PR in favour of https://github.com/apache/airflow/pull/46295

It needs newsfragment as breaking change for Airflow 3

@eladkal @ashb got a green CI finally. Can you take a look at this when you have some time? 

@vincbeck we will be adding the migration rules for this as requested. @sunank200 / @Lee-W will be assisting with that.

> @vincbeck we will be adding the migration rules for this as requested. @sunank200 / @Lee-W will be assisting with that.\r\n\r\nIt would still be nice if you could list down the migration rules so that I can focus on the implementation. Thanks!

Oops bad rebase. FIxing!

> May I know what is [test_zip.zip](https://github.com/apache/airflow/pull/46231/files#diff-e7119e5543b7e06772411ff4addb8796acc9dfa5d671bc937fce641ca6c01924) and why do we need it?\r\n\r\nYou can deploy dag in zip files.\r\nI assume that the example dag in that zip contains EmptyOperator

> > May I know what is [test_zip.zip](https://github.com/apache/airflow/pull/46231/files#diff-e7119e5543b7e06772411ff4addb8796acc9dfa5d671bc937fce641ca6c01924) and why do we need it?\r\n> \r\n> You can deploy dag in zip files. I assume that the example dag in that zip contains EmptyOperator\r\n\r\nIt does yeah, it contains a reference to the older EmptyOperator path, i updated that

@amoghrajesh Sorry for the CI error. The fix is created https://github.com/apache/airflow/pull/46404

> @amoghrajesh Sorry for the CI error. The fix is created #46404\r\n\r\nNo problem, thanks! I was wondering what the issue was!

I will wait with merging this time :)

Thanks! Was looking to do this

@ephraimbuddy Tests in api_connexion were broken.. related this PR were passed (in my local).

Hey @jedcunningham -> I am sorry but we have to revert that one AGAIN.\r\n\r\nDue to the mishap with K8S retry change masking errors in test - the #46100 got green - but in fact it failed. 

Merging. Failures ade about Amazon DMS not related to this PR

no longer needed

Seems there is some residual in back-compat provider tests.... in  providers/tests/amazon/aws/auth_manager/test_aws_auth_manager.py:37  - seems unrelated and needs another fix...

Note: Failures of core tests seems to be un-related to this fix, must be fixed in another PR

Fixes of core in PR https://github.com/apache/airflow/pull/46217

@bbovenzi Will there be a toggle to disable auto refresh that persists across views in 3.0 like legacy UI?

> @bbovenzi Will there be a toggle to disable auto refresh that persists across views in 3.0 like legacy UI?\r\n\r\nIs there a particular use case ? Otherwise IMO that just feels old and outdated without auto-refresh, I would be for completely removing that toggle in airflow 3 UI.\r\n

Gonna merge but we can keep talking about a pause autorefresh button

I think both of those use cases can be covered with us being smarter on when we invalidate queries and which queries need a refetchInterval function

Ah... conflicts again :( -> 

This one should be rebased after https://github.com/apache/airflow/pull/46289 -> the latest boto tests removed opensearch, but recent apache.beam releases already removed requests limitation that blocked it.

Thank you so much for helping to resolve it üôáüèª\u200d‚ôÇÔ∏è

Flaky tests addressed in main.

Conflict here too.

Rebae now - and all `flaky` tests shoudl finally be gone.

conflicts again (but I hope this time things will be green).

Thanks! Let me fix this first, I guess the patch for `move_providers.py` might affect other providers.

> Thanks! Let me fix this first, I guess the patch for `move_providers.py` might affect other providers.\r\n\r\nCool :)

Hmmm... I rebased after #46291 merged, and it indeed fixed the issues that caused the previous failed tests in `cncf.kubernetes` and `microsoft.azure` related to time.sleep. ~However, it looks like this is a different issue, and all the failed tests are using `caplog`~ (not related ü§î) .

Rebased - and the consensus is reached so ..... you can fix it :)

Some errors to look at :( . Happy to help if needed.

Yeah, let me circle back to this. Sorry for being busy the past two days, but thankfully, I just finished it and can now focus on this issue.

The wasb tests should be "fine" after this is merged https://github.com/apache/airflow/pull/46539  -> then you can rebase and ü§û we should get this one green.

OK. you can rebase it now :) 

Thanks, @potiuk! It turns out that the issue is caused from caplog, just as my initially assumed. However, I still can\

Pushed a fixup with conflict resolution :)

wooohooo\r\n

Looking into the failures... ü§î 

tests are failing

The failures do not seem related, @eladkal is there any place I can find out if these are flaky tests or have been failing off late?\r\nI see errors like so, which don\

~@vincbeck it looks like your change in #45986 might have broken the amazon provider test in 2.9.3 compat? I see the same error on your [pr](https://github.com/apache/airflow/actions/runs/12992179539/job/36231951612#step:11:1832) but somehow the ci passed? Could you take a look?~ fixed in #46245

Came across some issues while re-setting up airflow with pre-commit not running on python3.13 on mac. Had to force downgrade to python3.9 

> but given we do want to re-build these pages\r\n\r\nminor typo in description - we dop NOT want to rebuild them.

> > but given we do want to re-build these pages\r\n> \r\n> minor typo in description - we dop NOT want to rebuild them.\r\n\r\nOopsie üò® Description updated :) Thanks for the review!

Errors unrelated to the PR, merging

They are openlineage related :(

seems these failures are still in main

convert it to draft as I think the original idea is better

@jscheffl Let me know if this fixes it for you

Can we get this in: https://github.com/apache/airflow/pull/45749#discussion_r1925030404

I just merged it. Backport PR is here https://github.com/apache/airflow/pull/46243

> I just merged it. Backport PR is here #46243\r\n\r\nI have included this PR. 

I think https://github.com/apache/airflow/pull/46337 is missing

> Looks like you missed an import in the tests:\r\n> \r\n> ```\r\n> task_sdk/tests/definitions/test_mappedoperator.py:25: in <module>\r\n>     from airflow.models.param import ParamsDict\r\n> E   ModuleNotFoundError: No module named \

@ashb moved some more missing references and did some work on moving tests too. Can you take a look when you have some time?

Phew, just managed to add the tests, instead of relying on XCOMs, wrote the CustomOperator in such a way that we assert `expected` vs `actual` params per test case!

Very interesting pattern really: \r\n```\r\n    def test_dag_param_resolves_from_task(self, create_runtime_ti, mock_supervisor_comms, time_machine):\r\n        """Test dagparam resolves on operator execution"""\r\n        instant = timezone.datetime(2024, 12, 3, 10, 0)\r\n        time_machine.move_to(instant, tick=False)\r\n\r\n        dag = DAG(dag_id="dag_with_dag_params", start_date=timezone.datetime(2024, 12, 3))\r\n        dag.param("value", default="NOTSET")\r\n\r\n        class CustomOperator(BaseOperator):\r\n            def execute(self, context):\r\n                # important to use self.dag here\r\n                assert self.dag.params["value"] == "NOTSET"\r\n\r\n        task = CustomOperator(task_id="task_with_dag_params")\r\n        runtime_ti = create_runtime_ti(task=task, dag_id="dag_with_dag_params")\r\n\r\n        run(runtime_ti, log=mock.MagicMock())\r\n\r\n        mock_supervisor_comms.send_request.assert_called_once_with(\r\n            msg=SucceedTask(\r\n                state=TerminalTIState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]\r\n            ),\r\n            log=mock.ANY,\r\n        )\r\n```\r\n\r\nAt runtime, we parse the `ti` which will trigger reparsing our dag as per the fixtures and will add `params` to it if needed. At runtime, `self.dag.params` should have the required info for a task :)

added extension to INSTALL file to make it more readable \r\n![image](https://github.com/user-attachments/assets/ef8f5dd3-69c6-4efd-b0c8-bb30b65f4531)\r\n![image](https://github.com/user-attachments/assets/d5da020f-e60f-4b71-b26f-a66f867553b5)\r\n

Could you please rebase? There were some bugs in the CI recently

> For the problem in general, do you think if we tweak the validation/serialization alias for datamodels, for example, for connection_id, will the problem still persist?\r\n\r\nAt least in the ConnectionBody case, yeah. we can def just use `conn_id`(which is the column in db) instead of `connection_id`. we can just update tests to make it work. Should I make this change? Unless we need "connection_id" to be in the model\r\n

> I was talking about the usage of the model that is causing an issue (in the routes I guess), like where is that a problem ?\r\n>\r\n\r\nhttps://github.com/apache/airflow/blob/2f5cfbff3b5bd408d345dee5531b772b3b801bf1/airflow/api_fastapi/core_api/routes/public/connections.py#L188\r\n\r\n\r\nwhen we use by_alias=True, it throws the following error. Because, `connection_id` is a required field and because of serilization_alias, model_dump returns it as `conn_id`:\r\n\r\n```\r\n\

https://github.com/apache/airflow/pull/46245 should fix unrelated test failures\r\n

And rebase again :( 

‚ù§Ô∏è 

@jedcunningham  @dstandish @hussein-awala 

please stop @-ing me.

> **PR Description**\r\n> \r\n> This PR adds support for liveliness and readiness probe configuration to airflow sidecar container when gitsync dags enabled with probes defined.\r\n> \r\n> **Motivation** This is quite useful in places where companies has strict requirement of defining probes in all containers\r\n\r\n##Related issue: \r\nhttps://github.com/astronomer/issues/issues/6933

Is this PR good to merge?

@eladkal Hi can you review this PR

@jedcunningham Need your support closing this

@jedcunningham yea that works. Can you take a look again?

Lets wait for the checks to complete and we caan merge it

Could you please rebase? There were some bugs in the CI recently

Just approved few workflows, can merge when green

Could you please rebase? There were some bugs in the CI recently

passed tests :)

I think I have a better fix : https://github.com/apache/airflow/pull/46157

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Can you please change the title of your PR and commit meaningful ?

Also you have conflicts.

Yeah. Test needs fixing.

Conflicts to solve :)

Thx @eladkal 

This one will be a bit tricky and I think it will be best to move those failing tests to FAB @vincbeck ->  I think that was also your thought ?

> This one will be a bit tricky and I think it will be best to move those failing tests to FAB @vincbeck -> I think that was also your thought ?\r\n\r\nSome of these tests yes but some of them will be deleted when the legacy UI is deleted. For example, `test_views_variable` should not be moved to FAB provider. Actually just `test_views_acl` and `test_views_custom_user_views` should be moved. Adding that to my todo list :). (Or happy to review it someone wants to do it :))

We need also to move the 3rd party licenses of fab\r\nhttps://github.com/apache/airflow/blob/main/providers/src/airflow/providers/3rd-party-licenses/LICENSES-ui.txt

@josix Is there an ETA for us to close this PR?

> @josix Is there an ETA for us to close this PR?\r\n\r\nFYI: @utkarsharma2 -> if you are asking about release of Airflow - this one has nothing to do. We need to have fab provider released from the `providers-fab/v1_5` branch - but this one is not needed for that.

> Awesome job! Thank you for doing it!\r\n\r\nfantastic!

Rebased to account for HTTP fix in main

conflicts.

Conflicts - the tests should be green now

Conflicts.

The `Build documentation` failure is caused by a doc issue in the Google Provider, and the `Kubernetes tests` failure is due to `kerberos` dependency error.

There was a k8S problem fixed in `main`. Rebased to check if that solves the problems in this PR.

One more rebase

THis is REALLY strange those tests are failing - I tried to reproduce it and could not - so can you please rebaseit one more taime :) ?

That is a very interesting one. Let me take a look - the error is pretty strange.

Ok. I think I know what the problem was. I pushed a fixup. The problem was that `uv run` to run the pytest command creates automatically a complete local venv - which is not needed (and a bit harmful if some libraries cannot be installed). But we already have an environemtn there with all the libraries needed, so we should use that one, rather than create a new one

> Ok. I think I know what the problem was. I pushed a fixup. The problem was that `uv run` to run the pytest command creates automatically a complete local venv - which is not needed (and a bit harmful if some libraries cannot be installed). But we already have an environemtn there with all the libraries needed, so we should use that one, rather than create a new one\r\n\r\nThat‚Äôs really impressive to find out!\r\nI‚Äôve been encountering the same traceback since my first push and couldn‚Äôt figure it out.  \r\n\r\nShould we replace `"uv", "run"` in other scripts, even though the only occurrence right now is `"uv", "run", "airflow", "db", "reset", "--yes"` in `initialize_virtualenv.py`?  \r\nOr is it a good idea to add a `pre-commit` check to prevent passing `"uv", "run"` to `subprocess.run`?

> > Ok. I think I know what the problem was. I pushed a fixup. The problem was that `uv run` to run the pytest command creates automatically a complete local venv - which is not needed (and a bit harmful if some libraries cannot be installed). But we already have an environemtn there with all the libraries needed, so we should use that one, rather than create a new one\r\n> \r\n> That‚Äôs really impressive to find out! I‚Äôve been encountering the same traceback since my first push and couldn‚Äôt figure it out.\r\n> \r\n> Should we replace `"uv", "run"` in other scripts, even though the only occurrence right now is `"uv", "run", "airflow", "db", "reset", "--yes"` in `initialize_virtualenv.py`? Or is it a good idea to add a `pre-commit` check to prevent passing `"uv", "run"` to `subprocess.run`?\r\n\r\nNo. It\

Actually ... I got a feedback from `uv` guys and I can simplify it quite a bit. There is a magic `--no-sources` command which does pretty much what I am doing now when modifying the `pyproject.toml` and I totallly missed it !

Merging as-is for now - the mypy-proiders issues are unrelated and should be fixed separately, and the `--no-sources` has a bug that prevents us from using it for now - detail in https://github.com/astral-sh/uv/issues/10999\r\n\r\nHopefully this will be fixed soon by the astral team and we will be able to switch to `--no-sources` 

Aaaand Astral guys fixed it already :) . So we will be able to simplify the constraints generation soon.

Draft. Some refactoring + cleaning is required before being ready for review.

The `Build documentation` failure is caused by a doc issue in the Google Provider, and the `Build PROD images` failure is due to a dependency conflict.

After rebasing, only the `Build PROD images` failure is left.

 I will have to take a closer look at that after FOSDEM :)

> I will have to take a closer look at that after FOSDEM :)\r\n\r\nNo worries, thanks for taking a look! Enjoy FOSDEM! \r\n\r\n

OK. Easy. It will have to wait until `cncf.kubernetes` is migrated - when `cncf.kubernetes` is not yet migrated, flink has it as required dependency and it will not see `cncf.kubernetes` yet as a separate dependency - because `cncf.kubernetes` is still in `providers/src` as the "umbrella" provider package. Once cncf.kubernetes is migrated, rebasing this one should fix it.

cc: @o-nikolas ^^

> OK. Easy. It will have to wait until cncf.kubernetes is migrated - when cncf.kubernetes is not yet migrated, flink has it as required dependency and it will not see cncf.kubernetes yet as a separate dependency - because cncf.kubernetes is still in providers/src as the "umbrella" provider package. Once cncf.kubernetes is migrated, rebasing this one should fix it.\r\n\r\n> cc: @o-nikolas ^^\r\n\r\nAck, looks like @hardeybisey has taken on the kubernetes provider! https://github.com/apache/airflow/issues/46045#issuecomment-2626683707

> > OK. Easy. It will have to wait until cncf.kubernetes is migrated - when cncf.kubernetes is not yet migrated, flink has it as required dependency and it will not see cncf.kubernetes yet as a separate dependency - because cncf.kubernetes is still in providers/src as the "umbrella" provider package. Once cncf.kubernetes is migrated, rebasing this one should fix it.\r\n> \r\n> > cc: @o-nikolas ^^\r\n> \r\n> Ack, looks like @hardeybisey has taken on the kubernetes provider! [#46045 (comment)](https://github.com/apache/airflow/issues/46045#issuecomment-2626683707)\r\n\r\n@hardeybisey -> are you working on it? This one blocks us from HDFS move and we have just a few providers left, so that would need a bit quick turnaround ? Or you\

Rebased it after moving `http` provider - that was causing the failure of PROD build

Needs to be rebased now with livy hook fix

Conflicts - should be green after resolving

Trying out the example dags. Some failures, debugging it

The example dags are working now.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Rebased it after fixing the http provider issue 

One link to moved example needs to be fixed @Prab-27 

And conflicts to resolve now :( 

> And conflicts to resolve now :(\r\n\r\nSorry, I used GitHub workflow to resolve the conflict. I should have manually done a rebase\r\nShould I revert this changes ? 

No - you neeed to localy rebase and resolve conflicts (and run pre-coomit etc.).. But might be easier to do it again

Fine ?  \r\nShould I need close this PR and raise another PR ? \r\n

Sorry I accidently made mistake \r\nwill raise another PR 

This fixes the issue in my recent merge. Thanks!

Better error message in this case indeed :) 

Noticed that PR https://github.com/apache/airflow/pull/46093 was merged and it has failed static checks - https://github.com/apache/airflow/actions/runs/12983875648/job/36206050555

Closing in favour of https://github.com/apache/airflow/pull/46088

Actual fix: https://github.com/apache/airflow/pull/46119

Conflicts :( 

Ech.... conflicts too :(

Known Qdrant check that will be fixed in another PR.

This one also needs breeze unit test fix :(

Fixed it, @potiuk please feel free to take a look, thanks!

Conflicts :( 

@josix PR for common [compat](https://github.com/apache/airflow/pull/46063) is already raised

Oh, thanks for the reminder. Let me close this one.

conflicts :(

Merging. Qdrant. Will be fixed in another PR.

Thanks for this!\r\nI have to say that from my point of view this is a bug fix not a feature. Given https://github.com/apache/airflow/issues/43757\r\nI think we should backport it to 2.10 - this is really something that causes pain for users on 2.10 (including myself)

@shubhamraj-git Any reason to close this PR without merge?

This is just to prevent accidental merge till the tests are added, I will reopen in a while.

cc: @ashb 

> Task SDK does not support `extra links` yet.\r\n> \r\n> You can mark the test as `xfail` and add a comment:\r\n> \r\n> ```\r\n> # TODO: TaskSDK need to fix this\r\n> # Extra links should work for mapped operator\r\n> ```\r\n\r\nDo we still backport this fix? Or wait for Task SDK to support `extra links`\r\ncc: @eladkal @jscheffl @pierrejeambrun 

> > Task SDK does not support `extra links` yet.\r\n> > You can mark the test as `xfail` and add a comment:\r\n> > ```\r\n> > # TODO: TaskSDK need to fix this\r\n> > # Extra links should work for mapped operator\r\n> > ```\r\n> \r\n> Do we still backport this fix? Or wait for Task SDK to support `extra links` cc: @eladkal @jscheffl @pierrejeambrun\r\n\r\n\r\n\r\nhttps://github.com/apache/airflow/issues/43757 is a bug on 2.10 so we can raise PR to fix the issue directly on 2.10 test branch. This is not related to task SDK (in terms of we can fix it today on 2.10 and have a seperated fix for airflow 3 on main branch later on)

@eladkal raised PR for 2.10 https://github.com/apache/airflow/pull/46337

I think we can merge this, the fix needed on the task SDK can be taken later on. \r\nRaised a issue in case someone can pickup early https://github.com/apache/airflow/issues/46363\r\nTests are marked as xfail till then. we need to fix that post the fix for above issue gets merge.

conflicts :( 

close by #46045 [comment](https://github.com/apache/airflow/issues/46045#issuecomment-2614845077)

Close by https://github.com/apache/airflow/issues/46045#issuecomment-2614845077

But conflicts to be resolved :(

> But conflicts to be resolved :(\r\n\r\n@jarek resolved

Some qdrant immport needs fixing :( 

Qdrant wil be fixed in another PR. Merging.

conflicts :( 

still conflicts :( 

> still conflicts :(\r\n\r\nfixed

Cool

closing in favour of https://github.com/apache/airflow/pull/46102

Conflicts to resolve :(

> Conflicts to resolve :(\r\n\r\nFixed it. Thanks for the review! :)

Conflicts needs to be resolved

closing in favour of https://github.com/apache/airflow/pull/46102

conflicts

Still :( 

conflicts :(

OK. conflicts to resolve after fixing some http issue.

closing in favour of https://github.com/apache/airflow/pull/46102

Something strange going on with the CI. The dependency for http provider is open ended. Not very sure what the issue is 

I will take a look at those shortly - let it run to error again

Yes, I tried thrice.\r\n\r\nThe checks keeps on failing with Tests / Build PROD images / Build PROD Regular image 3.9 (pull_request)  failure.\r\n\r\n```\r\n  4.092 The conflict is caused by:\r\n  4.092     The user requested apache-airflow-providers-http 5.0.0.dev0 (from /docker-context-files/apache_airflow_providers_http-5.0.0.dev0-py3-none-any.whl)\r\n  4.092     The user requested (constraint) apache-airflow-providers-http==5.0.0\r\n  4.092 \r\n  4.092 To fix this you could try to:\r\n  4.092 1. loosen the range of package versions you\

Yeah. That requires a bit of deeper look :). On it\r\n\r\n

This has something to do with bad constraints generation (so an issue that I am already aware of):\r\n\r\nCyrrently the source constraints have this:\r\n\r\n```\r\n...\r\naliyun-python-sdk-kms==2.16.5\r\namqp==5.3.1\r\nanalytics-python==1.4.post1\r\nannotated-types==0.7.0\r\nansicolors==1.1.8\r\nanyio==4.8.0\r\napache-airflow-providers-http==5.0.0\r\napache-beam==2.62.0\r\napispec==6.8.1\r\napprise==1.9.2\r\n...\r\n```\r\n\r\nSo - for some reasons constraints generated in this build contain `apache-airflow-providers-http` where it should not. On it.\r\n

> This has something to do with bad constraints generation (so an issue that I am already aware of):\r\n> \r\n> Cyrrently the source constraints have this:\r\n> \r\n> ```\r\n> ...\r\n> aliyun-python-sdk-kms==2.16.5\r\n> amqp==5.3.1\r\n> analytics-python==1.4.post1\r\n> annotated-types==0.7.0\r\n> ansicolors==1.1.8\r\n> anyio==4.8.0\r\n> apache-airflow-providers-http==5.0.0\r\n> apache-beam==2.62.0\r\n> apispec==6.8.1\r\n> apprise==1.9.2\r\n> ...\r\n> ```\r\n> \r\n> So - for some reasons constraints generated in this build contain `apache-airflow-providers-http` where it should not. On it.\r\n\r\nI thought breeze generated that (could be wrong), maybe some bug there?

> I thought breeze generated that (could be wrong), maybe some bug there?\r\n\r\nNew structure of providers made a change how those are resolved an calculated (workspace feature is interfering with it) - I have a half-done fix already - PR will be there later today.

We will have to overcome some `uv` limitations - so I have to implement some hacks and open a feature request for `uv` to handle some cases - so that we can remove the hack eventually. Stay tuned.

https://github.com/apache/airflow/pull/46139 should fix it - together with constraint generation

Ok. you can rebase it @shubhamraj-git 

closing in favour of https://github.com/apache/airflow/pull/46102

### Backport failed to create: v2-10-test. View the failure log <a href=\

Can you verify extra link works also for mapped tasks? We have https://github.com/apache/airflow/issues/43757 open for the old ui

@tirkarthi Can you build using https://github.com/apache/airflow/pull/46107 and check your changes for mapped tasks ones? This should fix it.

Thanks @shubhamraj-git , I had a similar patch locally and I think the PR should work. I will wait for the changes to be merged to rebase here or take the frontend changes in a new PR after the PR. 

Is that working locally in breeze ? Are you using another executor because Task SDK does not support extra links so I expect errors there.

> Is that working locally in breeze ? Are you using another executor because Task SDK does not support extra links so I expect errors there.\r\n\r\n@pierrejeambrun  Yaa possible, I think `--executor CeleryExecutor` will work.

@pierrejeambrun I am using a virtual environment to pull from git and do `uv pip install .` with `airflow scheduler` and `airflow dag-processor` in different shells. I am using extra links as per the docs and using it on `BashOperator` from `from airflow.providers.standard.operators.bash import BashOperator` . So I guess I am not using the task sdk here.\r\n\r\nhttps://airflow.apache.org/docs/apache-airflow/stable/howto/define-extra-link.html

> @pierrejeambrun I am using a virtual environment to pull from git and do `uv pip install .` with `airflow scheduler` and `airflow dag-processor` in different shells. I am using extra links as per the docs and using it on `BashOperator` from `from airflow.providers.standard.operators.bash import BashOperator` . So I guess I am not using the task sdk here.\r\n> \r\n> https://airflow.apache.org/docs/apache-airflow/stable/howto/define-extra-link.html\r\n\r\nSee https://github.com/apache/airflow/blob/main/contributing-docs/07_local_virtualenv.rst - when we explain how to work with the new virtualenv model where we have mutliple packages: `uv sync --all-extras` and `uv sync --extra celery` are your friends as they are using the `uv workspace` feature which installs all internal airflow packages as needed and all necessary dependencies for the provider extras you specify.

Are we happy with having it in the details tab?\r\nI have to say, I click on the details tab rarely (for the info that we are having there).\r\nOn the other hand, I really need the extra link when I view failure logs.\r\n\r\nI am not UX expert but I think we need better place for the extra links than the details tab.

In Airflow 2 usually clicking on the task instance in grid leads to the task instance details tab. In Airflow 3 logs are made as the default tab. I am fine with moving extra links there if okay.

> In Airflow 2 usually clicking on the task instance in grid leads to the task instance details tab. In Airflow 3 logs are made as the default tab. I am fine with moving extra links there if okay.\r\n\r\nGetting to the extra links in 2.10 is very inconvenient to my taste.\r\nI would rather the links to be folded/expanded above the logs/events/xcom tabs, but I leave this choice to the UX experts :)

Thanks @jx2lee!

Unfortunately @jx2lee -> you will have to recreate that PR. For the last 2 days we had a change merged that masked failures of tests - and this one actually failed, but got green - it causes failures on our main, so we have to revert it. \r\n\r\nApologies for that!

Conflicts :)

> Conflicts :)\r\n\r\nResolved

Thanks @vatsrahul1001 for this PR :) 

@hussein-awala I see we have DRAFT [PR](https://github.com/apache/airflow/pull/46041) which relates to SMTP Provider, this migration can create conflicts. Are you okay with it or should we postpone this migration till you merge your PR? \r\nThis PR is ready to be merge state.

Yeah. It looks like we could start to batch those changes - since they are generating conflicts.

Instead of rebasing it I created https://github.com/apache/airflow/pull/46556 anew - it seems that  it should work out of the box - easier than solve conflicts with this one.

@potiuk Yaa, thanks!

Resolved conflicts via GitHub UI 

Looks good.

Some mypy fixes unfortunately :(

Static check is known and the Notebook one is something that MyPy is randomly unhappy about . Merging.

Some conflicts have been produced by other PRs, I assume this can be merged once conflicts removed

closing in favour of: https://github.com/apache/airflow/pull/46051

conflicts :( 

LLikely rebase/conflicts :( 

> LLikely rebase/conflicts :(\r\n\r\nResolved

again conflicts I am afraid :( 

The original diagram:\r\n![image](https://github.com/user-attachments/assets/e36b3b30-7278-4a01-9860-dd1ff5b979ae)\r\n\r\nDiagram after adding triggerer tasks:\r\n![image](https://github.com/user-attachments/assets/b683cdd4-a695-4c9a-adc2-993a6cc6b1aa)\r\n\r\n**Question: Does a triggerer task enter "queued" state during its lifecycle, or it\

Is there a way we can easily get to update that picture when it changes in the future ? \r\n\r\nI am a big fan of generating such images from text / code rather than drawing them by hand. We even already have a number of such images generated from the code via pre-commit - using the cool `diagrams` library:\r\n\r\nPython file: https://github.com/apache/airflow/blob/main/docs/apache-airflow/img/diagram_basic_airflow_architecture.py\r\nResulting diagram: https://github.com/apache/airflow/blob/main/docs/apache-airflow/img/diagram_basic_airflow_architecture.png\r\n\r\nAnd it\

I know it better goes into discussion... but `diagrams` is not included in the installed dependencies (if `uv sync --all-extras --all-groups` installs all dependencies possible, and of course I can\

@potiuk any further comments?

AI spam

Hi @eladkal \r\nI apologize. I am the new contributor to this project. Over the past few days, I had an offline handover with @josix  regarding this issue, and I will be responsible moving forward. However, while preparing the PR, I accidentally clicked on "create PR." I‚Äôm sorry for the disruption. Once the PR is ready, I will re-open it for your review. üôèüèΩ

Ah cool :)\r\n

> Ah cool :)\r\n\r\nah... yes, we can just reopen it haha. Thanks so much for helping out!

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

NICE! - Now the only thing missing is to fix the doc building.. From what I see - somewhere in docs (where the automation did not found it and fix automatically) you have\r\n\r\n`tests.system.providers.zendesk.example_zendesk_custom_get.fetch_organization` which is now a different module\r\n`tests.system.zendesk.example_zendesk_custom_get.fetch_organization`  I think - and it should be updated in the place it is used.

Just adding comments as copy of the 1:1 conversation:\r\nI doubt that pushing an XCom from an external system is a good way to signal data to an existing workflow. Technically it would be working but this opens the door to "hacky integrations" in my view. XCom (as far as I have seen it) was made for inter-task communication and not as an interface to external applications.\r\nI don\

> Just adding comments as copy of the 1:1 conversation:\r\nI doubt that pushing an XCom from an external system is a good way to signal data to an existing workflow.\r\n\r\nThis is not the use case. The use case is incident recovery. When on call fixes problems with data pipeline manual interventions sometimes required.

> > Just adding comments as copy of the 1:1 conversation:\r\n> > I doubt that pushing an XCom from an external system is a good way to signal data to an existing workflow.\r\n> \r\n> This is not the use case. The use case is incident recovery. When on call fixes problems with data pipeline manual interventions sometimes required.\r\n\r\nIf I read the PR description incident recovery is not the use case.\r\n\r\nFor incident recovery also patching of XCom would be needed. It is called as:\r\n\r\n> enabling dynamic, external updates to workflows

> Tests are missing\r\n\r\nYes, I was waiting for discussions to be done. I will include the tests before merging. @jscheffl 

ready to merge?

Yes, added the tests and good to merge.

@pierrejeambrun @jscheffl any further comments?

> @pierrejeambrun @jscheffl any further comments?\r\n\r\nNo. No objections.

I think we need one last rebase.

We need also to remove the email and smtp part of airflow.cfg\r\nhttps://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#email\r\n\r\nhttps://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#smtp

I merged https://github.com/apache/airflow/pull/47146\r\n@hussein-awala what is left to be completed on this PR?

Ai Spam

@jx2lee @jedcunningham This Pr is needed to finalize #43224 The metadata label in the scheduler deployment cannot contain a comma without being quoted and I found 1 occurrence that the previous PR #43606  did not cover.\r\n\r\nHope this can be merged just in time for the helm chart 1.16 release

Remaining failures are unrelated. Merging.

CC: @shubhamraj-git 

This looks good to me. I‚Äôd be even more confident if the map_index changes are taken out, but I‚Äôll take Ash‚Äôs words they don‚Äôt affect anything else if they‚Äôre left in. Aside from that it‚Äôs just the seemingly redundant `type()` calls left. We should remove them if possible, or find out why they are needed.

> Use the command line to resolve conflicts before continuing.\r\n> `providers/src/airflow/providers/cncf/kubernetes/operators/pod.py`\r\n\r\nThis is a "fake" conflict. `git` on the CLI does it without complaining (it just got renamed). If this current run passes I am merging after rebasing without waiting for next tests to pass.

NIIIIIIIICE!

Need to work on few things:\r\n1. Fix the task runner tests\r\n2. Add tests for supervisor\r\n3. Revisit the `# type: ignore` checks -> added some cos it felt easier.\r\n4. Add PR description with plenty of details.

Yeah, with the latest changes, the DAG passes:\r\n![image](https://github.com/user-attachments/assets/6686afd9-3606-460d-95f4-d99e3cfe0bfd)\r\n\r\nAnd the error handling is better:\r\n![image](https://github.com/user-attachments/assets/bcd74da7-4202-4d14-98ff-9bae63bd2a50)\r\n\r\n\r\nCC: @ashb

Well OK, turns out the test has to be updated now!

@ashb handled your comments, let me know if the approval still stands :)

Yup, still stands. When the changes requested are small enough I often "pre-approve" it to save an extra cycle, and then it\

The CI had some issues lately, just working on a green one so that this can be merged

After a hard battle with the CI, we have won! Merging this one

cc: @atul-astronomer

> This PR enables the task.scheduled_duration metric\r\n\r\nDoes this mean the metric was never produced in Airflow 2.x?

> > This PR enables the task.scheduled_duration metric\r\n> \r\n> Does this mean the metric was never produced in Airflow 2.x?\r\n\r\nYes, it was defined but never filled - as the scheduled time was not recorded. Only the queued time was written into the DB then the metric was able to be produced. Good overview is in https://github.com/apache/airflow/pull/30612

> > > > This PR enables the task.scheduled_duration metric\r\n> > > \r\n> > > \r\n> > > Does this mean the metric was never produced in Airflow 2.x?\r\n> > \r\n> > \r\n> > Yes, it was defined but never filled - as the scheduled time was not recorded. Only the queued time was written into the DB then the metric was able to be produced. Good overview is in #30612\r\n> \r\n> #45285 reports it was rarely sent. Not that it wasn\

@jscheffl That is a good question. I¬¥m not sure how this metric should be rarely exported. In the current implementation the metric would be exported if the start_date is available but from my point of view this start_date will be set after the task went from scheduled into queued state. \r\n\r\nMay be if a task reruns and the task_instance includes already a start_date and it is not moved into the task_instance_history then this metric could be exported but this value is wrong for this metric. But I do not know in which case we can end in such kind of state. :(

Nobody is objection - I thought it is a bit more of a discussion - I propose to merge this as an improvement for observability with the trade-off of an additional column in the DB

Why not pre-commit ? you will be able to run it locally way before it even hits the CI.

> Why not pre-commit ? you will be able to run it locally way before it even hits the CI.\r\n\r\nYep, sounds like a great idea! I just updated it to use pre-commit. Will merge it once the CI pass so that we can ensure the format is correct for future newsfragments.

> I understand the need and like the idea, but do we need templating?\r\n\r\nIn every component of airflow helm template it was defined that way - so i included the same approach. I doesnt require we can directly do a toYaml directly as well

Tests are failing:\r\n```\r\n\r\n=========================== short test summary info ============================\r\nFAILED helm_tests/airflow_core/test_dag_processor.py::TestDagProcessorLogGroomer::test_log_groomer_collector_custom_env - tests.charts.helm_template_generator.HelmFailedError: Helm command failed. Args: (1, [\

> Tests are failing:\r\n> \r\n> ```\r\n> \r\n> =========================== short test summary info ============================\r\n> FAILED helm_tests/airflow_core/test_dag_processor.py::TestDagProcessorLogGroomer::test_log_groomer_collector_custom_env - tests.charts.helm_template_generator.HelmFailedError: Helm command failed. Args: (1, [\

Oops, beat you @amoghrajesh :)

Hey @jedcunningham -> reverting it temporarily in #46037 as it seem to break main. You should redo it with "full tests needed" to double check what\

Thanks. I\

> Thanks. I\

Resolved in https://github.com/apache/airflow/pull/45954

sir @potiuk \r\ni fix the issue in my pull request please check out this \r\n

@dimberman -> just rebase your commit on top of `main` not my "move standard providers" change - that should fix it - and I am working on fixing the failures in "move standard providers"

Thanks @vincbeck 

Got some fixes in :crossed_fingers: . also found a few small things - now IDE integration works nicely with the providers in separate folders (it was broken for a few days for the new providers).

Also found out that `decorators/test_python.py` have not been moved to standards provider when we moved stuff :).\r\n\r\nThis is actually nice thing about this splitting - we will likely find out all the stuff where providers are  still imported from core tests :)

And there are still two things to fix I see :)\r\n

OK. looks like I have to convert common.sql first - to make that one succeed

Seems that easiest way is to :crossed_fingers: migrate common.sql and standard together

Some more fun to check - but at least the image builds now. I am glad I decided to do it slowly and incrementally ;) 

Hah... Found a few things missing from the previous moves - for example `task_sdk` was still present and installed in our compatibility tests - and started to mess around with pre-airflow-3 versions :).\r\n

And added few more replacements (logo/ .pre-commit-config.yaml) that could also be automated easliy (standard provider did not have them - but alibaba did).

> Hah... Found a few things missing from the previous moves - for example `task_sdk` was still present and installed in our compatibility tests - and started to mess around with pre-[airflow-3](https://issues.apache.org/jira/browse/AIRFLOW-3) versions :).\r\n\r\noh good catch, thanks Jarek, this move revealing good stuff. :) 

> Also found out that `decorators/test_python.py` have not been moved to standards provider when we moved stuff :).\r\n> \r\n> This is actually nice thing about this splitting - we will likely find out all the stuff where providers are still imported from core tests :)\r\n\r\nah i see, yes indeed. 

> oh good catch, thanks Jarek, this move revealing good stuff. :)\r\n\r\nYeah. I actually love to do these kinds of refactoring, they usually reval some `dead bodies in the closet` :D

And we need to add `task-sdk` at least for now - until we release it in PyPI - in order to run various provider verification tests as those are run with packages built and installed from sources :).\r\n\r\nAnother step closer to get to the package split - @ashb

Finally got it green... This single PR allowed me to trace and fix quite a number of small issues we had with moving to the "providers" and now to the "newer providers" strucuture... Next step will be to launch full-blown migration process :).

Merging as the failed mypy check is unrelated.

The only failures are caused by:  \r\n- UI static check  \r\n- Compatibility tests for `2.9.3:P3.9` and `2.10.4:P3.10` providers (likely broken due to the new provider structure?)  \r\n- Flaky test: `TestWorkflowTrigger.test_task_workflow_trigger_skipped`  \r\n\r\nDo we need to fix the compatibility tests, or is this PR ready to be merged?

Only non-related `ts-compile-format-lint-ui` static check fail üéâ

Not unrelated, those lines reference the removed `external_trigger` field.

I added a commit that should fix the static checks.

Thanks @amoghrajesh !

looks like `gh` issue ü§î will re-run those tests

> Then the worker will go to maintenance pending if there are running jobs, and maintenance mode if all jobs have finished.\r\n\r\nThe diagram suggest changing mode depends only on the count of jobs. Is there a way for cluster admin to force entering to mantanince mode? (Force kill all existed jobs)?

> > Then the worker will go to maintenance pending if there are running jobs, and maintenance mode if all jobs have finished.\r\n> \r\n> The diagram suggest changing mode depends only on the count of jobs. Is there a way for cluster admin to force entering to mantanince mode? (Force kill all existed jobs)?\r\n\r\nI think a force maintenance still could be implemented. If somebody wants/needs this. The intend of the current implementation is a graceful drain of running jobs. The "pending" stzate is the transition, same like if you send a SIGINT to a Celery worker, then also the worker does not pull new jobs (stps consuming from queue) but will attempt to complete all jobs and then terminate.\r\n\r\nYeah, if there is a urgent demand (that is how I do during testing to be faster) I check the jobs list page and then find the task in execution and mark as failed/success. Not a single click solution but basically a manual workaround. Not often used by me, just in testing :-D 

Closing this as it is also being done in [46232/](https://github.com/apache/airflow/pull/46232/)

> Please ammend the commit and PR title with meeaningful description\r\n\r\nDone

Still failing\r\nSee https://github.com/apache/airflow/actions/runs/12970501835/job/36178118560?pr=45954#step:6:8502

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Cool :) 

cc: @potiuk (It appears that Vite is a little bit more flexible on semver definition üòÇ, even if they seem to follow Semver stricktly https://vite.dev/releases)

> cc: @potiuk (It appears that Vite is a little bit more flexible on semver definition üòÇ, even if they seem to follow Semver stricktly https://vite.dev/releases)\r\n\r\nActually - they are pretty conistent. The change in question is a fix - so it turned out the previous default was wrong and this one is fixing it:\r\n\r\n![image](https://github.com/user-attachments/assets/2d9a8862-2480-4945-adf7-4ead607987f5)\r\n\r\nThis is precisely as I always repeat - pretty much any change is breaking, otherwise it would not be a change. Change == Behaviour change, which means that if you rely on the changed behaviour, it will break your use. But breaking your use does not mean breaking semver, it means that there was some unintentional (usually buggy) behaviour that you relied on which has been classified by the authors are exactly this - unintentional, or even buggy. And they fixed :).\r\n\r\nSo. ... All is good.\r\n

Simply -> Semver does not 100% guarantee that after upgrade to patchlevel version everything will work as it did. If it did, then it means that applying any fix is impossible - because someone could have relied on that thing being broken :)

@eladkal , please take a look at the updates.

I was initially against making it configurable, but seeing the simplicity and flexibility, I am in.

@eladkal ?

hi there! \r\n@potiuk \r\nCan we merge this one please?

Hi @potiuk @eladkal ! Are there some other changes we need to make here? Or we can merge this one?

We are on feature freeze for Airflow 3.\r\nhttps://lists.apache.org/thread/r26htzl0w3th7pw0l1y31g6s14qbtwwt

Yeah. I think that might be 3.1 

![image](https://github.com/user-attachments/assets/bae9b60e-3e04-493a-a779-2c44b58c2865)\r\n

Errors not related - they are being worked on in #45917  . Merging

I\

Working on fixing the tests

With the new changes, tested both for legacy and task sdk DAGs (cc @ashb)\r\n\r\nLegacy Results:\r\n![image](https://github.com/user-attachments/assets/78fbd716-b512-4f74-9d1c-36ef8b183b73)\r\n\r\n\r\nTask SDK Results:\r\n(asset_s3_bucket_producer first 2 failures are unrelated and the failure for asset_alias_example_alias_consumer is because of inlet_events not yet woirking)\r\n![image](https://github.com/user-attachments/assets/5f5b5178-ba07-4566-b1a4-2fe91690dec8)\r\n

Looks like the changes in this PR break some test cases for inlet_events. Looking into it

Ok I think I figured out the reason for failure, working on a fix

I will just resolve the conversations with relevant replies, rebase & merge this

Merging. The failures are unrelated and worked in on in #45917 

Yeah, looks nice!

backport of #45874

cc: @eladkal Precisely what I was foreseeing. I am glad I ported the "pull_request_target" removal before - it would have been so painful to recall all the details now :) (see https://github.com/apache/airflow/pull/45591#issuecomment-2585793226)

Yes. I think we should run all provider tests when task_sdk files change. I will add it to selective checks.

PR for selective checks  to avoid such failures in the future https://github.com/apache/airflow/pull/45921

ALMOST !

> ALMOST !\r\n\r\nShould be finally fully green ü§û 

:crossed_fingers: :crossed_fingers: 

Thanks @uranusjr  for reviewing!  \r\n\r\n> Generally looks good to me, but one question on rendering the log in API and web UI.  \r\n\r\nI just replied to the code review conversation above and also resolved the unnecessary type hint.

CI failures due to flaky tests:\r\n- [Additional PROD image tests / Docker Compose quick start with PROD image verifying](https://github.com/apache/airflow/actions/runs/13437479178/job/37544612350?pr=45914#logs)\r\n```\r\n  docker_tests/test_docker_compose_quick_start.py::test_trigger_dag_and_wait_for_result FAILED [100%]\r\n  \r\n  =================================== FAILURES ===================================\r\n  _____________________ test_trigger_dag_and_wait_for_result _____________________\r\n  \r\n  default_docker_image = \

Nice. We should certainly get it in before 2.10.6 or 2.11.0 (whichever will be released) is prepared. I rerun the failing tests - they seemed to be `flaky` / intermittent. 

> Nice. We should certainly get it in before 2.10.6 or 2.11.0 (whichever will be released) is prepared. I rerun the failing tests - they seemed to be `flaky` / intermittent.\r\n\r\nThanks @potiuk !\r\nBoth test reruns failed with the same error: `The image /mnt/ci-image-save-linux_amd64-3.9.tar does not exist.`

I see. yeah . Makes sense. I was mislead by "more admindeployment than authoring" - but yes Airflow 2 backport is likely not needed.

### Backport failed to create: providers-fab/v1-5. View the failure log <a href=\

@potiuk I am not sure this is possible. We might need to revert this one. Or maybe there is another solution I do not see. This PR introduces a bug in FAB auth manager, if you try to log in with wrong creds, the webserver will crash. The error is `unsupported hash type scrypt`. `scrypt` has been introduced in `werkzeug` in 2.3. Thus we need to set the minimum version of `werkzeug` to 2.3. When I do that, I got some conflict dependencies because `connexion[flask]` depends on `werkzeug < 2.3`. We would need then to upgrade `connexion` to 3, which is another story.\r\n\r\nWDYT?

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/45875"><img src="https://img.shields.io/badge/PR-45875-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

### Backport failed to create: v2-10-test. View the failure log <a href=\

Can you please add a unit test showing the issue and preventing regression in the future? Thanks in advance!

@potiuk - I looked at the code and how it can be unit-tested, and this is more challenging than I thought at first.\r\n\r\nInside the `start_spark_job` it calls `create_namespaced_custom_object` from the `custom_obj_api`, which makes a call to kubernetes API and returns the response object (simple dictionary).\r\n\r\nThe best option to implement the unit test was to mock the response of the `custom_object_api.create_namespaced_custom_object`.\r\n\r\n@potiuk - please let me know if this is what would be a good enough test or if you have any other options in mind. Thank you for your review in advance!

@potiuk @hussein-awala @jedcunningham - hey everyone,\r\n\r\nAppreciate you taking time to review this MR. Could you please take a look whenever you have time so that we can merge this simple change and include it into the next version of airflow release if possible. Thank you in advance!

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Thanks @chrisluedtke! Congrats on your first commit üéâ 

> Are these just not used _yet_, or they will never be used in AF3? Naively these seem like the former.\r\n\r\nNop, it is the latter. They should never be used in AF3.

Closing this one in favor of https://apache-airflow.slack.com/archives/C06K9Q5G2UA/p1737999509008529.

Hmmm... did not help in https://github.com/apache/airflow/pull/45875. I had to close/reopen it to run the workflows :( 

I think this changes also required in v2-10-test branch ?

### Backport failed to create: v2-10-test. View the failure log <a href=\

We are about to migrate Google to the new provider structure, so you will have to rebase your change with the moved files and fix the problems there.

Migrated. You should rebase @molcay 

@potiuk \r\nRebased, can you please check again? :)

>Yeah changes look good. Can we add TODO in taskinstance.py for the parts that we want to undo once Celery / K8s executors are integrated?\r\n\r\nMost of that file we get nuked :) 

Thanks, this should resolve https://github.com/apache/airflow/issues/45216

üò± \r\nüôÄ 

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Moar teething issues :) 

Nice: \r\n\r\n\r\n<img width="1556" alt="Screenshot 2025-01-20 at 22 44 25" src="https://github.com/user-attachments/assets/5adca2ac-124a-4d43-befd-572f494d09fb" />\r\n

yeah looks like a few formatting changes.

> yeah looks like a few formatting changes.\r\n\r\nYep, would be nice if we can take care of it. Thanks!

Re-based the PR against main. Should make this PR merge-able after the tests run. 

post re-base some core tests are failing. all the provider tests pass.  im stuck on how to fix this. 

@Lee-W  looks like this is good to go :) 

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

@jscheffl I thought the same, maybe we can introduce import/export with description, with an option to also perform the old way.

Did I say I love üíî  sphinx .... ? 

OK. I think I got to an agreement with Sphinx finally ü§û ...\r\n\r\nI also moved "example_dags" (i.e. system tests) from `tests.system.providers.<provider_id>` to `tests.system.<provider_id>`. I realized that `tests.system.` packages were previously exposed via inventory - and if there are any cross-providers references in the documentation, that would break the documentation links cross-providers. And we do not really need that extra "providers" package in the middle.\r\n\r\nI moved the `airbyte` system tests back as well in this change.\r\n\r\nThis is the current `apache.iceberg` provider\

> nice optimisation :)\r\n\r\nI got a bit annoyed with about a second and a half delay 

Example output: \r\n\r\n![image](https://github.com/user-attachments/assets/93edaaa3-3356-41dd-b58c-6640c20d9a6e)\r\n

### Backport failed to create: v2-10-test. View the failure log <a href=\

hi @potiuk ! Can you please check changes from this PR? :)

@eladkal I have updated the naming for hook and operators. Could you please check PR one more time?

Hi @eladkal ! Can you please check changes here? Thanks :)

@eladkal @potiuk could you please check this PR again?

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> Nice!\n\nThanks sir

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

cc: @jscheffl -> this one also includes fixing LICENSE generation for providers, and making tests adaptible - so that we don not have to modify them when we move next provider in.

Ah.. I need to do one more thing with "additional-extras" and comment removal during pyproject.toml generation !

Ok. I think I got all the things needed for celery one and added/fixed quite a few missing thing in the provider move.\r\n\r\nThere is still few things I need to add for other providers (devel deps support) and some pre-commits checking for completeness/consistency of workspace settings - but those will come as next steps when moving few more providers with those.

One more issue where pytest has a problem with duplicated modules of the same name inside providers :( 

Ok. I think I got that one ü§Ø 

OK. Looks good now :) 

Oh ... how cool is that :) ... üëÄ . I was just about to make two or three more providers, this weekend, so I might also verify that one together :)

> Oh ... how cool is that :) ... üëÄ . I was just about to make two or three more providers, this weekend, so I might also verify that one together :)\r\n\r\nOh, how cool. I thought notifications are only sent after leaving draft. Still this is a test baloon, let me find the basic glitches first :-D

> Oh, how cool. I thought notifications are only sent after leaving draft. Still this is a test baloon, let me find the basic glitches first :-D\r\n\r\nNotifications - yes. But I am looking at EVERY issue and PR in airflow repo.

You will also need to add the `apache-airflow-providers-edge` to .gitignore in `docs`

You will likely also have to copy "conftest.py" from `providers/airbyte/tests/conftest.py`. We would likely want to auto-generate this one as well (separate PR is coming).

> @ambika-garg, check our #46571 which just made it so more than 1 bundle can be passed via the cli. You\

Thanks @ambika-garg!

WHOA! 

Should we back-port this to 2.10?

> Should we back-port this to 2.10?\n\nAll the classes changed should all be just in main, since they are part of Task SDK.

Thanks for the feedback, I removed the newsfragment

> Thanks for the feedback, I removed the newsfragment\r\n\r\nPytests reveal... is is not a method but a property. Are you sure your fix is correct?

Yes I am sure it is correct. We use it in our production app and always get a reference to the method in the log, which is identical to what is descibed in the bug report 44022.\r\n\r\nAlso, if you take a look at the [pyexasol implementation of rowcount](https://github.com/exasol/pyexasol/blob/af98ab7305e90e26a0912eba77eb99f950ab84ba/pyexasol/statement.py#L228), you can see that it is actually a method instead of a property. There is a property `row_count`, but the rowcount method should be used.\r\n\r\nI will look into fixing the tests

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

I applied "full tests needed" and rebased it - I think that one should run full suite of tests due to potential compatibility issues.

Looks like the only failure is unrelated one (already failing on parent commit, same failure as here: https://github.com/apache/airflow/pull/45727#issuecomment-2599130122 - `providers/tests/common/sql/hooks/test_dbapi.py::TestDbApiHook::test_run_no_log `).

Added an issue for the flaky stuff https://github.com/apache/airflow/issues/45774 

It looks like another "caplog issue"

Hi! Thank you for preparing PR with changes :)\r\nCan you please add a system tests for this? As a new task with deferrable mode for every operator that you have modified.\r\nCan you please provide also screenshots of successfully executed system tests with your changes? Thanks!

Rebased to account for fixed main failure + flaky test

Merging now, one test failed, fix here https://github.com/apache/airflow/pull/45755

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/46243"><img src="https://img.shields.io/badge/PR-46243-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

sorry for the review request mess...

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> Thanks :)\r\n> \r\n> is it also comes under code interface change?\r\n\r\nI think `Dag changes` should be good enough for this one ü§î 

https://github.com/astral-sh/ruff/pull/15611 has been created to add these rules to ruff

forget to push the latest changes......

follow up PR https://github.com/apache/airflow/pull/46757

Hell yeah this works. We shouldn‚Äôt merge this before we resolve the discussion on this, but feel free to review the changes and get an idea what will change (not a lot).

cc: @dstandish 

Nice, look at all that deleted code üë®\u200düíª!!!

#protm

Can you please add a newsfragment before merging because from deployment point of view it is a breaking change.

Did you consider sending these via the initial context we send in the response to the Run request?

Following failure is unrelated\r\n\r\n```\r\nFAILED providers/tests/common/sql/hooks/test_dbapi.py::TestDbApiHook::test_run_no_log \r\n```

Python 3.10 builds failing due to python dependency issues.\r\n```\r\n  #45 0.924 Using Python 3.10.16 environment at: /usr/local\r\n  #45 2.028    Building apache-airflow @ file:///opt/airflow\r\n  #45 3.403       Built apache-airflow @ file:///opt/airflow\r\n  #45 4.229   √ó No solution found when resolving dependencies:\r\n  #45 4.229   ‚ï∞‚îÄ‚ñ∂ Because the current Python version (3.10.16) does not satisfy\r\n  #45 4.229       Python>=3.11 and sagemaker-studio==1.0.7 depends on Python>=3.11, we can\r\n  #45 4.229       conclude that sagemaker-studio==1.0.7 cannot be used.\r\n  #45 4.229       And because only sagemaker-studio<=1.0.7 is available and\r\n  #45 4.229       apache-airflow[devel-ci]==3.0.0.dev0 depends on sagemaker-studio>=1.0.7,\r\n  #45 4.229       we can conclude that apache-airflow[devel-ci]==3.0.0.dev0 cannot be\r\n  #45 4.229       used.\r\n  #45 4.229       And because only apache-airflow[devel-ci]==3.0.0.dev0 is available\r\n  #45 4.229       and you require apache-airflow[devel-ci], we can conclude that your\r\n  #45 4.229       requirements are unsatisfiable.\r\n  #45 ERROR: process "/bin/bash -o pipefail -o errexit -o nounset -o nolog -c bash /scripts/docker/install_airflow.sh" did not complete successfully: exit code: 1\r\n  ```\r\n

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> BTW. We could even FAIL if standalone dag processor is disabled for Airflow 3. But that might be another PR some day.\r\n\r\nProbably good thing to add when we completely remove the code for embedded DagFileProcessor.

PR https://github.com/apache/airflow/pull/45737 created

> Nice\r\n> \r\n> A few suggestions, and hint for follow up PRs. Almost ready to merge.\r\n\r\nThanks for the quick review! I updated the code according to the comments. \r\n\r\nI agree with all the other comments and created an issue for it too #45816.\r\nWhile making fields `enums`, I tried to create a baseline for unifying data models for bulk operations in general which could be applied for any unification for data models. I moved them in `common.py`. I have included `TODO` for further unification for `datamodels` from this story.

Nice

NIIIIICE!

Hey @bugraoz93 ,\r\nwe replaced `action_if_exists ` and `action_if_not_exists` with `action_on_existence`\r\n1. This contains, "overwrite", which would be not relevant in case of delete and update, and will fail.\r\n2. Also, `action_on_existence` seems misleading in case of update and delete, since the operations here were for if the value is not present and not on existence, when either it fails or skip.\r\n3. [In context of Pools bulk API] Since the PoolBulkCreateAction and PoolBulkUpdateAction models share the same structure (both have an action field and a pools list), pydantic may incorrectly parse an update action as a create action because it matches the first type (PoolBulkCreateAction) in the union. This issue here is because the PoolBulkCreateAction and PoolBulkUpdateAction have pools type as different list[PoolPostBody] and list[PoolPatchBody] respectively. [Can refer the below attached PR]\r\n\r\nWas this intentional? Or just a miss, in case I can rectify this in upcoming PR for bulk pool.\r\n\r\nFor now, based upon above comments, I have included that in https://github.com/apache/airflow/pull/45939

> Hey @bugraoz93 , we replaced `action_if_exists ` and `action_if_not_exists` with `action_on_existence`\r\n> \r\n> 1. This contains, "overwrite", which would be not relevant in case of delete and update, and will fail.\r\n> 2. Also, `action_on_existence` seems misleading in case of update and delete, since the operations here were for if the value is not present and not on existence, when either it fails or skip.\r\n> \r\n> Was this intentional? Or just a miss, in case I can rectify this in upcoming PR for bulk pool.\r\n\r\nHey @shubhamraj-git , \r\nThis was the baseline for making things common for the bulk endpoints so implementation is still from surface at the moment. I think things are getting clearer while we implement more use cases/endpoints. \r\n\r\nOn the documentation perspective `overwrite` is not relevant and it could be a side effect/bug since we are allowing to call the endpoints with `overwrite` in this case. I think we can drive from a parent `enum class` to make documentation nice and separate the `enum classes` according to action types. \r\n\r\nI am planning to do more unification for the datamodels in #45816. I can cover this one over there, it seems relevant. Please bring up anything in that issue to discuss more. Thanks for pointing out! I would be happy to bounce ideas and am going to tag you in the next PR. 

as explained in slack - you can replace the confilcting images / .txt with either versions, rebase and re-run pre-commit. The images wil get regenerated to be "latest" automatically (in fact it should happen automatically when you rebase your PR and resolve the conflicts if you did `pre-commit install` before).

Could you please clarify it for me?\r\n1 Is it okay to include` providers` in this pre-commit hook ?  \r\n2 Do I need to remove `session.query` from the codebases to pass this pre-commit check ?  

> Could you please clarify it for me? 1 Is it okay to include` providers` in this pre-commit hook ? 2 Do I need to remove `session.query` from the codebases to pass this pre-commit check ?\r\n\r\nProviders should not be part of this. You have to specify that this should check in airflow/ and task_sdk/ and not providers/.\r\nYou also need to remove all usage of session.query for this to pass. Sorry for this late reply

Sorry I have to close this PR 

> Can you provide more context on why you are doing this change, in the description?\r\n\r\nUpdated @phanikumv, I missed it.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Not specific to this PR, are patches to v2-10-test at this point going into a 2.10 patch release, or 2.11.0?

@eladkal I think we need to add `ignore::RemovedInAirflow3Warning` in [pyproject.py](https://github.com/apache/airflow/blob/f01c53a73573a5dacb2107944d32a5fd731d64f6/pyproject.toml#L468) for the tests to pass. 

Closed in favor of https://github.com/apache/airflow/pull/47146

> get_connection isn\

> > get_connection isn\

> Out of interest how long does this new test take? It\

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

### Backport failed to create: v2-10-test. View the failure log <a href=\

Closing after merging https://github.com/apache/airflow/pull/45723

tldr core + simple auth manager should not require flask to be installed at all.\r\n\r\nThat ship has already sailed if you are using the fab auth manager though, so might as well use the middleware and not rewrite stuff.\r\n\r\n

> tldr core + simple auth manager should not require flask to be installed at all.\r\n> \r\n> That ship has already sailed if you are using the fab auth manager though, so might as well use the middleware and not rewrite stuff.\r\n\r\nYep, this is what I did in #45765 for FAB :)

Any other concerns/questions?

Question for the audience :)\r\n\r\nI am looking into updating the basic tests to include tests from the simple auth manager UI as part of the [tests-ui](https://github.com/apache/airflow/blob/main/.github/workflows/basic-tests.yml#L87). Some caching is done that I am not familiar with. Do I need to do the same or just\r\n\r\n```\r\n- run: cd airflow/auth/managers/simple/ui && pnpm install --frozen-lockfile\r\n- run: cd airflow/auth/managers/simple/ui && pnpm test\r\n  env:\r\n    FORCE_COLOR: 2\r\n```

> > Any other concerns/questions?\r\n> \r\n> No, thanks for answering my questions.\r\n> \r\n> Overall code looks good and I agree with the general approach.\r\n> \r\n> I think Brent is already working on a detailed review and functional testing.\r\n\r\nFantastic! Thank you :)

> Question for the audience :)\r\n\r\n> I am looking into updating the basic tests to include tests from the simple auth manager UI as part of the [tests-ui](https://github.com/apache/airflow/blob/main/.github/workflows/basic-tests.yml#L87). Some caching is done that I am not familiar with. Do I need to do the same or just\r\n\r\n\r\nI am not super familiar with this workflow but by the look of it I would say that caching the node_modules is always helpful. I think we can just follow the same logic and apply that to the `airflow/auth/managers/simple/ui/node_modules` folder.\r\n(basically duplicating the Restore and Save cache steps before and after the `pnpm install`)

~~I dont understand what is going on ... The command `breeze release-management prepare-airflow-package --package-format wheel` fails because the pre-commit `Compile ui assets (manual)` fails because of `sh: 1: vite: not found`. See [error](https://github.com/apache/airflow/actions/runs/12916300336/job/36020735118?pr=45696).~~\r\n\r\n~~However when I run the script `scripts/ci/pre_commit/compile_ui_assets.py` locally, it works like a charm. `vite` is listed in `package.json` so I really dont understand why it cannot find it~~\r\n\r\nEDIT: I found it

Cool :)

> Looks good, but we likely need a compat import shim for `airflow/utils/dag_parsing_context.py` -- do we have a place to capture those anywhere.\r\n> \r\n> Also cc @Lee-W @uranusjr Another deprecated import to track in Ruff please.\r\n\r\nGood point, Yeah, https://github.com/apache/airflow/issues/41641\r\n\r\nAdded newsfragment too

ruff PR created https://github.com/astral-sh/ruff/pull/15525

Prepare package failed due to a node tls timeout

> That Bedrock one was on me. I thought I was just saving a bunch of repetition and didn\

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/45826"><img src="https://img.shields.io/badge/PR-45826-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

BTW. There was no need to backport this one - provider tests are skipped on v2-10-test.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

#protm

The list moved to the top.\r\n

A bit of "magic" scripting and I generated list of PRs + authors in a checklist form - this should make it easier to know who should review what and keep track of it.

> A bit of "magic" scripting and I generated list of PRs + authors in a checklist form - this should make it easier to know who should review what and keep track of it.\r\n\r\nSuper üòç

reviewed mine and marked them in the list 

@Lee-W -> If you want - you can move the list up from the comment I made to the description of the PR - to keep it at the top.

> A bit of "magic" scripting and I generated list of PRs + authors in a checklist form - this should make it easier to know who should review what and keep track of it.\r\n\r\nyep, it\

Thanks @Lee-W. I have reviewed my changes newsfragments/44533.significant.rst. LGTM!

I\

> I\

Thank you for all the effort! I have reviewed mine. It looks good. 

Actually better fix will be to link to the context class(you can see how such linka aeedone in other places) rather than link to URL. This way we will get an error whene context class is again moved 

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> @o-nikolas @vincbeck @eladkal\r\n> \r\n> Anything I\

> > @o-nikolas @vincbeck @eladkal\r\n> > Anything I\

> > > @o-nikolas @vincbeck @eladkal\r\n> > > Anything I\

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

@o-nikolas @vincbeck \r\n\r\nAny idea when a new release of [apache-airflow-providers-amazon](https://pypi.org/project/apache-airflow-providers-amazon/) will be made? Looking forward to using this.

Usually releases are done every 2 weeks but @eladkal should be able to give you more details about the next wave.

@eladkal Any chance of a release this week? Thanks.

@eladkal @vincbeck Any plans for making a release soon?

cc: @assignUser

Nice :) 

It does :) 

Ah no .. we need to add `full-tests-needed` - this one is not properly recognized in selective checks as triggering the tests for docker compose.

cc @potiuk any comments on this one? If not happy to merge it

@dabla can you rebase and resolve conflicts? I will merge after

> @dabla can you rebase and resolve conflicts? I will merge after\r\n\r\njust rebased and merged with main

@joellabes another MR related to dbt Cloud Operator from our side - could you have a look if you are fine with the approach?

Hi @ginone , great proposal and change. We use in our Airflow environment an own helper and retrieve job_ids by names. We figure out that in our case the payload from dbt Cloud REST API is quite big to retrieve it  for every dbt cloud job triggering. To reduce payload size we additionally filter rest api calls by filtering project name in API call, for example: \r\n project_url = (\r\n        f"{base_url_v3}accounts/{dbt_cloud_account_id}/projects/"\r\n        f"?account_id={dbt_cloud_account_id}"\r\n        **f"&name__icontains={project_name}**"\r\n    )\r\n\r\nor\r\njob_url = (\r\n        f"{base_url_v2}accounts/{dbt_cloud_account_id}/jobs/"\r\n        f"?account_id={dbt_cloud_account_id}"\r\n        f"&project_id={project_id}"\r\n        **f"&name__icontains={job_name}"**\r\n    )\r\nWhat do you think about such improvement?

@josh-fell @Lee-W thank you for the code review, I\

> @josh-fell @Lee-W thank you for the code review, I\

> @josh-fell @Lee-W thank you for the code review, I\

@Lee-W rebasing is done, please have a look again. Thanks!

> Looks good to me. Thanks for your prompt reply. I\

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

cc: @o-nikolas 

Hi @o-nikolas,  \nI ran the tests locally with Breeze, and they worked fine. However, the CI is failing mostly due to "Unknown executor being loaded: <executor name>" errors. Do you have any idea where I should look into this? (Could it be resolved by adding some additional configuration for Breeze tests or something similar?)  \n\nThanks!  

Likely this is a side effect of some other test that does not clean-up after itself - you can repeat what CI is doing - i.e. run the `Core` test type:\r\n\r\n```\r\nbreeze testing core-tests --test-type Core\r\n```\r\n\r\nAnd they should run the tests in the same sequence the "Core" tests are run in CI in your PR (`Core` test type is the one that is failing. \r\n\r\nThe problem is that some of the earlier tests likely modify a state stored in memory (likely list of executors stored in memory) and do not restore it properly - and the failing tests do not set up the expected state properly.\r\n\r\nNow - it\

Thanks, @potiuk, for pointing out that the error might be caused by side effects from other tests and for providing the detailed guide to resolve it! I will try using the bisect approach to identify which test case is causing the side effect.  \r\n\r\nUpdate:\r\nLuckily, I found that it was caused by the `tests/ti_deps/` module after a few iterations!  \r\n> `tests/ti_deps/deps/test_ready_to_reschedule_dep.py` 

Resolved the issue, and the CI has successfully passed! üéâ\r\ncc @o-nikolas 

> Resolved the issue, and the CI has successfully passed! üéâ cc @o-nikolas\r\n\r\nNice! LGTM, @Lee-W Has your feedback been addressed to your satisfaction? 

Covers issue #45630 

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> The fact that we didn\

Mypy is seriously unhappy. Oh well

![](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExMGNmNWRueGljc3Zic3pkMjdzMGticmZ0NG9iZHcyNzJuNjc5a2N5ZSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/145ttXPiU8q6E0286z/giphy.gif)\r\n\r\nüéâ üéâ üéâ 

#protm

Moved to https://github.com/apache/airflow/pull/45660 to avoid merge commits

That backport should fix pushing image cache in `v2-10-test` that is currently failing

Failure here: https://github.com/apache/airflow/actions/runs/12742913151/job/35513738172

Closing

@pierrejeambrun request your review on this

But it looks like we need a rebase

Rebased against main which had changes to markdown rendering components.

At this moment canary builds are failing in v2-10-test because of that.

https://github.com/apache/airflow/actions/runs/12737573988/job/35498983821

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

One more place where we had `workflow_run` rather than `pull_request_target` :)

I think this should be as simple as that one line change @shahar1 :)

With some unit tests of course :)\r\n\r\nLet\

OK. One more place where 3.8 had to be changed to 3.9 - we have static checks using it. Also some docs - just in case and error messages pointing to build 3.9 shoudl be updated. Will make @jscheffl 3.8-3.9 PR smaller :)

OK. Rebased it after merging 3.9 as default - but I guess it might fail for Python 3.8 until the conditional sphinx /docutils for Python 3.9+

HURRAY! FIRST STEP DONE :) 

Wrong one :)

This one synchronizes "pull_request_target" workflow changes with tip of the `providers-fab/v1-5` branch

Same story as for `v2-10-test` -> that `providers-fab/v1-5` branch also still has `pull-request-target` workflow and it needs to be removed.

> Same as previous PR - but hoping that not (many/any) patch cycles on this branch are needed.\r\n\r\nYeah. That one should be WAY easier to get green.

We need to merge v2-10-test (with fast-forward) now - in order to remove last "pull_request_target" workflow from v2-10-stable. @kaxil @utkarsharma2 @jscheffl -> This means that we need to merge stable before preparing 2.10.5 or 2.11.0 -> so that might change a bit the process of releasing - but for security reasons, we should do it now.

I think our changelog preparation will work anyway - because it is based on tags not branches, so it should work same way as usual (and we can fix it if not).

I have currently utilised the Run status colour which closely aligns with the pool slot options. Reason to that is, run status colours are used all over the UI from a single source of truth(airflow/ui/src/utils/stateColor.ts). Just to not confuse users I kept the same colours of that.\r\n\r\nThe stateColor currently used are:\r\n```\r\nexport const stateColor = {\r\n  deferred: "mediumpurple",\r\n  failed: "red",\r\n  null: "lightblue",\r\n  queued: "gray",\r\n  removed: "lightgrey",\r\n  restarting: "violet",\r\n  running: "lime",\r\n  scheduled: "tan",\r\n  skipped: "hotpink",\r\n  success: "green",\r\n  up_for_reschedule: "turquoise",\r\n  up_for_retry: "gold",\r\n  upstream_failed: "orange",\r\n};\r\n```\r\nOne advantage of this would be when users will personalise the status colours of runs (https://airflow.apache.org/docs/apache-airflow/1.10.13/howto/customize-state-colors-ui.html) these values will also change.\r\n\r\nIf we should have a fixed colour, we can  use the below options: (Open to discussion)\r\n```\r\nOpen Slots\t     Green\r\nScheduled Slots\t     Gray\t\r\nRunning Slots\t     Blue\t\r\nQueued Slots\t     Orange\r\nOccupied Slots\t     Yellow\t\r\nDeferred Slots\t     Red\t\r\n```\r\n\r\nWhich would look like\r\n<img width="1725" alt="Screenshot 2025-01-12 at 4 01 20\u202fPM" src="https://github.com/user-attachments/assets/5fda2655-45d7-48ce-86b7-60f92620787e" />

@bbovenzi Also, regarding slots icons, let me think of something, will try something COOL in upcoming PRs!

Random K8s issues 

### Backport failed to create: v2-10-test. View the failure log <a href=\

Even after removing the condition, validations are happening. 

@romsharon98  @amoghrajesh gentle ping :)

@romsharon98 @amoghrajesh gentle ping :)

hi @eladkal , could you pls review , thanks

Yup, issue is unrelated, fix is #45572.\r\n\r\nMerging this

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/45569"><img src="https://img.shields.io/badge/PR-45569-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

This was broken in #45546.

Failures are unrelated and will be fixed in #45567.

Drafting, weirdly on `v2-10-test` it worked fine but here there are errors

one more thing - last time when we tried to upgrade sphinx and docutils, the documentation looked ....weird .... lots of extra whitespace

#protm 

It needs conflict resolution though :) 

Merge conflicts on `hatch_build.py` @shahar1 and then hopefully we are good ü§û 

LGTM ! üéâ üéâ üéâ üéâ üéâ üéâ üéâ üéâ üéâ 

### Backport failed to create: v2-10-test. View the failure log <a href=\

I addressed your comments but I also added something new. It was actually a suggestion from @dstandish that I think is a great idea! I introduced the concept of `AssetWatcher` that is a thin wrapper around trigger. Explanations:\r\n\r\nInstead of having:\r\n\r\n```\r\ntrigger = FileTrigger(....)\r\nasset = Asset("<my_queue>", watchers=[trigger])\r\n```\r\n\r\nNow it is:\r\n\r\n```\r\ntrigger = FileTrigger(....)\r\nasset = Asset("<my_queue>", watchers=[AssetWatcher(name="my_queue_watcher", trigger=trigger)])\r\n```\r\n\r\n`AssetWatcher` does not add much besides just a name, but I think it is important because, in the future, we might want to surface the relations between assets and triggers in the graph view in the UI. Representing a trigger in the graph can be tricky because triggers have 2 pieces of information: `classpath` and `kwargs`. None of them are very suitable for displaying purposes. `classpath` is debatable but it is anyway not representable of the trigger. Example: you might have an asset with 2 watchers: each one uses the trigger `FileTrigger` to monitor a different file. Having only the classpath displayed will not help the user to understand which one is which. Hence the name. 

@uranusjr @ashb what do you guys think? 

Other questions/concerns? :)

All good, thanks @vincbeck 

Some tests are failing though :( 

Thanks

I‚Äôm on holidays this week.\r\n\r\nill try to find time to add the mentionned commit and update the PR.\r\n\r\nFeel free to update as needed.

Updated

CI is looking better. Thanks.

Nice!

Ah I see 

Just back to home 

> This is bad idea. You are about to reveal keyfile_dict which contains private key in the log.\r\n\r\nGood point! There definitely was an another way to improve. The code is updated

> Could you add a unit test to cover that?\r\n\r\n@ferruzzi  @vincbeck  We have updated the prior unit tests and added two of our own, the provider tests all pass.\r\n\r\nJust a small note the previous mock for time_ns was using an incorrect format that the time_ns() function would not return.\r\nWe updated the unit tests to use a representative integer for the tests.\r\n\r\nIn our updated function we limit the returned timestamp to 10 characters to capture only the date time up to the seconds as this is all that is needed.

@dirkrkotzeml @eladkal @o-nikolas \r\nUnit tests passing after latest commit.\r\n\r\n<img width="852" alt="Screenshot 2025-01-14 at 15 07 12" src="https://github.com/user-attachments/assets/5f1d48fb-0311-4288-901e-b5ca8ae47b54" />\r\n

Looks like you just need to run your static checks.  running  `breeze static-checks`  should fix the formatting issues that the CI is crying about.

> Looks like you just need to run your static checks. running `breeze static-checks` should fix the formatting issues that the CI is crying about.\r\n\r\nRan the checks now and pushed changes. Thanks @ferruzzi 

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Cool :) 

> NIce.\r\n> \r\n> Small nits before merging, but looking good.\r\n\r\nThank you for the suggestion. Please check the latest commit.

### Backport failed to create: v2-10-test. View the failure log <a href=\

cherry pick PR here:\r\nhttps://github.com/apache/airflow/pull/45560

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/45552"><img src="https://img.shields.io/badge/PR-45552-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

I built and pushed the image, running the tests to make sure it works in our K8S tests\r\n

Thanks for creating the PR @goktugkose 

Random unrelated test failing. Merging. Thanks for the PR.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

By having a separate subclass for execution we get rid of the need to have `if TYPE_CEHCKING: assert self.client` which feels nicer

Could you run `pnpm lint && pnpm format` locally? Looks like static checks are failing.

I have rebased my branch to the most updated `main` branch. Thank you for your time and guidance! 

Added some docs as I thought about that slightly the behavior changes - to ensure it is properly documented.

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/45581"><img src="https://img.shields.io/badge/PR-45581-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

> just notice this could potentially be a breaking change, but doesn\

> just notice this could potentially be a breaking change, but doesn\

Added as suggested. I am not familiar with code standards, so I was trying to minimize my "invasion".\n\nThank you!

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Hey here. I know this one is huge and difficult to review, but this was the easiest way I could bring the  "pull_request_target" removal to `v2-10-test`  branch. \r\n\r\nSince we stopped cherry-picking breeze changes to `v2-10-test` and made a LOT of chenges in main (removing Python 3.8, moving providers, adding test_sdk and so on -  cherry-picking individual commits was not an option. So I choose a different path - I copuied the latest `breeze`, `ci_scripts`, `Dockerfiles` and `.pre-commits` and adapted them to`v2-10-test` - mostly removing stuff that was not needed in v2-10-test (providers, charts, new api etc. etc., adding back Python 3.8). \r\n\r\nAll other changes were results of fixing the tests.\r\n\r\nI think the easiest way to review it is two-fold:\r\n\r\n1) you can compare all the breeze/ci stuff with `main` - and see the differences (mostly removals of the things above) \r\n2) then you can compare "airflow" and "tests" with `v2-10-test` and see that they only changed to accomodate to some tests scripts changes. \r\n\r\nI know I am asking a lot, but this is the easiest way we can remove last remnants of `pull_request_target" - which is still a potential security issue.

Yeah - some test failure fixes needed :( 

All problems solved. I also applied latest version of sphinx-theme limit and compared produced .whl files for airflow and the differences are pretty much as expected:\r\n\r\n> diff ./old ./new -r\r\n\r\n```diff\r\ndiff -r ./old/airflow/api/common/mark_tasks.py ./new/airflow/api/common/mark_tasks.py\r\n413a414,417\r\n>\r\n>     # Mark all task instances of the dag run to success - except for teardown as they need to complete work.\r\n>     normal_tasks = [task for task in dag.tasks if not task.is_teardown]\r\n>\r\n415c419\r\n<     if commit:\r\n---\r\n>     if commit and len(normal_tasks) == len(dag.tasks):\r\n418,419c422\r\n<     # Mark all task instances of the dag run to success.\r\n<     for task in dag.tasks:\r\n---\r\n>     for task in normal_tasks:\r\n422c425\r\n<         tasks=dag.tasks,\r\n---\r\n>         tasks=normal_tasks,\r\n469,472d471\r\n<     # Mark the dag run to failed.\r\n<     if commit:\r\n<         _set_dag_run_state(dag.dag_id, run_id, DagRunState.FAILED, session)\r\n<\r\n481c480\r\n<     tis = session.scalars(\r\n---\r\n>     running_tis: list[TaskInstance] = session.scalars(\r\n488c487\r\n<     )\r\n---\r\n>     ).all()\r\n490c489,490\r\n<     task_ids_of_running_tis = [task_instance.task_id for task_instance in tis]\r\n---\r\n>     # Do not kill teardown tasks\r\n>     task_ids_of_running_tis = [ti.task_id for ti in running_tis if not dag.task_dict[ti.task_id].is_teardown]\r\n492c492\r\n<     tasks = []\r\n---\r\n>     running_tasks = []\r\n496c496\r\n<             tasks.append(task)\r\n---\r\n>             running_tasks.append(task)\r\n499c499\r\n<     tis = session.scalars(\r\n---\r\n>     pending_tis: list[TaskInstance] = session.scalars(\r\n512a513,515\r\n>     # Do not skip teardown tasks\r\n>     pending_normal_tis = [ti for ti in pending_tis if not dag.task_dict[ti.task_id].is_teardown]\r\n>\r\n514c517\r\n<         for ti in tis:\r\n---\r\n>         for ti in pending_normal_tis:\r\n517,518c520,525\r\n<     return tis + set_state(\r\n<         tasks=tasks,\r\n---\r\n>         # Mark the dag run to failed if there is no pending teardown (else this would not be scheduled later).\r\n>         if not any(dag.task_dict[ti.task_id].is_teardown for ti in (running_tis + pending_tis)):\r\n>             _set_dag_run_state(dag.dag_id, run_id, DagRunState.FAILED, session)\r\n>\r\n>     return pending_normal_tis + set_state(\r\n>         tasks=running_tasks,\r\ndiff -r ./old/airflow/api_connexion/openapi/v1.yaml ./new/airflow/api_connexion/openapi/v1.yaml\r\n5735a5736\r\n>         format: path\r\ndiff -r ./old/airflow/cli/cli_config.py ./new/airflow/cli/cli_config.py\r\n67c67\r\n<         """Override error and use print_instead of print_usage."""\r\n---\r\n>         """Override error and use print_help instead of print_usage."""\r\ndiff -r ./old/airflow/datasets/metadata.py ./new/airflow/datasets/metadata.py\r\n19a20\r\n> import warnings\r\n40a42,50\r\n>         if isinstance(target, str):\r\n>             warnings.warn(\r\n>                 (\r\n>                     "Accessing outlet_events using string is deprecated and will be removed in Airflow 3. "\r\n>                     "Please use the Dataset or DatasetAlias object (renamed as Asset and AssetAlias in Airflow 3) directly"\r\n>                 ),\r\n>                 DeprecationWarning,\r\n>                 stacklevel=2,\r\n>             )\r\n45a56,63\r\n>             warnings.warn(\r\n>                 (\r\n>                     "Emitting dataset events using string is deprecated and will be removed in Airflow 3. "\r\n>                     "Please use the Dataset object (renamed as Asset in Airflow 3) directly"\r\n>                 ),\r\n>                 DeprecationWarning,\r\n>                 stacklevel=2,\r\n>             )\r\ndiff -r ./old/airflow/executors/executor_loader.py ./new/airflow/executors/executor_loader.py\r\n340c340\r\n<         if engine.dialect.name == "sqlite":\r\n---\r\n>         if engine and engine.dialect.name == "sqlite":\r\ndiff -r ./old/airflow/git_version ./new/airflow/git_version\r\n1c1\r\n< .release:c083e456fa02c6cb32cdbe0c9ed3c3b2380beccd\r\n\\ No newline at end of file\r\n---\r\n> .release:a9fe36219cd537af06708c9ed2efba86e3449f81\r\n\\ No newline at end of file\r\ndiff -r ./old/airflow/models/baseoperator.py ./new/airflow/models/baseoperator.py\r\n968d967\r\n<         validate_key(task_id)\r\n973a973,975\r\n>\r\n>         validate_key(self.task_id)\r\n>\r\ndiff -r ./old/airflow/models/mappedoperator.py ./new/airflow/models/mappedoperator.py\r\n832a833,834\r\n>             op.downstream_task_ids = self.downstream_task_ids\r\n>             op.upstream_task_ids = self.upstream_task_ids\r\ndiff -r ./old/airflow/models/skipmixin.py ./new/airflow/models/skipmixin.py\r\n164,165d163\r\n<         SkipMixin._set_state_to_skipped(dag_run, task_ids_list, session)\r\n<         session.commit()\r\n166a165,169\r\n>         # The following could be applied only for non-mapped tasks\r\n>         if map_index == -1:\r\n>             SkipMixin._set_state_to_skipped(dag_run, task_ids_list, session)\r\n>             session.commit()\r\n>\r\n179a183\r\n>     @staticmethod\r\n181d184\r\n<         self,\r\ndiff -r ./old/airflow/models/taskinstance.py ./new/airflow/models/taskinstance.py\r\n28a29\r\n> import traceback\r\n249c250\r\n<         TaskInstance.save_to_db(ti=ti, session=session)\r\n---\r\n>         TaskInstance.save_to_db(ti=ti, session=session, refresh_dag=False)\r\n1243c1244\r\n<         TaskInstance.save_to_db(failure_context["ti"], session)\r\n---\r\n>         TaskInstance.save_to_db(task_instance, session)\r\n3093a3095\r\n>             self.log.error("Stacktrace: \\n%s", "".join(traceback.format_stack()))\r\n3396c3398,3402\r\n<     def save_to_db(ti: TaskInstance | TaskInstancePydantic, session: Session = NEW_SESSION):\r\n---\r\n>     def save_to_db(\r\n>         ti: TaskInstance | TaskInstancePydantic, session: Session = NEW_SESSION, refresh_dag: bool = True\r\n>     ):\r\n>         if refresh_dag and isinstance(ti, TaskInstance):\r\n>             ti.get_dagrun().refresh_from_db()\r\ndiff -r ./old/airflow/providers_manager.py ./new/airflow/providers_manager.py\r\n534d533\r\n<     @provider_info_cache("hook_lineage_writers")\r\ndiff -r ./old/airflow/reproducible_build.yaml ./new/airflow/reproducible_build.yaml\r\n1,2c1,2\r\n< release-notes-hash: 0867869dba7304e7ead28dd0800c5c4b\r\n< source-date-epoch: 1733822937\r\n---\r\n> release-notes-hash: 7be47e2ddbbe1bfbd0d3f572d2b7800a\r\n> source-date-epoch: 1736532824\r\ndiff -r ./old/airflow/sensors/base.py ./new/airflow/sensors/base.py\r\n109c109,112\r\n<             TaskReschedule.try_number == try_number,\r\n---\r\n>             # If the first try\

@Ohashiro really nice work here well done!

@Ohashiro If you have time to apply changes from comments, then I think you could remove the draft en we merge this PR.

Thanks for all your feedbacks and your review!\r\nBefore merging, I just wanted to test the behavior of the `xcom_push` for the key `powerbi_dataset_refresh_status`. I think that we push this message only when the refresh is successful but I think that we should also push when the refresh fails 

Hi @dabla I made a quick change to push the `powerbi_dataset_refresh_status` even when the refresh fails. Should I let you review this commit before setting the PR as "Ready for review"?

@Ohashiro do not forget to remove the draft option

Sorry, where is the "draft option"? I thought it was "ready" as is

> Sorry, where is the "draft option"? I thought it was "ready" as is\r\n\r\nIt should be somewhere at the bottom, I cannot see it unfortunately.  It\

I don\

> I don\

@Ohashiro Could you rename the title of the PR into something like:\r\n\r\n"Fixed retry of PowerBIDatasetRefreshOperator when dataset refresh wasn\

OK I understand, thank you very much!

> OK I understand, thank you very much!\r\n\r\nThank you @Ohashiro for fixing this issue.

Not too sure where to add the tests for this.

@amoghrajesh I merged this to tick off items :) -- but feel free to change anything in follow-up

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Nice. All was green !

Thanks a lot for that one @sloria !

@sloria thats awesome , thank you so much for making changes. :) 

Did a small "smoke test" - if you select a mapped task and close the grid, the selected task navigation throws an error:\r\n![image](https://github.com/user-attachments/assets/dde3a00b-57b6-40f6-a021-49cd7adb3941)\r\n

Also second had "strange" results in the example task group, when I selected tasks in a group my browser (Firefox, Ubuntu 24.04 LTS) was sorting the runs and selecting the first execution if I clicked on the task of second/third run:\r\n![image](https://github.com/user-attachments/assets/412ada86-fb8e-4734-9ac7-6f87068a5c01)\r\n

So, a user can always sort by the dropdown but on the table view can also click on the column headers?

I realized later that table header do exactly that. Closing.

> Try Number is fine as the column entry. But I think "Attempt" is a more intuitive name for users.\r\n\r\nTry number is very well received term. I suggest to consider the implications...\r\nI find it easier when the UI speaks the same language as the code. try number is something that all airflow users are aware of and familiar with - changing the terminology just in the ui might cause confusion.\r\n\r\nI don\

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Can you add it to other logGroomer we have, and add tests [here](https://github.com/apache/airflow/blob/03349014513114f1eaa413a9831b0027e4fbfa67/tests/charts/log_groomer.py)?

Do we still need this PR after https://github.com/apache/airflow/pull/46003 was merged?

> Do we need to make any changes to the new FastAPI endpoint?\r\n\r\nYes. Added the needed changes now

Closing this PR since I actually do not think this is needed. All these tests will be deleted when the Airflow 2 UI is deleted

Sorry for the failed attempts :-( Sometimes I hate spell-checking :-(

@jscheffl it was slightly late for me, I slept! Will check now. Thanks though

@jscheffl it took me a few tries to fix spellcheck too!\r\nIt was the wordlist that needed update:\r\n```\r\n"""\r\nThis module contains Azure Data Explorer hook.\r\n\r\n.. spelling:word-list::\r\n\r\n    KustoResponseDataSet\r\n    kusto\r\n"""\r\n****\r\n```

Nice :) 

Looks like some of the existing tests need updating first as well

Does this fix solve https://github.com/apache/airflow/issues/45368 ?

> Does this fix solve #45368 ?\r\n\r\nOh yeah! I did not know there was a ticket for it. I updated the description to mention it

I made some edits for clarity, Thanks for the quick response!\r\n\r\n---\r\nEdit let me edit/read your stackoverflow a bit more. 

So I may be confused a little as well. \r\n\r\nAre the DAG Params when rendered in SQL, eg my first example, properly SQL escaped? If so that does alleviate our concern.  The team is worried about a future state where SQL injection might be possible. My assumption is that they are not and its mostly just a string insertion. \r\n\r\nPart of what might be confusing me is the [SQLExecuteQueryOperator](https://airflow.apache.org/docs/apache-airflow-providers-common-sql/stable/_api/airflow/providers/common/sql/operators/sql/index.html#airflow.providers.common.sql.operators.sql.SQLExecuteQueryOperator.template_fields) does have parameters in the `template_fields` which is leading to some inconsistency. \r\n\r\nThe second example I wrote is using the parameters option to allow SQLAlchemy to render the `select * from foo where date > %s` as a parameterized SQL statement passing in the values of `{{ params.date }}` as an argument instead of directly inline with the query. 

> Part of what might be confusing me is the [SQLExecuteQueryOperator](https://airflow.apache.org/docs/apache-airflow-providers-common-sql/stable/_api/airflow/providers/common/sql/operators/sql/index.html#airflow.providers.common.sql.operators.sql.SQLExecuteQueryOperator.template_fields) does have parameters in the template_fields which is leading to some inconsistency.\r\n\r\nTo my perspective it\

I\

> > Part of what might be confusing me is the [SQLExecuteQueryOperator](https://airflow.apache.org/docs/apache-airflow-providers-common-sql/stable/_api/airflow/providers/common/sql/operators/sql/index.html#airflow.providers.common.sql.operators.sql.SQLExecuteQueryOperator.template_fields) does have parameters in the template_fields which is leading to some inconsistency.\r\n> \r\n> To my perspective it\

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Thanks @eladkal 

This allows me to test something like:\r\n```\r\nfrom __future__ import annotations\r\n\r\nfrom airflow.models.baseoperator import BaseOperator\r\nfrom airflow.sdk import dag\r\nfrom airflow.sdk import Variable\r\n\r\n\r\nclass CustomOperator(BaseOperator):\r\n    def execute(self, context):\r\n        value = Variable.get(key="my_var")\r\n        print(f"The variable value is: {value}")\r\n\r\n\r\n@dag()\r\ndef var_from_defn():\r\n    CustomOperator(task_id="hello")\r\n\r\n\r\nvar_from_defn()\r\n\r\n```\r\n\r\n\r\nAdvantage is that now this can be used at task level as well at DAG parsing level.\r\n\r\nThe PR is pre mature, will add edge cases etc once we are OK with the general direction.

Tested it out, and yes that won\

New update:\r\n\r\nWhy don\

Interesting that I cannot reproduce the failures locally.

Closing this one in favour of https://github.com/apache/airflow/pull/46869

@potiuk @jscheffl As promised a document describing the dialects in common sql provider.  If any remarks or improvements are necessary please let me know.

> Where should I put those user facing docs? Is that under docs/apache-airflow-providers-common-sql? Should I create a new dialects.rst document?\r\n\r\nYes. You dont have to create new .rst \r\nDepends on the scope of what you write. If its 1-2 pargraphs it might fit in the current rst we have.\r\nHowever you may want to mention in the providers we have dedicated dialect how to use it and link to the doc in common.sql so there may be need for doc changea in several providers 

@dabla do you intend to add the docs in this PR?

> @dabla do you intend to add the docs in this PR?\r\n\r\nI just [committed ](https://github.com/apache/airflow/pull/45456/commits/3b78ec318097f90f03d65ea3ebe5d0ae1a3a8e5f) a new dialects.rst and adapted the index.rst this morning, what do you think about it?

> Sorry, "late to the party" now digging through my backlog... wanted to review earlier.\r\n> \r\n> I really like this and the description.\r\n> \r\n> For the docs build problem I see two options: (1) [not sure if this really works] outside the TOC create a small RST just with the heading and pointing to the other doctree - or (2) if you want to have exactly the same doc in each provider then use a symlink in GIT such that the RST is maintained in one place and each doctree has the same. (As long as we not move the providers the symlink is working in Sphinx)\r\n> \r\n> Can you add some example the the `dialects.rst` or is this already contained in the implementation PR?\r\n\r\nWill look into it.  I also opened another [PR](https://github.com/apache/airflow/pull/45640) related to this which fixes an issue with reserved words and special characters using the dialects.

> Sorry, "late to the party" now digging through my backlog... wanted to review earlier.\r\n> \r\n> I really like this and the description.\r\n> \r\n> For the docs build problem I see two options: (1) [not sure if this really works] outside the TOC create a small RST just with the heading and pointing to the other doctree - or (2) if you want to have exactly the same doc in each provider then use a symlink in GIT such that the RST is maintained in one place and each doctree has the same. (As long as we not move the providers the symlink is working in Sphinx)\r\n> \r\n> Can you add some example the the `dialects.rst` or is this already contained in the implementation PR?\r\n\r\nNot that found of symlinks, dunno how git will handle those.  Aren\

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Of course, we can definitely update that if this feels more natural.\r\n\r\n\r\n> Note to myself: Add an entry to the breaking change list when merging that.

You will need to bump HTTP provider version to new minor and make the other providers depend on it, otherwise someone can install old version of HTTP provider and new version of (say) livy and it wil not work.

> You will need to bump HTTP provider version to new minor and make the other providers depend on it, otherwise someone can install old version of HTTP provider and new version of (say) livy and it wil not work.\r\n\r\nOk thx @potiuk  bumped the version of the http, livy and dingding provider and made them dependent on version 5.0.1 of http provider.

> You will need to bump HTTP provider version to new minor and make the other providers depend on it, otherwise someone can install old version of HTTP provider and new version of (say) livy and it wil not work.\r\n\r\nI\

I think something went wrong with the branch. Maybe a bad rebase or something, can you check and clean the branch please ?

Note on the [error message](https://github.com/apache/airflow/actions/runs/13155177849/job/36710627700?pr=45451#step:10:198) in the CI:\r\n\r\n> There is a need to regenerate /home/runner/work/airflow/airflow/generated/provider_dependencies.json\r\n> You need to run the following command locally and commit generated generated/provider_dependencies.json file:\r\n> \r\n> breeze static-checks --type update-providers-dependencies --all-files\r\n> \r\n\r\n\r\nThere should be changes also to the `provider_dependencies.json` which breeze do automatically and it should be committed as part of your PR.

Tests are failing\r\n```\r\nFAILED providers/http/tests/provider_tests/http/sensors/test_http.py::TestHttpOpSensor::test_get - airflow.exceptions.AirflowException: 404:NOT FOUND\r\nFAILED providers/http/tests/provider_tests/http/sensors/test_http.py::TestHttpOpSensor::test_get_response_check - airflow.exceptions.AirflowException: 404:NOT FOUND\r\nFAILED providers/http/tests/provider_tests/http/sensors/test_http.py::TestHttpOpSensor::test_sensor - airflow.exceptions.AirflowSensorTimeout: Sensor has timed out; run duration of 15.730223178863525 seconds exceeds the specified timeout of 15.0.\r\n```

We better have the field as boolean rather than define it as json in extra.\r\nYou can see Snowflake connection (`insecure_mode`)for reference\r\nhttps://github.com/apache/airflow/blob/7e9b9edd76a9535fd6137ab33142f2c735eb4d4b/providers/snowflake/src/airflow/providers/snowflake/hooks/snowflake.py#L202

> We better have the field as boolean rather than define it as json in extra. You can see Snowflake connection (`insecure_mode`)for reference\r\n> \r\n> https://github.com/apache/airflow/blob/7e9b9edd76a9535fd6137ab33142f2c735eb4d4b/providers/snowflake/src/airflow/providers/snowflake/hooks/snowflake.py#L202\r\n\r\nAlright good catch @eladkal will adapt that thx üòÉ 

The failure was due to a timeout. Retriggering

The failure seems to be due to flakiness, retriggered.

Nice!

> The issue is that marshmallow-sqlalchemy (a dependency of Flask-AppBuilder) needs to be upgraded to version 1.1.1 to work correctly with marshmallow. So I think this should be fixed by relaxing the version constraint here: https://github.com/dpgaspar/Flask-AppBuilder/blob/418ab8a93907669be4ccbb99d7aefa5283f3e013/setup.py#L61\r\n\r\nNice find . Thanks for following it up!

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Unrelated failure. Merging.

Nice! Thanks @raboof !

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/45418"><img src="https://img.shields.io/badge/PR-45418-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

Closing - the fix aimed for 2.19.1 seems to work.

Should fix docs-build in `canary` 

FYI: The issue is scheduled to be fixed in 2.19.1

closing in favour of https://github.com/apache/airflow/pull/45410

Merging now, one test failed not related to this, it seems test related to auth manager. will look into separate.

> Good catch! Our CI seems to be full of surprises the more you look into it :-D\r\n\r\nYeah good that the skipped jobs are visible in UI :) 

> Good catch! Our CI seems to be full of surprises the more you look into it :-D\r\n\r\nIt is (or should be) very simple - we should strongly avoid complex conditions in the yaml files - and do all the logic calculation in selective_checks - they are unit testable and "python"  :).

going to run for all combinations just to make sure syntax are working :) 

Oops will update all the quotes :) 

> Nice one!\r\n\r\nIndeed :)

Small cleanup of unused functionality 

> Looks good! Small suggestions for improvements :)\r\n\r\nThanks for the review! Applied the changes

Do you mean for the text wrap in the dialog? @pierrejeambrun 

Oh, another leftover I seem to have missed when deprecating Python 3.8 :-D

> Oh, another leftover I seem to have missed when deprecating Python 3.8 :-D\r\n\r\nWe all did :) 

This should help: https://github.com/apache/airflow/pull/45399\r\n\r\nAlso follow-up #45398 - where pip, uv, pre-commit and pre-commit-uv should be updated everywhere automatically with pre-commit.

BTW nice refactoring and updates :) thanks jarek.

> This should help: #45399\r\n> \r\n> Also follow-up #45398 - where pip, uv, pre-commit and pre-commit-uv should be updated everywhere automatically with pre-commit.\r\n\r\nRecursive follow-up? :)

> Recursive follow-up? :)\r\n\r\nFollow up on folllow up on follow up on the follow-up before... ü™Ñ 

Nice!

Both of your PRs conflicts in the source tree, can you resolve this?

> Both of your PRs conflicts in the source tree, can you resolve this?\r\n\r\nResolved! Thanks for the review!  

One comment. I think we have one or two places where uv is not bumped automatically by precommit (I marked them with `TODO(potiuk)` - maybe worth finding it now and updating/reapplying the pre-commit?

> One comment. I think we have one or two places where uv is not bumped automatically by precommit (I marked them with `TODO(potiuk)` - maybe worth finding it now and updating/reapplying the pre-commit?\r\n\r\nyeah good call :) updated

accidental issue only :) 

All comments have been resolved. Thanks, @pierrejeambrun, for the code review!  

Rebase to latest main.

Hi @pierrejeambrun , I just fixed the broken test due to breaking change on `list_py_file__paths`.\r\nThe CI failure is caused by `CodeQL`.

Code QL seems happy üéâ 

> Code QL seems happy üéâ\r\n\r\nFinally passed all CI! Thanks, @pierrejeambrun, for the review!  \r\n

Thanks @jscheffl , Nice catch!\r\nI will take care of this while implementing the bulk delete action.

> Was the deprecation discussed anywhere\r\n\r\nThe PR to move `InventoryFileReader` is here: https://github.com/sphinx-doc/sphinx/pull/13215\r\n\r\nThis is preparatory work for a future v3 of the objects.inv file format.\r\n\r\nA

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/45538"><img src="https://img.shields.io/badge/PR-45538-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

Used `ruff check --fix --select PT006 --unsafe-fixes` to fix all the PT006 failures. But, also looking into what are the "unsafe" cases and update the PR accordingly.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

BTW. We could maybe add some pre-commit verifications for those. Why it will not work in general case we could agree on certain concentions we. Use and add verification if those conventions are followed 

got it!

Oh, there are more problems with curent code on main. Seems we need to revert the PR https://github.com/apache/airflow/pull/45273 and make it proper again...

There was a bug in CI and checks were skipped :-(

Ohh, we need to have it fix, do we have any issue number to track that?

Now green!

Nice!

hi @potiuk @eladkal ! Can you please check this PR?

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

re: Breaking change, I don‚Äôt think we need to since `create_dagrun` is considered private interface. If anyone is using it, the breakage is on them.

> One small improvement for the test, but looking good.\r\n\r\nThanks for the review! 

Re-running failed job that seem unrelated. 

Weird, failure persists. Rebasing the branch. ü§û 

Failure is unrelated (see #45329).

Hey @ashb -> you wanted this pre-commit removed, so we had first case where after @gopidesupavan contributed the dependabot fix, we finally CAN remove it.

@dependabot rebase

Images were built, so we can safely merge it.

Look @gopidesupavan -> Your contribution to dependabot in action !

> Look @gopidesupavan -> Your contribution to dependabot in action !\r\n\r\nhaha Yeah :) 

@potiuk could you please check this PR?

There are some conflicts to be resolved and we are about to migrate Google provider to the new provider structure, so you will have to rebase it and adapt to the moved files.

Static checks failing.

hi @potiuk and @eladkal !\r\nCan you please check changes here? All the conflicts are resolved and i  think this change will help google provider to improve :)

Can you add a unit test there to avoid regressions?

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

I you plan to update the API, this one (connexion) is the legacy one which will be dropped in Airflow 3. Can you please also change the fast_api which will be the future/Airflow 3 API backend?\r\n\r\nAs this change is on connexion API, is this to be back-ported to Airflow 2.10 as well? Then it should be classified as a bug, as we actually do not plan to have new features on Airflow 2.10

+1 @pierrejeambrun @jscheffl . I agree as its a new feature/Improvement this should only go to FastAPI. I will update this for only FastAPI.\r\n>  I think this one is really an improvement/feature. I would be for simply not releasing this in 2.x at all. Features are for 3.0, and just do the change in the fastapi API.\r\n\r\n

Nice, about to get to this one while looking at #44533  :) 

> Let\

https://github.com/apache/airflow/pull/45347 is the solution

Created https://github.com/apache/airflow/issues/45355 - there are other issues that are not relevant here.

@uranusjr   - can you please rebase that one -> we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests. I am asking in all affected PRs to rebase.

@amoghrajesh - can you please rebase that one -> we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests. I am asking in all affected PRs to rebase.

> @amoghrajesh - can you please rebase that one -> we found and issue with @jscheffl with the new caching scheme - fixed in #45347 that would run "main" version of the tests. I am asking in all affected PRs to rebase.\r\n\r\nOh ok, let me rebase this one

Hi @uranusjr are you ok with the reply on this comment? https://github.com/apache/airflow/pull/45344#discussion_r1900781515

@romsharon98  - I rebased this one -> we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests. 

I don‚Äôt remember the details, but from the things mocked, it looks like we changed when we filter files/folders from a scan to check whether a file actually contains a DAG later in the manager, making the mocks now useless (because they are no longer called at this part of the code). All of the mocks simply forces the manager to treat a Python file as one containing a DAG and not drop it prematurely.

@jedcunningham  - can you please rebase that one -> we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests. I am asking in all affected PRs to rebase.

Messed with this a bit more, still not able to get the tests to fail. I say we go ahead with removing it.

@jedcunningham  - can you please rebase that one -> we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests. I am asking in all affected PRs to rebase.

Hey @jedcunningham -> the bug in main image upload (fixed by #45347) masked test failures in that one  - I just reverted it, so you will have to re-do it

Hmm, not sure those failures are related? Or am I looking in the wrong spot?\r\n\r\nhttps://github.com/apache/airflow/actions/runs/12579028168/job/35066133378

I rebased it @mohamedmeqlad99  -> we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests. 

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Not that I am sure what it does - but it looks good :)

@potiuk  So, basically before the fix, The search bar, add and import button were not fix and that gets disappeared when table is scrolled. This fix it by setting overflow. 

> @potiuk So, basically before the fix, The search bar, add and import button were not fix and that gets disappeared when table is scrolled. This fix it by setting overflow.\r\n\r\nYep. Got it :)

You need to add unit tests exhibiting and testing the new behaviour

Add unit tests: Look at our [Contributing docs](https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) - particularly [testing](https://github.com/apache/airflow/blob/main/contributing-docs/09_testing.rst) - you can also look at other PRs for reference. We practically never accept PRs that have no accompanying unit tests and you need to learn how to write and run them if you want to contribute. \r\n\r\nOur contribution guides have quick-starts for various IDEs and when you follow them you can have working development environment where you can run unit tests in less than 10 minutes.

@hprassad FYI, I updated your PR description to link the issue.

@Lee-W do we have to add this one into migration rules too?

> @Lee-W do we have to add this one into migration rules too?\n\nYes, this can be easily detected. I‚Äôll create a PR for this later

@hprassad  - I rebased it  -> we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests. 

@amoghrajesh & @Lee-W , I have added file in newsfragments folder for the PR

@hprassad Few checks are failing. Can you please take a look and fix it?

@hprassad gentle ping. Could you please handle the review comments and rebase this Pull Request? We are waiting on this one for https://github.com/apache/airflow/issues/44951

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

@hprassad congratulations on your first PR merge! ü•á 

Hey @kacpermuda -> I rebased your PR -> we found and issue with @jscheffl with the new caching scheme - fixed in #45347 that would run "main" version of the tests-> so I am rebasing all PRs affected :)

I think we are downloading all the artifacts, https://github.com/apache/airflow/blob/main/.github/workflows/finalize-tests.yml#L190 that includes CI images aswell. thats why we are hitting space issue.\r\n\r\nmay be we can filter download part

Oh yeah.. And possibly we should not download all of them now ?

https://github.com/apache/airflow/pull/45323

Realized that after #45319 

Should prevent cases like https://github.com/apache/airflow/actions/runs/12563255232/job/35028354787

Nice. Thanks @jbampton !

Re-open to force a build...

And here is caching of the "cache-mount" as well cc: @gopidesupavan \r\n

First tests show that we can go down to < 2 minutes from ~6 minutes to build the image

> First tests show that we can go down to < 2 minutes from ~6 minutes to build the image\r\n\r\nNice improvement :)

Finally: 3m30 s.  to build the image instead of 5m47. Not as fast as the fastest "registry" build we had (1m20s or so ) but "fast enough" 

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/45315"><img src="https://img.shields.io/badge/PR-45315-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

Applied full tests needed to trigger full static checks

No short term objection... as not too much activity LGTM (and beat me up if this was too fast)

I found out that there are more places where the similar change is required. \r\n\r\nhttps://github.com/search?q=repo%3Aapache%2Fairflow%20github.com%2Fapache%2Fairflow%2Fblob%2Fmain%2Fairflow%2Fproviders&type=code\r\n\r\nAs this PR is now merged, I will be raising another PR for the same.

Will merge this PR with one approval as it is functional. Can handle any issues or shortcomings, if any, in follow ups.

@jscheffl this is a simple one, would like an approval if you are around.

> I am around... and I _thought_ I had viewed and approved? Anyway... here you are!\n\nThanks Jens. Had to check, cos holiday season :)

@amoghrajesh dag ones (DagCallbackRequest) should work, and are mostly implemented in some form. This PR makes basic ones work. There are missing things - like the context is empty dict - but neverthless is improving the situation.\r\n\r\nMy main motivation is unblocking ticket https://github.com/apache/airflow/pull/44547 that has been really hard to get to get merged.

I assume this PR broke main, failed tests are with https://github.com/apache/airflow/actions/runs/12657999447\r\n\r\nSomeone able to fix this or shall we revert to fix canary again?

> I assume this PR broke main, failed tests are with https://github.com/apache/airflow/actions/runs/12657999447\r\n> \r\n> Someone able to fix this or shall we revert to fix canary again?\r\n\r\nFixed in https://github.com/apache/airflow/pull/45482

Note: As PR #45312 has been merged, the code formatting rules have changed for new UI. Please rebase and re-run pre-commit checks to ensure that formatting in folder airflow/ui is adjusted.

**Note Update:** The missing parts are included in the PR. \r\n\r\n- Create default connections in connection_command\r\n- Include overwrite functionality for file import operations for connections and pool endpoint\r\n- Include overwrite functionality to connection_command\r\n\r\nThanks @jason810496 for your contributions!

Hey everyone, apologies for the delay! I have been feeling quite terrible since the town hall and have only just managed to pull myself together in recent days. I tried to address all the comments in the code. Could you check again when you have time? Thanks!

> LGTM :) Got a small comment, and also you should rebase from main and resolve conflicts.\r\n\r\nMany thanks for your time, Shahar! :) Rebased and resolved 

Thanks, Ash for your review! Updated the code accordingly.

The API is already integrated with the authentication. This is why I needed to add authentication here https://github.com/apache/airflow/pull/45300/commits/56c0cf96d53dd460cf8378be45c12cd73f617ca0. Otherwise, it started doesn\

> I think error handling needs some work -- returning an error feels wrong to me\r\n\r\nThanks for the review, Ash! I updated the code according to the comments. \r\nI aimed to directly cascade API-related errors to the user since there are good messages on API for each exception to reduce the code repetition, however, there could be cases where behaviour should be different in API and CLI cases. So updated all exceptions to raise it and not return while letting the caller decide the behaviour. 

Thanks for the major rework and updates. Looked at the code and this is okay.\r\n\r\nI wanted to test interactively and failed miserably because... when calling `airflow auth...` it seems I need an API token. `--help` was not able to help me, no docs are added. Also no PR hints. Where can I (and a potential user in future) create an API token? Via CLI? Either I am too stupid to read, lost the context... or we need to add some docs such that users also would know how... or some more hints needed in `--help` giving hints to generate an API-Token. I expected to be able to pass user+password (admin/admin)...

Okay, attempted (again) and... there is no "profile" page. Attempted to grab the "token" from the browser inspector: \r\n![image](https://github.com/user-attachments/assets/86c881a2-7546-42f6-8ece-20ebd905961e)\r\n\r\n...but this was not working, `airflow login auth` was accepting. But then got a 403 when attempting to `airflow connections list`. So\r\n\r\n(1) Still even after re-reading the Google spec doc I am not able to find the right token. Some description is needed, else almost nobody can use the CLI. (Hope it is not an individual short-coming just in front of my keyboard)\r\n(2) I would encourage that if a user calls `airflow auth login --api-token <something>` that the token is also validated. It seems it was "just" stored but was not working. Or the `airflow connections list` call is broken.\r\n(3) With every call I was asked for the keystore password. Is there a way to temporarily store this in the session? Would be tideous to enter the passphrase of the keystore every time. (Or is this a specific shortcoming of the breeze container?)

> Okay, attempted (again) and... there is no "profile" page. Attempted to grab the "token" from the browser inspector: ![image](https://private-user-images.githubusercontent.com/95105677/422528480-86c881a2-7546-42f6-8ece-20ebd905961e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDE5MDc3ODcsIm5iZiI6MTc0MTkwNzQ4NywicGF0aCI6Ii85NTEwNTY3Ny80MjI1Mjg0ODAtODZjODgxYTItNzU0Ni00MmY2LThlY2UtMjBlYmQ5MDU5NjFlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAzMTMlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMzEzVDIzMTEyN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQwYmEwNmZkYjc4MTNjNTI2YjdiNDY3NzYyYWE5OTcwM2I3N2Q1N2YwYTQyMTAwNmNiYWIxOWFjMjIwNTRkNGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.rKeBgDiysTv0GYTVLnQyuIY6-rfTMM2pUtm91AZBZ0Q)\r\n> \r\n> ...but this was not working, `airflow login auth` was accepting. But then got a 403 when attempting to `airflow connections list`. So\r\n> \r\n> (1) Still even after re-reading the Google spec doc I am not able to find the right token. Some description is needed, else almost nobody can use the CLI. (Hope it is not an individual short-coming just in front of my keyboard) (2) I would encourage that if a user calls `airflow auth login --api-token <something>` that the token is also validated. It seems it was "just" stored but was not working. Or the `airflow connections list` call is broken. (3) With every call I was asked for the keystore password. Is there a way to temporarily store this in the session? Would be tideous to enter the passphrase of the keystore every time. (Or is this a specific shortcoming of the breeze container?)\r\n\r\n> Okay, attempted (again) and... there is no "profile" page. Attempted to grab the "token" from the browser inspector: ![image](https://private-user-images.githubusercontent.com/95105677/422528480-86c881a2-7546-42f6-8ece-20ebd905961e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDE5MDc3ODcsIm5iZiI6MTc0MTkwNzQ4NywicGF0aCI6Ii85NTEwNTY3Ny80MjI1Mjg0ODAtODZjODgxYTItNzU0Ni00MmY2LThlY2UtMjBlYmQ5MDU5NjFlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAzMTMlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMzEzVDIzMTEyN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQwYmEwNmZkYjc4MTNjNTI2YjdiNDY3NzYyYWE5OTcwM2I3N2Q1N2YwYTQyMTAwNmNiYWIxOWFjMjIwNTRkNGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.rKeBgDiysTv0GYTVLnQyuIY6-rfTMM2pUtm91AZBZ0Q)\r\n> \r\n> ...but this was not working, `airflow login auth` was accepting. But then got a 403 when attempting to `airflow connections list`. So\r\n> \r\n> (1) Still even after re-reading the Google spec doc I am not able to find the right token. Some description is needed, else almost nobody can use the CLI. (Hope it is not an individual short-coming just in front of my keyboard) (2) I would encourage that if a user calls `airflow auth login --api-token <something>` that the token is also validated. It seems it was "just" stored but was not working. Or the `airflow connections list` call is broken. (3) With every call I was asked for the keystore password. Is there a way to temporarily store this in the session? Would be tideous to enter the passphrase of the keystore every time. (Or is this a specific shortcoming of the breeze container?)\r\n\r\n\r\nThanks for your feedback! They are valuable for me to shape this into better. I don\

Might the failure that I had in (2) be related with https://github.com/apache/airflow/pull/47754 ?\r\n\r\nIf (3) is only happening with breeze but a "normal" OS keeps the passphrase then it is okay.\r\n\r\nYeah and understand the aim. Supporting this and overall otherwise I am happy with the PR!

> Might the failure that I had in (2) be related with #47754 ?\r\n> \r\n> If (3) is only happening with breeze but a "normal" OS keeps the passphrase then it is okay.\r\n> \r\n> Yeah and understand the aim. Supporting this and overall otherwise I am happy with the PR!\r\n\r\n\r\nI couldn\

Thanks a lot, Jed for the review! 

> The idea is indeed to have this as a separate package for sure. We can think of all remote commands that will be in the new CLI package.\r\n\r\nMakes sense, but I think these should _also_ be remote commands, not _just_ remote commands. That\

Yeah. I am here with @jedcunningham -> all the commands should work "as is" (for scheduler, api server, triggerer) without keyring installation and authentication. - basically everywhere we have database access, all `airflow` commands should **just work**

Speaking of what we are going to release - we should be able to release a separate "apache-airflow-cli" distribution and only that distribution should have`keyring` as paraemeter and should authenticate with the server.\r\n\r\nNow - we need to figure out how to keep the `remote` commands still working remotely in a separate package, while also have them available as airflow commands in `apache-airflow-core` distribution (without the keyring).\r\n\r\nThat is a  bit tricky, but I am sure we can work it out.

> e.g. thinking in context of the docker compose example we publish, I doubt we want to add a keyring backend into the official docker images. So users either 1) need to modify the image before half the cli is usable or 2) install the cli separately out on their host, which isn\

> Speaking of what we are going to release - we should be able to release a separate "apache-airflow-cli" distribution and only that distribution should have`keyring` as paraemeter and should authenticate with the server.\r\n> \r\n> Now - we need to figure out how to keep the `remote` commands still working remotely in a separate package, while also have them available as airflow commands in `apache-airflow-core` distribution (without the keyring).\r\n> \r\n> That is a bit tricky, but I am sure we can work it out.\r\n\r\nThanks Jarek! How about I remove the connection command integration from the PR? Let\

> Thanks Jarek! How about I remove the connection command integration from the PR? Let\

I tried to get PROD image and realized it was not implemented :). So I added it, found a typo in PROD load/save and improved a bit github_token/  github_repository behaviour : @gopidesupavan :D

BTW. Side effect of the #45266 is that PR workflow sshould not work almost as well (minus caching speed from registry) for PRs internaly done in a different repository - without having to setup GitHub registry - which was huge limitation of the "pull_retquest_target" workflow. This means that it should be entirely possible for anyone to test their PR locally using their own github runners - in their own repository in-repo PR and it should **just work** - including storing the image artifacts - that\

> I tried to get PROD image and realized it was not implemented :). So I added it, found a typo in PROD load/save and improved a bit github_token/ github_repository behaviour : @gopidesupavan :D\r\n\r\nYeah good catch :) thank you 

@mobuchowski  - can you please rebase that one -> we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests. I am asking in all affected PRs to rebase.

Actually - I rebased it now.

@mobuchowski \r\nHope all is well. Just checking in how this is coming along? \r\n\r\nYou had also mentioned that you would be raising another PR once https://github.com/apache/airflow/pull/45732 was merged. Since that is already merged, is that follow-up PR already in progress. \r\n\r\nThank you! 

Looks like the stash action does not work exactly as advertised :) 

> Looks like the stash action does not work exactly as advertised :)\r\n\r\nthink we need to set env variable name `head_name` https://github.com/apache/infrastructure-actions/blob/main/stash/restore/action.yml#L108

> > Looks like the stash action does not work exactly as advertised :)\r\n> \r\n> think we need to set env variable name `head_name` https://github.com/apache/infrastructure-actions/blob/main/stash/restore/action.yml#L108\r\n\r\nah head_name is derived from github ref.

Evidently it tries with "overwite: true":\r\n\r\n![image](https://github.com/user-attachments/assets/2936fedb-7c01-4e42-9fff-131b7aa1da29)\r\n

If that\

cc: @assignUser - seems we are stressing your action to the limit :)

FYI. @assignUser and @gopidesupavan -> seems that this is a well known "feature" of the `@v4` action that caused a number of users a problems when migrating to `@v4`. The proposed `solution` (which IMHO is just a workaround) is to upload each artifact with a different key and use "merge-multiple" feature and glob pattern to download and merge all such uploaded artifacts (!??!!#$@#%!%!$%!#%!)....\r\n\r\nThis is even explained here: https://github.com/actions/download-artifact/blob/main/docs/MIGRATION.md#multiple-uploads-to-the-same-named-artifact as solution.\r\n\r\nhttps://github.com/actions/upload-artifact/issues/478\r\n\r\nExample solution:  https://github.com/actions/upload-artifact/issues/478#issuecomment-1885470013\r\n\r\nBut stash does not use the download-artitact action and "merge-multiple" ... And I really do not like the "solution". .. So we wil have to come up with a different approach.\r\n

OK. @assignUser and @gopidesupavan -> I think I found a solution (and actually this is a better one in general for performance, but slightly more "distributed" among the .yml files.\r\n\r\nInstead of heaving a clear save/restore around installation, I only do:\r\n\r\n* restore\r\n* installl\r\n\r\nAnd I make sure to have one separate job that is prerequisite of all other jobs (`build-info` which was already there as a single job that kicks-off the rest - for uv cache and `install-pre-commit` that is added as "needs" for all jobs that need the cache and those "needs" job are the only ones that upload the artifact. \r\n\r\nThis way we get a little longer bootstrap, but then all the other jobs should use the cache uploaded by the prerequisite job. Plus the bootstrap will also use the artifact from previous runs (or target branch) if corresponding pyproject.toml / pre-commit config files did not change.\r\n\r\n

Ha. TIL....\r\n\r\nFirst of all It turns out that `uv` is DAMN FAST. I\r\n\r\nInstalling breeze with uv from scratch = 6s, just downloading breeze cache is 5s \r\n\r\n<img width="826" alt="Screenshot 2024-12-31 at 00 42 24" src="https://github.com/user-attachments/assets/036c15a9-4789-432f-953c-a7f1d146cdb7" />\r\n\r\nNot mentioning upload delay.... So using cache for breeze makes no sense..\r\n\r\nI expected a bit different numbers for "pre-commit" ... but... \r\n\r\nInstalling all our pre-commits with uv from scratch = 1m45 s, uploading the resulting cache = 1m56 s (!) \r\n\r\n<img width="774" alt="Screenshot 2024-12-31 at 00 31 14" src="https://github.com/user-attachments/assets/bc64adb9-94a3-4f18-b019-c2f14eeef4ec" />\r\n\r\nDownloading the artifact cache and installing pre-commit is ..... 2 m (!)\r\n\r\n<img width="761" alt="Screenshot 2024-12-31 at 00 33 01" src="https://github.com/user-attachments/assets/7ce9c3c5-90d6-4be2-b126-6ec78017e204" />\r\n\r\nBut I think there is something wrong with cache restoration - not sure what though (yet):\r\n\r\nThe cache is downloaded in 30 seconds:\r\n\r\n<img width="1313" alt="Screenshot 2024-12-31 at 00 47 47" src="https://github.com/user-attachments/assets/ba523821-be1b-4ff1-bf86-0e6b35952682" />\r\n\r\nBut then it takes almost as much time to install all pre-commits. Which I think is because we neeed both `uv` and  `pre-commit` cache. which means that I need to upload whole `~/.cache/` folder.... But... the problem is that this is a hidden file - and the stash action does not allow to pass `include-hidden-files` :( Need to contribute it..\r\n\r\n

https://github.com/apache/infrastructure-actions/pull/81

Also https://github.com/apache/infrastructure-actions/pull/82

Ok. @assignUser @gopidesupavan -> I believe I also found out why caching did not speed up pre-commit. The problem was that `stash/restore` action (unlike download-artifact) did not expand `~` to home directory of the user - and this is where our cache is stored. \r\n\r\nEffectively the cache was restored to `~/.cache/` directory inside the checked out repository (with actual `~` as directory name) :( .. Took me a bit of time to figure it out.\r\n\r\n\r\nFix in https://github.com/apache/infrastructure-actions/pull/84\r\n

> FYI. @assignUser and @gopidesupavan -> seems that this is a well known "feature" of the `@v4` action that caused a number of users a problems when migrating to `@v4`. The proposed `solution` (which IMHO is just a workaround) is to upload each artifact with a different key and use "merge-multiple" feature and glob pattern to download and merge all such uploaded artifacts (!??!!#$@#%!%!$%!#%!)....\r\n> \r\n> This is even explained here: https://github.com/actions/download-artifact/blob/main/docs/MIGRATION.md#multiple-uploads-to-the-same-named-artifact as solution.\r\n> \r\n> [actions/upload-artifact#478](https://github.com/actions/upload-artifact/issues/478)\r\n> \r\n> Example solution: [actions/upload-artifact#478 (comment)](https://github.com/actions/upload-artifact/issues/478#issuecomment-1885470013)\r\n> \r\n> But stash does not use the download-artitact action and "merge-multiple" ... And I really do not like the "solution". .. So we wil have to come up with a different approach.\r\n\r\nOh yeah this is new breaking change they released with v4, actually i got to know this, when try different patterns for our trusted publisher faced this while upload files to artifact

> OK. @assignUser and @gopidesupavan -> I think I found a solution (and actually this is a better one in general for performance, but slightly more "distributed" among the .yml files.\r\n> \r\n> Instead of heaving a clear save/restore around installation, I only do:\r\n> \r\n> * restore\r\n> * installl\r\n> \r\n> And I make sure to have one separate job that is prerequisite of all other jobs (`build-info` which was already there as a single job that kicks-off the rest - for uv cache and `install-pre-commit` that is added as "needs" for all jobs that need the cache and those "needs" job are the only ones that upload the artifact.\r\n> \r\n> This way we get a little longer bootstrap, but then all the other jobs should use the cache uploaded by the prerequisite job. Plus the bootstrap will also use the artifact from previous runs (or target branch) if corresponding pyproject.toml / pre-commit config files did not change.\r\n\r\ngood point make sense :)

> Ha. TIL....\r\n> \r\n> First of all It turns out that `uv` is DAMN FAST. I\r\n> \r\n> Installing breeze with uv from scratch = 6s, just downloading breeze cache is 5s\r\n> \r\n> <img alt="Screenshot 2024-12-31 at 00 42 24" width="826" src="https://private-user-images.githubusercontent.com/595491/399386334-036c15a9-4789-432f-953c-a7f1d146cdb7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzU2MTQ1ODAsIm5iZiI6MTczNTYxNDI4MCwicGF0aCI6Ii81OTU0OTEvMzk5Mzg2MzM0LTAzNmMxNWE5LTQ3ODktNDMyZi05NTNjLWE3ZjFkMTQ2Y2RiNy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMjMxJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTIzMVQwMzA0NDBaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03YmFhZTEzZDViM2RiMTViODdjYzliMWI1MWY5MmIyMDFlMTU1OWZmZDBlOTBjZTUzMWI4NmI5OTgzNGZhNzdhJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.R5sjOUSawYLdSi0vvgc5AvdmGxShQDdmhULC7DAE3DA">\r\n> Not mentioning upload delay.... So using cache for breeze makes no sense..\r\n> \r\n> I expected a bit different numbers for "pre-commit" ... but...\r\n> \r\n> Installing all our pre-commits with uv from scratch = 1m45 s, uploading the resulting cache = 1m56 s (!)\r\n> \r\n> <img alt="Screenshot 2024-12-31 at 00 31 14" width="774" src="https://private-user-images.githubusercontent.com/595491/399385418-bc64adb9-94a3-4f18-b019-c2f14eeef4ec.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzU2MTQ1ODAsIm5iZiI6MTczNTYxNDI4MCwicGF0aCI6Ii81OTU0OTEvMzk5Mzg1NDE4LWJjNjRhZGI5LTk0YTMtNGYxOC1iMDE5LWMyZjE0ZWVlZjRlYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMjMxJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTIzMVQwMzA0NDBaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1iYjk0NGViYzg2OTBiYTY4NWUwNDJiZTgwMTgwZWJkNmFhODUzMWZjYWQ2MGRkYjcwOTVjZTIzN2Y5ZjAxNzA3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.4MBPMfnmMFbvJ1QRZ3NuAXg4xefR_TKZZ6pgOpB4QVw">\r\n> Downloading the artifact cache and installing pre-commit is ..... 2 m (!)\r\n> \r\n> <img alt="Screenshot 2024-12-31 at 00 33 01" width="761" src="https://private-user-images.githubusercontent.com/595491/399385698-7ce9c3c5-90d6-4be2-b126-6ec78017e204.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzU2MTQ1ODAsIm5iZiI6MTczNTYxNDI4MCwicGF0aCI6Ii81OTU0OTEvMzk5Mzg1Njk4LTdjZTljM2M1LTkwZDYtNGJlMi1iMTI2LTZlYzc4MDE3ZTIwNC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMjMxJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTIzMVQwMzA0NDBaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lOWY4MDA5YjkxNGYzMDQ0Mjk5NmY1Mzg4MWJkOWQ2M2ZjODNjNzIzOTg2Nzk1MmVhMjAzZWJlYzIwNThjN2NhJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.hE_kQn7yAHb4cP86ykrLhHj018f8hR5i_HcFidr5mJ8">\r\n> But I think there is something wrong with cache restoration - not sure what though (yet):\r\n> \r\n> The cache is downloaded in 30 seconds:\r\n> \r\n> <img alt="Screenshot 2024-12-31 at 00 47 47" width="1313" src="https://private-user-images.githubusercontent.com/595491/399386703-ba523821-be1b-4ff1-bf86-0e6b35952682.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzU2MTQ1ODAsIm5iZiI6MTczNTYxNDI4MCwicGF0aCI6Ii81OTU0OTEvMzk5Mzg2NzAzLWJhNTIzODIxLWJlMWItNGZmMS1iZjg2LTBlNmIzNTk1MjY4Mi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMjMxJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTIzMVQwMzA0NDBaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xOGY4Mjk0NDAwOTE2NGFiMTY0Y2Y3OWY5YWQzOTgxZWQ2NTYyZjg2NDgwYmQ1NTNkYzk1MTBiNDg4NzdmZjdiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.ZX9MZbDvHCQiP6nPYjUxIxXiz9AyBxmHGuqVkwM9o24">\r\n> But then it takes almost as much time to install all pre-commits. Which I think is because we neeed both `uv` and `pre-commit` cache. which means that I need to upload whole `~/.cache/` folder.... But... the problem is that this is a hidden file - and the stash action does not allow to pass `include-hidden-files` :( Need to contribute it..\r\n\r\ninteresting ... Yes absolutely UV is super fast , like they release new versions super fast üòÑ 

> Ok. @assignUser @gopidesupavan -> I believe I also found out why caching did not speed up pre-commit. The problem was that `stash/restore` action (unlike download-artifact) did not expand `~` to home directory of the user - and this is where our cache is stored.\r\n> \r\n> Effectively the cache was restored to `~/.cache/` directory inside the checked out repository (with actual `~` as directory name) :( .. Took me a bit of time to figure it out.\r\n> \r\n> Fix in [apache/infrastructure-actions#84](https://github.com/apache/infrastructure-actions/pull/84)\r\n\r\nah gotcha good find :)  and we always point `~/.cache/` so that path becomes after download `{github_workspace}~/.cache/` 

Nice. 1m35 s -> 30s  for pre-commit environment. 

Nice! One small nit 

Merging this, its a simple one

This will make things like https://github.com/apache/airflow/actions/runs/12544405502 build in seconds.

Follow up after #45266

Note: As PR #45312 has been merged, the code formatting rules have changed for new UI. Please rebase and re-run pre-commit checks to ensure that formatting in folder airflow/ui is adjusted.

I have rebased, made changes and run pre-commit checks as request, thank you!

> Thanks @jscheffl for working on this. Had the first look. Looks great. The base is almost ready.\r\n> \r\n> As mentioned in TODO, we will support the accordion and not flat.\r\n> \r\n> Some points which i noticed, just writing it down incase we don\

Had a few "sleeps" over the discussion I had in Slack with @shubhamraj-git about Accordion.. and after Pierre also assigned it to the Note for Clear tasks... I am somewhat open and ajusted... as forms can get long. All is not (last commit) in one accordion group. Looks like this and parts are nicely folding:\r\n\r\n![image](https://github.com/user-attachments/assets/f39c2a4d-94bc-4591-9dc8-3fa3a3a5d857)\r\n\r\nChanging to Advanced options:\r\n\r\n![image](https://github.com/user-attachments/assets/c56f7ee1-7af2-49a1-8d14-0b11595e686d)\r\n\r\nWDYT?

Hi @bbovenzi I aligned in Slack with @shubhamraj-git that he would take the "wire-up" of reset-button and sync of form fields <-> Advanced Options JSON in a follow up PR the next days. So this PR is mainly the general component start and layout, function coming in the next PR.\r\n\r\nWould it be OK frmo point of code/layout/structure mo merge? Then would need your review/approval.

> Found two things with the datetime field.\r\n> \r\n> Also, did you see my comment on renaming the Fields and NormalRow?\r\n\r\nNext round of reviews applied.\r\nSaw the request to rename "NormalRow" ... renamed it to proposed "FieldRow" - is this OK?\r\nThe other comment about renaming the Fields... I did not understand. Can you help me what you wanted me to rename to?

I still have some tests to fix, but this one should be generally ready to review. There is a corresponding #45261 "regular PR" (this is canary build equivalent).\r\n\r\nIt should be **HUGE** simplificaiton of our CI workflows and Dockerfile -> and huge improvement in security - as we are getting rid of the `pull_request_target` workflow.\r\n\r\nI am planning to backport that one to `v2-10-test` and `providers-fab/v1-5` so that we will be able to get rid of the `pull_request_target` workflows altogether.

I added comments/conversations explainining some detailed reasoning for the changes. The change is huge (of course) - and sorry for that size, but I could not really make it smaller. I separated two folow-ups that do not need to be added now in this PR:\r\n\r\n* https://github.com/apache/airflow/issues/45268\r\n* https://github.com/apache/airflow/issues/45269\r\n\r\nbut all the rest is essentially "needed" as single PR. 

Oh, this is BIG. How much time do you give me/us to review? Are we in a hurry?

> Oh, this is BIG. How much time do you give me/us to review? Are we in a hurry?\r\n\r\nNot THAT big - most of it is generated images/hashes and removal of some parts of the workflow that are not used any more. There is no "huge" hurry with merging it but this one will all also fix some of the current caching issues (i.e. fixing #42999 ) -> so also local breeze dev env should be quite a bit snappier to rebuild images locally. So the sooner - the better :)

> One question-> does the PR workflow build uses cache? from where it uses is it from github registry?\r\n\r\nPRs are using cache from the artifacts. If you look at the `ci-image-build.xml`, we are attempting to download the cached artifact image and load it to the registry (the load command will only run when "stash/restore" command will have a cache hit:\r\n\r\n```yaml\r\n      - name: "Restore CI docker image ${{ inputs.platform }}:${{ env.PYTHON_MAJOR_MINOR_VERSION }}"\r\n        uses: apache/infrastructure-actions/stash/restore@c94b890bbedc2fc61466d28e6bd9966bc6c6643c\r\n        with:\r\n          key: "ci-image-save-${{ inputs.platform }}-${{ env.PYTHON_MAJOR_MINOR_VERSION }}"\r\n          path: "/tmp/"\r\n        id: restore-ci-image\r\n      - name: "Load CI image ${{ inputs.platform }}:${{ env.PYTHON_MAJOR_MINOR_VERSION }}"\r\n        run: breeze ci-image load --platform ${{ inputs.platform }}\r\n        shell: bash\r\n        if: steps.restore-ci-image.stash-hit == \

> > One question-> does the PR workflow build uses cache? from where it uses is it from github registry?\r\n> \r\n> PRs are using cache from the artifacts. If you look at the `ci-image-build.xml`, we are attempting to download the cached artifact image and load it to the registry (the load command will only run when "stash/restore" command will have a cache hit:\r\n> \r\n> ```yaml\r\n>       - name: "Restore CI docker image ${{ inputs.platform }}:${{ env.PYTHON_MAJOR_MINOR_VERSION }}"\r\n>         uses: apache/infrastructure-actions/stash/restore@c94b890bbedc2fc61466d28e6bd9966bc6c6643c\r\n>         with:\r\n>           key: "ci-image-save-${{ inputs.platform }}-${{ env.PYTHON_MAJOR_MINOR_VERSION }}"\r\n>           path: "/tmp/"\r\n>         id: restore-ci-image\r\n>       - name: "Load CI image ${{ inputs.platform }}:${{ env.PYTHON_MAJOR_MINOR_VERSION }}"\r\n>         run: breeze ci-image load --platform ${{ inputs.platform }}\r\n>         shell: bash\r\n>         if: steps.restore-ci-image.stash-hit == \

> This is the reason i tried slight different, instead of upload image artifact, upload cache file into artifact and restored this cache and used as type=local,src=/tmp/local-cache with buildx. and worked fine\r\n\r\nAh. Indeed. I have not yet **looked** at this - I was going to look at it after we merge it. BUT there is an easy solution - we need to stop using buildx for CI image building - and revert to default builder. 

> This is the reason i tried slight different, instead of upload image artifact, upload cache file into artifact and restored this cache and used as type=local,src=/tmp/local-cache with buildx. and worked fine\r\n\r\nI also tried it but this has problematic effect  - the cache also contains whole `uv` cache which is additional 10 GB  or so (!) and downloading that much of an artifact is actually **slower** than `uv` recreating the cache in CI :D

> Ah. Indeed. I have not yet looked at this - I was going to look at it after we merge it. BUT there is an easy solution - we need to stop using buildx for CI image building - and revert to default builder. \r\n\r\nI removed the `--builder` and pushed all the changes now.

> I removed the `--builder` and pushed all the changes now.\r\n\r\nI believe in this case it will use the `docker` builder - so it should also use the loaded images.\r\n\r\n> Buildx supports the following build drivers:\r\n> docker: uses the BuildKit library bundled into the Docker daemon.

> I believe in this case it will use the docker builder - so it should also use the loaded images.\r\n\r\nOr not... I will have to take a closer look :). Seems indeed the loaded images from the previous run are not used as cache  .... `cache invalidation is the most difficult thing in computer science` and yeah - I had a fair share of similar issues with it in the past :).\r\n

> Or not... I will have to take a closer look :). Seems indeed the loaded images from the previous run are not used as cache .... cache invalidation is the most difficult thing in computer science and yeah - I had a fair share of similar issues with it in the past :).\r\n\r\nOK. So we will have to revert back to `--cache-from` for CI image building. That\

> > I believe in this case it will use the docker builder - so it should also use the loaded images.\r\n> \r\n> Or not... I will have to take a closer look :). Seems indeed the loaded images from the previous run are not used as cache .... `cache invalidation is the most difficult thing in computer science` and yeah - I had a fair share of similar issues with it in the past :).\r\n\r\nYeah indeed üòÑ 

@jscheffl \r\n\r\nYaa, In starting I thought to add this as private endpoint for UI.\r\nBut we have a option to do it from CLI.\r\nSo, thought to be consistent and add public API.\r\n\r\nAlso, regarding the static checks, I tried running it locally. It suggesting me to change the name of `ImportVariableBody` to `Body_import_variables`, Not sure, why there is constraint to change it.

> Thanks for the contribution! I always wanted to make this but the big shift to Airflow 3 distracted me from making this...\r\n> \r\n> Once check that we need to discuss on attribute model...\r\n> \r\n> ... and besides this, would it make sense to consider adding UI changes in the same PR as well? Else the group display name would have no effect other than just adding an attribute.\r\n\r\nThank you for taking the time to review this and for your thoughtful feedback!\r\n\r\nTo clarify, when you mention the display name having no effect, are you suggesting it doesn‚Äôt currently reflect in the UI? In this implementation, when the group display name is provided, it is displayed in the UI as expected as shown below.\r\n\r\n<img width="1018" alt="image" src="https://github.com/user-attachments/assets/137b9d4b-e8c7-4f5f-af3a-c76638036ad1" />\r\n\r\n\r\n\r\n\r\n\r\nRegarding the UI changes, I‚Äôd be happy to extend this PR to also include a new Group Display Name key as part of the task group details on this page if that is what you are suggesting. \r\n<img width="840" alt="image" src="https://github.com/user-attachments/assets/2372e01a-7d6b-4449-b782-1853753965bb" />\r\n\r\n\r\nPlease let me know the direction you‚Äôd like me to proceed. Thank you!

> > Thanks for the contribution! I always wanted to make this but the big shift to Airflow 3 distracted me from making this...\r\n> > Once check that we need to discuss on attribute model...\r\n> > ... and besides this, would it make sense to consider adding UI changes in the same PR as well? Else the group display name would have no effect other than just adding an attribute.\r\n> \r\n> Thank you for taking the time to review this and for your thoughtful feedback!\r\n> \r\n> To clarify, when you mention the display name having no effect, are you suggesting it doesn‚Äôt currently reflect in the UI? In this implementation, when the group display name is provided, it is displayed in the UI as expected as shown below.\r\n> <img alt="image" width="1018" src="https://private-user-images.githubusercontent.com/104902482/400136461-137b9d4b-e8c7-4f5f-af3a-c76638036ad1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzU5OTc4ODEsIm5iZiI6MTczNTk5NzU4MSwicGF0aCI6Ii8xMDQ5MDI0ODIvNDAwMTM2NDYxLTEzN2I5ZDRiLWU4YzctNGY1Zi1hZjNhLWM3NjYzODAzNmFkMS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTA0JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEwNFQxMzMzMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05ZmVjNWZjN2Y5NDlkZDA0ODIyYmM2ZmIzYjMzOTVhZjRmNzg2ZjQyMWNkZWExODU5YTUzOGUyYWE0OWFkYmM3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.2yVDKByjc3ZS0nUGb1objoQ0ZireZ3jfkHq254u3yi0">\r\n> \r\n> Regarding the UI changes, I‚Äôd be happy to extend this PR to also include a new Group Display Name key as part of the task group details on this page if that is what you are suggesting. <img alt="image" width="840" src="https://private-user-images.githubusercontent.com/104902482/400136149-2372e01a-7d6b-4449-b782-1853753965bb.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzU5OTc4ODEsIm5iZiI6MTczNTk5NzU4MSwicGF0aCI6Ii8xMDQ5MDI0ODIvNDAwMTM2MTQ5LTIzNzJlMDFhLTdkNmItNDQ0OS1iNzgyLTE4NTM3NTM5NjViYi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTA0JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEwNFQxMzMzMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wOGE0ZWJkOWFiMzk5MTM2NGEzYjJmYWE0YWI3ZDEzYTk3NTViOGZjMDk5MTc4MDAzMTU0NGY3MDJmMTUwMDZjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.CWqrbEQTc-s7083D8RZYn_nXnMwbfNuGGLh3AvdCaJQ">\r\n> \r\n> Please let me know the direction you‚Äôd like me to proceed. Thank you!\r\n\r\nOh, did not notice that UI changed w/o touching the code. Cool.\r\nI assume on current (legacy) UI no changes are needed. But if you like... the "new" UI is under construction and if you want to contribute there... any contribution is welcome! https://github.com/orgs/apache/projects/400

> > > Thanks for the contribution! I always wanted to make this but the big shift to Airflow 3 distracted me from making this...\r\n> > > Once check that we need to discuss on attribute model...\r\n> > > ... and besides this, would it make sense to consider adding UI changes in the same PR as well? Else the group display name would have no effect other than just adding an attribute.\r\n> > \r\n> > \r\n> > Thank you for taking the time to review this and for your thoughtful feedback!\r\n> > To clarify, when you mention the display name having no effect, are you suggesting it doesn‚Äôt currently reflect in the UI? In this implementation, when the group display name is provided, it is displayed in the UI as expected as shown below.\r\n> > <img alt="image" width="1018" src="https://private-user-images.githubusercontent.com/104902482/400136461-137b9d4b-e8c7-4f5f-af3a-c76638036ad1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzU5OTc4ODEsIm5iZiI6MTczNTk5NzU4MSwicGF0aCI6Ii8xMDQ5MDI0ODIvNDAwMTM2NDYxLTEzN2I5ZDRiLWU4YzctNGY1Zi1hZjNhLWM3NjYzODAzNmFkMS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTA0JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEwNFQxMzMzMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05ZmVjNWZjN2Y5NDlkZDA0ODIyYmM2ZmIzYjMzOTVhZjRmNzg2ZjQyMWNkZWExODU5YTUzOGUyYWE0OWFkYmM3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.2yVDKByjc3ZS0nUGb1objoQ0ZireZ3jfkHq254u3yi0">\r\n> > Regarding the UI changes, I‚Äôd be happy to extend this PR to also include a new Group Display Name key as part of the task group details on this page if that is what you are suggesting. <img alt="image" width="840" src="https://private-user-images.githubusercontent.com/104902482/400136149-2372e01a-7d6b-4449-b782-1853753965bb.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzU5OTc4ODEsIm5iZiI6MTczNTk5NzU4MSwicGF0aCI6Ii8xMDQ5MDI0ODIvNDAwMTM2MTQ5LTIzNzJlMDFhLTdkNmItNDQ0OS1iNzgyLTE4NTM3NTM5NjViYi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTA0JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEwNFQxMzMzMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wOGE0ZWJkOWFiMzk5MTM2NGEzYjJmYWE0YWI3ZDEzYTk3NTViOGZjMDk5MTc4MDAzMTU0NGY3MDJmMTUwMDZjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.CWqrbEQTc-s7083D8RZYn_nXnMwbfNuGGLh3AvdCaJQ">\r\n> > Please let me know the direction you‚Äôd like me to proceed. Thank you!\r\n> \r\n> Oh, did not notice that UI changed w/o touching the code. Cool. I assume on current (legacy) UI no changes are needed. But if you like... the "new" UI is under construction and if you want to contribute there... any contribution is welcome! https://github.com/orgs/apache/projects/400\r\n\r\nThanks for the feedback! Just to confirm, does this mean the PR is ready to go and meets the current requirements?\r\n\r\nAlso, I‚Äôm happy to contribute to other areas that are not UI-related, as I‚Äôm not an expert with frontend libraries or code :smile: 

Nice looking forward for this :) 

Also opened a "canary" build from "apache" repo to check how caching will work #45266\r\n\r\nI will eventually close that one and will mark "ready for review" the other one, as we cannot get this one to be green due to pull_request_target workflow failing.

Merged in https://github.com/apache/airflow/pull/45266

\r\nWhen attempting to remove the lineage logic from the core module, I noticed that it causes failures in tests related to **OpenLineage listener capturing hook-level lineage** (https://github.com/apache/airflow/pull/41482).  \r\n\r\nFor example, removing `apply_lineage` and `prepare_lineage` from `BaseOperator`:  \r\nhttps://github.com/apache/airflow/blob/0efd9e6a2fa8bde0f6c14e88951b44badca063a2/airflow/models/baseoperator.py#L705-L739  \r\n\r\nResults in the following test failure:  \r\n```\r\nFAILED providers/tests/openlineage/extractors/test_manager.py::test_extractor_manager_gets_data_from_pythonoperator - assert 0 == 1\r\n +  where 0 = len([])\r\n +    where [] = HookLineage(inputs=[], outputs=[]).outputs\r\n```\r\nhttps://github.com/apache/airflow/blob/0efd9e6a2fa8bde0f6c14e88951b44badca063a2/providers/tests/openlineage/extractors/test_manager.py#L301  \r\n\r\nIt seems OpenLineage is still coupled with the lineage module and might need to be moved to `compact.lineage` for now (or to the OpenLineage module in a future PR ).  \r\n\r\nAfter some experimentation, implementing an `on_load` callback in the `OpenLineageProviderPlugin` to monkey-patch the core module at runtime prevents the OpenLineage test failures, even with the lineage module removed from the core. \r\n> Based on https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/plugins.html#interface\r\n\r\nI‚Äôm not sure if this is a suitable long-term solution for maintaining OpenLineage compatibility while cleaning up the lineage module ?\r\n\r\ncc @Lee-W @uranusjr  \r\n

The CI failure is caused by a flaky K8s test (#45145) and a breaking change in the compatibility tests for providers 2.9.3 and 2.10.4 .  \r\n\r\nShould we fix the tests to pass the compatibility checks, or is it acceptable to ignore the compatibility tests since all tests for 3.0 have passed ?

We should not remove the entire lineage mechanism. The issue is only meant to remove the old lineage constructs in `airflow.lineage.entities`. The rest of the mechanism integrates modern Airflow assets with OpenLineage, and they should be kept.

Just as TP said. Removing [airflow/lineage/hook.py](https://github.com/apache/airflow/pull/45260/files#diff-da82ad12f9efa71b6de517375641691e1b83a69f66eb79fa4887b197b15c4147) is wrong

@jason810496  I rebased it -> we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests.

Fixed: I only moved `airflow.lineage.entities` to `airflow.providers.common.compat.lineage.entities`.  \r\nThe CI failure is unrelated, as no relevant frontend tests failed.  \r\n\r\nOne small question: Does this PR need to be backported to 2.10 ?  \r\n

Hi @jason810496, I just found the PR we discussed yesterday. https://github.com/apache/airflow/pull/44720 I think this one would be useful for this PR.

> One small question: Does this PR need to be backported to 2.10?\r\n\r\nNo since you only moved the classes. The providers are always released from main, and it‚Äôs OK for those classes to be available in two places in 2.10.

This is not yet complete, and I also extract some things out of that to separate PR to make it smaller/simpler. Just wanted to test the image build process.

Once we sort out what needs to be done to move one provider, an automated script to move each one is maybe possible.

> Once we sort out what needs to be done to move one provider, an automated script to move each one is maybe possible.\r\n\r\nThat\

What about system tests? Is it going to stay cenerlized or per provider?

> What about system tests? Is it going to stay cenerlized or per provider?\r\n\r\nThey are under "tests/system" in  the provider. 

Indeed - worth to describe the proposal how tests should work - this is one of the changes that is not yet "final" setup but more of a step in the right direction. \r\n\r\nIt\

> Looks good! Our sphinx friend is angry though :)\r\n\r\nYeah. And as usual speaking riddles, so I have to somehow walk around the beast.

OK. Sphinx riddles and resulting Drachenfutter continued ....\r\n\r\nI **think** I got it right now.  I had to make some "dirty" solution to speed it up for docs building. There are far too many moving parts now to fix it for docs building individually "per-package" now, so for now I am  just dynamically copying the docs to "docs/apache-airflow-providers-*/ and build docs from there. (those new providers are gitignored and by default the  generated files are deleted after building to avoid the docs to show up in your IDE in two places normally (you can add `--skip-deletion` flag to leave the copied docs in place).\r\n\r\nEventually, I think we shoudl be able to generate docs separately for each provider - in each provider on it\

Woohoo nice Jarek :) 

@kacpermuda @potiuk could you please take a look at the PR and approve?

@rahul-madaan  -> I rebased it. we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests. 

@kacpermuda @potiuk A gentle reminder, please review the PR whenever you find some time this week.

errors are not getting resolved even after rebasing. It somehow started coming after Jarek rebased it. Should I reset the branch and  cherrypick my commits to resolve this?

> errors are not getting resolved even after rebasing. It somehow started coming after Jarek rebased it. Should I reset the branch and cherrypick my commits to resolve this?\r\n\r\nUsually not needed - but I agree it seems the errors are unrelated to your changes... on first view.

@kacpermuda I have addressed all the comments, please take a look. I have tested it on s3 and it is working perfectly.

Hmm. Interesting it was not caught by the compatibility tests 

And some tests still need fixing.

>Hmm. Interesting it was not caught by the compatibility tests\r\n\r\nYeah, I would expect some tests to fail.\r\n> And some tests still need fixing.\r\n\r\nYes, I am fixing them now. Thanks for the quick review!

This how the logout button shall look in the GUI:\r\n### GUI with the new Logout button:\r\n<video src="https://github.com/user-attachments/assets/049f129a-dbc1-4abb-96f7-df7810d6962e" width="320" height="240" controls></video>\r\n\r\n### GUI after removing additional "login" text:\r\n<video src="https://github.com/user-attachments/assets/1c149afe-c0d3-4630-b30c-842d1d24881a" width="320" height="240" controls></video>\r\n

The current fix is not a clean one. I did go through the code to make a cleaner fix which would involve including the "logout" in one of the functions in \r\n[base_auth_manager.py](https://github.com/apache/airflow/blob/7f2b8ef5acf95b7fb8faa38a10caf94c043f5019/airflow/auth/managers/base_auth_manager.py#L398C1-L423C1):\r\n```\r\n    def filter_permitted_menu_items(self, menu_items: list[MenuItem]) -> list[MenuItem]:\r\n        """\r\n        Filter menu items based on user permissions.\r\n\r\n\r\n        :param menu_items: list of all menu items\r\n        """\r\n        items = filter(\r\n            lambda item: self.security_manager.has_access(ACTION_CAN_ACCESS_MENU, item.name), menu_items\r\n        )\r\n        accessible_items = []\r\n        for menu_item in items:\r\n            menu_item_copy = MenuItem(\r\n                **{\r\n                    **menu_item.__dict__,\r\n                    "childs": [],\r\n                }\r\n            )\r\n            if menu_item.childs:\r\n                accessible_children = []\r\n                for child in menu_item.childs:\r\n                    if self.security_manager.has_access(ACTION_CAN_ACCESS_MENU, child.name):\r\n                        accessible_children.append(child)\r\n                menu_item_copy.childs = accessible_children\r\n            accessible_items.append(menu_item_copy)\r\n        return accessible_items\r\n```\r\nThe reason being that this menu list is called in the [```navbar_menu.html```](https://github.com/apache/airflow/blob/7f2b8ef5acf95b7fb8faa38a10caf94c043f5019/airflow/www/templates/appbuilder/navbar_menu.html#L36C1-L60C13)\r\n\r\nEnabling these changes further requires changes in the [security_manager.py](https://github.com/apache/airflow/blob/7f2b8ef5acf95b7fb8faa38a10caf94c043f5019/airflow/www/security_manager.py#L119C4-L140C68). Is this the correct approach? I did attempt to implement these but that just lead to the need for more changes.\r\n\r\nAlso, I think that the version checking condition in the [override.py](https://github.com/apache/airflow/blob/7f2b8ef5acf95b7fb8faa38a10caf94c043f5019/providers/src/airflow/providers/fab/auth_manager/security_manager/override.py#L135C1-L140C24) can now be removed if it had been implemented solely due to the GET request at /logout endpoint.

From the screencast I see that you have not properly compiled www-assets before running the UI. This has side effects of missing scripts.\r\n\r\nWhat I do not understand is the fix you are attempting to contribute. There IS already a logout menu entry behind the profile:\r\n![image](https://github.com/user-attachments/assets/e1c2e133-1ce1-4150-a850-710becd6738f)\r\nWhy do you plan to add another in this PR?

@jscheffl @shahar1 my sincere apologies for this mishap. I have compiled the www-assets (sorry for this mistake). In my previous comment under the issue, I had presumed that the person who opened the issue tried to go to the /logout/ endpoint in the browser to logout. I should have asked them how exactly was the error occurring. The only instance of 405 error I got was when I tried to enter the /logout/ in the browser. Hence my assumption. \r\n\r\nAgain, my apologies for this bad PR. I will make a comment under the issue to understand how to recreate the error. Should I close my PR until then?

@pierrejeambrun @jscheffl @bbovenzi \r\n\r\nI can see that the patch variable api have a check \r\n```\r\nif patch_body.key != variable_key:\r\n        raise HTTPException(\r\n            status.HTTP_400_BAD_REQUEST, "Invalid body, key from request body doesn\

@jscheffl I am open to suggestion on this, if we have something better.\r\nMy only take on this was, we are going to have import vars, export var, delete multiple, action bars soon. So, it will contain lot of files, so just made the proper CRUD under a folder.

> @pierrejeambrun @jscheffl @bbovenzi\r\n> \r\n> I can see that the patch variable api have a check\r\n> \r\n> ```\r\n> if patch_body.key != variable_key:\r\n>         raise HTTPException(\r\n>             status.HTTP_400_BAD_REQUEST, "Invalid body, key from request body doesn\

> @pierrejeambrun @jscheffl @bbovenzi\r\n> \r\n> I can see that the patch variable api have a check\r\n> \r\n> ```\r\n> if patch_body.key != variable_key:\r\n>         raise HTTPException(\r\n>             status.HTTP_400_BAD_REQUEST, "Invalid body, key from request body doesn\

@potiuk @pierrejeambrun @jscheffl \r\nBased upon the discussion, https://apache-airflow.slack.com/archives/C07813CNKA8/p1735405229925579\r\nI have disabled the variable key edit from UI side.

Discovered when looking why https://github.com/apache/airflow/pull/45249 has been failing - very interesting side-effect in our tests

> Interesting one :)\r\n\r\nIndeed - we should likely improve some of the diagnostics there and fail "earlier" and with "clearer" message - but for now this one should be "good enough" 

Also, if this gets merged, how can I release a new chart version?

![Screenshot 2024-12-29 at 12 58 32\u202fPM](https://github.com/user-attachments/assets/23bd5bde-e4c9-4fe8-b35a-6a242c8fee89)\r\n\r\nI see, it has to be maintainer-approved. Curious if at least some tests can be made to run by default. That can greatly speed up the iteration.

But you can run exactly the same tests locally -> All tests that are run in CI are runnable locally - that is basic assumptions for our CI system https://github.com/apache/airflow/tree/main/contributing-docs/testing -> you can see more details about all the types of tests we have and how to reproduce them locally 1:1

Tho I might be running into some incompatibility issues with pytest\r\n```\r\nERROR: while parsing the following warning configuration:\r\n\r\n  error::pytest.PytestReturnNotNoneWarning\r\n\r\nThis error occurred:\r\n\r\nTraceback (most recent call last):\r\n  File "/Users/andrii.korotkov/.pyenv/versions/3.10.12/lib/python3.10/site-packages/_pytest/config/__init__.py", line 1638, in parse_warning_filter\r\n    category: Type[Warning] = _resolve_warning_category(category_)\r\n  File "/Users/andrii.korotkov/.pyenv/versions/3.10.12/lib/python3.10/site-packages/_pytest/config/__init__.py", line 1677, in _resolve_warning_category\r\n    cat = getattr(m, klass)\r\n  File "/Users/andrii.korotkov/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pytest/__init__.py", line 165, in __getattr__\r\n    raise AttributeError(f"module {__name__} has no attribute {name}")\r\nAttributeError: module pytest has no attribute PytestReturnNotNoneWarning\r\n```

> Thanks for the hints, I\

> Tho I might be running into some incompatibility issues with pytest\r\n\r\nJust follow the docs.  Breeze is best to reproduce the env. You can also use `uv` for local venv. But the docs are complete and explain what to do. 

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

lso it is likely that this one was impacting some other ways Airflow DB is initialized: https://github.com/apache/airflow/discussions/45236

I am not sure if we are missing other models here, but generally, depending on what sequence imports happened our DB scripts could "forget" to create certain tables, and our tests were failing with missing associations if referred DB models were not imported in earlier tests.\r\n\r\nThis is somewhat a weakness of our "imports" doing a lot of work implicitly (and sometimes forgetting to do it).\r\n\r\n

Though @uranusjr @ashb -> I am not sure if this is right - I also saw that there are those __getrr_ calls for all lazy imports, so I am not at all sure if this is the right approach what I am doing - this mechanism of sometimes lazy, sometimes not importing of modules is .... complex ... and I am not sure what the intentions were to have some of the modules lazy and some not... So I am not all sure if what I am doing here is fine.

Nope. It does not help;

@potiuk could you please check this PR?

@Lee-W Thank you for reviewing! I will resume work on this matter from January 6th, so please wait for two days.

The CI seems weird ü§î  let me rebase and see how it works

Thank you!\r\nIt was a great experience working with you.

i did rebase and work on the quick fix!

@Lee-W Corrections have been made concerning the linter! Please check.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

I have added three new commits from the following commitments.\r\n\r\ncommit ef012cf4bb1af675cd09c5f0f21d26df45901214 (origin/v2-10-test, v2-10-test)\r\n\r\nPlease let me know if there are any corrections needed!

I will cut back from the main branch and re-submit a PR.

### Backport failed to create: v2-10-test. View the failure log <a href=\

Do you want me to "forward-port" it to main - or will you do it @eladkal  ?

> Do you want me to "forward-port" it to main - or will you do it @eladkal ?\r\n\r\nIt\

Note: As PR #45312 has been merged, the code formatting rules have changed for new UI. Please rebase and re-run pre-commit checks to ensure that formatting in folder airflow/ui is adjusted.

Lgmt looks like we need to rebase though

@bbovenzi Thanks, rebased and fixed conflicts.

@amoghrajesh It probably needs some unit tests adding though :) 

@ashb I have added UT coverage for this, the UT added makes sure that we hit the get_run_data_interval flow and we validate if certain fields have been set or not.

@pierrejeambrun Also regarding the placement of this button? do we need to follow the leagcy? It was in starting of the row, Should i add there?\r\nBut if we want consistency throughout all pages, it should be at end.

> @pierrejeambrun Also regarding the placement of this button? do we need to follow the leagcy? It was in starting of the row, Should i add there?\r\nBut if we want consistency throughout all pages, it should be at end.\r\n\r\nActions at the end of the row seems fine by me.

Before any more progress on that - please read and follow https://github.com/apache/airflow/blob/main/PROVIDERS.rst#accepting-new-community-providers on discussing, proposing and accepting new providers . PR is not enough to propose new provider. You will find some example past discussions there as well and that should guide you on how you should approach proposing a new provider

I will try the third-party-airflow-plugins-and-providers.\r\nCLOSE now

> **^ Add meaningful description above** Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information. In case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed. In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x). In case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).\r\n\r\nWelcome to Apache Airflow and thanks for your contribution! \r\nEvery good journey starts with removing a letter (my first PR was like that as well :D ) - feel free to challenge yourself a bit more with any of the [good first issues](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22).

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

@shubhamraj-git  The search is substring search like the legacy view. Legacy view has limit of 10 entries and I am fine with increasing the limit here. Thanks.\r\n\r\nhttps://github.com/apache/airflow/blob/b13ed892b1fcc98b1bc9896dd4b2273a21121705/airflow/www/views.py#L5522

> @shubhamraj-git The search is substring search like the legacy view. Legacy view has limit of 10 entries and I am fine with increasing the limit here. Thanks.\r\n> \r\n> https://github.com/apache/airflow/blob/b13ed892b1fcc98b1bc9896dd4b2273a21121705/airflow/www/views.py#L5522\r\n\r\nActually yes, I saw in the legacy. Hence removed the comment, since i got my answer. Yes, maybe increasing to 10 will be good.

Thanks, increased it to 10 and also increased width to view longer names.

Note: As PR #45312 has been merged, the code formatting rules have changed for new UI. Please rebase and re-run pre-commit checks to ensure that formatting in folder airflow/ui is adjusted.

I think we should style this as a universal search like how the Chakra 3 docs work:\r\n\r\n<img width="1412" alt="Screenshot 2025-01-03 at 10 04 54\u202fAM" src="https://github.com/user-attachments/assets/14ee9d8e-4e4c-4409-b5a3-b7750b300e97" />\r\n

Thanks @bbovenzi , updated the PR so that "search dags" at top right of the header is a button and it opens the modal with the search implementation. Updated screenshot in the PR description.

Chakra search is also triggered by "ctrl+k" . I guess there needs to be a tracking ticket to track porting keyboard shortcuts in legacy UI to new UI too.

Thanks, refactored to `SearchDags` folder exporting only `SearchDagsButton` in index.ts . I have also set `menuIsOpen` so that default dags are always shown on modal open and not just on clicking the select input.

Starting with a draft with a POC. Please, let me know if this is a good direction.

Before we go any deeper. I would like to dicuss the reasoning of doing it\r\n\r\nI simply think it does not achieve any "more" security than we currently have. Both parameters and decryption key are necesarlly in the same environment and you neeed to have access to both - if you don\

Just to note - not that I am against it totally, but I want you to think and come up with proposal how to tell the user to maintain "secret" status of such values?\r\n\r\nIf you are keeping both - secret values and private key in the same configuration place (secure perimeter), then you can use the key to decrypt the values. What your advise should be for the users who are going to use that feature, how they should keep the key and what exactly will the encryption will be protecting against?\r\n\r\nIf we are going to accept some form of it, we need have detailed documentation describing the users what kind of security they can expect and how they should protect (and against what) - the thing is that often when people see some "Encrypted" values, they **think** they are generally protected against the values leaking (but they don\

Also. I think it needs an explanation on why you cannot use "k8s secrets" (if you are talking about k8s) - and why is it problematic to deploy them separately and mount as config files. 

One alternative to the `_encrypted` suffix is to have encrypted values start with some prefix, e.g.\r\n```\r\nclient_secret: aes256encrypted:<base64-encoded-ciphertext>\r\n```

Why not having it all configured through env variables? All airflow configurations can be configured via ENV variables  in one place - same source for every secret value https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#configure-all-key-value-pairs-in-a-secret-as-container-environment-variables \r\n\r\nThe thing is that we tend to avoid having to add "features" to Airflow where they can be provided via "external" means in this case deployment that might be responsible to keep all secrets (i.e. env variables) that are providing all secret values from a single "secret" that can even be backed by a secret manager. \r\n\r\nGenerally we are now more in the mode (for Airlfow 3) to search what  non core issues we can remove from Airflow rather than what we can add to it. Similalry as your other issue  (now discussion) https://github.com/apache/airflow/discussions/45221 - we are very, very, very cautious about adding things to Airflow that  could be done "externally" - we are good at scheduling workfloads, we are not good in keeping secret configurations (K8S is way better for that) and authentication (KeyCloak is way better in this). You have to remember, that whatever we approve as contributions to Airlfow, we also have to maintain it - and if there are other ways to achieve it - for example via deployment features - we tend to be very hesitant about accepting those.\r\n\r\nYou seem to be very focused on K8S / ArgoCD environment, so my question is - why "standard" feature where all the configurationsecrets can be configured in one place and exposed as env variables are not "good" for you ?\r\n\r\n

Nice!

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

@alexott Please could you give your review comments when possible.

> Thanks for the great addition @HariGS-DB . I have few minor suggestions. Happy to take another pass once you add your thoughts or have addressed the comments.\r\n> \r\n> Thanks @eladkal for requesting review here\r\n\r\nHi @pankajkoti I have incorporated your review comments. I had to close and reopen the PR due to some merge conflicts which I have resolved. Please could you check and give me your new feedback

@pankajkoti I have resolved the issues and the error check in the test and docs. But I am still getting a mypy error in the CI for code which I have not even touched. I might need some advice on how should I resolve them

@pankajkoti I see you have pushed a new change which seem to have fixed the CI pipelines. Is there anything else need to be done for merging this. Please let me know

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Thanks, @HariGS-DB, for the fantastic contribution! I‚Äôve merged the PR.\r\n\r\nSince this was your first contribution, the CI pipeline required maintainer approval to run. Going forward, for your subsequent PRs, the CI pipelines will run automatically without needing approval. üéâ

@pankajkoti Do you know when this PR will be released in the next airflow version? I will need to add another PR to introduce a validation for serverless (check for presence of environment_key  in the json payload when submitting it for serverless ). I will submit this PR this week, is it possible to hold this PR in the next release until then?

Hi @HariGS-DB,\r\n\r\nProviders are typically released on a bi-weekly cycle, as outlined here: https://github.com/apache/airflow/blob/main/PROVIDERS.rst#community-providers-release-process. While it‚Äôs not possible to delay a release for a single PR or skip a PR from the release candidate, you can cast a negative vote if the provider release includes this PR and it introduces a breaking change. Release managers initiate a voting thread once the provider release candidates are created. If you‚Äôre not already subscribed to the dev list (https://airflow.apache.org/community/#mailing-list), I recommend doing so to stay informed about upcoming releases, allowing you to test and vote accordingly.\r\n\r\nIn this case, it looks like the change is an addition rather than a regression, so proceeding with the release seems reasonable. Additional validation can be incorporated in the next release if needed, without delaying the current one. Does that sound good to you?

Same as https://github.com/apache/airflow/pull/45164 but opened from the "apache/airlfow" repository so that after approval I can just push the `providers-fab/v1-5` branch to the tip of the approved PR - to keep commit history.

I had to rebase it one-more time to make it `fast-forward` one.

I created an issue https://github.com/apache/airflow/issues/45192 in our CI/DEV ENV project that describes how we can make future cases like that simpler to handle backporting.

Note: As PR #45312 has been merged, the code formatting rules have changed for new UI. Please rebase and re-run pre-commit checks to ensure that formatting in folder airflow/ui is adjusted.

Thanks @pierrejeambrun and @bbovenzi 

@fdemiane is currently working on some fixes in the Google system tests dashboard; @fdemiane can you please help @kacpermuda ?

@kacpermuda should we draft this PR until you verify if it works?

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

This should remove warning when `uv` performs resolution.

Not needed any more.

cc: @shubhamraj-git -> small thing I found out when backporting - isabs should be imported from os.path :)

I am so glad we have compatibility tests :)

> +1, nice catch.\r\n\r\nIt were the tests that caught it - not me :)

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/45179"><img src="https://img.shields.io/badge/PR-45179-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

Where is the checkbox to be used  in future? Or any ticket associated to the demand creating it?

Yaa, actually I have started working on it, faced the issue, thought might rebase it, that PR will take longer to be raised.  But will keep this in mind from next time.

Related issue https://github.com/apache/airflow/issues/43328

Note: As PR #45312 has been merged, the code formatting rules have changed for new UI. Please rebase and re-run pre-commit checks to ensure that formatting in folder airflow/ui is adjusted.

I think some rebase went wrong. Can you fix the branch and resolve possible conflicts to facilitate the review please.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

One more thing needed - pull_request_target needs to be configured for `provider-*` branches too :)

I think I need to merge this one to `providers-fab/v1-5` to make it works

Nice!

Hmm. Still something wrong :( 

All right ! Now all workflows seems to be triggering ! 

cc: @shubhamraj-git 

> Do we need any change log update?\r\n\r\nIt will be generated automatically - following the usual release provider process. 

Ah... I need to backport the click changes too :)\r\n

And some more :)

I had to backport few fixes since that branch was relased - so it was a "bit" more involved - but all of them applied cleanly 

Everything is green here, so cool. But I want to keep history of the commits, so I had to redo it from the "apache/airflow" repository and will merge it by pushing the `providers-fab/v1-5` branch once approved there.\r\n\r\nhttps://github.com/apache/airflow/pull/45185

Tested here, yes hope it works this time :)\r\n\r\n<img width="699" alt="image" src="https://github.com/user-attachments/assets/eca7cb14-99de-48c1-b44d-04c655cb9d87" />\r\n\r\nhttps://regextester.github.io/

Continue on other PR...

cc: @shubhamraj-git 

@tirkarthi @eladkal -> I think you were more involved in https://github.com/apache/airflow/issues/44759 - that one looks nice and good I will have a few nits.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

cc: @shubhamraj-git 

Re-opened to trigger (hopefully) the build.

Hmmmmmm\r\n

Hmm. I think I need to close that PR and open a new one.

The failing test is intermittent\r\n

### Backport failed to create: v2-10-test. View the failure log <a href=\

I will take a close look what has changed eventually but evidently on Mac the hash is different :)

(or at least it looks like - it also might be that there is other factor that makes it flip-flop).\r\n

I got it i think its the ubuntu upgrade that github release recently ? ubuntu-24.04. might it is default now in our repo

Our CI using now the latest version ubuntu-24.04, https://github.com/apache/airflow/actions/runs/12454695812/job/34766359224#step:1:4

Yeah... It flip-flops randomly so we will keep on seing it... very interesting :)

Ok. Found it - new click does not keep stable sequence of options:\r\n\r\n<img width="1807" alt="Screenshot 2024-12-22 at 16 59 21" src="https://github.com/user-attachments/assets/69e1fb97-e4a5-4619-9dec-0921ba6790ed" />\r\n

Fix here: https://github.com/apache/airflow/pull/45156\r\n

Nice! Thank you for this one 

Should move v2-10-test PRs and canary builds back to the "green" zone.

Thanks for your help , I rebased and it still failing, any idea ?

Yeah. You need to run `breeze static-checks --last-commit` and it should fix itself.

(or so I think)

yes I tried `breeze static-checks --last-commit` after the rebase , no effect

Try  `breeze ci-image build --python 3.9` and repeat it.

yes I already tried to rebuild my breeze context , no effect

You can also (though it should happen automatically) remove and re-install breeze `uv tool remove airflow_breeze` followed by `uv tool install --editable ./dev/breeze`.  Maybe also you did the rebuild before constraints were updated - then `breeze ci-image build --upgrade-to-newer-dependencies` might help.

I re-run it locally and those hashes were properly calculated - I pushed a fixup. 

`breeze ci-image build --upgrade-to-newer-dependencies` fixed it :+1: \r\n\r\nthanks

> `breeze ci-image build --upgrade-to-newer-dependencies` fixed it üëç\r\n> \r\n> thanks\r\n\r\nAh. I found out the root cause - we should also upgrade minimum version of click in "airflow" so that `breeze ci-image build` will upgrade it as well - I missed that in https://github.com/apache/airflow/pull/45156. \r\n\r\nThis should be fixed now for everyone else via: https://github.com/apache/airflow/pull/45177

Small fix :) 

> Oh oops. The zero indexing error!!\r\n\r\noff-by-one :)

Cool :) retries always helps.. 

oh nice good to know , due to the click version difference it produced different hashes :)

> oh nice good to know , due to the click version difference it produced different hashes :)\r\n\r\nYeah. We are hashing dictionaries of command information that click produces, I think the difference is that we have a `""` as default value and click 8.1.8 implemented a fix for that.\r\n\r\n```python\r\noption_chicken_egg_providers = click.option(\r\n    "--chicken-egg-providers",\r\n    default="",\r\n    help="List of chicken-egg provider packages - "\r\n    "those that have airflow_version >= current_version and should "\r\n    "be installed in CI from locally built packages with >= current_version.dev0 ",\r\n    envvar="CHICKEN_EGG_PROVIDERS",\r\n)\r\n```\r\n\r\nRelease notes: https://github.com/pallets/click/releases/tag/8.1.8\r\nIssue: https://github.com/pallets/click/issues/2500\r\nFix: https://github.com/pallets/click/pull/2724\r\n

Actually it is something else - because it was only setup :)... But .... It works, so no need to dig-in

Complete fix in #45142 

Just a static check failure :)

This is provider-only change - so we do not need to backport it.

> This is provider-only change - so we do not need to backport it.\r\n\r\nNot so simple. It was decided that fab provider next release is breaking change and compatible with Airflow 3 only.\r\nhttps://github.com/eladkal/airflow/blob/c8c5756530b95de7f53b1f4cfc296d04627c7b25/providers/src/airflow/providers/fab/provider.yaml#L53\r\n\r\nto release this change to fab provider that is compatible with Airflow 2.9 we need to create a branch from fab tag 1.5.1 and follow on:\r\nhttps://github.com/apache/airflow/blob/main/PROVIDERS.rst#mixed-governance-model-for-3rd-party-related-community-providers

> > This is provider-only change - so we do not need to backport it.\r\n> \r\n> Not so simple. It was decided that fab provider next release is breaking change and compatible with Airflow 3 only. https://github.com/eladkal/airflow/blob/c8c5756530b95de7f53b1f4cfc296d04627c7b25/providers/src/airflow/providers/fab/provider.yaml#L53\r\n> \r\n> to release this change to fab provider that is compatible with Airflow 2.9 we need to create a branch from fab tag 1.5.1 and follow on: https://github.com/apache/airflow/blob/main/PROVIDERS.rst#mixed-governance-model-for-3rd-party-related-community-providers\r\n\r\nIndeed. First time!. So wiil simply need to create a branch from FAB and use cherry-picker to cherry-pick it there after merging. Probably manual `cherry_picker` command. I can do it once this one is merged. \r\n\r\n

no one review. it goes to close.

(but yes milestone was missing).

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/45137"><img src="https://img.shields.io/badge/PR-45137-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Ho Ho Ho ! Nice present for contributors üéÖ \r\n

Thank you, I also plan to do the same for the other contributing docs when I find the time. :santa: 

no one review. it goes to close.

> It seems there are might be some pr\

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Rebased after we fixed main issue

Applied and closed/reopened to trigger the build

Fix the provider tests that explicitly use the `read` or `_read` methods. 

Rebase to latest main, wait for review.

@jason810496  I rebased it  -> we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests.

Just rebase to latest main, nothing update.

@dstandish @ashb - can we merge it ? That one seems like a good cnadidate for 2.10.5 ?

It only really waits for your review/approval and it solves real issue.

Hi @potiuk, may I ask whether this PR is considered a refactor for 3.0 or 2.10? I saw the 3.0 feature freeze mentioned in the dev list, so I‚Äôm not sure which version this PR will be counted for.\r\n\r\nHere is the related discussion on Slack: https://apache-airflow.slack.com/archives/CCZRF2U5A/p1736767159693839

Do we really need to ignore these deprecations? or we should remove those deprecated classes? it looks like they have date mentioned target around sept 2025.

looks like selective checks ignoring some of the file checks conditions. trying to look more..

Fix here #45131

I fixed the records during release so no need to change anything

The build failure appears unrelated to my re-wording of the comment :)

Yeah. Fixed in main (including #45131 fixing selective checks that caused broken main). Rebased it now. 

Thank you to both of you!\n

> Thank you to both of you!\r\n\r\nNicely done, good work!

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Merging as (still) no response from Kaxil/Ash... let me know "post mortem" if some polishing is needed.

@potiuk are we ok with merging this PR on `main`? \r\nYou approved and merged the backport, just wondered if you just missed this one or is it on purpose :)

Ups

Thanks üòì .... Not sure how I missed it :)

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/45115"><img src="https://img.shields.io/badge/PR-45115-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

### Backport failed to create: v2-10-test. View the failure log <a href=\

If we agree on the approach, I will work on the tests.

It probably might also be a good idea to de couple the entire payload construction out of `TaskState`. We might need to handle future cases where "fail" ti state has specific attributes like `fail_stop` for example.\r\n\r\nSomewhat like: \r\n```\r\nclass TaskState(BaseModel):\r\n    """\r\n    Update a task\

This still doesn\

@ashb tested out with retries as 0, 3, None. Works as expected.\r\n<img width="320" alt="image" src="https://github.com/user-attachments/assets/b5b78a53-6ad6-433f-b5bd-946fd657b74b" />\r\n

Resolved all conversations. Merging this PR.

the docker image of 2.10.4 is still using uv  0.4.29\r\n\r\ncould we backport this PR on 2.10.X ? thanks all

You can do it yourself. Maintainers don\

hello potiuk , thanks for the answser, my question was not explicit.\r\n\r\nabout `there is no partucular need to wait for "someone else" to do it.` \r\n\r\nI was aksing for guidance to apply the good process, cause I seen the doc https://github.com/apache/airflow/blob/main/dev/README_AIRFLOW3_DEV.md#how-to-backport-pr-with-github-actions ( and many discussion on the slack `contributors` about backport ) and I\

See https://github.com/apache/airflow/blob/main/dev/README_AIRFLOW3_DEV.md#how-to-backport-pr-with-cherry-picker-cli \r\n\r\nFollowing this process you can cherry-pick and open PR regardless if you are committer or not.

> CI need fixing, most likely CLI tests need to be updated.\r\n\r\nFixed tests after updating `setproctitle `

Ok, I think we left enough time for people to participated in case they feel that some other options are necessary. We can always improve later, merging.

Very weird. @tirkarthi, Do you want to open those changes as PR?

Trying to fix vs revert here: #45092

@jscheffl @pierrejeambrun @bbovenzi \r\nThis PR will handle the validation and checks and will unblock the\r\nhttps://github.com/apache/airflow/pull/44942

One test need fixing.

> One test need fixing.\r\n\r\nUps, had the same comment but now realize at the point coming back that it was pending :face_in_clouds: 

Closing, was just testing here.

cc @Owen-CH-Leung

LGTM!

Looks good to go :)

> Yeah. I am not sure why it has been missing, but a user noticed that it was not in https://github.com/apache/airflow/pull/41995#issuecomment-2554458054  even if it had 2.10.2 as milestone, so we missed it somehow (cc: @utkarsharma2 )

Also looks good to go... 

I have not seen it to be flaky. I think caplog behaviour is bad in **some** cases only.

@radujica We switched to uv recently. Something like this should work\r\n\r\n```\r\nuv venv\r\nuv sync  --extra devel --group dev\r\n.venv/bin/pytest tests/models/test_dagbag.py::test_dagbag_dag_collection\r\n```

TL;DR; is directly there in the contributing docs BTW: https://github.com/apache/airflow/blob/main/contributing-docs/07_local_virtualenv.rst

Also @ashb - not sure if you know but `uv run pytest ...` is  a simpler replacement when you already did sync once. It will repeat the latest sync (if needed) and activate the default venv for the command :)

Thank you for the swift input both :)\r\n\r\nHad lazily assumed I could just use `conda` but clearly it misses something that `uv` does not, as the error went away after using `uv`. For posterity, this works fine on my mac:\r\n\r\n```bash\r\nuv venv\r\nuv sync --extra devel\r\nuv run pytest tests/models/test_dagbag.py::TestDagBag::test_dagbag_dag_collection\r\n```\r\n\r\nNever used `uv` before and funny enough, it seems to override what `conda` does, i.e. I created a new conda env and installed pytest inside however pytest still points to the pytest installed by uv "outside" the conda env.\r\n\r\nAnyway, will update PR with test soon enough.

@ashb @XD-DENG not urgent, enjoy holidays! just to say this is ready for review IMO

> Actually - this is not securre enough.\r\n> \r\n> The problem is that you can pass many, many bad things via JDBC url - when the values are not properly escaped. It\

> I guess this also applies to the already existing Principal and Proxy User fields? I see that they only validate if a ; character was passed\r\n\r\nI think there are more validations needed. Passing arbitrary parameter as path to jdbc is dangerous (what happens if for example jdbc driver displays content of the file when it is wrong and you pass "/etc/passwd"` ?. This is just example, it could be even more diastrous - printing more secret keys and secret variables stored somewhere on remote system.  I am not sure if you can make it "secure" when this parameter is passed via UI and free-form.\r\n\r\nThere are only few values allowed for transportMode I guess, so it is safer to enumerate them rather than pass directly. When it comes to password, there is a question how `;` is going to be passed (i..e what form of escaping should be there)?\r\n

> > I guess this also applies to the already existing Principal and Proxy User fields? I see that they only validate if a ; character was passed\r\n> \r\n> I think there are more validations needed. Passing arbitrary parameter as path to jdbc is dangerous (what happens if for example jdbc driver displays content of the file when it is wrong and you pass "/etc/passwd"` ?. This is just example, it could be even more diastrous - printing more secret keys and secret variables stored somewhere on remote system. I am not sure if you can make it "secure" when this parameter is passed via UI and free-form.\r\n> \r\n> There are only few values allowed for transportMode I guess, so it is safer to enumerate them rather than pass directly. When it comes to password, there is a question how `;` is going to be passed (i..e what form of escaping should be there)?\r\n\r\nThanks for the insight. I will see what I can do.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

We do not need this PR anymore, handled using #45106

### Backport failed to create: v2-10-test. View the failure log <a href=\

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

@potiuk Can we merge this please?

This is how it works for me:\r\n\r\n![image](https://github.com/user-attachments/assets/237558dd-9154-4449-bceb-5a0f3f3464b8)\r\n

> checkout airflow repo\r\ninstall breeze: uv pip install -e ./dev/breeze\r\nbreeze release-management prepare-provider-packages databricks --version-suffix-for-pypi dev0\r\nInstall resulting package that will be placed in dist.\r\n\r\nHey @potiuk I followed your steps to install and validate. But got stuck post that. \r\n\r\n\r\n1. I executed the commands which you shared and it generate .whl file in dist directory. `dist/apache_airflow_providers_databricks-7.0.0.dev0-py3-none-any.whl`\r\n2. I googled and found the command `pip install dist/apache_airflow_providers_databricks-7.0.0.dev0-py3-none-any.whl` to install the wheel packages.\r\n3. I executed the command in the same directory (airflow source code) and I guess it install airflow there. (This is my assumption)\r\n\r\nOther things which I tried was,\r\n\r\n1. Build a docker container in local and tried to substitute it with the apache/airflow docker image in my custom airflow code to bring up the airflow env. Post this, I triggered the DAG and I saw only notebook params field in `rendered template` section.\r\n<img width="672" alt="image" src="https://github.com/user-attachments/assets/20897eb5-8abd-4eea-a8cd-b0ec5b42982e" />\r\n\r\n2. I did some digging and went through the docker file and found `ARG AIRFLOW_VERSION="2.10.4"` in docker file. \r\n3. I also went through the `INSTALL` doc within the airflow source code repo to bring up the test env.\r\n\r\nIf you don\

We need to backport it, to keep CI from failing.

Note: As PR #45312 has been merged, the code formatting rules have changed for new UI. Please rebase and re-run pre-commit checks to ensure that formatting in folder airflow/ui is adjusted.

Running all tests just to be sure

I updated this PR to create a new pre-commit to generate assets from FAB provider. It is basically the solution 2 described by @potiuk in [this comment](https://github.com/apache/airflow/pull/45060#discussion_r1943332556). I duplicated the script `scripts/ci/pre_commit/compile_www_assets.py` to `scripts/ci/pre_commit/compile_fab_assets.py`to make it easier when we remove the Airflow 2 Flask UI, we will only have to delete `scripts/ci/pre_commit/compile_www_assets.py` (and its associated pre-commit).\r\n\r\nThe artifacts from FAB provider are not built on build time but are now part of the codebase.

I dont quite understand why the static checks are failing üëÄ 

The static check seems to delete the generated files I checked in. I dont understand why, I am still trying to find out why

> The static check seems to delete the generated files I checked in. I dont understand why, I am still trying to find out why\r\n\r\nWithout looking into details, it might be that generating the files (and pre-commit in general) that generates them has a slightly different environmen in CI than in your env - maybe the local env has different npm/yarn version (not pinned) or maybe there is another configuration setting in CI that makes the result different.\r\n\r\nThere are a number of "tricks" to make things reproducible regardless of the environment (for example when we generate image snapshot of breeze we fix the terminal width, and hard-code some variable things (like default number of processors) that can vary between the different systems.\r\n\r\nSometimes this is unavoidable - for example when things are run on Mac and Linux there might be some real differences - then we might revert to running stuff in docker (to ensure maximum environment similarity) or have another trick - often we can calculate hash of the "source" files (and we do it in a few places) and implement the pre-commit to only actually do something when the hash changes - we do that with breeze help snapshots. We store the hash as part of the commit and pre-commit will only actually regenerate stuff if the hash of sources changes. That might be an easy way out as well. 

> > The static check seems to delete the generated files I checked in. I dont understand why, I am still trying to find out why\r\n> \r\n> Without looking into details, it might be that generating the files (and pre-commit in general) that generates them has a slightly different environmen in CI than in your env - maybe the local env has different npm/yarn version (not pinned) or maybe there is another configuration setting in CI that makes the result different.\r\n> \r\n> There are a number of "tricks" to make things reproducible regardless of the environment (for example when we generate image snapshot of breeze we fix the terminal width, and hard-code some variable things (like default number of processors) that can vary between the different systems.\r\n> \r\n> Sometimes this is unavoidable - for example when things are run on Mac and Linux there might be some real differences - then we might revert to running stuff in docker (to ensure maximum environment similarity) or have another trick - often we can calculate hash of the "source" files (and we do it in a few places) and implement the pre-commit to only actually do something when the hash changes - we do that with breeze help snapshots. We store the hash as part of the commit and pre-commit will only actually regenerate stuff if the hash of sources changes. That might be an easy way out as well.\r\n\r\nYep, that\

> > > The static check seems to delete the generated files I checked in. I dont understand why, I am still trying to find out why\r\n> > \r\n> > \r\n> > Without looking into details, it might be that generating the files (and pre-commit in general) that generates them has a slightly different environmen in CI than in your env - maybe the local env has different npm/yarn version (not pinned) or maybe there is another configuration setting in CI that makes the result different.\r\n> > There are a number of "tricks" to make things reproducible regardless of the environment (for example when we generate image snapshot of breeze we fix the terminal width, and hard-code some variable things (like default number of processors) that can vary between the different systems.\r\n> > Sometimes this is unavoidable - for example when things are run on Mac and Linux there might be some real differences - then we might revert to running stuff in docker (to ensure maximum environment similarity) or have another trick - often we can calculate hash of the "source" files (and we do it in a few places) and implement the pre-commit to only actually do something when the hash changes - we do that with breeze help snapshots. We store the hash as part of the commit and pre-commit will only actually regenerate stuff if the hash of sources changes. That might be an easy way out as well.\r\n> \r\n> Yep, that\

Finally!

Okay, I _thought_ this is an easy fix... but it seems the pre-commit is also failing on a clean worktree - not an expert but smells like the yarn.lock is missing and needs to be added. But it is not generated so I can not add it.\r\n\r\nI propose to go rather the reversion route and then make it "right".

Attempting another yarn.lock, maybe this fixes it...

Okay, giving.up here :-(

Note: I#ll have another PR as fix in 5min, maybe that saves a reversion...

I propose to close this in favor of #45058

Yep.

Other PR as quick fix failed, so I change my opinion and would propose the reversion route. Then we can re-apply and check fully.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

I assume unrelated to this PR but on the task there is a `NaN` displayed for duration when it is still running:\r\n![image](https://github.com/user-attachments/assets/eae0d4b6-d761-40b2-ac5f-9ec739d48662)\r\n(Did not know that there is a plural form of `NaN` existing. :-D )

Yes I can add autorefresh and fix the duration in other PRs

@bbovenzi @jscheffl   I have a PR open to handle NaN across API calls and in UI for running dagruns and task instance. Happy to know your thoughts. https://github.com/apache/airflow/pull/44989

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> Needs also a test for the success scenario. You can add the needed changes to `test_execute_success` or write a separated test if needed\r\n\r\nDone! 

@eladkal ?

BTW. That issue picked my interest - and I looked closely - it seems that the "constraint" cryptography is held by yandexcloud - and I created an issue for them to address it https://github.com/yandex-cloud/python-sdk/issues/131 \r\n\r\nI think it\

I am closing this issue as this was really wrongly addressing the issue.

@potiuk - seems like the cryptography version for yandex-cloud has already been bumped to 43.0.1 which should address the issue.\r\nhttps://github.com/yandex-cloud/python-sdk/pull/127/files

I should have looked closer.  Thanks for checking @potiuk 

Comments addressed, screenshot updated.

Cool!

Addressed the comments. CI is failing not able to debug why CI pipeline is failing.

Except some minor comment I think it is good, let me check what is broken on CI. Nothing related to your code.\r\nIgnore that it is "red" atm, will be better once you push the next change on the PR.\r\n\r\nThen I think... LGTM!

@jscheffl I have fixed the unit test. The CI is passing. and Thanks for the help in dubuging the CI issue.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

While necessary to unblock main, doesnt look like it can cause the errors. Lets try a retrigger or we can merge this

@amoghrajesh Retrigger as in retrigger the failed workflow?

Simplifies it better now. This is a working example:\r\n```\r\nclass A(BaseModel):\r\n    """Schema for updating TaskInstance to a deferred state."""\r\n    trigger_kwargs: Annotated[dict[str, Any], Field(default_factory=dict)]\r\n    @field_validator("trigger_kwargs")\r\n    def validate_moment(cls, v):\r\n        if "moment" in v:\r\n            v["moment"] = AwareDatetimeAdapter.validate_strings(v["moment"])\r\n        return v\r\n\r\nA(trigger_kwargs={"key": "value", "moment": "2024-12-18T00:00:00Z"})\r\nOut[27]: A(trigger_kwargs={\

random issue.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

We also plan some refactor for clear https://github.com/apache/airflow/issues/44867#issuecomment-2548587582

@pankajkoti The scope of #42438 fix seems to be very small. Can you please review and let me know if I am missing anything?

Thanks :)

nice!

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

I also just noticed it was on the "wrong" branch - but only because I had a test on v2-10-test running that was cancelled due to the merge.

> I also just noticed it was on the "wrong" branch - but only because I had a test on v2-10-test running that was cancelled due to the merge.\r\n\r\nApologies for that :)\r\n@potiuk fair enough. Is there any stance about removing the providers from `v2-10-test` to avoid such confusion? I assume that it hasn\

> > I also just noticed it was on the "wrong" branch - but only because I had a test on v2-10-test running that was cancelled due to the merge.\r\n> \r\n> Apologies for that :) @potiuk fair enough. Is there any stance about removing the providers from `v2-10-test` to avoid such confusion? I assume that it hasn\

> > > I also just noticed it was on the "wrong" branch - but only because I had a test on v2-10-test running that was cancelled due to the merge.\r\n> > \r\n> > \r\n> > Apologies for that :) @potiuk fair enough. Is there any stance about removing the providers from `v2-10-test` to avoid such confusion? I assume that it hasn\

Merging, other DB isolation test is fixed in #45021

Nice!

Nice, should we convert it to a draft PR to avoid an accidental merge, since this is for 3.1

> Nice, should we convert it to a draft PR to avoid an accidental merge, since this is for 3.1\r\n\r\n@kaxil \r\nI can update the PR to add more gating logic. Right now there is some in the executor loader, but I can add another one on the use of the `core.multi_team_configurations` config (such that users cannot enable multi team configurations).\r\n\r\nOtherwise it will be difficult to develop the rest of the features without the merging of this one. WDYT?  

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

@amoghrajesh @romsharon98 any comment/review here?

(Yes, this def needs a news fragment, and likely a rule in our Ruff checks)

cc @uranusjr @Lee-W (I think you are working on ruff rules)

Do we want to remove `dag_default_view` from the airflow config at the same time or in another PR?

Re-building...

Tests are passing ü•≥ 

Let me know if you have more questions/concerns @jedcunningham 

@ashb @kaxil ...forgot to "un-draft", after the base has been merged, this would be the next small step.

> Does this need to be back-ported to v2-10-test branch?\r\n\r\nWe can attempt to do it, but pretty much only release managers and people helping with backporting need to use breeze. The links in docker-stack might actually be useful though

Merging... tests are failing but also failing with the same on main: https://github.com/apache/airflow/actions/runs/12380031516/job/34555973915\r\nUnrelated to this PR.

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/45010"><img src="https://img.shields.io/badge/PR-45010-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

Interesting :) Need to take a look.

I think the auto-upgrade of breeze is interfering with dependabot, I will need to take a closer look - as we are both using and upgrading `breeze` at the same time, which might be a bit tricky - also there is the stored "hash" in breeze README that is used for auto-upgrade of breeze that might make it difficult for the dependabot upgrades in breeze.

Closing in favour of #45041 that also contains the fix to failing "get-workflow-info" steps.

@kacpermuda  - can you please rebase that one -> we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests. I am asking in all affected PRs to rebase.

Actually.. I rebased it myself :) 

I think this is something easier to forget as APIs are more widely used in UI and should be enforced or as a convention someway but not sure how.

Adding `extra` field in xcoms/variable datamodels, test passed.. so I added only task_instance test. Do i miss anything?\r\ne.g in, [TestGetVariable.test_variable_get_from_db](), request with `params` extra fields but test passed (expected this test to fail, but...üò≠)\r\n```\r\n    def test_variable_get_from_db(self, client, session):\r\n        Variable.set(key="var1", value="value", session=session)\r\n        session.commit()\r\n\r\n        response = client.get("/execution/variables/var1", params={"foo": "bar"})\r\n\r\n        assert response.status_code == 200\r\n        assert response.json() == {"key": "var1", "value": "value"}\r\n\r\n        # Remove connection\r\n        Variable.delete(key="var1", session=session)\r\n        session.commit()\r\n```\r\n

One test to remove and we should be good.

Re-running failed job\r\n

@pierrejeambrun Do i need to modify code?

CI is green, merging, thanks.

Hi @potiuk , could we merge it please?

> Hi @potiuk , could we merge it please?\r\n\r\nThere was a comment from @eladkal that needs to be addressed 

Thanks @kaxil and @dstandish. I got confused here with similar names "up_for_retry" and "upstream_failed". Will take a look and redo it tomorrow. Thanks

Closing this PR as it is easier to create a new one

>Right now this puts a\r\nnasty/unwanted depenednecy between the Dag Parsing code upon the TaskSDK.\r\n\r\nI think that is unavoidable, as user code will come from Task SDK\r\n\r\n\r\n>This will be addressed before release (we have talked about introducing a\r\nnew "apache-airflow-base-executor" dist where this subprocess+supervisor\r\ncould live, as the "execution_time" folder in the Task SDK is more a feature\r\nof the executor, not of the TaskSDK itself)\r\n\r\nRight, I think processor will have to depend on both Task SDK (user-facing code) + Base Executor dist -- after that separation

Right, I think this should now pass the tests, the only thing I\

(I still need to rename a class and file, but that is a non-meaningful/non-review-impacting change.

> Right, I think this should now pass the tests, the only thing I\

cc: @MaksYermak ?

Thanks for fixing it Jed

@jscheffl / @uranusjr -- Does this good look for 2.10.5/2.11?

> Look good.\r\n> \r\n> Can you please also add a deprecation note to https://github.com/apache/airflow/blob/v2-10-test/docs/apache-airflow/templates-ref.rst#deprecated-variables ?\r\n> \r\n> Newsfragment would also be good!\r\n\r\nDone in https://github.com/apache/airflow/pull/44968/commits/a28a1189394b6fc7b9258231cbdcacd89a98515a

Should this target 2.11.0 instead? It may be weird for users if a deprecation warning starts showing up when they do a patch version upgrade.

CC @amoghrajesh @potiuk 

Do we need to group any package updates ? \r\n\r\nhttps://docs.github.com/en/code-security/dependabot/dependabot-version-updates/configuration-options-for-the-dependabot.yml-file#groups\r\n\r\nhttps://github.blog/changelog/2023-06-30-grouped-version-updates-for-dependabot-public-beta/

Thanks for the merge :) was bit of on travel today. it looks like dependaboat already started playing his role. hahah

https://github.com/apache/airflow/pulls/app%2Fdependabot 

> LGTM - but added one more comment about adding `./dev/breeze` as well.\r\n\r\nThanks for pointing missed that :)

Can you resolve conflicts and rebase?

@eladkal Yes, Done!

cc: @uranusjr @Lee-W for feedback on asset events in UI

Note: As PR #45312 has been merged, the code formatting rules have changed for new UI. Please rebase and re-run pre-commit checks to ensure that formatting in folder airflow/ui is adjusted.

Nice!

Also we should remove all conditional code with `AIRFLOW_V_2_9_PLUS`. \r\n\r\nI\

@kaxil I will merge this PR once the CI is green to unblock the other PRs which are based on this branch

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

> It seems dependaboat is not working properly on our repo for pip packaging upgrades, some time back we have made changes at dependaboat repo to upgrade dependency under build-requires.\r\n\r\n> I think it might require some config changes or workflow changes in this repo. I will look into tomorrow.\r\n\r\nIs that change released already ? If so, then I am quite sure we need to configure it - and then we will be able to disable the manual check we have now that breaks canary for build-dependencies.

Nice one BTW!

> > It seems dependaboat is not working properly on our repo for pip packaging upgrades, some time back we have made changes at dependaboat repo to upgrade dependency under build-requires.\r\n> \r\n> > I think it might require some config changes or workflow changes in this repo. I will look into tomorrow.\r\n> \r\n> Is that change released already ? If so, then I am quite sure we need to configure it - and then we will be able to disable the manual check we have now that breaks canary for build-dependencies.\r\n\r\nYes its released , oh okay , i thought dependaboat already configured on our repo and could see frequently prs from dependaboat for UI library upgrades. 

Interesting. How did it work before?!?

> Interesting. How did it work before?!?\r\n\r\nSlack 2.0.0 introduced breaking changes. I think now we have to provide explicitly which method we are calling. Ex: they have multiple methods updateMessage, postMessage etc;\r\n\r\nNot sure how to test this, it might end up multiple turns to workout these if again fails or best case hopefully it works after this merge 

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Re-trigger build...

before fix, able to re produce.\r\n\r\n<img width="1334" alt="image" src="https://github.com/user-attachments/assets/966c3a07-4f6d-41bc-b441-af7442a48934" />\r\n\r\n\r\n\r\n\r\nAfter fix, it will show as dag import errors.\r\n\r\n![image](https://github.com/user-attachments/assets/edcbf109-3f08-4153-bf38-a881576bf548)\r\n\r\n

@shahar1 Could you verify your changes again -- I needed to force-push on v2-10-test to sync it back with v2-10-stable. 

> @shahar1 Could you verify your changes again -- I needed to force-push on v2-10-test to sync it back with v2-10-stable.\r\n\r\nVerified and it works great :)

Failures are un related, merging now.

Some checks failing

> https://infra.apache.org/github-actions-policy.html\r\n\r\noh yeah my bad, missed. will close this one :) 

closing this one, as github/* namespace allowed to use without commit sha.

> Some checks failing\r\n\r\nsorry closed as it not needed :)

Thanks for the reminder! I‚Äôll wait until https://github.com/apache/airflow/issues/44847 is resolved.\r\n

Lets also please amment the commit message and change the title as this is a fix that we will note in the change log so users needs to know what this is about.

Somehow that fix (while necessaryf for now) seems to be just workaround cc: @o-nikolas -> was it deliberate change or side-effect ?

And good job @amoghrajesh on quick-fixing.

Thanks! Yeah i didnt get enough time to actually debug what went wrong. So, workaround for now

> Somehow that fix (while necessaryf for now) seems to be just workaround cc: @o-nikolas -> was it deliberate change or side-effect ?\r\nThere is a lot of context to explain haha, here goes!:\r\n\r\nThis was a deliberate change. Until now we\

yeah. makes perfect sense. I think we should not compare it to the name but determine the class name from the "default_executor" and compare it with K8SExecutor class. \r\n

(and later when team_id is served when team_id is set for executor, it should only query for tasks with the team_id of course).

Thanks @o-nikolas! Yeah that makes good sense.\r\n\r\n> I could go back and remove this update, but I really do like that the "core executors" are moving closer inline with other executors. The problem comes with how the code here is fetching an executor name (not the executor itself) and comparing it to the airflow.executors.executor_constants.KUBERNETES_EXECUTOR which are NOT the same thing.\r\n\r\nYea as Jarek mentioned, we should just try to compare the class names or the `repr` values instead. The logic in `clear_not_launched_queued_tasks` and related places should probably be updated to consider the match with team id too

Small issue in static check, can you resolve this? Best is to use pre-commit locally then you can catch it before submitting to CI

> I actually am not a fan that autocomplete is turned off just in sake of "security" as usability suffers with this. In my view this is more convenience and is something to be considered on a shared PC... but anyway.\r\n\r\nYea I can see that. Perhaps maybe next iteration can turn this into a Webserver environment variable/Airflow Configuration setting?

> Small issue in static check, can you resolve this? Best is to use pre-commit locally then you can catch it before submitting to CI\r\n\r\nRight on, thanks for the heads up. All fixed now.

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/44940"><img src="https://img.shields.io/badge/PR-44940-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

> UV is very super fast doing releases :)\r\n\r\n100%

Failure is unrelated (should be fixed in https://github.com/apache/airflow/pull/44931), merging

No worries, this is just a standard "branch wasn\

Cross-merging PRs. yeah. \r\n\r\nAnd actually yes we **could** do something about it - that could go away if we had the possibility of using "Merge Queue" functionality: https://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/configuring-pull-request-merges/managing-a-merge-queue\r\n\r\nThe merge queue functionality works in the way, that instead of merging, committer adds PR to merge queue, and they are run in sequence - automatically rebasing them after the previously merged PR is merged - and only actually merging the PR when that "queued" PR had succeded again after rebase. \r\n\r\nSo far it was blocked as  INFRA tooling is unable to retrieve "merger identity" for necessary ICLA "audit logs" when merge queue is used. But @davidarthur from Kafka team run a sandbox environment https://issues.apache.org/jira/browse/INFRA-25932 where   he had shown how it is possible - so now it hangs a bit on INFRA making a decision whether to invest in it, implement missing piece and enable it. \r\n\r\nIf people here would like to comment on ths INFRA ticket and encourage it, having the possibility of using Merge Queues could help us to avoid this kind of issues, without impacting the velocity of merges. 

Merge queue are a double edge sword, especially with our full test matrix taking ~1 hour and our sometimes flakey tests and it would greatly limit the number of PRs we could merge.

> Merge queue are a double edge sword, especially with our full test matrix taking ~1 hour and our sometimes flakey tests and it would greatly limit the number of PRs we could merge.\r\n\r\nNot when we go back and have ARC enabled, our tests elapsed time will go back to 20 minutes, and we already have done pretty damn good job (and continue doing so) on battling flaky tests. With our team effort we already have sometimes days without flaky tests, and yes while I would not want to use merge queues two months ago, I think we are pretty much ready to get it and get it much more stable (especially after we bring back 16 processor machines in our S3 to complete our tests 4 and sometimes even 8 times faster than the current builds. CC: @hussein-awala  :D 

PLus merge queue will STILL use the selective checks optimizations - that is not going to change - so most of the PRS will be done in 10 minutes even without ARC (and maybe less than 4-5 minutes with ARC).

I also think consistency is very important and worth paying a time cost for. Something could have been dropped that was harder to detect except in narrow runtime circumstances.

Irrelevant commits are here, closing to resolve this.

Drafting, as I start working on implementing the ShortCircuit operator for Airflow 3.\r\nThe contents of this PR might be integrated there already.

Will be merged as part of #46584

Remaining failure is unrelated.

Nice!  thank you !

Nice .. I like the idea. Tried to do similar thing to `common.sql` with stubgen (after initial attempt of doing it similarly to you) - but this explicit approach is I think better - even if requires more code. \r\n\r\nOne other option I saw in Android OS (and maybe that is a good idea for you to explore) - was to turn the signatures into a plain-text file where you keep signatures of public methods - and keep it in the repository next to the code. Then you do not have to check-out the previous version of the code (Which is not always available - in CI we only retrieve single commit and we do not have parent commit by default) - you just generate a new version of such signature "dump" file - then pre-commit framework will automatically see that the dump has changed and will fail precommit  (and you would have to commit such modifed file to accept  changed signatures)

Smth like that:\r\n\r\n```txt\r\nmethod1(a:string, b:string) -> int\r\nmethod2(a:string, b:int) -> bool\r\n```

This is such a "current.txt" generated in Android AOSP file https://android.googlesource.com/platform/frameworks/base/+/160bb7fa60e8ece654e6ce999b6c16af50ee7357/api/current.txt

>One other option I saw in Android OS (and maybe that is a good idea for you to explore) - was to turn the signatures into a plain-text file where you keep signatures of public methods - and keep it in the repository next to the code. Then you do not have to check-out the previous version of the code (Which is not always available - in CI we only retrieve single commit and we do not have parent commit by default) - you just generate a new version of such signature "dump" file - then pre-commit framework will automatically see that the dump has changed and will fail precommit (and you would have to commit such modifed file to accept changed signatures)\r\n\r\n@potiuk I like it - I think the advantage of this idea is that it allows you to _break_ the compatibility way easier - for example, if the old version of Airflow goes out of scope and providers can move to regular import from core Airflow.\r\n\r\nHowever, the two main issues remain: handling of imports - maybe mark them as API too somehow? This would allow us only to track relevant ones.\r\n\r\nThose nested classes - still not sure how to handle those. And I think if we\

> However, the two main issues remain: handling of imports - maybe mark them as API too somehow? This would allow us only to track relevant ones.\r\n\r\nYes. We could generate that text file as part of imports.\r\n\r\n> Those nested classes - still not sure how to handle those. And I think if we\

@potiuk see that example from example: \r\n\r\n```\r\ndef get_hook_lineage_collector():\r\n    # Dataset has been renamed as Asset in 3.0\r\n    if AIRFLOW_V_3_0_PLUS:\r\n        from airflow.lineage.hook import get_hook_lineage_collector\r\n\r\n        return get_hook_lineage_collector()\r\n\r\n    # HookLineageCollector added in 2.10\r\n    if AIRFLOW_V_2_10_PLUS:\r\n        return _get_asset_compat_hook_lineage_collector()\r\n\r\n    # For the case that airflow has not yet upgraded to 2.10 or higher,\r\n    # but using the providers that already uses `get_hook_lineage_collector`\r\n    class NoOpCollector:\r\n        """\r\n        NoOpCollector is a hook lineage collector that does nothing.\r\n\r\n        It is used when you want to disable lineage collection.\r\n        """\r\n\r\n        # for providers that support asset rename\r\n        def add_input_asset(self, *_, **__):\r\n            pass\r\n\r\n        def add_output_asset(self, *_, **__):\r\n            pass\r\n\r\n        # for providers that do not support asset rename\r\n        def add_input_dataset(self, *_, **__):\r\n            pass\r\n\r\n        def add_output_dataset(self, *_, **__):\r\n            pass\r\n\r\n    return NoOpCollector()\r\n```\r\n\r\nThis could be done in a different way:\r\n\r\n```\r\n# Top-level code\r\n# Dataset has been renamed as Asset in 3.0\r\nif AIRFLOW_V_3_0_PLUS:\r\n    from airflow.lineage.hook import get_hook_lineage_collector\r\n\r\n    return get_hook_lineage_collector()\r\n\r\n# HookLineageCollector added in 2.10\r\nelif AIRFLOW_V_2_10_PLUS:\r\n    return _get_asset_compat_hook_lineage_collector()\r\n\r\nelse:\r\n    # For the case that airflow has not yet upgraded to 2.10 or higher,\r\n    # but using the providers that already uses `get_hook_lineage_collector`\r\n    def get_hook_lineage_collector():\r\n        class NoOpCollector:\r\n            """\r\n            NoOpCollector is a hook lineage collector that does nothing.\r\n\r\n            It is used when you want to disable lineage collection.\r\n            """\r\n\r\n            # for providers that support asset rename\r\n            def add_input_asset(self, *_, **__):\r\n                pass\r\n\r\n            def add_output_asset(self, *_, **__):\r\n                pass\r\n\r\n            # for providers that do not support asset rename\r\n            def add_input_dataset(self, *_, **__):\r\n                pass\r\n\r\n            def add_output_dataset(self, *_, **__):\r\n                pass\r\n\r\n        return NoOpCollector()\r\n ```\r\n\r\nHowever, it still would require parsing version variables - and this is pretty important since it\

\r\n@shahar1 Could you verify your changes again -- I needed to force-push on v2-10-test to sync it back with v2-10-stable.\r\n\r\n

> @shahar1 Could you verify your changes again -- I needed to force-push on v2-10-test to sync it back with v2-10-stable.\r\n\r\nVerified and it works great :)

NAAAAJS

> Awesome, looks really nice! @sunank200 one comment but thats just a thought. Will it also be possible to add a short README or so in order for someone to deprecate any future options? (Although i think all are done)\r\n\r\nReadme for in how to deprecate options in future? @amoghrajesh can you elaborate on this, please

> > Awesome, looks really nice! @sunank200 one comment but thats just a thought. Will it also be possible to add a short README or so in order for someone to deprecate any future options? (Although i think all are done)\r\n> \r\n> Readme for in how to deprecate options in future? @amoghrajesh can you elaborate on this, please\r\n\r\nYeah, that is what I was hinting at. But looking at the state of this PR, it is a good example for how to do it later in future. So I wouldnt ask for that anymore

re: The README topic, I feel at least we should introduce a news fragment on this.\r\n\r\nWe should probably add a page in documentation that guides people through the migrations. This would be useful for us in blogposts and discussions on Airflow 2-to-3 upgrade path. (This should be a separate PR.)

Also tentitively backporting this to 2.11 since it would be very useful for users if it is possible to run the check before upgrading. We need to check if the command actually works manually too.

one test case failed

> one test case failed\r\n\r\nIt was from different section and fixed it now.

### Backport failed to create: v2-10-test. View the failure log <a href=\

@kaxil merging this one. I think I have handled the comments on this

Why are those things failing‚Ä¶

I don‚Äôt know why it failed, but I found a way that works.

Re-triggering with full tests

Closing in favour of #44895

@potiuk is going to pick this up and open a replacement PR.

Replacement PR in #44888

Can you explain why this is useful helpful? I imagine the stack trace is going to be large and mostly not relevent to when a task was SIGTERMd.

> Can you explain why this is useful helpful? I imagine the stack trace is going to be large and mostly not relevent to when a task was SIGTERMd.\r\n\r\nI agree the traceback can be large in some cases, but the idea is to get the code line on which the task was killed. Ideally it will help to troubleshoot issues that we get

> Following Ash\

> > Following Ash\

### Backport failed to create: v2-10-test. View the failure log <a href=\

currently, the pod where the log is collected and the pod where xcom sidecar is located should be same. but for rayjob. log is on submitter, but xcom should be mounted on head. it seems that airflow cannot support this.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Needs rebase now.

> Needs rebase now.\r\n\r\nThank you! Rebased.

PR is ready to be merged, there is some unrelated flaky test that makes TestLocalExecutor timeout.

Not sure why my pre-commit run and the one in CI are different.

In the legacy UI in 2.10 I invested a bit of efforts making XCom better readable for users in https://github.com/apache/airflow/pull/40640 . Especially for Dicts and lists. Can you re-apply such feature also for the new XCom display, such that dicts are not just dumped as text? (I know I generated a set of side-effects as bugs which needed to be fixed, so in the new UI we have the chance to make it "right the first time" also with a re-usable component for DAG Run Conf.

Thanks @jscheffl, it needs porting `ReactJSON` related component from legacy UI and would like to tackle it in a separate PR as noted in the PR description since it seems to take fair bit of work.

> Thanks @jscheffl, it needs porting `ReactJSON` related component from legacy UI and would like to tackle it in a separate PR as noted in the PR description since it seems to take fair bit of work.\r\n\r\nFair. As long as it is not forgotten :-D And does not need to be e exactly the same component as in legacy if there are better components nowadays.\r\n\r\nRegarding (3) in your list above: In the past "Rest API" call the public XCom endpoint was used and for adding the "ReactJSON" I needed to add the "stringify" option as workarouns as the stringified version before used Python-style quotes which were not JSON parsable in the React / Javascript code. In the legacy this is really bad (in my view) and it would be better if in the new UI it is made "right" from the beginning. I wanted to prevent this in the past not having a breaking change in the REST API - but now would be the time with FastAPI anyway. So `stringify` might need to be set to `false` later back again to get a native object, else would be a waste to re-parse the string to JSON in TypeScript again after REST endpoint

> Thanks @jscheffl, here is one rough attempt I can think of. Pass `stringify: false` in the API. Try `JSON.stringify()` and set highlight to true to use `<SyntaxHighlighter />`. On error just convert to string using `String()` and set highlight to be false to render it as normal `<Text>`. But this also means if there is `__str__` implemented for the custom object in Python land then JS `String()` value might be different in case the object doesn\

Still some problems. Don‚Äôt merge this yet.

I can‚Äôt come up with a good way to test this (our test setup does not lend well toward real scheduler isolation). I‚Äôm going to just cross fingers and merge this if @Lee-W does not reject.

Good catch, thanks!

@kaxil if we agree with my comment https://github.com/apache/airflow/pull/44843#discussion_r1881472643 here, then this payload:\r\n```\r\nJsonAbleValueTypes = Union[\r\n    dict[str, JsonValue],\r\n    list[JsonValue],\r\n    JsonValue,\r\n]\r\n```\r\n\r\nWill do the job really well. I just pushed a commit with those changes. Added some tests for variety of payloads too

FYI, I ran the DAG from ash that found this issue:\r\n```\r\nfrom __future__ import annotations\r\n\r\nimport sys\r\nimport time\r\nfrom datetime import datetime\r\n\r\nfrom airflow.decorators import dag, task\r\n\r\n\r\n@dag(\r\n    # every minute on the 30-second mark\r\n    catchup=False,\r\n    tags=[],\r\n    schedule=None,\r\n    start_date=datetime(2021, 1, 1),\r\n)\r\ndef hello_dag():\r\n    """\r\n    ### TaskFlow API Tutorial Documentation\r\n    This is a simple data pipeline example which demonstrates the use of\r\n    the TaskFlow API using three simple tasks for Extract, Transform, and Load.\r\n    Documentation that goes along with the Airflow TaskFlow API tutorial is\r\n    located\r\n    [here](https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html)\r\n    """\r\n\r\n    @task()\r\n    def hello():\r\n        print("hello")\r\n        time.sleep(3)\r\n        print("goodbye")\r\n        print("err mesg", file=sys.stderr)\r\n\r\n    hello()\r\n\r\n\r\nhello_dag()\r\n\r\n```\r\n\r\n\r\nusing this UT:\r\n```\r\ndef test_startup_dag_with_no_templates_mixed_types(mocked_parse, test_dags_dir):\r\n    """Test startup of a simple DAG."""\r\n    what = StartupDetails(\r\n        ti=TaskInstance(id=uuid7(), task_id="hello", dag_id="hello_dag", run_id="c", try_number=1),\r\n        file=str(test_dags_dir / "mydag.py"),\r\n        requests_fd=0,\r\n    )\r\n    ti  = parse(what)\r\n\r\n    with mock.patch(\r\n        "airflow.sdk.execution_time.task_runner.SUPERVISOR_COMMS", create=True\r\n    ) as mock_supervisor_comms:\r\n        mock_supervisor_comms.get_message.return_value = what\r\n        startup()\r\n        run(ti, log=mock.ANY)\r\n\r\n```\r\n\r\nResult:\r\n```\r\nHome of the user: /Users/amoghdesai\r\nAirflow home /Users/amoghdesai/airflow\r\nPASSED [100%][2024-12-12T15:16:57.104+0530] {dagbag.py:535} INFO - Filling up the DagBag from /Users/amoghdesai/Documents/OSS/airflow/task_sdk/tests/dags/mydag.py\r\n[2024-12-12T15:16:57.175+0530] {dagbag.py:535} INFO - Filling up the DagBag from /Users/amoghdesai/Documents/OSS/airflow/task_sdk/tests/dags/mydag.py\r\n2024-12-12 15:16:57 [debug    ] DAG file parsed                [task] file=/Users/amoghdesai/Documents/OSS/airflow/task_sdk/tests/dags/mydag.py\r\nhello\r\ngoodbye\r\n[2024-12-12T15:17:00.178+0530] {python.py:197} INFO - Done. Returned value was: None\r\n```

War to fix the CI starts!

@amoghrajesh Created https://github.com/apache/airflow/pull/44920

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Looks like I left a load of `validate_database_executor_compatibility` in the tests.

This is a fantastic improvement. And it will make "airflow standalone" finally getting really usefuil for "local experience".

> Would it be better to just have a checkbox for this instead? Prone to error.\r\n\r\ngood point! let me add it

2 test cases still need to be fixed but the overall implementation is wrapped up

Should we also fail if an inactive asset is added by the user to an asset alias? I feel we should for consistency. If that‚Äôs the case, this should better be done _after_ the task is run (during the event-pushing phase) instead.

> Should we also fail if an inactive asset is added by the user to an asset alias? I feel we should for consistency. If that‚Äôs the case, this should better be done _after_ the task is run (during the event-pushing phase) instead.\r\n\r\njust handled `AssetAlias.add`

Ok I decided to make the list of TIs only ever link to a single TI and not confuse users with links to Task or Dag Run too.\r\nI also turned "Latest Run" and "Latest Instance" fields on the Dag and Task cards into links.

Slowly piecing it together - they can [come from KE](https://github.com/apache/airflow/blob/42dfa7eee1d4816dfb95863c7e472c250eb5765b/providers/src/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py#L809-L813), but the cleanup pods is also useful for KPO... guess no one cares since no one has complained

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

I am sorry for all of this 

> I am sorry for all of this\r\n\r\nNo worries :). We reviewed it and did not notice! We are humans and we make mistakes. That what makes us humans.

:robot: \r\n

You broke integration tests with this change :P - https://github.com/apache/airflow/actions/runs/12262659739/job/34212827799?pr=44823\r\n\r\nI veto this change ;)-- jk ofcourse -- failure is unrelated\r\n\r\n\r\n![](https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExenZiamVueWtqZnk2bWJnbm53NHR4OXR2ZHgzMHFrMDh6cnk0Z2xyayZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ZkUlTZe2cgpjL6FfKW/giphy.gif)\r\n\r\n![](https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExaGQ2dGNvcngzeHptcndhMWR2bWQ0b2JqMWV5amdhMGZteTZpd3FtZyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/VHxLlZT4w1NZYYQdJt/giphy.gif)\r\n

:scream: :scream: :scream: :scream: :scream: :scream: :scream: :scream: :scream: :scream: 

Hahah good one! Its been fixed by https://github.com/apache/airflow/pull/44829

Reopening to trigger tests.

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/44827"><img src="https://img.shields.io/badge/PR-44827-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

static checks :(

> static checks :(\r\n\r\nthanks :)\r\nAgain this static checks üåû \r\nFixed!

Following dependency check is failing in breeze:\r\n\r\n```\r\npytest.param(\r\n            ("providers/src/airflow/providers/standard/operators/bash.py",),\r\n            {\r\n                "selected-providers-list-as-string": "common.compat standard",\r\n                "all-python-versions": "[\

> Following dependency check is failing in breeze:\r\n> \r\n> ```\r\n> pytest.param(\r\n>             ("providers/src/airflow/providers/standard/operators/bash.py",),\r\n>             {\r\n>                 "selected-providers-list-as-string": "common.compat standard",\r\n>                 "all-python-versions": "[\

> @potiuk Okay but this would then have an impact on imports no? Or would you keep same structure as is and move the GenericTransfer from standard providers to common sql?\r\n\r\nGeneric Transfer has **only** been moved to "standard" provider recently as part of the preparation for Airflow 3. And the "standard" provider is not YET released in a `1.0.*` version - it is 0.0.3 now - because we have not completed yet extraction of everything there, and we expected that we might have some changes here and there, so Generic Transfer moved to the standard provider can be classified as mistake - should be moved to common.sql in the first place, and we can do it without taking care about back-compatibility.\r\n\r\nThe only back-compatibiity issue is that the old generic transfer should be redirected in Airflow 3 - but we can simply redirect it to the new place in common.sql, no problem with it whatsoever:\r\n\r\n* https://github.com/apache/airflow/blob/main/airflow/operators/__init__.py#L45

@dabla  - I rebased it  -> we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests. 

> @dabla - I rebased it -> we found and issue with @jscheffl with the new caching scheme - fixed in #45347 that would run "main" version of the tests.\r\n\r\nThx @jscheffl and @potiuk 

@potiuk @eladkal Once [PR 45478](https://github.com/apache/airflow/pull/45478) is merged less files will be impacted here as I moved that extraction to a dedicated [PR](https://github.com/apache/airflow/pull/45478) to simplify the review of this one.

> > @potiuk Okay but this would then have an impact on imports no? Or would you keep same structure as is and move the GenericTransfer from standard providers to common sql?\r\n> \r\n> Generic Transfer has **only** been moved to "standard" provider recently as part of the preparation for Airflow 3. And the "standard" provider is not YET released in a `1.0.*` version - it is 0.0.3 now - because we have not completed yet extraction of everything there, and we expected that we might have some changes here and there, so Generic Transfer moved to the standard provider can be classified as mistake - should be moved to common.sql in the first place, and we can do it without taking care about back-compatibility.\r\n> \r\n> The only back-compatibiity issue is that the old generic transfer should be redirected in Airflow 3 - but we can simply redirect it to the new place in common.sql, no problem with it whatsoever:\r\n> \r\n> * https://github.com/apache/airflow/blob/main/airflow/operators/__init__.py#L45\r\n\r\nMoved the GenericTransfer to the common sql provider as this makes more sense.

The root cause is here: https://github.com/apache/airflow/actions/runs/12743121090/job/35512486483?pr=44809\r\n\r\nYou need to run pre-commit with your change to regenerate .json file where we keep dependencies cross-providers.

I\

@eladkal @potiuk  test are green again

> I wonder if this might be best done as a fixture builder -- i.e. a fixture which returns a function:\r\n> \r\n> ```python\r\n> @pytest.fixture\r\n> def mocked_parse(spy_agency):\r\n>     def set_dag(dag_id, task):\r\n>         dag = get_inline_dag("super_basic", task)\r\n>         agency.spy_on(parse, call_fake=lambda what: mocked_parse(what, dag, task.task_id))\r\n>     return set_dag\r\n> ```\r\n> \r\n> Which we could then use like this:\r\n> \r\n> ```python\r\n> def test_run_basic(time_machine, mocked_parse):\r\n>   mocked_parse(dag_id="super_basic_run", task=CustomOperator(task_id="hello"))\r\n> ```\r\n\r\nActually I really like this, the tests can be made much simpler with this, no need to worry about manually adding spy_agency and complicating things.

Rebased with main after merging #44725

Some static check failures

> Some static check failures\r\n\r\nHaha! Was just fixing it

Finally green CI!

I force-pushed to rebase on top of main -- I wish GitHub makes "rebase" the default instead of "merge", the commit history to review becomes a lot cleaner

Can you also add support for catalog in `GlueCatalogPartitionSensor` ?

Code looks good, I agree with @eladkal suggestion, it would be awesome if you could do the same kind of changes in `GlueCatalogPartitionSensor `

> Only if you (contributor) can fulfill the expectation of the generative tooling policy of the ASF. Otherwise we cannot accept it.\r\n\r\nAck. I am looking into the generative tooling policy + Devin policy as well

As explained in the issue, this is licencing not ownership issue potentially - so you and devin should seek confirmation if this is OK to use such code via https://www.apache.org/legal/#communications

Ack. Asked the Cognition team to reach out through the official channels + the issue thread. Will keep the further updates in the issue to avoid duplicate conversation threads. Ty for the prompt guidance here, interested to see if this goes through :) \r\n\r\n*will leave the PR open until this is resolved

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Static check (formatting) needs fixing before merging.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

One tests is failing it looks like flaky.

Yep. Re-run it just in case.

This one is pretty straight forward. Merging it

Nice. But tests have to pass :)

> Nice, tested locally, working as expected.\r\n> \r\n> Should we target `main` and backport instead of specifically targeting `v2-10-test`. When changes are compatible I believe main then backport is better to limit drift between the two branches, I tend to directly target `v2-10-test` for changes that are completely different and incompatible with main anymore. @ephraimbuddy what do you think ?\r\n\r\nI agree with you. Main then backport, for changes that are a bit compatible otherwise a different PR targeting the test branch would be better

Closing this PR, since we decided to push this one to `main` and backtrack to `v2-10-test`. \r\n\r\nNew PR: https://github.com/apache/airflow/pull/44929

BTW: @ashb @kaxil -> while this should fix the test, I wonder if this is our concern that stdout /stderr might come in different order than they were produced. That is generally I think quite impossible to prevent if you have parallel threads reading from two different pipees, the only "practical" way I know is to make sure both stdout and stderr are joined as single stderr+out pipe by the process that produces them. So i am not sure if there is anything we can do here if we want to keep the stdout/stderr separation. \r\n\r\nThat\

>This is a good fix, or another option is to compare stdout and stderr messages independently.\r\n\r\nI think doing independently might be better as we do want to make sure this come ordered, no @ashb ?

Side comment. Regardless - looked at the code and see there are selectors used - nice :) 

Interestingly enough - that one got a conversation started by someone who has been apparently malicious user and deleted since ... And I cannot merge it as the conversation is gone. I will raise the issue to Github and open a new PR.

@hussein-awala @gopidesupavan @jscheffl -> https://github.com/apache/airflow/pull/44774 kind request to transfer your approvals there :)

Ticket created in Github Support: https://support.github.com/ticket/personal/0/3130863

Ah okay I see what has happened, when we receive the failed response we enter the kill loop, and as part of letting the task shutdown cleanly we can _re-enter_ the send_heartbeat if needed bath.\r\n\r\nPR coming to prevent that

> Ah okay I see what has happened, when we receive the failed response we enter the kill loop, and as part of letting the task shutdown cleanly we can _re-enter_ the send_heartbeat if needed bath.\r\n> \r\n> PR coming to prevent that\r\n\r\nCool.

Spun the removal of the kubernetes commands from core out of this PR here: #44826

Thanks @potiuk for running this one down with me üçª 

> Thanks @potiuk for running this one down with me üçª\r\n\r\nI am sorry you had to suffer it :( . And yes we should definietely - when we restructure our packaging and "distributions" simplify the configuration loading and removing the double-loading if possible. 

Working on a backport :)

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/44758"><img src="https://img.shields.io/badge/PR-44758-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

Closed in favor of #44633

> Overall, lgtm. Do we need to add a step to run this into the provider release process for FAB?\r\n\r\nI think so. Do you know where is this provider release process?

I just had a double thought. Would not it be better to copy paste this script in FAB provider and generate only the assets for FAB provider in this script, update the release process of FAB provider to run this script? That way, when we remove the entire Flash application from core, we can safely remove the script from core?

Oh, that probably is a better idea.\r\n\r\nThese are the docs for the release process: https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md

I think we no longer need this PR? cc @potiuk 

Yep

Replaced by #44924, which moves this into config.

There is something wrong with selective tests,  making full tests here, then I assume good to merge if green

Need to close/open to force a re-build...

Just resolve, move `QueryTaskDisplayNamePatternSearch` to common paramters.

Ah yes. cross-PRs merged

Thanks for the review, merging this one.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

@potiuk modified the code as you asked I think this one is ok now

Can you fix the tests please ?

Would you please tell me how to fix this test ? \r\n\r\nThere is only one deprecated class  `TrinoOperator` in [providers/src/airflow/providers/trino/operators/trino.py](url) \r\n\r\nWhen I fixed `mypy` and removed that files , they showed me deprecated module error \r\n\r\nTo fix I have added that files again but but it failed here.\r\n

This tests uses TrinoOperator:\r\n\r\n```python\r\n    @mock.patch.dict("os.environ", AIRFLOW_CONN_TRINO_DEFAULT="trino://airflow@trino:8080/")\r\n    def test_openlineage_methods(self):\r\n        op = TrinoOperator(task_id="trino_test", sql="SELECT name FROM tpch.sf1.customer LIMIT 3")\r\n        op.execute({})\r\n        lineage = op.get_openlineage_facets_on_start()\r\n        assert lineage.inputs[0].namespace == "trino://trino:8080"\r\n        assert lineage.inputs[0].name == "tpch.sf1.customer"\r\n        assert "schema" in lineage.inputs[0].facets\r\n        assert lineage.job_facets["sql"].query == "SELECT name FROM tpch.sf1.customer LIMIT 3"\r\n\r\n```\r\n\r\nReplace with SQLExecuteQueryOperator -as suggested by deprecation message.

Done!

I am Sorry \r\nCould you please tell me where I can add this test code ?\r\n\r\n[providers/tests/trino/hooks/test_trino.py](url) ? here ?

> [providers/tests/trino/hooks/test_trino.py](https://github.com/apache/airflow/pull/url) ? here ?\r\n\r\nThat would be ok

Would you please tell me which test I should run to check if it is okay or not ? \r\n\r\nDoes this work ? \r\n` breeze testing providers-tests --test-type "Providers[trino]"`\r\n\r\nit passed at that time\r\n\r\n

[Here](https://github.com/apache/airflow/actions/runs/12254530942/job/34193202147?pr=44477) is an example where this test failed. It was triggered with `./scripts/ci/testing/run_integration_tests_with_retry.sh providers "trino"`, maybe try that?

Ah, sorry you meant the other test. Yes, the breeze command looks good.

No, you are right. I am asking about the provider test, not Breeze, so I can know about that.\r\nThe Breeze tests failed due to my mistakes. \r\nSorry, I am a newbie, so I don‚Äôt know about all this. I am trying to learn 

@eladkal  ,I am confused. Would You please tell me if I should add this method here \r\n\r\n[providers/tests/integration/trino/hooks/test_trino.py](url)  because this file contains similar methods\r\nor [[providers/tests/trino/hooks/test_trino.py]](url)

Sorry , You are right I have noticed a similar method `test_openlineage_methods`  at here so I asked [providers/tests/integration/trino/hooks/test_trino.py](url)

Extracted from #44686 - I found this unnecessary compatibility code while looing and consistency of version checks across all providers.

Extracted  from #44686

Why do you need the label "airflow3.0:breaking"?

Cool, looks like that fixed the migration part.   I have to fix static checks and rework the unit tests now to account for those changes.  Thanks to Ephraim and Daniel for your help on that.\r\n

CC @potiuk 

Can you please also update the docs? https://github.com/apache/airflow/blob/main/docs/apache-airflow-providers-sqlite/operators.rst?plain=1#L20

Doc updates please https://github.com/apache/airflow/blob/main/docs/apache-airflow-providers-oracle/operators/index.rst?plain=1#L29?

> Doc updates please https://github.com/apache/airflow/blob/main/docs/apache-airflow-providers-oracle/operators/index.rst?plain=1#L29?\r\n\r\nuuups, yeah, totally over-looked this :-( Thanks for the hints!

> > Doc updates please https://github.com/apache/airflow/blob/main/docs/apache-airflow-providers-oracle/operators/index.rst?plain=1#L29?\r\n> \r\n> uuups, yeah, totally over-looked this :-( Thanks for the hints!\r\n\r\nBetter to verifying docs if the deprecation is completely removing the class :) 

@jscheffl it seems the test failures are because of this https://github.com/apache/airflow/pull/44704/files#diff-a1ced97245f107006f7a972734c2e5e7ff79da3816f5762c6c2bc1fc2e17f806R26. may be removing these deprecations  `providers/src/airflow/providers/common/sql/hooks/sql.py` solve ?

> @jscheffl it seems the test failures are because of this https://github.com/apache/airflow/pull/44704/files#diff-a1ced97245f107006f7a972734c2e5e7ff79da3816f5762c6c2bc1fc2e17f806R26. may be removing these deprecations `providers/src/airflow/providers/common/sql/hooks/sql.py` solve ?\r\n\r\nor is it something related dag versioning 

> example_trigger_controller_dag`: the dependency comes back as upstream when it should be downstream\r\n\r\nGood catch, fixed, also added a test for that.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Thanks for the review, I will merge this one @ashb

closing in favour of https://github.com/apache/airflow/pull/44704

Maybe we should add some code to fail, rather than skip if db_tests are run in `task_sdk`? 

@potiuk, is it a good idea to remove `task_id`, `run_id`, `dag_id` and `map_index` ?

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Great idea, thanks Jarek !

> Such constants can be imported in the code and used to implement conditional logic for different versions of Airflow.\r\n\r\nWhy do we even need this?

> to me, this seems like too much. all we really need is for it to be just a little bit easier to know the core airflow version, it seems to me.\r\n\r\nWhich we already have:\r\n\r\n```\r\nAIRFLOW_VERSION = Version(airflow_version)\r\nAIRFLOW_V_3_0_PLUS = Version(AIRFLOW_VERSION.base_version) >= Version("3.0.0")\r\n```\r\n\r\n

There are also changes in this PR like version bumps etc that are entirely unrelated to the PR as given in the description.

Well previous atetmpts led to multiple errors - like updating `__init__.py` that is already auto generated, importing version checks from multiple unrelated places or lack of dependency to a common compat provider where it was needed. \r\n\r\nNone of the previous proposals solved all those problems, so I am not sure what is the constructive idea to solve them differently. I would really like to see another, simpler solution for those.

> There are also changes in this PR like version bumps etc that are entirely unrelated to the PR as given in the description.\r\n\r\nYep. the changelog was excessive - I just fixed the generateio in latest push and I am going to remove those changes from this PR

After - unnecessary i think - heated discussion I decided to split off (following suggestion of @dstandish) the "non-controversial part of this PR to https://github.com/apache/airflow/pull/44713/ - and will follow up with rebasing/fixing this one after a devlist discussion I am going to start in a moment. 

Devlist discussion started https://lists.apache.org/thread/px36w3ph2mf0pmv377dtfc2nhpq8dqw1

> Very cool - THANKS!\r\n\r\nThanks @jscheffl and @kacpermuda  for that, really appreciated.  I was really buffled and I thought I am loosing sanity as I saw very clearly how this one is needed - seeing how strongly @ashb and @dstandish reacted to it. But I see that this is maybe they did not understand the full context and reasoning for the change, or that it is just strong difference in opinions (and that others have different opinions). \r\n\r\nSo I extracted all pieces that should be "uncontroversial" from it and started the devlist  discussion above and I hope we can  get to consensus, rebase this PR eventually and maybe apply some comments that might arise separately. I think this is one is pretty useful to have in this form or another, because we already experienced problems with inconsistent version handling. \r\n

A quick POC knocked up using `ruff analyze graph` output:\r\n\r\n```\r\npython ./scripts/ci/pre_commit/check_provider_cross_deps.py\r\nwarning: `ruff analyze graph` is experimental and may change without warning\r\n\

Interestig. This "ruff analyze graph" is cool. We could use it to replace some of the stuff we do when we generate cross-provider dependencies - we were using it by AST analysis, but we can likely get the speed of rust for that . Really cool tool.\r\n\r\nThings like this - make it really useful to get it even more accurate:\r\n\r\n```\r\n  --detect-string-imports\r\n          Attempt to detect imports from string literals\r\n```\r\n

OK. I reworked it to follow the suggestion by @ashb (i.e. using `ruff analyze graph`. I managed to work out to work it out to catch both `other provider` imports and `tests_common` wrong import - and added documentation explaining why we are doing it and what the contributors should do, when they want to add version check support to their provider:\r\n\r\nExample pre-commit errors:\r\n\r\nWhen you import from tests_common:\r\n\r\n![image](https://github.com/user-attachments/assets/76d7164e-fdd9-43e9-869c-cc7503fedab4)\r\n\r\nWhen you import from another provider:\r\n\r\n![image](https://github.com/user-attachments/assets/d74eae7c-2349-4568-a86a-f357534f5287)\r\n\r\nThe change is way bigger than the original one, because I had to extract the `version` commands from `tests_utils.compat` to `version_compat` - because `ruff analyze graph`  produces output on a file level, and that was the easy way to check - if `version_compat` is being imported, it should be from the same provider.\r\n\r\nLookig forward to feedback and comments.\r\n

BTW. If needs be - I can very easily split that change into extracting the version constants in tests to `version_compat.py` (big but straightforward change) and separately making the "provider version code" usage consistent + pre-commit.

Thanks very much for making these changes Jarek

I think this one should be green. I also extracted out the pure refactor of test version constants to "version_compat" module - that will make this one much smaller (and makes it way easier to see if version_compat is used properly in the `ruff analyze graph` json output - #44770. If we merge #44770, this one will be much easier to review

Rebased after merging #44774  - this should be now much easier to review :)

Merging it `as is` then. We can always iterate on it later.

This PR might not be needed

Thanks for the PR!\r\n\r\nI don\

Thank you everyone for your review! I have made changes as requested.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Thank you for the review.

Just want to check, right now several of the existing operators get a handle to the connection and call the Microsoft Azure library directly. These should really be refactored down into the hook as well, right? For example this: https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/microsoft/azure/operators/asb.py#L316-L339

It seems to me if a message is sent from an airflow DAG then the DAG author probably wants a message back at some point to confirm completion, check for errors, et cetera. To the best of my knowledge, the logic in this PR implements the standard design pattern for doing that. \r\n\r\nAlso, after the refactors that dabla requested this will be much smaller and the hooks will be more useful.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Need to rewrite to use a dedicated response queue because there is a race condition between adding the subscription and modifying the filter. The alternatives are to discard any messages before sending the request or to fix the python SDK for ASB but seems simpler to use a queue.

The components and pages can later have clipboard button in relevant places to copy dag_id, run_id, task_id etc. which was present in the old UI and was handy.\r\n\r\nhttps://www.chakra-ui.com/docs/components/clipboard

One way to handle mapped task instances might be to pass `map_index` as a query parameter in the URL and then handle it. Below is a rough approach but this could be discussed in a separate issue since this will involve URL changes.\r\n\r\n```patch\r\ndiff --git a/airflow/ui/src/pages/Events/Events.tsx b/airflow/ui/src/pages/Events/Events.tsx\r\nindex 60663aefec..d311471252 100644\r\n--- a/airflow/ui/src/pages/Events/Events.tsx\r\n+++ b/airflow/ui/src/pages/Events/Events.tsx\r\n@@ -18,7 +18,7 @@\r\n  */\r\n import { Box } from "@chakra-ui/react";\r\n import type { ColumnDef } from "@tanstack/react-table";\r\n-import { useParams } from "react-router-dom";\r\n+import { useSearchParams, useParams } from "react-router-dom";\r\n \r\n import { useEventLogServiceGetEventLogs } from "openapi/queries";\r\n import type { EventLogResponse } from "openapi/requests/types.gen";\r\n@@ -113,6 +113,8 @@ const eventsColumn = (\r\n \r\n export const Events = () => {\r\n   const { dagId, runId, taskId } = useParams();\r\n+  const [searchParams, setSearchParams] = useSearchParams();\r\n+  const mapIndex = searchParams.get("map_index");\r\n   const { setTableURLState, tableURLState } = useTableURLState({\r\n     sorting: [{ desc: true, id: "when" }],\r\n   });\r\n@@ -132,6 +134,8 @@ export const Events = () => {\r\n     offset: pagination.pageIndex * pagination.pageSize,\r\n     orderBy,\r\n     runId,\r\n+    taskId,\r\n+    mapIndex,\r\n   });\r\n \r\n   return (\r\ndiff --git a/airflow/ui/src/pages/Run/TaskInstances.tsx b/airflow/ui/src/pages/Run/TaskInstances.tsx\r\nindex e96ff55d46..4e4127cdfc 100644\r\n--- a/airflow/ui/src/pages/Run/TaskInstances.tsx\r\n+++ b/airflow/ui/src/pages/Run/TaskInstances.tsx\r\n@@ -35,7 +35,7 @@ const columns: Array<ColumnDef<TaskInstanceResponse>> = [\r\n     cell: ({ row: { original } }) => (\r\n       <Link asChild color="fg.info" fontWeight="bold">\r\n         <RouterLink\r\n-          to={`/dags/${original.dag_id}/runs/${original.dag_run_id}/tasks/${original.task_id}`}\r\n+          to={`/dags/${original.dag_id}/runs/${original.dag_run_id}/tasks/${original.task_id}?map_index=${original.map_index}`}\r\n         >\r\n           {original.task_display_name}\r\n         </RouterLink>\r\ndiff --git a/airflow/ui/src/pages/TaskInstance/TaskInstance.tsx b/airflow/ui/src/pages/TaskInstance/TaskInstance.tsx\r\nindex b9e41de93d..d0f5ca9930 100644\r\n--- a/airflow/ui/src/pages/TaskInstance/TaskInstance.tsx\r\n+++ b/airflow/ui/src/pages/TaskInstance/TaskInstance.tsx\r\n@@ -17,37 +17,51 @@\r\n  * under the License.\r\n  */\r\n import { LiaSlashSolid } from "react-icons/lia";\r\n-import { useParams, Link as RouterLink } from "react-router-dom";\r\n+import {\r\n+  useParams,\r\n+  useSearchParams,\r\n+  Link as RouterLink,\r\n+} from "react-router-dom";\r\n \r\n import {\r\n   useDagServiceGetDagDetails,\r\n   useTaskInstanceServiceGetTaskInstance,\r\n+  useTaskInstanceServiceGetMappedTaskInstance,\r\n } from "openapi/queries";\r\n import { Breadcrumb } from "src/components/ui";\r\n import { DetailsLayout } from "src/layouts/Details/DetailsLayout";\r\n \r\n import { Header } from "./Header";\r\n \r\n-const tabs = [\r\n-  { label: "Logs", value: "" },\r\n-  { label: "Events", value: "events" },\r\n-  { label: "XCom", value: "xcom" },\r\n-  { label: "Code", value: "code" },\r\n-  { label: "Details", value: "details" },\r\n-];\r\n-\r\n export const TaskInstance = () => {\r\n   const { dagId = "", runId = "", taskId = "" } = useParams();\r\n+  const [searchParams, setSearchParams] = useSearchParams();\r\n+  const mapIndex = searchParams.get("map_index");\r\n+\r\n+  const tabs = [\r\n+    { label: "Logs", value: "" },\r\n+    { label: "Events", value: `events?map_index=${mapIndex}` },\r\n+    { label: "XCom", value: `xcom?map_index=${mapIndex}` },\r\n+    { label: "Code", value: `code?map_index=${mapIndex}` },\r\n+    { label: "Details", value: `details?map_index=${mapIndex}` },\r\n+  ];\r\n \r\n   const {\r\n     data: taskInstance,\r\n     error,\r\n     isLoading,\r\n-  } = useTaskInstanceServiceGetTaskInstance({\r\n-    dagId,\r\n-    dagRunId: runId,\r\n-    taskId,\r\n-  });\r\n+  } = Boolean(mapIndex) && mapIndex > -1\r\n+    ? useTaskInstanceServiceGetMappedTaskInstance({\r\n+        dagId,\r\n+        dagRunId: runId,\r\n+        taskId,\r\n+        mapIndex,\r\n+      })\r\n+    : useTaskInstanceServiceGetTaskInstance({\r\n+        dagId,\r\n+        dagRunId: runId,\r\n+        taskId,\r\n+      });\r\n \r\n   const {\r\n     data: dag,\r\n```

> One way to handle mapped task instances might be to pass `map_index` as a query parameter in the URL and then handle it. Below is a rough approach but this could be discussed in a separate issue since this will involve URL changes.\r\n\r\nUpdated everything to accept `map_index=X` search param or to at least forward the param around. Also, I realized we can just use `useTaskInstanceServiceGetMappedTaskInstance` and pass `-1` for regular task instances.\r\n\r\n

Hi @nishant-gupta-sh,\r\n\r\nAfter discussion with other contributors, this cannot be release in airflow 2.x. (Next 2.10 patch release cannot hold feature, and next 2.11.0 feature release cannot either because as mentioned in [this thread](https://lists.apache.org/thread/0d3dlly0mbps8n58hlxmmpvdcv9kx68s) it holds no feature.\r\n\r\nI am closing this one.

Just a static check is failing, when this is fixed I assume this can be merged.

> > Just a static check is failing, when this is fixed I assume this can be merged.\r\n> \r\n> I have modified test_pagerduty.py and another check has failed due to this statement from [providers/src/airflow/providers/pagerduty/hooks/pagerduty.py](url) `airflow.providers.pagerduty.hooks.pagerduty_events import PagerdutyEventsHook`\r\n> \r\n> However, fixing the second static check caused the unit test to fail.\r\n> \r\n> ```\r\n> __ ERROR collecting providers/tests/pagerduty/hooks/test_pagerduty_events.py ___\r\n> ImportError while importing test module \

Done !

Green! Merged! Thanks!

cc @mobuchowski @kacpermuda\r\nI am approving the workflow

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

my pr https://github.com/apache/airflow/pull/44148 (causing the the CI to break on branch v2-10-test) was to fix issue https://github.com/apache/airflow/issues/43794, which, i guess, is caused by harmless miscommunication among contributors (https://github.com/apache/airflow/pull/43040 and https://github.com/apache/airflow/pull/42126 plus only https://github.com/apache/airflow/pull/43040 is incorporated into 2.10.3) `¬Ø\\_(„ÉÑ)_/¬Ø`

Please add a meaningful description to this pr, and likely unit tests too

I thi k we mostly overlooked it :(

What is the target release version for this fix? 

Currently Airflow 3. Why? Is this worthwhile to make an attempt to backport it to Airflow 2.* ? If so - why? What would be your arguments @kulkarni-sp ?

> Currently Airflow 3. Why? Is this worthwhile to make an attempt to backport it to Airflow 2.* ? If so - why? What would be your arguments @kulkarni-sp ?\r\n\r\nWe are currently using Airflow 2.9.3 and encountered the same errors during our attempt to upgrade to 2.10.3. Therefore, this fix is essential to unblock our upgrade path.

> We are currently using Airflow 2.9.3 and encountered the same errors during our attempt to upgrade to 2.10.3. Therefore, this fix is essential to unblock our upgrade path. \r\n\r\nCould you please be more specific - what exactly error you experienced - i think that one has no clear issue that it marks as "solving" ? And is it possible that you apply that patch to verify that this one solves it ?

> > We are currently using Airflow 2.9.3 and encountered the same errors during our attempt to upgrade to 2.10.3. Therefore, this fix is essential to unblock our upgrade path.\r\n> \r\n> Could you please be more specific - what exactly error you experienced - i think that one has no clear issue that it marks as "solving" ? And is it possible that you apply that patch to verify that this one solves it ?\r\n\r\nWe are utilizing Airflow with KubernetesExecutor and have several long-running pods that continuously monitor specified locations for input files. Initially, we used mapped tasks to specify different input locations, but this led to a higher number of pods and caused stability issues on our AKS cluster. Now, we are employing ThreadPoolExecutor for parallel processing, which triggers the processor DAG upon receiving a valid input file. However, with Airflow 2.10.3, we encounter the following errors when triggering the processor DAG:\r\n\r\n\r\n\r\n**[2025-01-10, 12:53:29 UTC] {ThreadPoolExecutor-1_3 logging_mixin.py:190} INFO - pa: Error while calling processor DAG: \

@potiuk We have also applied this patch, Requires minor change for it to work in multithreaded env i.e. getattr check and initialization should be done before sentinel check OR It could work with this PR https://github.com/apache/airflow/pull/44240/files\r\n\r\n            sentinel_key = f"{self.__class__.__name__}__sentinel"\r\n            sentinel = kwargs.pop(sentinel_key, None)\r\n            //Initialize attribute callers\r\n             if not getattr(cls._sentinel, "callers", None):\r\n                cls._sentinel.callers = {}\r\n            if sentinel:                \r\n                cls._sentinel.callers[sentinel_key] = sentinel\r\n            else:                \r\n                sentinel = cls._sentinel.callers.pop(f"{func.__qualname__.split(\

I guess this issue fixes https://github.com/apache/airflow/issues/44648 which is a bug on 2.10.3\r\n@utkarsharma2 can we backport the fix to v2-10 branch?

I marked it as 2.10.5 milestone to not forget about it.

@utkarsharma2  Is it possible for you to include this make minor change? Essentially doing getattr check before if statement.\r\n         //Initialize attribute callers\r\n         if not getattr(cls._sentinel, "callers", None):\r\n            cls._sentinel.callers = {}\r\n        if sentinel:                \r\n            cls._sentinel.callers[sentinel_key] = sentinel\r\n        else:                \r\n            sentinel = cls._sentinel.callers.pop(f"{func.__qualname__.split(\

No. This is not going to work. Both pandas and pyarrow are required dependencies of `databricks-sql-connector`. We only have them added to workaround the fact that there are other providers that also need them and we want to keep all the minimum versions in sync - but they will come anyway from the connector.\r\n\r\nHere are deps of the `databricks-sql-connector` 3.6.0 for example. \r\n\r\n```\r\nRequires-Dist: alembic (>=1.0.11,<2.0.0) ; extra == "alembic"\r\nRequires-Dist: lz4 (>=4.0.2,<5.0.0)\r\nRequires-Dist: numpy (>=1.16.6,<2.0.0) ; python_version >= "3.8" and python_version < "3.11"\r\nRequires-Dist: numpy (>=1.23.4,<2.0.0) ; python_version >= "3.11"\r\nRequires-Dist: oauthlib (>=3.1.0,<4.0.0)\r\nRequires-Dist: openpyxl (>=3.0.10,<4.0.0)\r\nRequires-Dist: pandas (>=1.2.5,<2.3.0) ; python_version >= "3.8"\r\nRequires-Dist: pyarrow (>=14.0.1,<17)\r\nRequires-Dist: requests (>=2.18.1,<3.0.0)\r\nRequires-Dist: sqlalchemy (>=2.0.21) ; extra == "sqlalchemy" or extra == "alembic"\r\nRequires-Dist: thrift (>=0.16.0,<0.21.0)\r\nRequires-Dist: urllib3 (>=1.26)\r\nProject-URL: Bug Tracker, https://github.com/databricks/databricks-sql-python/issues\r\nProject-URL: Homepage, https://github.com/databricks/databricks-sql-python\r\nDescription-Content-Type: text/markdown\r\n```

cc: @VladaZakharova -> any comments?

> cc: @VladaZakharova -> any comments?\r\n\r\nLGTM for these changes,\r\nThank you :)

I will follow-up with a PR to handle signals for the actual Task process

Parking this for now to work on https://github.com/apache/airflow/issues/44481. If someone wants to take it (and propagating signals to subprocess & its childrens) on, go for it

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Intermittent error - not related. Merging.

CI need fixing I think.

@pierrejeambrun Need one help from you, currently I am using this `breeze testing core-tests --test-type "API"` to run the unit test cases which takes a lot of time as it runs for all the APIs. Is there any command to make it run for a specific file?\r\nI tried to find in doc but for breeze didnt find any command for single file for API

One comment here @omkar-foss.  This is quite some change in how we treat errors, so it would be great to announce intention to implement those error numbers and messages at the devlist. While there was survey and few people discussed that this is a good idea, "What did not happen on devlist, did not happen" - so likely start a discussion on devlist - with intention to run lazy consensus / (or vote in case there will be any doubts).

> One comment here @omkar-foss. This is quite some change in how we treat errors, so it would be great to announce intention to implement those error numbers and messages at the devlist. While there was survey and few people discussed that this is a good idea, "What did not happen on devlist, did not happen" - so likely start a discussion on devlist - with intention to run lazy consensus / (or vote in case there will be any doubts).\r\n\r\nDone, sent on devlist ‚úÖ\r\n\r\nApologies for the delayed response! I\

As discussed in slack - value of that list and the page is going to be WAY better if there is an action that the user can make for all of those. Users often do not look for description of what is going on, they are looking after the solutions. And in a number of cases we can at the very least guide them where to look for such solutions, which part ofthe documentation should they look for (i.e. link to relevant documentation) . In some other cases we can suspect that this is a deployment issue and tell the users to look there, In many other cases we can even point them to actual configuration parameters that could be changed, or typical resolutions and aras they should look for. In many other cases you can add some examples what could be done.\r\n\r\nThe ruff rules for one are very good way of approaching it  lilke https://docs.astral.sh/ruff/rules/#legend - many of those rules explain what happen, and a number of thos provide a proposal for a solution/example of fixes. While it\

I like the thing as it is now. This has all the basic structure and information about the errors I want to see. And while I do not have concrete comments to all the issues I think it\

Sounds like a nice plan @potiuk üëçüèΩ  

> Sounds like a nice plan @potiuk üëçüèΩ\r\n\r\nDo it :) . Merged!

Better and more complete ("eat cake and have it too") in https://github.com/apache/airflow/pull/44686

cc: @uranusjr 

This should be not needed any more (at least for providers) after the better and more complete fix ("eat cake and have it too") in https://github.com/apache/airflow/pull/44686

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Thanks @bbovenzi 

nice! Thanks @uranusjr !

Looks like it does?

Done

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Renamed the files as per convention since they were generated using the chakra cli.

Code looks good

One side note, not related to the implementation... but rather to UI. I was wondering also in 2.10 that the "reparse DAG" button is so prominent. I would like tpo understand what the reason is making this so prominent. Usually DAG parsing should be running in background silently. Are there really many cases where an individual trigger is needed? Is the system not rather missing a different function? For me this looks very much like an admin/troubleshooting option.\r\n\r\nVery much reminds me to https://github.com/apache/airflow/pull/34487

Yeah, the button does feel too prominent to me. I am happy to move it inside of a "..." more options button.

Really strange it works.

I KNOW\r\n

We need to add-back the workaround until we release and start using for backport Airflow 2.10.4. Detailed explanation in https://github.com/apache/airflow/pull/44589

I added "include success outputs" to see the output of what happens in the successful builds here. And If I am right, the back-compat test for 2.10.3 installs the "eval-types-backport" because of a fallback of not using constraints, but I need to be sure.

> We need to add-back the workaround until we release and start using for backport Airflow 2.10.4. Detailed explanation in #44589\r\n\r\nAs I see #44589 is merged - should we merge this then?\r\n\r\n(I would have waited the "few days" until 2.10.4 is released to clean this PR up but if the test ENV is now fixed anyway... we just need to remember that we clean it up post 2.10.4 release.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Related issue for tracking https://github.com/apache/airflow/issues/44200

I also remember seeing some errors in backend from pydantic where `rx` and `ry` were missing. Sample example dag example_setup_teardown had the issue.

Yes, one of the issue we have is that I am not aware yet of a way to have a datamodel generates something like:\r\n\r\n```python\r\n{\r\n    field?: string\r\n}\r\n```\r\n\r\ni.e have the field `optional/can be missing from the payload` (`?:`), but has no default value / not nullable...\r\n\r\nInstead we currently have:\r\n```python\r\n{\r\n    field?: string | null\r\n}\r\n```\r\n

> I also remember seeing some errors in backend from pydantic where rx and ry were missing. Sample example dag example_setup_teardown had the issue.\r\n\r\nBoth setup and teardown example dags (`example_setup_teardown` and `example_setup_teardown_taskflow`) seem to render properly (things have been updated since you last tried, especially the API side):\r\n![Screenshot 2024-12-03 at 11 09 18](https://github.com/user-attachments/assets/13e3ed18-40ff-4cd6-9284-b70cf5a08e31)\r\n![Screenshot 2024-12-03 at 11 09 36](https://github.com/user-attachments/assets/54cbfc2f-8ec0-406e-8bfd-34c8e086ef4b)\r\n

> Both setup and teardown example dags (`example_setup_teardown` and `example_setup_teardown_taskflow`) seem to render properly (things have been updated since you last tried, especially the API side): \r\n\r\nThanks for validating. I rebased my branch with latest main and now it\

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/44573"><img src="https://img.shields.io/badge/PR-44573-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

> @eladkal , @potiuk,\r\n> \r\n> This PR contains changes for both Google and cncf-kubernetes providers, and the Google provider depends on changes in cncf-kuberneets. For this reason I had to bump cncf-kubernetes version in Google provider\

Also updated documentation about it here https://github.com/apache/airflow/pull/44720

> > @eladkal , @potiuk,\r\n> > This PR contains changes for both Google and cncf-kubernetes providers, and the Google provider depends on changes in cncf-kuberneets. For this reason I had to bump cncf-kubernetes version in Google provider\

> This is one option yes. There is another option. You can add the `cncf.kubernetes` provider to the list of "chicken-egg" providers in `src/airflow_breeze/global_constants.py` -> similarly to what I\

Tests failing - breeze (easier to fix) and compatibility tests with airflow 2.8 - 2.9 (a bit more difficult). The way how to run compatibilty test is described in detail in https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#compatibility-provider-unit-tests-against-older-airflow-releases - with some examples how to deal with the tests.

Hi @potiuk ,\r\nThe CI is finally green. Thank you very much for your guidance!\r\nThe PR is ready for review and merging hopefully.

Cool. The good things that all deprecations have been removed already in the previous wave - so we will not have another bump in k8s provider MAJOR version, so this one is good to go.

LGTM\r\n

Tests need to be fixed @ajitg25 - apparently those tests depend on ncat being available in the environment, and we should not rely on it.

> Tests need to be fixed @ajitg25 - apparently those tests depend on ncat being available in the environment, and we should not rely on it.\r\n\r\n@potiuk Apologies, it was a miss from my side. Now, I have fixed the unit test cases in both files. PTAL!!\r\n\r\ntest_ssh\r\n<img width="1109" alt="Screenshot 2024-12-03 at 8 31 33\u202fAM" src="https://github.com/user-attachments/assets/596fda4b-ae36-4d08-9b48-c31b9b2e8e4d">\r\n\r\ntest_sftp\r\n![WhatsApp Image 2024-12-03 at 08 39 54](https://github.com/user-attachments/assets/f80d9c71-51ab-4173-8c2b-251b05cd43cd)\r\n\r\n

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

cc: @VladaZakharova -> maybe you can take a look as well?

Hi! Please disregard this merge attempt "cc2f73c". I accidentally clicked the update branch before the review was complete. Thanks!

1. The CloudSQLExecuteQueryOperator is not yet offering the impersonation_chain parameter.\r\n\r\nhttps://github.com/apache/airflow/blob/a242ff6edb51db6e8ac1ced72b5bf327dd98c1b4/providers/src/airflow/providers/google/cloud/operators/cloud_sql.py#L1290\r\n\r\n\r\n2. The download mechanism currently supports only CloudSQL Proxies with versions below 2. Updating this method and always downloading the latest version by default could introduce backward compatibility issues, such as conflicts with user-specified command line parameters.\r\n\r\nhttps://github.com/apache/airflow/blob/cc2f73c934526a13decee02db88a93561560b4fa/providers/src/airflow/providers/google/cloud/hooks/cloud_sql.py#L595\r\n\r\n3. Ruff tells me there are some blank lines that contain whitespaces.\r\n4. It looks like there a some merge commits in the PR. I believe Airflow recommends to use rebase commits.

Hi @jasonmar310 !\r\nCould you please share the status of this PR? Are you planning to finish the implementation?

We are moving Google to the new provider structure, so you will have to rebase your PR and adapt to moved files.

Looks good, but some static checks failed. I recommend installing pre-commit, rebasing and running `breeze static-checks --only-my-changes` before re-submitting 

small pre-commit issue :).

> small pre-commit issue :).\r\n\r\n![image](https://github.com/user-attachments/assets/138a0dc4-b8f1-409a-8875-5a5000f225bc)\r\n

Hi @kandharvishnu !\r\nIs there anything that needs to be added to move it forward?

> Hi @kandharvishnu ! Is there anything that needs to be added to move it forward?\r\n\r\ntest cases were pending and completed them now

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

BTW. This is the last one in the #44436 series !

Speaking of which.  I think we have to agree on Pydantic strategy here.\r\n\r\nThere are two reasons why this one is "red"\r\n\r\n* as discussed before - we are passing Context in PythonVirtualenv operator to virtualenv when `use_airflow_context=True` - which is likely not good idea for Standard provider.\r\n\r\nSo far it was really a "future" feature - only working with Airflow 3  and in  Airflow 2 it was only working with AIP-44 enabled (which was never a production option, except a few crazy people (cc: @jscheffl ;). So we could possibly even drop it. Or we can likely serialize it using the same mechanism serialization will happen for TaskSDK (@kaxil @ashb)? What would it be? I looked at the datamodels in task_sdk and have not seen context autogenerated yet (did I look in the right place?) maybe it\

So from those two responses:\r\n\r\n> We haven\

Reopen with full-tests-needed

Ok. Looks like Pydantic classes removal is ready to merge - any last pass? I moved the DagTag to fast_api models, and added error messages/conditional tests/ left TODOs to bring `use_airflow_context` when we have serializable context for AIP-72.\r\n\r\nIf anyone want to have last pass -> feel free.

Good idea with the full tests! Then we see if I missed something...

Added `full tests needed` and `all versions` - to test for all versions of python/k8s/airlflow back-compatibility tetss 

cc: @jason810496 \r\n

> ...ah wheres still some cleaning and removal in airflow/serialization/pydantic/** is needed as well?\r\nWill airflow/serialization/** actually also be removed or will this stay? (I assume we don\

CC: @jason810496  

relates to #44559

We need also update to the changelog (simiar to what we did for cncf.kubenetes and amazon). I will create tracking issue with all the details tomorrow 

> We need also update to the changelog (simiar to what we did for cncf.kubenetes and amazon). I will create tracking issue with all the details tomorrow\r\n\r\nIf you give me a hint I do that... do I need to edit the CHANGELOG.rst and add the future major version like in k8s? Or do I need to drop a newfragment somewhere that is sourced in the provider production process?\r\n\r\nLike in `providers/src/airflow/providers/http/CHANGELOG.rst`\r\n```\r\n5.0.0\r\n......\r\n\r\n\r\nBreaking changes\r\n~~~~~~~~~~~~~~~~\r\n\r\n.. warning::\r\n  All deprecated classes, parameters and features have been removed from the Kubernetes provider package.\r\n  The following breaking changes were introduced:\r\n\r\n  * Operators\r\n     * Remove ``airflow.providers.http.operators.http.SimpleHttpOperator``. Use ``airflow.providers.http.operators.http.HttpOperator`` instead.\r\n\r\n```

> > We need also update to the changelog (simiar to what we did for cncf.kubenetes and amazon). I will create tracking issue with all the details tomorrow\r\n> \r\n> If you give me a hint I do that... do I need to edit the CHANGELOG.rst and add the future major version like in k8s? Or do I need to drop a newfragment somewhere that is sourced in the provider production process?\r\n> \r\n> Like in `providers/src/airflow/providers/http/CHANGELOG.rst`\r\n> \r\n> ```\r\n> 5.0.0\r\n> ......\r\n> \r\n> \r\n> Breaking changes\r\n> ~~~~~~~~~~~~~~~~\r\n> \r\n> .. warning::\r\n>   All deprecated classes, parameters and features have been removed from the Kubernetes provider package.\r\n>   The following breaking changes were introduced:\r\n> \r\n>   * Operators\r\n>      * Remove ``airflow.providers.http.operators.http.SimpleHttpOperator``. Use ``airflow.providers.http.operators.http.HttpOperator`` instead.\r\n> ```\r\n\r\nThe former :)

relates to #44559

One thing more I realized - I think we need to add a CHANGELOG entry - that will eventually will land in a single "1.0" changelog for "standard" provider. Cc: @eladkal -> we need to find out a way to tell our users - surely, you can use the "standar" operators from here - but here is the list of things that changed in it since the "built-in" ones.

Yes, actually discussed the same with @eladkal in #44542 - just someone need to tell me "how to make it right"

> Yes, actually discussed the same with @eladkal in #44542 - just someone need to tell me "how to make it right"\r\n\r\nJust add messages to CHANGELOG.rst for the provider ?  - we can then combine all the changelog entries for 0.* versions when we release 1.0. Does it sound good as "making things right" @eladkal @jscheffl :D ? 

> I don\

relates to #44559

I want to make this one pass - and then - separately - do refactoring with merging those changes in task instance.

I\

You need to rebase / solve conflicts now @bugraoz93 

Rebased, thanks a lot for the reviews! :heart: 

Thanks for the catch. When is this fix expected to be released?

Any chance for making the test method name shorter üòõ ?

(alternatively you just can announce you won that competition ;)

> LGTM +1 with respect to changes in this PR.\r\n> \r\n> On related note, I see that there are no tests present for the api_client at all, @jscheffl. Is that intentional or was it missed out?\r\n\r\nThanks for hinting me to missing pytests... added them and by this also found an error... so good to have tests now.

I\

> I\

This is a very interesting one. It looks like for **some** reason the compat tests were running the version of tests that were there before #41327   - which it should not - tests should always be taken from main.\r\n\r\nFirst time where #45287 might prove to be useful @gopidesupavan !

Ah yeah. I think it just needs rebase. 

@potiuk I have rebased. Test are running now. 

Oh, we had another issue fixed in https://github.com/apache/airflow/pull/45347 - maybe you need to re-base another time, not generating the same again :-(

Can you rebase AGAIN @vatsrahul1001 :)  -> we found and issue with @jscheffl with the new caching scheme - fixed in https://github.com/apache/airflow/pull/45347 that would run "main" version of the tests. So I think that could be the reason

:D @jscheffl -> same thought

> This is a very interesting one. It looks like for **some** reason the compat tests were running the version of tests that were there before #41327 - which it should not - tests should always be taken from main.\r\n> \r\n> First time where #45287 might prove to be useful @gopidesupavan !\r\n\r\nI think we are uploading some images differently by seeing workflows?

> I think we are uploading some images differently by seeing workflows?\r\n\r\nWe were not uploading them AT ALL

:scream: 

I am sorry @vatsrahul1001  -> you have to rebase AGAIN - the #45347 caused that #45335 was merged as "green" but it had some issues. It\

thanks @jedcunningham. @potiuk all test passed now :)

> thanks @jedcunningham. @potiuk all test passed now :)\r\n\r\nUFF! Goood.... That was it then :)

BTW. @vatsrahul1001  I also submitted this https://github.com/apache/infrastructure-actions/pull/88 so that in the futur we can protect against similar mistakes (of mine this time). That was a good learning :) 

Nice!

Once https://github.com/apache/airflow/pull/44510 is merged, I need to pull changes to fix failing tests

The bindings you want to remove in this PR are actually cleaned in #44434 - hoping to have this merged... sooon.

Merging - the other tests need to be handled differently (will do it shortly).

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Note: As PR #45312 has been merged, the code formatting rules have changed for new UI. Please rebase and re-run pre-commit checks to ensure that formatting in folder airflow/ui is adjusted.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Failure unrelated.

BTW. @mobuchowski -> I think you won the price for the longest test name :D

Once https://github.com/apache/airflow/pull/44496 is merged, I need to pull changes and fix remaining failing tests

> Once #44496 is merged, I need to pull changes and fix remaining failing tests\r\n\r\nMerged ! Go ahead!

Rebased and mered in #44540 .. Almost out-of the box after I rebased - and you are co-author @rawwar :)

MyPy does not agree with it :)

> LGTM +1 @kaxil?\r\n\r\nDoes this look good to you, @kaxil?

When backporting, I guess that it would make more sense to add `?branch=v2-10-test` to the path :)

> When backporting, I guess that it would make more sense to add `?branch=v2-10-test` to the path :)\r\n\r\nI am okay, but one question do we run schedule runs in v2-10-test? 

> > When backporting, I guess that it would make more sense to add `?branch=v2-10-test` to the path :)\r\n> \r\n> I am okay, but one question do we run schedule runs in v2-10-test?\r\n\r\nNope. We do it only in main. There is no way to run scheduled runs on non-default branch https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/events-that-trigger-workflows#schedule

NICE ONE !\r\n

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/44587"><img src="https://img.shields.io/badge/PR-44587-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

Resolved conflicts :). 

(we are CLOSE) :) 

> ### Note\r\n> I thinks this method is not the case that:\r\n> \r\n> > re-join back methods that were separated out from the main code - when methods start with `_`\r\n> \r\n> Only Edge-Worker is depends on this `_update_runtime_evaluated_template_fields` method, and there isn\

Fixed in #44546

I think that one calls for "joining" - seems that `_get_task_map_length` is only used in `get_task_map_length` really - - so we should just remove the `_get_task_map_length` and move all the code to `get_task_map_length`

Hi @potiuk While I like the cleanup and reversion of AIP-44 you are running faster than I can make the other PR to main to move EdgeExecutor off the AIP-44.\r\n\r\nThe method map is broken on main and for Airflow 3 can be completely removed. BUT we need it further for the backcompat case in Airflow 2.10. There I need to restore the state of Airflow 2.10, else it is both broken in Airflow 2.10 as well as in Airflow 3.\r\n\r\nWhile I accept it is temporarily broken in Airflow 3 (Still running behind trying to make EdgeExecutor working on main (again after the stone was hitting the glass) with the ongoing changes and merges to EdgeExecutor it is also throwing stones on the ability to further use it in Airflow 2.10.

> While I accept it is temporarily broken in Airflow 3 (Still running behind trying to make EdgeExecutor working on main (again after the stone was hitting the glass) with the ongoing changes and merges to EdgeExecutor it is also throwing stones on the ability to further use it in Airflow 2.10.\r\n\r\nI think we should not hold back the changes for Airflow 2.10 compatibilty. This is the main thing we discussed before that it will have to end at some point of time - when it starts blocking our Airflow 3 development, And I think this is the time. \r\n\r\nI\

Also technically speaking- when it comes to the mechanics of it. \r\n\r\n That should be of course a bit of a burden for you and your team - but this is completely no problem to fork Airflow and run `breeze prepare-proivders-package edge` from it - update versions there, test it etc. \r\n\r\nSo this is just a matter of backporting any changes in `main` edge executor to that fork to make it works with Airflow 2.10 / AIP-44. \r\n\r\nYou can also easily run the same  compatibility tests  in your CI - using your fork, so technically speaking you could do everything to release next "2.10" compatible edge worker on your own - without having to keep the compatibility in Airflow repo. \r\n\r\nSo all that is doable - the question is - should it slow down the current Airflow 3 development, or should it slow down Bosh\

For review - best is to  look at history of adding those removed methods here - from #34026 

<img width="918" alt="Screenshot 2024-11-30 at 00 30 40" src="https://github.com/user-attachments/assets/7c969345-f0b4-4ef0-81a5-59a2136a250a">\r\n

Tests are üî¥

One of those I want to get rid in airflow 3 by explicit rather than implicit initialization via "airflow.__init__". \r\n

Hopefully green now :)

> Hopefully green now :)\r\n\r\nNot yet üòÖ I was too optimistic

Variable too... I hate circular imports

Uff :) 

Sure, here is step to reproduce the issue with the the following dag\r\n\r\n```python\r\nfrom __future__ import annotations\r\n\r\nimport pendulum\r\n\r\nfrom airflow.decorators import dag, task\r\nfrom airflow.operators.empty import EmptyOperator\r\n\r\n\r\n# [START dag_decorator_usage]\r\n@dag(\r\n    schedule=None,\r\n    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),\r\n    catchup=False,\r\n    tags=["example"],\r\n    dag_display_name="Sample DAG with Display Name",\r\n)\r\ndef example_display_name():\r\n    sample_task_1 = EmptyOperator(\r\n        task_id="sample_task_1",\r\n        task_display_name="Sample Task 1",\r\n    )\r\n\r\n    @task(task_display_name="Sample Task 2")\r\n    def sample_task_2():\r\n        pass\r\n\r\n    sample_task_1 >> sample_task_2()\r\n\r\n\r\nexample_dag = example_display_name()\r\n```\r\ntrigger a dag run of this dag\r\nclear "Sample task 2"\r\n\r\nwith the fix, the gantt gaph should look to something like this\r\n![image](https://github.com/user-attachments/assets/dee3d772-08fa-4e49-afae-2c204bc2182a)\r\n\r\nwithout it will result as something like this:\r\n![2024-11-30 16-10-10](https://github.com/user-attachments/assets/dc12a0c0-c747-457e-a12e-a466e24e2cfb)\r\n

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/44517"><img src="https://img.shields.io/badge/PR-44517-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

As part of this task, #44436 mentions as well "re-join back methods that were separated out from the main code - when methods start with _". Are you planning to do it in this PR? I\

> As part of this task, #44436 mentions as well "re-join back methods that were separated out from the main code - when methods start with _". Are you planning to do it in this PR? I\

> Yes, that\

> > Yes, that\

> In this case, (please @potiuk correct me if I am wrong) I dont think we should re-join this method. _execute_task_callbacks is called in different methods and the separation makes sense.\r\n\r\nYes. We can maybe even nicely refactor it to make more sense and rename such methods, place them in the "right" place etc. This should be case-by-case decision and that\

And yes. Cool you are looking at it @shahar1 -> i think a good idea will be by anyone who starts looking at removing the method from one of those files announce "I am taking this part" in #446436 in order to avoid duplication :)

Then - this one can be rebased gradually until we get to "no change" :D

I have realized that we forgot to remove method maps: https://github.com/apache/airflow/pull/44494\r\nAnd here is an example of "joining" back the methods that is more than just joining @shahar1 https://github.com/apache/airflow/pull/44493

Nice it change is !

## Some screenshots for reference below.\r\n\r\n![after_1](https://github.com/user-attachments/assets/976e43ea-e23b-40cd-b918-7bffd0c63933)\r\n![after_2](https://github.com/user-attachments/assets/1cff9c56-9137-4aa7-a075-c7aa73fa298d)\r\n![after_3](https://github.com/user-attachments/assets/364edea0-5fb8-4110-904b-1996801a73f1)\r\n

Looks nice!

Indeed!

Hi, are we planning on also emitting the lineage events from Airflow itself? I think we have other services that emit lineage (for example, BigQuery) where we also still emit this lineage from Airflow. For example, in Composer, we generate the events based on the SQL query of Hive, SparkSQL, Presto and Trino jobs.

Correct, this feature is only about automatically passing some OpenLineage information from Airflow to Spark to automate the process of configuring the OpenLineage/Spark integration.

Thanks @kacpermuda, we would like to contribute the logic we used in Composer to generate the events from the SQL queries in other DataprocSubmitJob types. I think this PR and what we want to contribute are not incompatible, does it sound good to you? (also @mobuchowski )

Since"Please ask maintainer to assign the \

ok too complicated for me

Rebased to account for the workaround in `main`.

> (@potiuk was throwing the stone into the window :-D)\r\n\r\nIndeed. I was. Calling for glass-maker now\r\n 

Like it is ready to merge, looks,It, so I will do.  

@jedcunningham 

I\

> I\

This is a preview of the page when accessing a plugin.\r\n\r\nURL is `<fast_api_endpoint>/pluginsv2/`\r\n\r\n<img width="1720" alt="Screenshot 2024-12-03 at 3 31 25\u202fPM" src="https://github.com/user-attachments/assets/8bc2be87-1f7d-49bb-af60-944fa1f23fe6">\r\n

> Can we also reuse that same application to do the flask auth server ? (When people are use FABAuthManager and we need to reach for the flask webserver) ?\r\n\r\nI was wondering the same and I think we should do that

Tests are passing, let me know if there are more comments/feedbacks

Whoo! Nice!

"Tests / Kubernetes tests / K8S System:LocalExecutor - false - v1.28.13 (pull_request)" is still failing which is really unfortunate, I was hoping that would pass.

Okay, the "Tests / Kubernetes tests / K8S System:LocalExecutor" tests aren\

Hi @Lee-W, I think this issue should be back ported to `2.9.x`, `2.8.x`, and `2.7.x` and also fixed on the latest `main` branch once it is merged. Does that sound correct? \r\n

Thanks @pierrejeambrun ! \r\n\r\n@jason810496 To create a "backport" PR to 2.10.x, you\

> @jason810496 To create a "backport" PR to 2.10.x, you\

Refactored the test cases to include error cases with traceback.\r\n\r\n> I think we should also consider applying the colors to those lines. (only certain lines appear red.)\r\n\r\nThis has been addressed. Screenshot:\r\n![Êà™Âúñ 2024-11-30 ‰∏ãÂçà1 30 11](https://github.com/user-attachments/assets/221ea48a-c81a-4444-abd3-ba707d0265bb)\r\n

Rebased to the latest `v2-10-test`. Looking forward to any advice or feedback before merging. \r\ncc @Lee-W , @pierrejeambrun 

> Looks good thanks.\r\n> \r\n> Some of the questions of the previous review are left un answered. Just for my comprehension, if do you mind giving me a few hints.\r\n\r\nSure!\r\n\r\n1. The `mockExtraLog` in the previous version was just a variable to reuse in the regex instead of directly adding `----...` to the regex. In the refactored version, I removed the regex and matched the expected result with `.toContain`, which is more readable than handling multiple different matches in a single regex.\r\n\r\n2. There was an empty newline in the previous version of the test cases, causing an empty line in `parsedLogs`. This issue is resolved in the current version:\r\n   ```js\r\n   // Previous version with an empty line:\r\n   const mockLog = `line 1\r\n   line2\r\n   `;\r\n\r\n   // Current version without an empty line:\r\n   const mockLog = `line 1\r\n   line 2`;\r\n   ```\r\n\r\n3. Updated the comments for the new regex in the current version.\r\n\r\n

Great thanks @jason810496 for the details.

K8S tests failure unrelated (work in progress to fix them). Merging.

@gopidesupavan the CI fails

> @gopidesupavan the CI fails\r\n\r\nfixed :)

Ah nice !

It‚Äôs been this way for a while now. I think it only applies to a couple of jobs; the main jobs (tests) still run automatically.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Should we remove this now https://github.com/apache/airflow/blob/main/scripts/docker/entrypoint_ci.sh#L341 ?

> Should we remove this now https://github.com/apache/airflow/blob/main/scripts/docker/entrypoint_ci.sh#L341 ?\r\n\r\nNext step. I will add  `database_isolation` removal as a point in #44436 

> > Should we remove this now https://github.com/apache/airflow/blob/main/scripts/docker/entrypoint_ci.sh#L341 ?\r\n> \r\n> Next step. I will add `database_isolation` removal as a point in #44436\r\n\r\nah ok fine :) 

Nice

> Now after Part 3 is merged looks much better:\r\n\r\n\r\n:) 

I will wait for 1 and 2 to be merged :)

> I will wait for 1 and 2 to be merged :)\r\n\r\nAs internal API is now "gone" I will have a hard time getting #1+#2 merged. As there are some merge conflicts as well I\

> UPDATE: Okay now I realize that the internal API is gone but all the previous functions are moved to provider package... need to catch-up coming home...\r\n\r\n:eyes: 

> I will wait for 1 and 2 to be merged :)\r\n\r\nYou just merged 2 after I merged 1... so "ball is in your field" now :-D

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Hi There, had a quick question why the variable `client_store_temporary_credential ` is not available in version 5.8.1 of the provider. 

> Hi There, had a quick question why the variable `client_store_temporary_credential ` is not available in version 5.8.1 of the provider.\r\n\r\n@haarismirza this PR was merged after 5.8.1 release. It may be included in the next release. I am not a release manager so I am not 100% sure.

That makes total sense. Thanks for the reply @yzliao. Looks like @vatsrahul1001 last modified the provider change log - @vatsrahul1001 are you able to tell us when a new release for this provider will happen? 

Yes, windows supports the socketpair etc (it\

Yeah - as already planned - can you please add a unit test and get a better description in commit message @SuccessMoses ?

@potiuk I made a minor change to the test. Do you think more test is needed?

Yep. Nice work - the failing tests are also failing in `main` and we need to fix them separately, so this one is goood to go.

Workarounded already - and waits for a permanent fix here https://github.com/apache/airflow/issues/44513

@pierrejeambrun @eladkal I sincerely apologize for the issues with the previous PR. As a result, I have created a new one. Moving forward, I will pay closer attention to the PR versions. Thank you for sharing your thoughts!

Hi @pierrejeambrun ,\r\n\r\nThanks for your feedback! I‚Äôve updated the implementation to address the issue with missing DAGs causing partial success. Now, the process ensures clarity by stopping and raising an appropriate error if a DAG is missing.\r\n\r\nLet me know if this resolves your concern!

@pierrejeambrun Do we need this for 2.10.4?

Hi @pierrejeambrun, I just wanted to let you know that I‚Äôll need a bit more time to write the test as I‚Äôm still figuring out how Airflow defines its URLs. Thank you for your understanding!

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

up

closed and reopened to run tests

Fixing the checks.

Rebased and all PR comments addressed.. FYI @pierrejeambrun 

Thanks! 

Nice

This seems to work well.

> LGTM +1 Should we just mark 2-10* specifically?\r\n\r\nWe have the same pattern elsewhere - and we are going to have 2.11 in the future, so better to keep it this way I think. 

And in the future we will have 3_0, 3_1 etc. ....

Few small typoes and extra spaces to fix.

No worries :)

All good! Thank you :)

Require any full tests?

Do you have a hint how to locally test? VSCode and use DevContainers?

Use vscode and local docker. I checked it out to my local file system and opened in vscode. vscode shows an option to open in devcontainers when you open the folder. The devcontainer will then start docker and vscode will connect to the devcontainer.

Running codespaces with this change leads to this:\r\n\r\n![image](https://github.com/user-attachments/assets/e96ef17d-7384-4ad4-9655-7465bef070cc)\r\n\r\nThe script exits with 0 and there is no terminal accessible. Yuo can see yourself by opening your own codespace:\r\n\r\n![image](https://github.com/user-attachments/assets/84933dfd-bc05-4ccd-8906-757ba9592e0c)\r\n

Converting the PR to draft until I have time to address this.

Merged, @uranusjr feel free to rebase and report back if the issue persists.

nice!

Needed to produce a nicely formatted message - see https://github.com/apache/airflow/pull/44371#issuecomment-2501185577

The  failure fixed already in main.

I don‚Äôt know about the new implementation, but in the Connextion implementation, the result _is_ actually sorted, but it‚Äôs against a very weird default (`id`) that isn‚Äôt really deterministic in the tests because when you insert things through the ORM it‚Äôs not guaranteed the `id` would be the same as the order you insert them (due to both database and ORM implementation). So the tests will need fixing in any case unless we change the default.

> This error happens on a distributed system when the user has trigger and worker on different machines.\r\n\r\nDo we have a way to verify if other operators / other providers suffer from this problem? If so, lets open dedicated issues for them so we can keep track on it.

> > This error happens on a distributed system when the user has trigger and worker on different machines.\r\n> \r\n> Do we have a way to verify if other operators / other providers suffer from this problem? If so, lets open dedicated issues for them so we can keep track on it.\r\n\r\nTo be honest I do not see any way how we can check the existence of this problem for other operators/providers. 

Thanks for looking into this @pierrejeambrun üëçüèΩ 

### Backport failed to create: v2-10-test. View the failure log <a href=\

Ah... Bad formatting  in my PR ... needs to fix it :)

Much nicer now (manually corrected it):\r\n\r\nFrom:\r\n\r\n<img width="868" alt="Screenshot 2024-11-26 at 16 39 27" src="https://github.com/user-attachments/assets/b693946a-7a4c-4d01-a5cc-f425d6adbfbc">\r\n\r\nTo:\r\n\r\n<img width="867" alt="Screenshot 2024-11-26 at 16 35 26" src="https://github.com/user-attachments/assets/5311b8b7-4b42-41bb-9b9d-63d8822d96bd">\r\n

> Much nicer now (manually corrected it):\r\n> \r\n> From:\r\n> \r\n> <img alt="Screenshot 2024-11-26 at 16 39 27" width="868" src="https://private-user-images.githubusercontent.com/595491/390023655-b693946a-7a4c-4d01-a5cc-f425d6adbfbc.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzI2NDcwMjcsIm5iZiI6MTczMjY0NjcyNywicGF0aCI6Ii81OTU0OTEvMzkwMDIzNjU1LWI2OTM5NDZhLTdhNGMtNGQwMS1hNWNjLWY0MjVkNmFkYmZiYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTI2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEyNlQxODQ1MjdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1mMTE4Y2E5YzE0NjMyMWU4MWExZTRlM2RmM2I0MGFhODlmOTJmZmQ2ZTg3NGFjZjRiOTJjNmRjNDI5YWU2NmM1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.UIdrxqX5gdaxwYdpDUPeA2uXr52SslFEZotg6G0jM-Y">\r\n> To:\r\n> \r\n> <img alt="Screenshot 2024-11-26 at 16 35 26" width="867" src="https://private-user-images.githubusercontent.com/595491/390021767-5311b8b7-4b42-41bb-9b9d-63d8822d96bd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzI2NDcwMjcsIm5iZiI6MTczMjY0NjcyNywicGF0aCI6Ii81OTU0OTEvMzkwMDIxNzY3LTUzMTFiOGI3LTRiNDItNDFiYi05YjlkLTYzZDg4MjJkOTZiZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTI2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEyNlQxODQ1MjdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05MzVjZDVlZTM1ZTY1ZGZhMTI2YTlkNTIwYWJhZTAyNWE4OWFlZDliOWJmNDQwOGExZDRlOGU4MzNjYWFiY2E2JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.oS8NM5_a7ZZ4nMUxQtnyRg__WCO9wohsoXqgClGDps0">\r\n\r\nNice, manually verified , otherwise it may have to go multiple rounds :)

On trigger restart or reassignment to another triggerer process the coroutine is cancelled and a check for `trigger_timeout` is done on `task_instance` where `task_instance` is None in this case and could be checked with sample patch as below to handle this.\r\n\r\nTraceback on trying this out locally with sample dag and ctrl+c to stop the triggerer\r\n\r\n```\r\n[2024-11-26T20:58:28.870+0530] {base_events.py:1744} ERROR - unhandled exception during asyncio.run() shutdown\r\ntask: <Task finished name=\

> On trigger restart or reassignment to another triggerer process the coroutine is cancelled and a check for `trigger_timeout` is done on `task_instance` where `task_instance` is None in this case and could be checked with sample patch as below to handle this.\r\n> \r\n> Traceback on trying this out locally with sample dag and ctrl+c to stop the triggerer\r\n> \r\n> ```\r\n> [2024-11-26T20:58:28.870+0530] {base_events.py:1744} ERROR - unhandled exception during asyncio.run() shutdown\r\n> task: <Task finished name=\

I tested this feature locally and will be useful at work once we upgrade for certain use cases. I am looking forward to how the "infinite scheduling" part is handled in future as noted in the AIP which will further improve usability for us. Thanks @vincbeck .

> I tested this feature locally and will be useful at work once we upgrade for certain use cases. I am looking forward to how the "infinite scheduling" part is handled in future as noted in the AIP which will further improve usability for us. Thanks @vincbeck .\r\n\r\nThanks for testing it!!

Looking into CI failures

> Looking into CI failures\r\n\r\nHi @michaeljs-c, I think the current CI failure for static is cause by frontend generated code, which could be resolve by:\r\n```\r\npre-commit run ts-compile-format-lint-ui --all-files\r\n```\r\nrefer: https://github.com/apache/airflow/pull/43859#issuecomment-2488570257\r\n

> > Looking into CI failures\r\n> \r\n> Hi @michaeljs-c, I think the current CI failure for static is cause by frontend generated code, which could be resolve by:\r\n> \r\n> ```\r\n> pre-commit run ts-compile-format-lint-ui --all-files\r\n> ```\r\n> \r\n> refer: [#43859 (comment)](https://github.com/apache/airflow/pull/43859#issuecomment-2488570257)\r\n\r\nThanks! for some reason it got skipped during my initial pre-commit

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Got it thanks.

Some static checks to solve after rebase 

Note: We are in the process of moving all providers to a new structure - as part of https://github.com/apache/airflow/issues/46045 so if you would like to avoid having to resolve conflicts, speedy fix and rebase is something that you might want to do @johnhoran 

See also devlist announcement: https://lists.apache.org/thread/dzbj5yx5kwpbwyr5yscp4wnlsp6p9v8l\r\n

I fixed the static checks but the PR has conflicts @johnhoran can you please revase and resolve them?

Failures are on papermill provider, not related to this PR

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> Failures are on papermill provider, not related to this PR\r\n\r\nYep. already fixed in main

One pre-commit validation failed, fixed here https://github.com/apache/airflow/pull/44363. merging it now.

Interesting.  I wonder if it might be safer to add some code which looks for that envvar and keeps json as the default, add a deprecation note that this will change in the future, and after the deprecation period remove the default and let the envvar and OTel default behavior do its thing.\r\n\r\nThat would let you get the functionality you need without breaking anyone else without notice.  But I also do not have any statistics on how many people might be affected either way so, it\

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Need to resolve conflicts. Good to merge

Resolve conflict, wait for CI ‚öôÔ∏è 

Just adapt https://github.com/apache/airflow/pull/44393 to this PR.

overall looks good, but look like we need to update some test cases ü§î 

Actually that‚Äôs a bug in my implementation. Fixed.

@potiuk @eladkal @gopidesupavan can you take a look when you have some time?

@potiuk @gopidesupavan i handled it using the requirements parser, its much cleaner now!

Interesting error. The "get_install_requiresments" is probably not the best idea to use because it adds escaping to `"` which is needed to produce  "pyproject.toml". \r\n\r\n```\r\ndependencies = [\r\n{{- INSTALL_REQUIREMENTS }}\r\n]\r\n```\r\n\r\nInstead you should just use `PROVIDER_DEPENDENCIES.get(provider_id)["deps"]` @amoghrajesh  - that one returns unescaped dependencies.\r\n\r\n(and we do not need to us version suffix - this one is merely used to filter out extras - so version suffix does not matter).

Maybe worth to add a comment in `get_install_requirements` that it escapes the dependencies :).

> Interesting error. The "get_install_requiresments" is probably not the best idea to use because it adds escaping to `"` which is needed to produce "pyproject.toml".\r\n> \r\n> ```\r\n> dependencies = [\r\n> {{- INSTALL_REQUIREMENTS }}\r\n> ]\r\n> ```\r\n> \r\n> Instead you should just use `PROVIDER_DEPENDENCIES.get(provider_id)["deps"]` @amoghrajesh - that one returns unescaped dependencies.\r\n> \r\n> (and we do not need to us version suffix - this one is merely used to filter out extras - so version suffix does not matter).\r\n\r\nOh i see, let me correct that!

@potiuk the tests expect a version suffix, do we need it or not at all?

@potiuk i removed the `version_suffix` related changes. I will do it in a follow up

Yep. Working on it. Looks like i did uv sync and precommit never ran to tell me this!

NICE!

Closing as tests passed.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

@kaxil , Should I be following what you did for XCOM value for DagRun.conf as well? \r\n\r\nYou mentioned following in the migration:\r\n\r\n```\r\n    # Summary of the change:\r\n    # 1. Create an archived table (`_xcom_archive`) to store the current "pickled" data in the xcom table\r\n    # 2. Extract and archive the pickled data using the condition\r\n    # 3. Delete the pickled data from the xcom table so that we can update the column type\r\n    # 4. Update the XCom.value column type to JSON from LargeBinary/LongBlob\r\n```

> @kaxil , Should I be following what you did for XCOM value for DagRun.conf as well?\r\n> \r\n> You mentioned following in the migration:\r\n> \r\n> ```\r\n>     # Summary of the change:\r\n>     # 1. Create an archived table (`_xcom_archive`) to store the current "pickled" data in the xcom table\r\n>     # 2. Extract and archive the pickled data using the condition\r\n>     # 3. Delete the pickled data from the xcom table so that we can update the column type\r\n>     # 4. Update the XCom.value column type to JSON from LargeBinary/LongBlob\r\n> ```\r\n\r\nYeah, that\

So, I\

I addressed the comments. Thanks for the review! Some parts turned out to be more complex than I imagined. It still needs small touches and should be ready with unit tests soon. :sweat_smile: 

@pierrejeambrun this is ready for review. I removed the WIP earlier but forgot to ping you again :sweat_smile: Please when you have time, thanks!

> Great work! This will power so much of the new UI!\r\n> \r\n> In the old UI we also included the note for both dag runs and task instances. We should still include it for the new UI.\r\n\r\nI was writing my message and saw your review now :slightly_smiling_face:  Thanks for the review, Brent! 

Can you also plug the new common filters `include_upstream` and `include_downstream` in `structure_data` (`structure.py`)

> Can you also plug the new common filters `include_upstream` and `include_downstream` in `structure_data` (`structure.py`)\r\n\r\nI already included this in the previous commit, updating that endpoint params according to changes on those command `upstream|downstream` variables :) 

> After more manual testing. It looks like we have a few issues.\r\n> \r\n> Using the `example_task_group` Dag:\r\n> \r\n> * `section_1` is correct if everything runs normally. But manually mark a child task as failed. Then the `states` dict is correct but overall_state was not updated\r\n> * `section_2` the `task_count` is correct at 2 if you assume its nested task group is a single task but the list of `states` shows 4 states which would be correct if you ignore task groups and only count the actual number of tasks no matter how nested they may be.\r\n\r\nThanks a lot for the review and additional tests! \r\nI have adjusted the code accordingly. Indeed it was missing recursive `task_groups` and `overall_state` calculation had problem with the order of the loops :sweat_smile: 

> This is coming along well! Thanks for all your patience as I test this endpoint out more and more.\r\n\r\nMy pleasure! I am happy that we ensuring everything will run as expected before shipping this one. Thanks for having been testing multiple times throughout the PR! :) \r\nI will take a look at these soon. Thanks!

>We only want to count its immediate children and their states. So section_2, should only have a count of 2 and states of failed: 1, success: 1\r\n\r\nI misunderstood your earlier message from this one related to the representation of nested task groups. I have now included smaller steps to ensure the correct calculation.

Thanks so much. We are nearly there!\r\n\r\n1. Can you check that `limit` is actually being respected? In my manual testing, I was trying to only fetch 14 runs but I always got the default of 100. @pierrejeambrun would you have any ideas here?\r\n\r\n2. Can we add a test case for a triply nested dag group? Because it looks like we\

Amazing, thanks both of you for the open-minded approach and constructive feedback! :)\r\nI really appreciate the great work done here. Handling recursive logic like this isn‚Äôt easy. This work would take much longer without that logic already in place. I am focusing on adding the necessary tests and checking the areas you‚Äôve highlighted. The PR will be updated soon.

I double-checked the parameters created with `filter_param_factory`, and they weren‚Äôt working as expected. üòï To address this, I needed to adjust the `run_types` and `run_states` parameter names to `run_type` and `state`, respectively. It seems that `filter_param_factory` requires the parameter names to match the column names to function correctly. \r\n\r\nAdditionally, I have included the remaining two filter tests to ensure full coverage. With this update, all filters have been tested.\r\n\r\nThanks!

> Code and test cases look good, beside the couple of nits, ready to merge.\r\n\r\nAmazing news, just pushed the changes and resolved the threads. Thanks a lot for your detailed review!

cc: @bbovenzi just merged. Hoping this can enable further development for the front end side. 

nice

Re-reviewed... Looks good :)

Full tests not triggering , might be because of changes to workflows? @potiuk does this change need to come from the apache repo?

<img width="1701" alt="image" src="https://github.com/user-attachments/assets/1e4c1d3a-f18b-421a-8667-3efe2edda13c">\r\n

ah it seems this one:\r\n\r\n<img width="1459" alt="image" src="https://github.com/user-attachments/assets/a74a8b40-7e4f-4e51-86ac-0ecd6ccce3c2">\r\n

> ah it seems this one:\r\n> \r\n> <img alt="image" width="1459" src="https://private-user-images.githubusercontent.com/31437079/389285779-a74a8b40-7e4f-4e51-86ac-0ecd6ccce3c2.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzI0ODEzNzIsIm5iZiI6MTczMjQ4MTA3MiwicGF0aCI6Ii8zMTQzNzA3OS8zODkyODU3NzktYTc0YThiNDAtN2U0Zi00ZTUxLTg2YWMtMGVjZDZjY2NlM2MyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMjQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTI0VDIwNDQzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmMjRkZGRlNWZhZGNhNTMzNDYyNGQ5NDkwOTc4ZGRjODdjZjcxYjdiZGUxMWM5MDA4MGNiMWY2NGY2OWE5MDImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.iJsao7qUG9nAj89IobSGOAcxowqEVQvJv6NVLsPlz8E">\r\n\r\nThis looks like github actions have hard limit. 

Alright one test failing but, its not related to this i believe. i will check that separate..

> Full tests not triggering , might be because of changes to workflows? @potiuk does this change need to come from the apache repo?\r\n\r\nThis is fixed after removing openapi tests workflow :) 

<img width="1461" alt="image" src="https://github.com/user-attachments/assets/65d94c1b-b3ba-42ce-b00c-80a5eb9ca153">\r\n

Added some comments - it looks fantastic in general, but I found a few places where things might be improved. We could also use the opportunity to rename those tests to be "python-api-client-tests" across the board. \r\n\r\n

> Added some comments - it looks fantastic in general, but I found a few places where things might be improved. We could also use the opportunity to rename those tests to be "python-api-client-tests" across the board.\r\n\r\ncool, thanks those really great improvement suggestions let me have a updates on those :) 

BTW. I really like how this `tests openapi-tests` (or whatever we name it eventually) is running all the tests of the client as well that gets generated :)\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/f241de12-db8d-40ad-af8a-c6c5f3eab031)\r\n

Cool!

Things have been merged in between, can you rebase and update the PR, then we can merge it before more changes come through.

Hi @pierrejeambrun, @ephraimbuddy,\r\n\r\nI‚Äôve refactored the `_UniqueConstraintErrorHandler` to provide unified error details across all database dialects by parsing errors with `re2`. If the parsing fails, it defaults to returning the original database error.\r\n\r\nI‚Äôve also added tests for the unique constraint handler, covering the following cases:\r\n- Inserting a duplicate entry for a single-column unique constraint (e.g., `Pool`, `Variable`).\r\n- Inserting a duplicate entry for a multi-column unique constraint (e.g., `dag_id` and `run_id` pair in `DagRun`).\r\n- Handling cases where parsing the database error message fails.\r\n\r\nLooking forward to your feedback and suggestions!\r\n

Rebase to latest main, looking forward to feedback.\r\ncc @pierrejeambrun , @ephraimbuddy 

Thank you for the feedback, @pierrejeambrun and @kaxil!\r\nI realize I overlooked the maintenance effort required for directly parsing errors from different dialects.\r\n( Hope we‚Äôll see a PEP someday that standardizes the error details across various DBAPI drivers, then we could adopt the parsing solution. )\r\n\r\nJust refactored the implementation to include only the `statement` and `orig` in the error response.  

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> Should we keep them with the original PR id, or change it to the backport\

So I am not sure if the rename was a good idea. :)

> > Should we keep them with the original PR id, or change it to the backport\

Green!

Nice :) think this solves the current ci failures related to pydantic.

Yeah, good one :) 

I am cherry-picking that one to 2-10-test - targetting for 2.10.4 - just in case https://github.com/pydantic/pydantic/issues/10958 will not be quickly resolved and Pydantic team will be reluctant to make `eval-type-backport` as required dependency for Pydantic (which IMHO is the best solution to the problem they introduced in Pydantic 2.10.*. This is not ideal as it only protects the 2.10.4 users on Python 3.8/3.9 but I want to have it handy.

I believe failed steps are not relevant to the changes. It seems like a couple of tests are still using old configuration.

> I believe failed steps are not relevant to the changes. It seems like a couple of tests are still using old configuration.\r\n\r\nYeah, we need to fix it separately.

Thanks for fixing it üôè 

> One "bigger" thing about revealing too much for auth errors.\r\n\r\nPoint taken. Thanks for the review. Actually it was a take-over from existing internal API. Would we need to harden this as well before we do a 2.10.4? See https://github.com/apache/airflow/blob/2.10.3/airflow/api_internal/endpoints/rpc_api_endpoint.py#L190 (now removed on main...)

@kaxil / @potiuk Thanks for the review! All things adjusted... but as AIP-44 needed to re-work a lot I needed to fully re-base and restore AIP-44 broken function. As Airflow 3 is now broken... after this PR v2.10 is working again.\r\n\r\nDo you want to have a second round or good to merge as is? (And follow-ups will be taken care...)

I had a quick look - I am good to go. I think we need to align on the near future strategy for breaking/non-breaking strategy for the edge worker - see https://github.com/apache/airflow/pull/44494#issuecomment-2509012064 but this one is good to go I think 

@shahar1 when I moved Connection instance to test class, tests can pass!\r\nReview test code, please. üôáüèΩ \r\n\r\n(Thanks, @potiuk , for the hint)

@potiuk Thanks for the review. I have made the test conditional based on the version it is running against.

Thanks for the review

Note: Add this to the breaking change of the API tracked here:\r\nhttps://github.com/apache/airflow/issues/43378\r\n\r\n(We do not create the newsfragments yet, format is not settled for those).

Note: As PR #45312 has been merged, the code formatting rules have changed for new UI. Please rebase and re-run pre-commit checks to ensure that formatting in folder airflow/ui is adjusted.

@rawwar any progress on that one ? https://github.com/apache/airflow/pull/44986 also depends on that, I think it would be a great improvement for the API.\r\n\r\nLet me know if you need some help.

@pierrejeambrun , while i was working on this, I noticed the following: \r\nhttps://github.com/apache/airflow/blob/f7a9a1581b58c296d8b6edfe71bb72efebb32943/airflow/api_fastapi/core_api/routes/public/connections.py#L206\r\n\r\nAs the models can have an alias, this `ConnectionBody(**patch_body.model_dump())` will fail when there are alias. Especially when few are validation/serialization alias. \r\n\r\n\r\nExample test: https://github.com/apache/airflow/blob/823ffde75de0f1b75ee12d82059099f504240f53/tests/api_fastapi/core_api/routes/public/test_connections.py#L316\r\n\r\nHere, it always fails when extra=forbit with following error:\r\n```\r\n\

> Also, Do we even need to have this specific check? ConnectionBody(**patch_body.model_dump()) feels redundant, given input is already validated during request.\r\n\r\nThe idea behind this check is to ensure that the given payload is a fully formed entity before moving further and saving that to the DB. Why we do that is because the endpoint can accept partial updates, when we give a partial `PathBody` with a lot of None field and without specifying an update mask, we verify that this payload can be saved to the db.\r\n\r\nI would say that we need to keep that, but we still need to figure out the other problem that you are mentioning. `extra_forbidden` and `aliases` should play nicely together.

Need rebasing and conflict solving.

Thank you, @shahar1, for your detailed review and valuable feedback! I‚Äôve addressed your comments by:\r\n1. Adding a detailed docstring for the `__init__` method, including the missing `adapter` parameter.\r\n2. Removing the redundant instantiation of `TCPKeepAliveAdapter` in the `run` method.\r\n\r\nPlease let me know if there‚Äôs anything else I can improve. I really appreciate your time and support!

> Thank you, @shahar1, for your detailed review and valuable feedback! I‚Äôve addressed your comments by:\r\n> \r\n> 1. Adding a detailed docstring for the `__init__` method, including the missing `adapter` parameter.\r\n> 2. Removing the redundant instantiation of `TCPKeepAliveAdapter` in the `run` method.\r\n> \r\n> Please let me know if there‚Äôs anything else I can improve. I really appreciate your time and support!\r\n\r\nThanks :) Regarding the 2nd point - could you please explain why it makes sense to relocate the instanation of `TCPKeepAliveAdapter` to the `__init__`?

Tests currently seem to fail :(

I need some time to figure out how to fix this issue.:)

running `pre-commit run --all-files` locally can help you resolve issues earlier üôÇ 

> running `pre-commit run --all-files` locally can help you resolve issues earlier üôÇ\r\n\r\nOr `breeze static-checks --only-my-changes` which runs only on files you changed so it is way faster.\r\n

Thanks for your suggestion!

Thank you all for taking the time to work on this together! üòä

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> Thank you all for taking the time to work on this together! üòä\r\n\r\nThank you for your great first contribution :)\r\nLooking forward!

Rebasing again, need latest main fix for the CI.

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/44319"><img src="https://img.shields.io/badge/PR-44319-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

Thanks @ferruzzi for noticing :)

Unrelated errors. Merging

Nice - but seems there are still some troubles with collection changes.\r\n\r\nBTW. I would like - before merge and when green re-run it locally and reapply ruff auto-changes - to see if I come up with small set of reviewable changes (aka - reproducibility check :)\r\n

> Nice - but seems there are still some troubles with collection changes.\n> \n> \n> \n> BTW. I would like - before merge and when green re-run it locally and reapply ruff auto-changes - to see if I come up with small set of reviewable changes (aka - reproducibility check :)\n> \n> \n\nDone, rebased, ran ruff, force pushed locally

> Interesting. How can it be that I over-looked the changes in the first commit when removing Python 3.9 support?\n\nYou already had to many changes to handle :)

And one more here @jscheffl ;) https://github.com/apache/airflow/pull/44328

> And one more here @jscheffl ;) #44328\r\n\r\n:D

> Interesting. How can it be that I over-looked the changes in the first commit when removing Python 3.9 support?\r\n\r\nAll good man

> And one more here @jscheffl ;) #44328\r\n\r\nHope this was the last one... or shall I go hunting again? :-D

NICE!.. Our test harness seems to work well :)

Also see https://github.com/apache/airflow/pull/44249#issuecomment-2495077341 for context.

Issues unrelated. Merging.

### Backport failed to create: v2-10-test. View the failure log <a href=\

> Backport failed to create: v2-10-test. View the failure log [ Run details](https://github.com/apache/airflow/actions/runs/11991248507)\r\n\r\nYeah - expected that. Will cherry pick manually.

:D 

The failure is unrelated:\r\n\r\n```\r\n=========================== short test summary info ============================\r\nFAILED tests/serialization/test_dag_serialization.py::TestStringifiedDAGs::test_extra_operator_links_logs_error_for_non_registered_extra_links - assert "Operator Link class \

> Nice - just one small thing to fix with the modified release notes.\r\n\r\nThanks for pointing this @potiuk, I have reverted the release notes changes.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Thanks @AlexWaygood! Appreciate you coming over and doing the update for us. Congrats on your first Airflow commit üéâ 

No problem, glad I could help!

The whole issue is not solved yet BTW.\r\n\r\nSee: \r\n* https://github.com/pydantic/pydantic/issues/10924#issuecomment-2493313194\r\n* https://github.com/pydantic/pydantic/issues/10910\r\n* https://github.com/databricks/dbt-databricks/pull/843\r\n* https://github.com/apache/airflow/pull/44249#issuecomment-2491919741

Is full tests require?

Ah I see it has changed to dependency file, so it should trigger full tests :) ?

The failing tests are "dataplex" - unrelated fixed in https://github.com/apache/airflow/pull/44281

@utkarsharma2 will we still have 2-10 releases? If not, should we mark it as ready to review and get it merged

> @utkarsharma2 will we still have 2-10 releases? If not, should we mark it as ready to review and get it merged\r\n\r\nGood question :) 

Actually, we might want to remove this argument entirely instead, and prefer data interval fields. Converting this to draft for now.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

This seems version bump is required. in canary its installing latest version.

merging this one now, failures are not related to this change :)

Closes this issue - https://github.com/apache/airflow/issues/40839

close in favor of https://github.com/apache/airflow/pull/44384

> Great thanks.\r\n> \r\n> Looking good overall, a few suggestions, and should be ready to merge once other comments are resolved.\r\n\r\nRebased and resolved all comments.

Thanks üëç 

I think it\

statement is fine by me too :)

Since `try_number` is present in the task_instance probably a filter to get `task_instance.try_number > 1` could be added to the API to fill retried tasks section. Something like below but the color palette needs a fix since up_for_retry has gold color which is not compatible with Chakra.\r\n\r\n```patch\r\ndiff --git a/airflow/api_fastapi/core_api/openapi/v1-generated.yaml b/airflow/api_fastapi/core_api/openapi/v1-generated.yaml\r\nindex 2f74f22689..5238ad1f91 100644\r\n--- a/airflow/api_fastapi/core_api/openapi/v1-generated.yaml\r\n+++ b/airflow/api_fastapi/core_api/openapi/v1-generated.yaml\r\n@@ -3894,6 +3894,22 @@ paths:\r\n           - type: number\r\n           - type: \

> Since `try_number` is present in the task_instance probably a filter to get `task_instance.try_number > 1` could be added to the API to fill retried tasks section. Something like below but the color palette needs a fix since up_for_retry has gold color which is not compatible with Chakra.\r\n\r\nNice! Do you want to make a pull request with that after this is merged? 

Sure @bbovenzi 

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

> Looking good.\r\n> \r\n> Just a few suggestions/questions.\r\n> \r\n> Are we missing the `standalone_dag_processor` ?\r\n\r\nAs per below inline from Brent we have to do this in [issue](https://github.com/apache/airflow/issues/44253)\r\n\r\n```\r\nOther config:\r\nstandalone_dag_processor [This should be checked in API, not the UI](https://github.com/apache/airflow/issues/44253)\r\n```

> Are we missing the standalone_dag_processor ?\r\n\r\nAt first glance, it feels like we just need to add an extra field to `additional_config` with `conf.getboolean("scheduler", "standalone_dag_processor")` ?\r\n\r\nMaybe i\

> > Are we missing the standalone_dag_processor ?\r\n> \r\n> At first glance, it feels like we just need to add an extra field to `additional_config` with `conf.getboolean("scheduler", "standalone_dag_processor")` ?\r\n> \r\n> Maybe i\

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

You can try against v2.10.1 now :)

Oh my bad for merging too fast!

Perhaps you all could test against our `main` branch before we do our patch release to confirm that all is working as expected on your end?

> Perhaps you all could test against our `main` branch before we do our patch release to confirm that all is working as expected on your end?\r\n\r\nIt would be easier if you have an rc or beta/alpha released in PyPI - because I am not sure if we can trigger the whole test suite against Github-installed version. Can it be done @sydney-runkle ?

You can try with `pydantic==git+https://github.com/pydantic/pydantic@main`

Just released v2.10.1 with fixes for the issues you folks reported :)

> What\

> I think this change is breaking far too much of an existing code base\r\n\r\nSince the release of 2.10, we only had one report (https://github.com/pydantic/pydantic/issues/10924) related to evaluation of forward annotations.\r\n\r\n> Pydantic is so popular and used in many, many dependencies\r\n\r\nWe are aware of that, it is, in some cases, extremely challenging for us to make changes/cleanups, especially because in the field of forward annotations evaluation.\r\n\r\nThe standard library utilities are far from perfect, so we had to implement our own logic to support edge cases that unfortunately led to a messy implementation, accepting invalid annotations to evaluate (this can be dangerous as names in forward annotations could resolve to the wrong type) [^1].\r\n\r\n> it\

> If we do get more reports, depending on their validity, we will then consider tweaking the current logic.\r\n\r\nSure. It is indeed up to you. We are pretty well protected - we have "constraint mechanism" and we can restric airlfow 2.10.4 - and we are rather happy to adjust future versions of airflow, I was just merely suggesting that this change might have more consequences for you - so It was more of a friendly advice than complaint.\r\n\r\nBut yes - I agree my assesment might be wrong.\r\n\r\n> Your usage of type annotations is inconsistent here, as you are mixing the new union syntax with Optional. Note that if this class were to be defined in the same module as [TaskInstancePydantic](https://github.com/apache/airflow/blob/1307e37bc6338c43aa5099752d012e244f8494ba/airflow/serialization/pydantic/taskinstance.py#L84), the issue would have appeared already.\r\n\r\nYes -  but to be perfectly honest it\

> Indeed this is not great. We have all the codebase updated to the new style type annotation. (`|`, `list`, etc...) and having to use the old one everywhere pydantic is involved is not great. (maybe I didn\

Sounds great thanks Jarek. \r\n\r\nYes indeed when I first introduced it, I really didn‚Äôt know it would be all that important and have to be cherry picked.\r\n\r\nIt was really just to fix issues I was facing with initial development of the FastAPI API. Now looking back, indeed a PR of its own would have been nice üòÑ

I think we should only merge one of this and https://github.com/apache/airflow/pull/44223 and include setting a note and all the `include_*` filters to update other task instance states at the same time.

Closed this PR as this functionality is handled by Patch Task Instance API in https://github.com/apache/airflow/pull/44223 using `update_mask = ["new_state"]`.

@kaxil I wrote a new integration test that tests out the deferred exception functionality in task runner as well as tests the message sent across to the SUPERVISOR_COMMS through the `send_request` function\r\n\r\nIn this commit: https://github.com/apache/airflow/commit/dc39226f6659c36db83322bfed75faba274ee32a

Squashed commits into 1 and resolved conflicts.\r\n\r\nDoing one final pass now

What was the stactrace of such issue @dabla -> I have a hard time to figure out what race/parallelism could have caused it.

> > What was the stactrace of such issue @dabla -> I have a hard time to figure out what race/parallelism could have caused it.\r\n> \r\n> Or maybe this is a bug in the Python 3.9 implementation of threadlocal?\r\n\r\nYeah. that why I wanted to see stacktrace, because I cannot find a path where you would have missing attribute - when I compare "before" and "after" the change i cannot see anything that could have fixed that problem, other than slightly bigger overhead of executing the method, which could have mitigitated some race condition, having such a stack trace would definitely help us to see the root cause. I am pretty reluctant to approve and merge change that I do not understand how it works :).\r\n\r\n

> > > What was the stactrace of such issue @dabla -> I have a hard time to figure out what race/parallelism could have caused it.\r\n> > \r\n> > \r\n> > Or maybe this is a bug in the Python 3.9 implementation of threadlocal?\r\n> \r\n> Yeah. that why I wanted to see stacktrace, because I cannot find a path where you would have missing attribute - when I compare "before" and "after" the change i cannot see anything that could have fixed that problem, other than slightly bigger overhead of executing the method, which could have mitigitated some race condition, having such a stack trace would definitely help us to see the root cause. I am pretty reluctant to approve and merge change that I do not understand how it works :).\r\n\r\nYes, I completely understand, me personally I also don\

Which version of Airflow the stacktrace was from @dabla ?

> Which version of Airflow the stacktrace was from @dabla ?\r\n\r\nIt was on 2.10.3

> We should probably directly use the state Enum at some point. (That\

Looks like the original one is the correct one

These are autogenerated files. Is this a manual change or is it fixing an accidental manual change?

Hope this change is not required, it looks like the side effect of pydantic version issue?\r\n\r\nThis is fine now, https://github.com/apache/airflow/pull/44249

> Hope this change is not required, it looks like the side effect of pydantic version issue?\r\n\r\nI think too. Can we close this one in favor of https://github.com/apache/airflow/pull/44249 ?

Yep it is due to the underlying pydantic bug fixed by https://github.com/apache/airflow/pull/44249. Closing this one

This got merged but tests are failing @bbovenzi 

@eladkal since you reviewed #37655 would you mind reviewing this as well? (this is a fix for issues with #37655)

@bbovenzi For now, removed the search options, this PR will just have list variables, all other parts will be merged separately in smaller PRs.\r\n

All checks passed, PR is ready for review ‚úÖ

All conversations resolved, PR rebased with `main` and all conflicts resolved, please have a look when possible @pierrejeambrun @bbovenzi. Thanks.

@pierrejeambrun all checks passing, and have rebased with `main` just now. Please review when you get a chance, thanks.

@pierrejeambrun PR rebased with `main` and ready to merge. Thank you.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Resolving conflicts from recent PR merges into `main`.

> Resolving conflicts from recent PR merges into `main`.\r\n\r\nThis is done, PR synced with `main` and all conflicts resolved ‚úÖ 

All checks passed, PR ready to review ‚úÖ 

I just rebased the branch, to solve additional conflicts introduced by recent merge.\r\n\r\nShould be good to merge.

1 test failing after rebase, rebased again. It should pass now.

Failure is unrelated:\r\n\r\n```\r\n=========================== short test summary info ============================\r\nFAILED tests/jobs/test_scheduler_job.py::TestSchedulerJob::test_setup_callback_sink_not_standalone_dag_processor - sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) server closed the connection unexpectedly\r\n\tThis probably means the server terminated abnormally\r\n\tbefore or while processing the request.\r\n\r\n```

Will need to get the tests fixed

Sure, will look into that 

@Lee-W can you please review and merge this bug fix PR\r\n

> @Lee-W can you please review and merge this bug fix PR\r\n\r\nSure, will take a look once the test and CI pass

@Lee-W Made some changes. Those failing CI tests should be fixed now. Could you please review?

rebased from the latest main to see whether the last test passes

@Lee-W Passed the CI tests. Could you please review now?

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Nope. This is still very wrong. "common" sql provider should not know anything about "oracle" - this is the main reason why we introduced "common.sql" and all the "database" specific calls should come to the provider - so you have to modify it in the way that all "oracle" specific code lives in the "oracle" provider. You should implement a new feature in the common.sql where you can replace the template and then make oracle provider to replace it.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Note to myself to not merge too early:\r\n\r\nDepends on https://github.com/apache/airflow/pull/44267 

Hmm... there seem to be error generated by that however (see failing tests) - looks like it expects to have provider in the dictionary and it does not find it.

Ah yes, let me try and fix it\r\n

> Looking good overall beside @rawwar comments.\r\n\r\nyes, removed the print statements

Just noticed logical_date is missing in response model(TaskInstanceHistoryResponse). Is this intentional ?

> Just noticed logical_date is missing in response model(TaskInstanceHistoryResponse). Is this intentional ?\r\n\r\nTaskInstanceHistoryResponse return `logical_date` \r\n<img width="1658" alt="image" src="https://github.com/user-attachments/assets/7358e036-6fe0-4536-b1fe-c368a326fd40">\r\n

I saw the response [Here](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_mapped_task_instance_try_details) and saw it had execution_date

> I saw the response [Here](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_mapped_task_instance_try_details) and saw it had execution_date\r\n\r\nI have raised a PR to fix that - https://github.com/apache/airflow/pull/43830

Nested task groups, setup/teardown, mapped tasks, edge labels:\r\n\r\n<img width="1295" alt="Screenshot 2024-11-19 at 5 56 47\u202fPM" src="https://github.com/user-attachments/assets/08820d3a-8cec-4550-a83f-59a1c4a6788d">\r\n

Mhm, I don\

> Mhm, I don\

@jscheffl\r\n\r\nIn non `dev-mode` do you still have the issue mentioned above ?

Fixed!

Can you update change log similar to how you did it for AWS?

> Can you update change log similar to how you did it for AWS?\r\n\r\nSorry I forgot! Sure, doing it right now

> Can you update change log similar to how you did it for AWS?\r\n\r\nDone :)

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Regarding my previous comment is there any way that the project would handle an upgrade that requires manually effort like that? 

Nice!

@shahar1 Static check was performed in different part of the code. Non-DB tests that failed are running fine locally‚Äîdo I need to address something?

@shahar1 I need help, is there any way to pass this test?\r\nhttps://github.com/apache/airflow/actions/runs/11965697415/job/33360361183?pr=44190

> @shahar1 I need help, is there any way to pass this test? https://github.com/apache/airflow/actions/runs/11965697415/job/33360361183?pr=44190\r\n\r\nIt seems that it has to do with the the following definition in the tests:\r\n```python\r\nPYMSSQL_CONN = Connection(\r\n    conn_type="mssql", host="ip", schema="share", login="username", password="password", port=8081\r\n)\r\n```\r\n\r\nI assume that behind the scenes it initiates `PymssqlConnection(...)` with the `**extra_conn_args` that you just added, and now it fails all the tests that use it, as the `sqlalchemy` doesn\

> > @shahar1 I need help, is there any way to pass this test? https://github.com/apache/airflow/actions/runs/11965697415/job/33360361183?pr=44190\r\n> \r\n> It seems that it has to do with the the following definition in the tests:\r\n> \r\n> ```python\r\n> PYMSSQL_CONN = Connection(\r\n>     conn_type="mssql", host="ip", schema="share", login="username", password="password", port=8081\r\n> )\r\n> ```\r\n> \r\n> I assume that behind the scenes it initiates `PymssqlConnection(...)` with the `**extra_conn_args` that you just added, and now it fails all the tests that use it, as the `sqlalchemy` doesn\

And make sure to rebase the PR @jx2lee 

Oops, I had a problem and closed & recreated the PR.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

https://docs.sqlalchemy.org/en/14/orm/contextual.html#using-thread-local-scope-with-web-applications says how we should be doing this. Creating a session object per request is not it.

> https://docs.sqlalchemy.org/en/14/orm/contextual.html#using-thread-local-scope-with-web-applications says how we should be doing this. Creating a session object per request is not it.\r\n\r\nI don\

https://github.com/fastapi/fastapi/issues/726#issuecomment-584371305 I think this comment is relevant.

Failure was unrelated

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

WDYT? I noticed that some of the tests are passing but also failing in a few runs. is that fine check minimum 1 , i see it is completely depending on the timing. :) 

updated lower bound value to spy call here: https://github.com/apache/airflow/pull/44177

> updated lower bound value to spy call here: #44177\r\n\r\none test failed, after merging above and rebase this PR. it will work..

Hmm. I wonder.. Looking at #43464 - yes, when you have K8S executor, you will not be able to work without serviceAutomount set to true. But is it generally true for LocalExecutor and CeleryExecutor for example? \r\n\r\nI believe should work without automount for those executors?

Two small comments. 

> Two small comments.\r\n\r\nThank you for the suggestions. I have implemented the suggested changes.

Can you elaborate what broke in OL? I‚Äôm not sure `airflow/dag_processing/collection.py` and `airflow/timetables/assets.py` need to trigger OL tests. Those aren‚Äôt really about assets, but more _using_ assets.

> #44026\r\n\r\n\r\n\r\n> Can you elaborate what broke in OL? I‚Äôm not sure `airflow/dag_processing/collection.py` and `airflow/timetables/assets.py` need to trigger OL tests. Those aren‚Äôt really about assets, but more _using_ assets.\r\n\r\n@uranusjr This [issue](https://github.com/apache/airflow/issues/44026) explain more about what broke in OL. Also, this [PR](https://github.com/apache/airflow/pull/44025) was raised to fix OL tests. I asked @Lee-W for the assets file we can remove these files `airflow/dag_processing/collection.py` and `airflow/timetables/assets.py` if needed.

Good to go, merged :)

Closing and opening to run full tests

Why?  IMHO the way it is now is better.\r\n\r\nIThis is fake data that looks like real - it\

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Static failure is unrelated

Thanks for the PR - I LIKE it! Sorry entering too late for review :-(

<img width="829" alt="Screenshot 2024-11-18 at 23 14 04" src="https://github.com/user-attachments/assets/5e9c4307-af35-4a2c-bf13-7ae4046a1363">\r\n\r\nWhy were there two workflows awaiting maintainer approval in a PR from one of the PMC members, am I missing something?

\r\n> Why were there two workflows awaiting maintainer approval in a PR from one of the PMC members, am I missing something?\r\n\r\nI raised question about it through GitHub support and still wait for answer (we discussed about it in Slack).\r\n

> > Why were there two workflows awaiting maintainer approval in a PR from one of the PMC members, am I missing something?\r\n> \r\n> I raised question about it through GitHub support and still wait for answer (we discussed about it in Slack).\r\n\r\nI followed up.

might need this as well @jscheffl https://github.com/apache/airflow/pull/44093

> might need this as well @jscheffl #44093\r\n\r\nYeeah, figured out the same commit right at the same time :-D Added to the PR!

We release providers from main branch. There is no need to have it on v2.10

Agreed.

Needs rebasing, can merge after\r\n

### Backport failed to create: v2-10-test. View the failure log <a href=\

Nice!

Sorry, I was afk for a bit. Thanks for taking care of the fixes!

ü§î having some troubles when testing more extensively.

ready for review, https://github.com/apache/airflow/pull/44187 should be merged first but with this additional change solve the issues for SQLite.

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/44167"><img src="https://img.shields.io/badge/PR-44167-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

the current like is dead in the documentation , so we need to fix it ( otherwise it will be dead until release of airflow 3 )

Oh my mistake! I will create a PR for backporting it

### Backport successfully created: v2-10-test\n\n<table>\n                <tr>\n                    <th>Status</th>\n                    <th>Branch</th>\n                    <th>Result</th>\n                </tr>\n                <tr>\n                    <td>‚úÖ</td>\n                    <td>v2-10-test</td>\n                    <td><a href="https://github.com/apache/airflow/pull/44157"><img src="https://img.shields.io/badge/PR-44157-blue" alt="PR Link"></a></td>\n                </tr>\n            </table>

Can you amend the commit to a meaningful one? We use commit titles for change log

> Can you amend the commit to a meaningful one? We use commit titles for change log\r\n\r\nDone. Thanks for noticing!

@Lee-W @rawwar I pushed a fix for the reviews, can you check again?

Only last commit is relevant

@Dawnpool You seem to have a whole test non-db test suite failing. Can you check it on your local breeze and see if you can fix it?

> @Dawnpool You seem to have a whole test non-db test suite failing. Can you check it on your local breeze and see if you can fix it?\r\n\r\nHi, I guess there was an issue with the test code at the time I forked the repository. It might have been resolved by merging the main branch. There is no problem on my local breeze environment for now.

errrors, errors everywhere :D

PR has conflicts that needs to be resolved

Can you please rebase/resolve conflicts and ping me if it fails again.

@potiuk \r\nit failed againüò• same error with the non-db tests

:scream: \r\n

I think this should fix it: https://github.com/apache/airflow/pull/45244

Nope - there is another issue

All right. I think I found it https://github.com/apache/airflow/pull/45249 - very interesting issue (and your change accidentally revealed it because it selectively run non-db `microsoft.azure` test collection as the first group of tests to run - and it turned out that those tests relied on a side-effect of other tests being run before. 

OK. The fix is merged. You can rebase again and I :crossed_fingers: that it should work now.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Finally!

Finally! Thank you allüòÅ

Thank you for your contribution @Dawnpool!\r\n\r\nIn this implementation, files are downloaded sequentially, right? If so - when downloading folders containing a large number of small files, this approach will be quite inefficient. I think we should consider expanding the method, or adding another one that downloads files concurrently.  \r\n\r\nPossibly related to [SFTPHookAsync](https://airflow.apache.org/docs/apache-airflow-providers-sftp/stable/_api/airflow/providers/sftp/hooks/sftp/index.html#airflow.providers.sftp.hooks.sftp.SFTPHookAsync).

Just rebased the branch to get the latest, I think we can merge then if the CI is green.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Looks good! Thanks for the changes @jason810496! I believe the CI failure is likely a transient error caused by a DNS issue that couldn‚Äôt resolve AWS S3 at that moment.

> Looks good! Thanks for the changes @jason810496! I believe the CI failure is likely a transient error caused by a DNS issue that couldn‚Äôt resolve AWS S3 at that moment.\r\n\r\njust reran. seems to work fine üëÄ

Just resolved the issue with inserting duplicate pools by handling the database exception rather than adding extra logic to the `datamodels` or performing additional database queries.  \r\nBy the way, I‚Äôm happy to work further on refactoring to handle duplicate insertion cases across all endpoints  üôå\r\n

Just fixed the nit. I will refactor all endpoints to adopt the global error handler for managing unique constraint violation errors in further PR.

Merged previous one too fast :)

> sorry my bad, that suggestion added properly.\r\n\r\nAnd I did not check how it renders :)

> Cool! Maybe should we also just model `cherry-picker`being a dependency in breeze that this is automatically installed when setting up breeze? :-D\r\n\r\nI think that is mostly an "external" dependency that should be use outside of breeze and independently which environment you are in.  If you install it with `breeze` it will shadow the one that you have installed with `uv tool` or `pipx`. Not sure if that is what we want :)

NICE! Good job @jx2lee !

@potiuk Thank you for reviewing! üíü 

The lowest-dep failure was accidental

Wating for https://github.com/apache/airflow/pull/44138 before merging. (This CI is shorter to run, and PR is easier to update if there are merge conflicts)

@amoghrajesh I see this `.isoformat().replace("+00:00", "Z")` usage in a couple of other places too. Do you suggest moving to the same formatter for consistency?\r\n\r\nhttps://github.com/kunaljubce/airflow/blob/c269be90c9a840d159fd38c03459bc6aeaf5708d/tests/api_fastapi/core_api/routes/public/test_import_error.py#L118\r\nhttps://github.com/kunaljubce/airflow/blob/c269be90c9a840d159fd38c03459bc6aeaf5708d/tests/api_fastapi/core_api/routes/public/test_event_logs.py#L163\r\nhttps://github.com/kunaljubce/airflow/blob/c269be90c9a840d159fd38c03459bc6aeaf5708d/tests/api_fastapi/core_api/routes/public/test_event_logs.py#L170

@iprithv I also see static checks failing. We have a pre-commit to maintain quality standards. You can install pre-commit using this: https://github.com/apache/airflow/blob/d43052e53bcf9bd8772484b8be4590f869932330/contributing-docs/03_contributors_quick_start.rst#configuring-pre-commit

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Hope CLI tool is very useful üòÉ

### Backport failed to create: da9c4017d19bf17a8f3c3015602e54c919be2e2a. View the failure log <a href=\

Let me merge and try to cherry-pick manually my .dockerignore PR

> Let me merge and try to cherry-pick manually my .dockerignore PR\r\n\r\nYeah which is cool :)

Good hint with whitespace\r\n

> P.S.: Time to have AIP-72 working to remove the internal API...\r\n\r\nOh yes.

even with this changes there is still a bug:\r\n```\r\nairflow version\r\n```\r\nObservation:\r\n```\r\nairflow) success@success-HP-Laptop-14-cf3xxx:~/Desktop/airflow$ airflow version\r\nTraceback (most recent call last):\r\n  File "/home/success/Desktop/airflow/.venv/bin/airflow", line 5, in <module>\r\n    from airflow.__main__ import main\r\n  File "/home/success/Desktop/airflow/airflow/__init__.py", line 78, in <module>\r\n    settings.initialize()\r\n  File "/home/success/Desktop/airflow/airflow/settings.py", line 787, in initialize\r\n    configure_orm()\r\n  File "/home/success/Desktop/airflow/airflow/settings.py", line 494, in configure_orm\r\n    async_engine = create_async_engine(SQL_ALCHEMY_CONN_ASYNC, future=True)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File "/home/success/Desktop/airflow/.venv/lib/python3.12/site-packages/sqlalchemy/ext/asyncio/engine.py", line 43, in create_async_engine\r\n    sync_engine = _create_engine(*arg, **kw)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File "<string>", line 2, in create_engine\r\n  File "/home/success/Desktop/airflow/.venv/lib/python3.12/site-packages/sqlalchemy/util/deprecations.py", line 375, in warned\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File "/home/success/Desktop/airflow/.venv/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 544, in create_engine\r\n    dbapi = dialect_cls.dbapi(**dbapi_args)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File "/home/success/Desktop/airflow/.venv/lib/python3.12/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py", line 321, in dbapi\r\n    __import__("aiosqlite"), __import__("sqlite3")\r\n```

Should I add `aiosqlite` to dependencies in `./pyproject.toml`

No - it\

Nice!

ah, oops, thanks

We need to revert that one - because bug of no error fixed in #44087  did not catch the API errors introduced in #43934 . @amoghrajesh  - you will have to recreate the PR unfortunately.

now we have Copilot as reviewer ü§î 

We will also have to merge #44088 to revert the failing API tests.

Now - we detect those errors properlu.

added fix here for the failed tests: https://github.com/apache/airflow/pull/44093

IMHO I think those lazy imports (below) are evil.\r\n\r\nI think we should not try to repeat the same mistake we did with "airflow.". In theory it\

This is what we have in Airflow pre-commit now because of that. I would not want to maintain another pre-commit like that.\r\n\r\n```\r\n      - id: check-base-operator-usage\r\n        language: pygrep\r\n        name: Check BaseOperator core imports\r\n        description: Make sure BaseOperator is imported from airflow.models.baseoperator in core\r\n        entry: "from airflow\\\\.models import.* BaseOperator\\\\b"\r\n        files: \\.py$\r\n        pass_filenames: true\r\n        exclude: >\r\n          (?x)\r\n          ^airflow/decorators/.*$|\r\n          ^airflow/hooks/.*$|\r\n          ^airflow/operators/.*$|\r\n          ^providers/src/airflow/providers/.*$|\r\n          ^airflow/sensors/.*$|\r\n          ^dev/provider_packages/.*$\r\n      - id: check-base-operator-usage\r\n        language: pygrep\r\n        name: Check BaseOperatorLink core imports\r\n        description: Make sure BaseOperatorLink is imported from airflow.models.baseoperatorlink in core\r\n        entry: "from airflow\\\\.models import.* BaseOperatorLink"\r\n        files: \\.py$\r\n        pass_filenames: true\r\n        exclude: >\r\n          (?x)\r\n          ^airflow/decorators/.*$|\r\n          ^airflow/hooks/.*$|\r\n          ^airflow/operators/.*$|\r\n          ^providers/src/airflow/providers/.*$|\r\n          ^airflow/sensors/.*$|\r\n          ^dev/provider_packages/.*$\r\n      - id: check-base-operator-usage\r\n        language: pygrep\r\n        name: Check BaseOperator[Link] other imports\r\n        description: Make sure BaseOperator[Link] is imported from airflow.models outside of core\r\n        entry: "from airflow\\\\.models\\\\.baseoperator(link)? import.* BaseOperator"\r\n        pass_filenames: true\r\n        files: >\r\n          (?x)\r\n          ^providers/src/airflow/providers/.*\\.py$\r\n        exclude: ^.*/.*_vendor/|providers/src/airflow/providers/standard/operators/bash.py|providers/src/airflow/providers/standard/operators/python.py\r\n```

What was the error?

> What was the error?\r\n\r\n```log\r\nTraceback (most recent call last):\r\n  File "/home/airflow/.local/bin/airflow", line 8, in <module>\r\n    sys.exit(main())\r\n  File "/opt/airflow/airflow/__main__.py", line 62, in main\r\n    args.func(args)\r\n  File "/opt/airflow/airflow/cli/cli_config.py", line 48, in command\r\n    func = import_string(import_path)\r\n  File "/opt/airflow/airflow/utils/module_loading.py", line 39, in import_string\r\n    module = import_module(module_path)\r\n  File "/usr/local/lib/python3.9/importlib/__init__.py", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import\r\n  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load\r\n  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked\r\n  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked\r\n  File "<frozen importlib._bootstrap_external>", line 850, in exec_module\r\n  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed\r\n  File "/opt/airflow/airflow/cli/commands/task_command.py", line 46, in <module>\r\n    from airflow.models.dag import DAG, _run_inline_trigger\r\n  File "/opt/airflow/airflow/models/dag.py", line 87, in <module>\r\n    from airflow.models.baseoperator import BaseOperator\r\n  File "/opt/airflow/airflow/models/baseoperator.py", line 80, in <module>\r\n    from airflow.sdk import DAG, BaseOperator as TaskSDKBaseOperator, EdgeModifier as TaskSDKEdgeModifier\r\nImportError: cannot import name \

Yes, we need to sort out our init files "long" term before we cut Airflow 3 release.

> To be clear, this PR doesn\

Once we change the core to airflow-core and have task sdk / core completely isolated (from import POV) ‚Äî things should be better.\r\n\r\nIn the coming weeks, Ash & I are also planning to take de-couple Core & SDK completely \r\n\r\n

> i don\

>FYI @kaxil . Actually just separating airflow core and sdk is not going to help if we keep aliases in sdk I am afraid. This is not because we mix "airlfow" and "sdk" - this is because we are lazy importing modules nested below sdk as part of sdk.__init__.py.\r\n\r\nYeah but there are actual cross deps too right now with SDK relying on Core and Core relying on SDK -- that is what I am referring to

This has been detected by CodeQL scanning (cc: @vikramkoka  -> FIPS compliance, that what we talked about on Thursday).

@potiuk , I notice `hashlib.sha1` being used in two other places that can use this argument:\r\n\r\n1. https://github.com/apache/airflow/blob/19303ca655ad8f977642be49dc225433ffa3cef5/airflow/models/dagcode.py#L197\r\n2. https://github.com/apache/airflow/blob/19303ca655ad8f977642be49dc225433ffa3cef5/airflow/models/taskinstance.py#L2543

> @potiuk , I notice `hashlib.sha1` being used in two other places that can use this argument: \r\n\r\nOh nice. I wonder why CodeQL has not detected those :)

Updated it/

### Backport failed to create: v2-10-test. View the failure log <a href=\

> ### Backport failed to create: v2-10-test. View the failure log [ Run details ](https://github.com/apache/airflow/actions/runs/11874200699)\r\n> Status\tBranch\tResult\r\n> ‚ùå\tv2-10-test\t[![Commit Link](https://camo.githubusercontent.com/955fee2413ee8217f153d861d4b0e068a59a457b74923079660fab04540db8df/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6d6d69742d613835643934652d726564)](https://github.com/apache/airflow/commit/a85d94e6cdcd09efe93c3acee0b4ce5c9508bc23)\r\n\r\nLooks like it failed, due to conflicts

Do we need newsfragment for this one?

Also this just makes me realize that `http://localhost:29091/ui/dags/recent_dag_runs?dag_id_pattern=xxxxx` endpoint should most certainly also support a `dag_id` query params or `dag_ids`.\r\n\r\nI am affraid that using a `pattern` like this could end up matching multiple dags while we are really trying to get the dag_runs from a specific dag like here.

@pierrejeambrun I have `redacted` for `create Asset response` as well let me know if its not needed I can remove that

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Does it mean that our packages from release are broken ? cc: @eladkal ? I guess we need to re-release them? Or do they work as usual? (I did not have time to check them)

> Does it mean that our packages from release are broken ? cc: @eladkal ? I guess we need to re-release them? Or do they work as usual? (I did not have time to check them)\r\n\r\nThey are good as while preparing provider packages we copy the a specific provider directory e.g. `providers/src/airflow/providers/apache/hdfs`.\r\n\r\nhttps://github.com/apache/airflow/blob/f7270c8a2026b8da07590623560ba58e9da38d7f/dev/breeze/tests/test_packages.py#L153-L156\r\n\r\nhttps://github.com/apache/airflow/blob/f7270c8a2026b8da07590623560ba58e9da38d7f/docs/exts/provider_yaml_utils.py#L30\r\n\r\n```\r\n‚ùØ tree apache_airflow_providers_amazon-9.1.0rc4-py3-none-any\r\napache_airflow_providers_amazon-9.1.0rc4-py3-none-any\r\n‚îú‚îÄ‚îÄ airflow\r\n‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ providers\r\n‚îÇ\xa0\xa0     ‚îî‚îÄ‚îÄ amazon\r\n```

UFF üòì 

> suppress the warnings in this particular case where we check for sensitive values.\r\n\r\nokie doke\r\n\r\nhttps://github.com/apache/airflow/pull/44148

Closing. Fixed by #44148

sorry :)

created an issue for the TODO: https://github.com/apache/airflow/issues/44062

Merging. Those are intermittent issues only

### Backport failed to create: v2-10-test. View the failure log <a href=\

Good job! I like that version

Should be ready for merge

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Only last 2 commits are relevant

Closing in favour of https://github.com/apache/airflow/pull/44130

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

closing in favor of https://github.com/apache/airflow/pull/44138

That was intentional just to confirm the dagParams generated. But its fine.

needs rabasing + conflict resolution

closing in favour of https://github.com/apache/airflow/pull/44139

>But what Kaxil wants, Kaxil gets ;)\r\n\r\n<img src="https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExMnFsM2prd2s0cWd0YW4zeXQzeTU0bXV3ZXBzbG82NzNlZzRjcGwzZSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/kHOts8xerNiFI6KdCO/giphy.gif" />

PR of the month ;)

> Indeed. Good candidate for PR of the month.\r\n\r\nYeah if this does not win it‚Äôs a totally rigged competition 

FastAPI itself returns `str` for `datetime` objects in both requests and responses. As mentioned earlier, I‚Äôm not certain if there would be a meaningful difference since `pendulum.parse()` already supports the `ISO 8601` format.\r\n\r\n> * datetime.datetime:\r\nA Python datetime.datetime.\r\nIn requests and responses will be represented as a str in ISO 8601 format, like: 2008-09-15T15:53:00+05:00.\r\n\r\nhttps://fastapi.tiangolo.com/tutorial/extra-data-types/?h=datet#other-data-types

> Looks good, let us just merge again the PR introducing `OptionalDateTimeQuery` (it got reverted because of CI issues not catching tests etc...) and we should be good to go\r\n\r\nThanks for the quick reviews and for merging the changes again so promptly! I aimed to replicate the changes and test the endpoint accordingly. I also followed up on the CI failure, which helped me realize I needed to apply a similar fix to the CI script updates in another PR later on :)

Restarting failed job, ready to merge when CI is green.

Testing complete, failing tests are passing. Closing this PR.

sure. Go ahead.

Yeah I was afraid of this - you cannot us `dict in dict` assert :( .

https://github.com/pytest-dev/pytest/issues/2376#issuecomment-852366588

Python 3.9+\r\n\r\n```\r\nassert {**dict2, **dict1} == dict2\r\n```

oh whoops, fixing

looks good. Maybe one nit - would be great to add comment with link to the issue in provider.yaml

(we have it in commit so no strictly necessary - but it safes git blame etc.)

> (we have it in commit so no strictly necessary - but it safes git blame etc.)\r\n\r\nDone

Damn! last commit msg was used as commit title :( 

Good we have the comment :)

Could you please update the PR title? The current one is a bit vague and doesn‚Äôt clearly communicate the purpose of the change. I recommend reading [this blog post](https://cbea.ms/git-commit/) for guidance.\r\n\r\nIn particular, it would be helpful to follow this principle:\r\n\r\n> **_A properly formed Git commit subject line should always be able to complete the following sentence: ‚ÄúIf applied, this commit will [your subject line here].‚Äù_**\r\n\r\nSince this entry will appear in the release notes, it‚Äôs important that the PR title and description clearly explain the changes and their impact.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

error already fixed in main.

I don‚Äôt think we are moving core Airflow concepts like BaseOperator into the provider? Unless we are, at least `@task` should stay in core. Likely `@setup` and `@teardown` as well (since `as_setup()` and `as_teardown()` are on BaseOperator).

> But isn\

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Not sure which PR causes this failure trying to find :)

One of the few latest asset changes. I think we should automatically add providers open-lineage tests after any changes in assets, as their test heavily depend on them.\r\n\r\nThis will be quite a bit easier I think after https://github.com/apache/airflow/pull/43979 lands.

Issue created : https://github.com/apache/airflow/issues/44026

Yeah it was caused by #41325 - and there the test were green because openlineage tests were skipped by selective checks., so #44026 when implemented might help to avoid such breakages.

(or maybe @mobuchowski @kacpermuda - we could make those open-lineage tests less depending on actual dataset implementation ? Not sure).

One point to add is that current import errors are shown all at once in the legacy home page which makes using `Ctrl+F` easier to search especially on large Airflow instances with lot of users and hundreds of import errors which might not be possible anymore with pagination.

> One point to add is that current import errors are shown all at once in the legacy home page which makes using `Ctrl+F` easier to search especially on large Airflow instances with lot of users and hundreds of import errors which might not be possible anymore with pagination.\r\n\r\n@tirkarthi I have added a search option to do that.

@bbovenzi \r\nAlso, since this is an error showing page, I saw the old UI have names too in red, So what do you think, which of the two options look good? Red or normal black?\r\n<img width="996" alt="image" src="https://github.com/user-attachments/assets/84383519-7933-4e8e-aa3a-b2e459bb53a7">\r\n

cc @potiuk 

This is strange error

Not sure one test is failing.. \r\nhttps://github.com/apache/airflow/actions/runs/11835585982/job/32982431658?pr=44018#step:12:1349

Applying full tests needed should help

The problem is that "sdist" test build new providers and try to install them, and they do it in "chunks" - and only for providers affected in this PR. This is a bit missing piece - this PR  only modifies the standard provider but no common.sql 1.20 that it depends on, so it misses 1.20 version locally built.\r\n\r\nWe could likely solve it by smarter selection which providers should be built for sdist builds, but for now "full tests needed"  should be enough - also this will go away after we merge this one an release common.sql 1.20 , and applying "full tests needed" should solve the problem as all provider\

> We could likely solve it by smarter selection which providers should be built for sdist builds, but for now "full tests needed" should be enough - also this will go away after we merge this one an release common.sql 1.20 , and applying "full tests needed" should solve the problem as all provider\

\r\n> We can open a followup task in Github issue to get it done when we have the time\r\n\r\nYeah. I was adding it as you wrote it :) https://github.com/apache/airflow/issues/44023

> The problem is that "sdist" test build new providers and try to install them, and they do it in "chunks" - and only for providers affected in this PR. This is a bit missing piece - this PR only modifies the standard provider but no common.sql 1.20 that it depends on, so it misses 1.20 version locally built.\r\n> \r\n> We could likely solve it by smarter selection which providers should be built for sdist builds, but for now "full tests needed" should be enough - also this will go away after we merge this one an release common.sql 1.20 , and applying "full tests needed" should solve the problem as all provider\

tests are failing for openlineage, created this https://github.com/apache/airflow/pull/44025

ah i see you already created this :) , have made change here https://github.com/apache/airflow/pull/44018 do you want me to close that?

Even though it is green, converting to DRAFT as long as devlist discussion is ongoing.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Only last 2 commits are relevant

Closing in favour of https://github.com/apache/airflow/pull/44128

Hmm. Interesting thing - now we have circular dependency: compat -> standard , standard->compat ... which is generally fine but we might have some interesting CI issues because of that :)

> That will also mean @eladkal that we MUST release standard provider with this wave - otherwise common won\

Very interesting error. Side effect of another test. Fix is coming. \r\n\r\n```\r\n_______________________ TestAutoMLHook.test_get_dataset ________________________\r\n[gw2] linux -- Python 3.9.20 /usr/local/bin/python\r\n/opt/airflow/providers/tests/google/cloud/hooks/test_automl.py:258: in test_get_dataset\r\n    self.hook.get_dataset(dataset_id=DATASET_ID, location=GCP_LOCATION, project_id=GCP_PROJECT_ID)\r\n/opt/airflow/providers/src/airflow/providers/google/common/hooks/base_google.py:560: in inner_wrapper\r\n    return func(self, *args, **kwargs)\r\n/opt/airflow/providers/src/airflow/providers/google/cloud/hooks/automl.py:669: in get_dataset\r\n    client = self.get_conn()\r\n/opt/airflow/providers/src/airflow/providers/google/cloud/hooks/automl.py:97: in get_conn\r\n    self._client = AutoMlClient(credentials=self.get_credentials(), client_info=CLIENT_INFO)\r\n/usr/local/lib/python3.9/site-packages/google/cloud/automl_v1beta1/services/auto_ml/client.py:793: in __init__\r\n    self._transport = transport_init(\r\n/usr/local/lib/python3.9/site-packages/google/cloud/automl_v1beta1/services/auto_ml/transports/grpc.py:145: in __init__\r\n    if isinstance(channel, grpc.Channel):\r\nE   TypeError: isinstance() arg 2 must be a type or tuple of types\r\n```\r\n

The failure is fixed in https://github.com/apache/airflow/pull/44029

> The failure is fixed in #44029\r\n\r\nCool so no need to regenerate docs as the fix is only in a test

> Need a newsfragment for it, otherwise LGTM.\r\n\r\nJust added! Thanks!

> Why this PR targets v2-10-test branch? Helm chart is released from main branch\r\n\r\nHmmm... this is just for fixing the CI failure on `v2-10-test`

> > Why this PR targets v2-10-test branch? Helm chart is released from main branch\r\n> \r\n> Hmmm... this is just for fixing the CI failure on `v2-10-test`\r\n\r\nThen why the newsfragment? I think me and Jed see this as feature not an internal fix to the CI.\r\nIs this already avaliable in main branch and this is just the "chery pick" to fix the CI?

> > > Why this PR targets v2-10-test branch? Helm chart is released from main branch\r\n> > \r\n> > \r\n> > Hmmm... this is just for fixing the CI failure on `v2-10-test`\r\n> \r\n> Then why the newsfragment? I think me and Jed see this as feature not an internal fix to the CI. Is this already avaliable in main branch and this is just the "chery pick" to fix the CI?\r\n\r\nJust notice there\

> > > > Why this PR targets v2-10-test branch? Helm chart is released from main branch\r\n> > > \r\n> > > \r\n> > > Hmmm... this is just for fixing the CI failure on `v2-10-test`\r\n> > \r\n> > \r\n> > Then why the newsfragment? I think me and Jed see this as feature not an internal fix to the CI. Is this already avaliable in main branch and this is just the "chery pick" to fix the CI?\r\n> \r\n> Just notice there\

> > > > > Why this PR targets v2-10-test branch? Helm chart is released from main branch\r\n> > > > \r\n> > > > \r\n> > > > Hmmm... this is just for fixing the CI failure on `v2-10-test`\r\n> > > \r\n> > > \r\n> > > Then why the newsfragment? I think me and Jed see this as feature not an internal fix to the CI. Is this already avaliable in main branch and this is just the "chery pick" to fix the CI?\r\n> > \r\n> > \r\n> > Just notice there\

No need for newsfragment on this PR after the fix in main

it seems to be fixed already. close this one

converted to draft for now\r\n

I will create an issue for that maybe @gopidesupavan or others would like to take that :)

I added https://github.com/apache/airflow/issues/44020 describing what needs to be done

> I will create an issue for that maybe @gopidesupavan or others would like to take that :)\r\n\r\nCool , Interesting one to solve :)

> > I will create an issue for that maybe @gopidesupavan or others would like to take that :)\r\n> \r\n> Cool , Interesting one to solve :)\r\n\r\nIndeed :). It touches a bit of everything :D

Non-DB tests are passing. Merging.

Actually .... even better fix coming

Even better fix in https://github.com/apache/airflow/pull/44006

Cool! Thanks!

Added the missing example.

Test fail :(\r\n\r\n```\r\ntests/always/test_example_dags.py::test_should_be_importable[providers/tests/system/amazon/aws/example_dms_serverless.py] - AssertionError: import_errors={\

The import error should be fixed now.

still seeing error\r\n```\r\nproviders/tests/system/amazon/aws/example_dms_serverless.py:472: error: Module\r\n"tests_common.test_utils.system_tests" has no attribute "watcher" \r\n[attr-defined]\r\n        from tests_common.test_utils.system_tests import watcher\r\n        ^\r\nFound 1 error in 1 file (checked 3356 source files)\r\n```

It seems the merge of this PR broke tests on main/canary builds:\r\nhttps://github.com/apache/airflow/actions/runs/12380031516/job/34555973915\r\n\r\nSomebody having an idea how to fix? Shall we revert?

Okay, I thought I make a "quick fix" and add aiobotocore as dependency... but there is a bit of history and even as pre-commit check NOT to add this.\r\nSo loading/testing it seems need to be selective.

Fix merged in #45013

Looks like something went bad in my rebase. I did not change 5000+ files. I need to look into it.

Something is "wrong" with this PR :-D

New PR: #43988

This PR (when you open failing output you can see it) revealed  limitation on parsing development dependencies that should be installed for compatiblity testing.\r\n\r\nThe fix is here: https://github.com/apache/airflow/pull/43995 - but I think this limit should not be placed in devel-deps. I will take a look in a moment.\r\n\r\n

This is a better fix https://github.com/apache/airflow/pull/43998 - aiohttp is already a dependency of `adlfs` so we can add it there.

Even better fix https://github.com/apache/airflow/pull/44006

Closing -> the fix from https://github.com/apache/airflow/pull/44006 is much more "appropriate".

> i approve so you can move fast but gave some comments\r\n\r\nI appreciate it, going to keep this PR limited to just the set_attributes change, will follow-up it with other things

Changed my mind around some UX and colors. Screenshots are updated.

@pierrejeambrun PR synced with `main` and all conversations resolved. Thank you! ‚úÖ 

OK. I moved it here from https://github.com/apache/airflow/pull/43965 .\r\n\r\nThat one should be ready for review. I got it green, all the tests are nicely split between core and providers. I will be adding a little more diagnostics (Right now it is not obvious if the successful tests are doing what they are supposed to do) and running it for all versions of Python etc. to check that everythong is as expected + I will add runnig system tests - but other than that, I think it is in the shape that is pretty much ready to merge (and for sure ready to review).\r\n\r\nLeter we can run a number optimizations, but the way it is and after I check it for all typos and run in multiple versions, we should be ready to go. I am quite away next week (in Ireland) - and will be less available again in the next week, but I am confident it\

@ashb \r\n> Interface question should the db/non-db be a flag that operates like a filter, so something like breeze testing core-tests --non-db etc?\r\n\r\n@vincbeck \r\n> \r\n> I agree, a flag to filter db/non-db tests would make more sense to me instead of having separate commands\r\n\r\nOK. That\

> \r\nJust so I understand the plan, this splits out the running of tests, but doesn\

The whole  set of test commands is simplified now:\r\n\r\n<img width="971" alt="Screenshot 2024-11-15 at 02 21 55" src="https://github.com/user-attachments/assets/1c865a38-08bf-4258-aed6-66d7a91eefed">\r\n\r\nI also updated docs and examples and contributing docs.\r\n\r\nI also reviewed and updated docs and fixed and simplified how sytem tests are run.\r\n\r\nThere is no more `--system SYSTEM` or `pytest.mark.system("SYSTEM")` but simply `--system`  and `pytest.mark.system`. Also the example dags in "tests/system" and "providers/tests/system" are automatically marked with the "system" marker.\r\n\r\nSo tests shoudl be run:\r\n\r\nIn venv/inside breeze:\r\n\r\n```bash\r\npytest --system providers/tests/system/google/cloud/bigquery/example_bigquery_queries.py\r\n```\r\n\r\nvia Breeze - there is one command only:\r\n\r\n```bash\r\nbreeze testing system-tests  providers/tests/system/google/cloud/bigquery/example_bigquery_queries.py\r\n```\r\n\r\ncc: @ahidalgob @kosteev @pankajkoti @fdemiane @sc250072 \r\n\r\nnce we merge it you will have to update the dashboards

And merged!

Thanks for heads up, Jarek.\r\nWe will update System Tests Dashboard for google provider package with regard to these changes.

Could you please add a unit test covering this case?

> Maybe add one then - following the same structure in `tests` as you have for the source\r\n\r\nSure. Since it\

> > Maybe add one then - following the same structure in `tests` as you have for the source\r\n> \r\n> Sure. Since it\

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

### Backport failed to create: v2-10-test. View the failure log <a href=\

hmmm.. not quite sure, but I can take a look

@johncmerfeld I guess it was due to conflict. Created backport PR https://github.com/apache/airflow/pull/46707

I appreciate it @Lee-W - looks like that backport PR was successful so I will assume this change will be in the 2.10.6 release. Thanks!!

Find this approach already using in airflow, so followed similar one here. is that fine?

It looks it works nicely :)\r\n

The failed checks are unrelated and due to "docker timeouts"

I like the name change, but the default being "on" was discussed in the PR where it was added](https://github.com/apache/airflow/pull/39908), why are we changing it now?  This is a "breaking change" in that it requires adjusting StatsD dashboards, but it is being applied in 3.0 which is the time to do such changes, right?  The last I heard in discussions was that StatsD was being moved to second-class and OTel Metrics were supposed to be treated as the standard starting in 3.0 \r\n\r\nPlease read through the conversation on the other PR and reconsider this change.

> I like the name change, but the default being "on" was discussed in the PR where it was added](#39908), why are we changing it now? This is a "breaking change" in that it requires adjusting StatsD dashboards, but it is being applied in 3.0 which is the time to do such changes, right? The last I heard in discussions was that StatsD was being moved to second-class and OTel Metrics were supposed to be treated as the standard starting in 3.0\r\n> \r\n> Please read through the conversation on the other PR and reconsider this change.\r\n\r\nA few things:\r\n- The version-added in this one was still `version_added: 2.10.0`\r\n- Airflow 3 will already remove this, check PR description where it says: "We should backport this to 2.11 and remove this setting from Airflow main" including the newsfragment. The idea is to keep it "off" for 2.11 so users get advanced warning and they can migrate early if they want without breaking and of their current tools. The main behaviour still says the same once we remove it, there is also a TODO I had added in this PR, please let me know if that isn\

The only thing different is instead of getting it in 3.0 with default "on" and breaking compact and removing it in 3.1 -- here we make it consistent that 3.0 will break it -- and we warn users in advance about this in 2.11 -- our bridge release.\r\n\r\nIt will make it consistent with how we are breaking other things.

Thank you.

Also cc: @ahidalgob @kosteev @pankajkoti @fdemiane @sc250072  except the reviewers that I pinged.\r\n\r\nThis is something I worked on for last few days as first stage of #42632 - it will be quite big eventually and it is still work in progress (I am still working on fixing all the tests in breeze selective checks and need to apply changes to github workflows to run the tests but wanted to give you a heads up of what I am proposing as a change.\r\n\r\nSo do not (yet) comment on subtle details - that will come, but I\

!!!!!!!!!!  Please, Please, Pretty please - committers do not approve the workflow. This is an example for Github Ticket I opened to show that I need to approve my own workflows now. !!!!!!!\r\n:scream: :scream: :scream: :scream: :scream: :scream: :scream: :scream: :scream: :scream: :scream: :scream: :scream: 

Looks good so far!\r\nDo we need to make the db/non-db part of the command (e.g. `core-tests`, `core-db-tests`, `core-non-db-tests`)? It feels like we should have just one group command and then an option to toggle db/non-db/both.\r\n\r\nOther than that I like the changes!

> Do we need to make the db/non-db part of the command (e.g. core-tests, core-db-tests, core-non-db-tests)? It feels like we should have just one group command and then an option to toggle db/non-db/both.\r\n\r\nIt makes it quite a bit easier to handle. DB paralel tests are run using multi-processing and running parallel docker-composes. Where Non-DB parallel tests are run using xdist. This makes the two commands pretty distinct - they not only behave diferently but also they have various other parameters applicable on in one of those cases (mostly the DB).\r\n\r\nThose are the options that only make senses for DB tests:\r\n\r\n* `--db-reset`\r\n* `--backend`\r\n* `--no-db-cleanup`\r\n* `--postgres-version`\r\n* `--mysql-version`\r\n\r\nThanks to that teh `non-db-tests` have way smaller set of parameter than `db-tests`.\r\n\r\nTechnically we could pass all the necessary parametters for non-db option in the "regular" (that handles both db and non-db) - parallelism, backend=none and few others, But then that makes `non-db` commands way simpler when we select which command to use based on parameters passed - for example:\r\n\r\n```\r\n    if [[ "${TEST_SCOPE}" == "DB" ]]; then\r\n        set -x\r\n        breeze testing core-db-tests\r\n        set +x\r\n    elif [[ "${TEST_SCOPE}" == "Non-DB" ]]; then\r\n        set -x\r\n        breeze testing core-non-db-tests\r\n        set +x\r\n```\r\n\r\nI experimented with it before, and that one produced the nicest and simplest set of commands to execute without specifying too many parameters.

I moved the change to https://github.com/apache/airflow/pull/43979.  Closing this one - seems workflows have been run eventualy BTW

Yeah. I really like how the "lowest dependency" tests are keeping thing in order - this is a result of #41916 telling us "Not good version" :) 

close in favor of #44639

Only the last 2 commits are relevant

Closing in favour of https://github.com/apache/airflow/pull/44129

Hi @potiuk @eladkal @kosteev !\r\nThis PR removes operators, hooks and other stuff that was planned for removal after November 1st, so based on our deprecation policy (6 months notice period)

#protm

I think you got big chances for PR of the month @moiseenkov :+1: \r\n\r\n![image](https://github.com/user-attachments/assets/60e73f6e-60d5-4e17-94ed-539eceede48f)\r\n

Nice ! thanks!

Maybe also we should attempt to keep old serialized DAGS and attempt to load them (Assuming that our serialization is forward-compatible) . That could also be done on a "best effort" case.

> Maybe also we should attempt to keep old serialized DAGS and attempt to load them (Assuming that our serialization is forward-compatible) . That could also be done on a "best effort" case.\r\n\r\nYeah, old serialized dags won\

> with DAG bundles we could actually in the future to reserialize also history ? It sounds feasible, we would just have to go through version history and checkout the bundle in each version and reserialize it then.\r\n\r\nThis can already be pretty slow. Doing it for every historical version isn\

Will appreciate another review @jedcunningham @pierrejeambrun 

Hi @pierrejeambrun, \r\n\r\nWhat are your thoughts on the current `filter_param_factory` implementation? The current approach adds a new parameter, `_type`, to specify the type of query parameter at runtime. This implementation consolidates multiple factories like `str_filter_param_factory`, `str_list_filter_param_factory`, etc., into a single `filter_param_factory`.\r\n\r\nI have checked the OpenAPI schema at `http://localhost:29091/docs`, and the correct type is displayed in the documentation. \r\n\r\nLooking forward to your feedback!\r\n

I think it\

Hi @pierrejeambrun,\r\nI chose the schema with only basic functionalities defined in `common/parameters.py` but used solely in a single endpoint.\r\nThe refactor will not affect the OpenAPI schema or test cases. The current differences in `v1-generated.yaml` are caused by the `Offset`, `Limit`, and `OrderBy` parameters.\r\n\r\n### Follow-Up\r\nI noticed that `_SearchParam` could leverage the factory pattern as well. I plan to refactor this part after this PR is merged to avoid making the scope of this PR too large.\r\n\r\n### Question\r\nFor some parameter schemas like `QueryTIStateFilter`, which are only used twice in `task_instance.py`, should such parameter schemas be placed directly in `task_instance.py` rather than in `common/parameters`?  \r\n\r\nThanks !\r\n

Rebased to the latest `main`. Looking forward to further feedback before merging üôå\r\n

Rebasing, merging on green CI.

really interesting one :) 

> really interesting one :)\r\n\r\nIndeed - that is about third time when our test harness (which I am reeally happy about) uncover a bug in CPython.\r\nFor me this one case is a single most important reason why we should have all the matrices and canary builds running everything. Once evey half a year or so, they discover an issue in one or two of the combos that would take us weeks of investigation if it would have happened in "production" - and it happens way, way, way before it is even close to being released.\r\n

The failure in 3.11 and 3.12 will be handled by reverting MetaPath redirection in https://github.com/apache/airflow/pull/43946

merged the reverted changes for redirect module.

Is this related to AIP-70?\r\n\r\nhttps://cwiki.apache.org/confluence/display/AIRFLOW/%5BWIP%5D+AIP-70+Migrating+to+asynchronous+programming

> Is this related to AIP-70?\r\n> \r\n> https://cwiki.apache.org/confluence/display/AIRFLOW/%5BWIP%5D+AIP-70+Migrating+to+asynchronous+programming\r\n\r\nI think you prob know the answer.  The AIP deals with asyncio, so yes strictly speaking related in that sense.  But not "part of".  That AIP has been in draft a long time not sure actively worked on.  Anyway, I don\

There is a similar draft PR open as POC for it though not active. So just wanted to confirm. Thanks for the details.\r\n\r\nhttps://github.com/apache/airflow/pull/36504

@pierrejeambrun A FastAPI dep to give an async session would be good, but I think anything in the models etc should be passed an explicit session.

NICE! Yeah. I really like to see it working with mysql compatibility and benchmarks :package: \r\n\r\n(because of course MySQL is our beloved database)

Lovely! Thank you for adding this, will try to start using this soon :)

giving-up but the problem anyway was in another side

If I may, can you add some indication to the title that this is related to OTel Traces specifically?   We also have OTel Metrics implemented and it would be nice to minimize confusion.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

I need to work on the tests since it seems that setting the state of an already installed provider from ready to non-ready is not that trivial

> I need to work on the tests since it seems that setting the state of an already installed provider from ready to non-ready is not that trivial\r\n\r\nI dont think this is right approch. We may still release 1.x versions. For example if we find security risk.\r\nProvider should stay in ready state.\r\nAlso, we are going to have airflow 3 alpha/beta which might need also alpha/beta of the provider\r\n\r\nI can simply skip its release manually if needed.

Sounds good!

The compat tests are failing because Airflow 2.9 and 2.10 tries to install the latest fab provider that is only compatible with Airflow 3. In `hatch_build`, the fab provider constraint is `fab>=1.0.2`. How can we solve that?  Should we, somehow, update this constraint to `fab>=1.0.2<=2.0.0` for Airflow 2.9 and 2.10? 

Or should `uv` figure it out on its own?

> Or should `uv` figure it out on its own?\r\n\r\nI think we will have to simply skip fab provider for Airflow 2.9 and 2.10. That should do the job. It will be installing applicable FAB from PyPI then

In global_constants.py in Breeze:\r\n\r\n```python\r\nBASE_PROVIDERS_COMPATIBILITY_CHECKS: list[dict[str, str | list[str]]] = [\r\n    {\r\n        "python-version": "3.9",\r\n        "airflow-version": "2.8.4",\r\n        "remove-providers": "cloudant fab edge",\r\n        "run-tests": "true",\r\n    },\r\n    {\r\n        "python-version": "3.9",\r\n        "airflow-version": "2.9.3",\r\n        "remove-providers": "cloudant edge",\r\n        "run-tests": "true",\r\n    },\r\n    {\r\n        "python-version": "3.9",\r\n        "airflow-version": "2.10.3",\r\n        "remove-providers": "cloudant",\r\n        "run-tests": "true",\r\n    },\r\n]\r\n```

> > Or should `uv` figure it out on its own?\r\n> \r\n> I think we will have to simply skip fab provider for Airflow 2.9 and 2.10. That should do the job. It will be installing applicable FAB from PyPI then\r\n\r\nMakes sense. Thank you!

All tests are passing! 

Providers are now released. Can we proceed with this one?

can we have unit test to cover this change?

Nice

Is not that something that should be back-ported to 2.10.4 ? It certainly looks like

@pierrejeambrun FYI, I have pushed the changes for all the "resolved" comments. The rest are open for input from you.\r\n\r\nEDIT: I kept the endpoint at public/dags.py cos if I move to assets, the path will become: `/public/assets/{dag_id}/assets/queuedEvent` due to the underlying router registration. I moved the datamodels and tests to assets.py however

> > There\

> > > There\

Here is one attempt: https://github.com/apache/airflow/pull/43990

You will need to fix tests

@dominikhei can you rebase and fix the tests?

> @dominikhei can you rebase and fix the tests?\r\n\r\n@eladkal Thank you for the reminder. I will fix it in the next few days!

Looks like this was already added in https://github.com/apache/airflow/pull/44565

Thanks Kaxil

Nice optimisation !

Thanks @potiuk 

Do we need to fix up the selective check logic?\r\n\r\nhttps://github.com/apache/airflow/actions/runs/11750492342/job/32738823728?pr=43521#step:8:878\r\n\r\n> skip-pre-commits = ...,ts-compile-format-lint-ui,ts-compile-format-lint-www\r\n\r\nhttps://github.com/apache/airflow/blob/main/dev/breeze/src/airflow_breeze/utils/selective_checks.py#L1077-L1133 will need updating.\r\n

Oops, wrong branch ü§¶ 

Yes I completely agree :)

mypy failures are not related to this PR

Yeah. We should solve it separately (if not solved already).

Could you include before and after pics please?\r\n\r\nAnd if we use `.. warning::` anywhere else before and after of one those too please

Screenshots are here: https://github.com/apache/airflow/issues/41532#issuecomment-2467139167

**For `BigQueryExecuteQueryOperator`:**\r\n\r\n**Before:**\r\n<img width="628" alt="image" src="https://github.com/user-attachments/assets/4ac926d8-9a7e-4565-8ced-deddac061a91">\r\n\r\n**After:**\r\n<img width="608" alt="image" src="https://github.com/user-attachments/assets/8b1f6609-8f53-440e-b7de-bb68e3d398ba">\r\n\r\n\r\n\r\n\r\n\r\n\r\nThere are other operators that use `.. warning::` in their docstring, here is `CloudDataTransferServiceCreateJobOperator`\r\n\r\n**Before:**\r\n<img width="609" alt="image" src="https://github.com/user-attachments/assets/29d62ef0-22ac-4ba3-8ef0-5ced4f3d0edc">\r\n\r\n**After:**\r\n<img width="608" alt="image" src="https://github.com/user-attachments/assets/1cc619ce-eb8c-4698-a8d1-a12c18be7b5e">\r\n\r\n\r\n\r\n\r\n\r\nSo the styling appears to apply to all docstrings, which use the `.. warning::` admonition (since the CSS is in `docs/sphinx_design/static/custom.css`) . If that is an issue, I could create a `.css` specifically for Google Providers if that helps. Then that would restrict the styling to Google Providers. 

@ashb Did I provide enough screenshots? Are there any other concerns?

We should do the same for all deprecated operators too

You need to rebase/resolve conflicts @geraj1010 

> @kaxil @potiuk Shall I raise a separate PR with commits from this PR or something else, please let me know. Thank you :)\r\n\r\nIf @geraj1010 is ok with it, absolutely - feel free.

I resolved the conflict, and merged

Verified it locally:\r\n\r\n<img width="1108" alt="image" src="https://github.com/user-attachments/assets/bb02a45f-2f6b-438f-af36-355fb20aef90">\r\n

@eladkal Worth including this change in the next batch of providers

Greetings, I was away for the holidays. Thank you all for review!\r\n\r\n@kaxil Thank you for resolving the conflict. Can you please tell me what it was?

> Greetings, I was away for the holidays. Thank you all for review!\r\n> \r\n> @kaxil Thank you for resolving the conflict. Can you please tell me what it was?\r\n\r\nHi @geraj1010 , I hope you had a good holiday. Your base branch was 1000 commits behind the Airflow main -- Example: https://github.com/apache/airflow/commits/39ebd75c5cb066218a2b6b3940eddc217dd816d9/docs/sphinx_design/static/custom.css -- your branch\

I like the drop of the pickle type - but I\

I will create a separate PR to handle the migration so that can be reviewed independently -- will have a PR by EOD today

PR created: https://github.com/apache/airflow/pull/44166

Nice. never used it but might be a good idea :)

This needs a news fragment. Rewording the above summary would work fine. See existing files in `newsfragments` (the `significant` ones) for some examples.

> For templates, replace `{{ ds }}` with `{{ data_interval_start | ds }}`.\r\n\r\n~~This should probably use `logical_date` for max compatibility. Also should mention `ts` (same rewrite).~~\r\n\r\nOh wait, we‚Äôre not removing those in this PR yet, so this should not be mentioned at all.

Is this change backward compatible for all providers?

Providers don‚Äôt generally use `execution_date`; in the rare cases they do, a compatibility layer is provided so they work on both Airflow 2 and 3. No official providers have interface that expose `execution_date` to the user.

> Providers don‚Äôt generally use `execution_date`; in the rare cases they do, a compatibility layer is provided so they work on both Airflow 2 and 3. No official providers have interface that expose `execution_date` to the user.\r\n\r\nYeah. I think we need to bite the bullet and change it - even if we know that **some** things outside of our providers will be broken. Our compat tests are passing, which means that the change is "good to go" from our provider\

> The shift towards `logical_date` helps move away from the limitations of `execution_date`, particularly with dynamic DAG runs and cases where multiple runs occur at the same time.\r\n\r\nAdded the newsfragment

cc @JDarDagran 

Static check failure unrelated and will be fixed by #43923

Nice!

@potiuk Please merge this, I have created a fresh pr for the change.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> do you plan to make `runner` mode customizable at task level ( like executor ) ?\r\n> \r\n> cause I have multiple `operational` pythonoperator that manage heavy database operations like this\r\n\r\n@raphaelauv No plans today. This is not something in scope for AIP-72 (nor is it possible today)

> lgtm. Worth splitting certain methods/func into more granular funcs and adding more docstrings wherever you can.\r\n\r\nFor sure, I will do that before merging and ping you tomrorow for another look

nice!

Applied all the PR comments locally and rebased with latest main branch. Thanks.

Are screenshots up to date following latest changes ? Just to get a rough idea of the rendered UI.

Latest screenshots after the review changes. Most of it has been moving `Metrics.tsx` to `HistoricalMetrics` with one component per file. Moving from text customization to using `Badge` and a few text size, padding, margin changes.\r\n\r\n![image](https://github.com/user-attachments/assets/3e95ff72-cac5-4d3a-ac71-80a7b424a3ae)\r\n\r\n![image](https://github.com/user-attachments/assets/c458055c-0961-4f6f-a538-97a2770de244)\r\n

It appears okay in Brave on Ubuntu. Probably a Firefox/Linux issue as many have reported it in https://github.com/chakra-ui/chakra-ui/issues/983 , https://github.com/chakra-ui/chakra-ui/issues/2314\r\n\r\n![localhost_8000_webapp_ (1)](https://github.com/user-attachments/assets/255f1923-34fb-4823-9831-b4dcc5708c37)\r\n\r\n

> It appears okay in Brave on Ubuntu. Probably a Firefox/Linux issue as many have reported it.\r\n\r\nNice to hear, thanks for investigating.

Lgtm! We can iterate on a more detailed loading state and update `end_date` later.

Thanks @bbovenzi and @pierrejeambrun .

Nice!

@amoghrajesh thanks for the suggestion :) 

> I fail to see where we configure the target branch.\r\n\r\nah it would take from the label, backport-to-v2-10-test, so here `v2-10-test` is the target branch

> > I fail to see where we configure the target branch.\r\n> \r\n> ah it would take from the label, backport-to-v2-10-test, so here `v2-10-test` is the target branch\r\n\r\nYeah, We will just have to create appropriate labels for all past versions.

I rebased it to account for latest static check fixes.

> > > I fail to see where we configure the target branch.\r\n> > \r\n> > \r\n> > ah it would take from the label, backport-to-v2-10-test, so here `v2-10-test` is the target branch\r\n> \r\n> Yeah, We will just have to create appropriate labels for all past versions.\r\n\r\nYes agreed, not sure how to test? do we have any prs to backport? üòÑ 

> Yes agreed, not sure how to test? do we have any prs to backport? üòÑ\r\n\r\nAbsolutely: https://github.com/apache/airflow/pull/43885 should be backported to all branches. Perfect candidate :D. I will rebase and create/apply backport branches when this one is merged :)\r\n\r\n

> Looks cool! Do you prepare labels with a description such that we know what to pick.\r\n> \r\n> Looking forward to automate the manual burden!\r\n\r\nI will do it when backporting #43885 to ALL past branches :)

And I will delete the old branches after testing :). We want to have quick label selection.

Ah.... We will need to get approval from INFRA\r\n\r\n```\r\nsorenlouv/backport-github-action@ad888e978060bc1b2798690dd9d03c4036560947 is not allowed to be used in \r\napache/airflow. Actions in this workflow must be: within a repository that belongs to your Enterprise account, created by \r\nGitHub, verified in the GitHub Marketplace, or matching the following:\r\n */*@[a-f0-9][a-f0-9][a-f0-9][a-f0-9][a-f0-9][a-f0-9][a-f0-9]+, AdoptOpenJDK/install-jdk@*, \r\nTobKed/label-when-approved-action@*, actions-cool/issues-helper@*, actions-rs/*, al-cheb/configure-pagefile-action@*, amannn/action-semantic-pull-request@*, apache/*, burrunan/gradle-cache-action@*, \r\nbytedeco/javacpp-presets/.github/actions/*, chromaui/action@*, codecov/codecov-action@*, \r\ncontainer-tools/kind-action@*, container-tools/microshift-action@*, \r\ndawidd6/action-download-artifact@*, delaguardo/setup-graalvm@*, \r\ndocker://jekyll/jekyll:*, docker://pandoc/core:2.9, eps1lon/actions-label-merge-conflict@*, \r\ngaurav-nelson/github-action-markdown-link-check@*, golangci/*, gr2m/twitter-together@*, \r\ngradle/wra...\r\n```

Created a JIRA issue https://issues.apache.org/jira/browse/INFRA-26262 

> Ah.... We will need to get approval from INFRA\r\n> \r\n> ```\r\n> sorenlouv/backport-github-action@ad888e978060bc1b2798690dd9d03c4036560947 is not allowed to be used in \r\n> apache/airflow. Actions in this workflow must be: within a repository that belongs to your Enterprise account, created by \r\n> GitHub, verified in the GitHub Marketplace, or matching the following:\r\n>  */*@[a-f0-9][a-f0-9][a-f0-9][a-f0-9][a-f0-9][a-f0-9][a-f0-9]+, AdoptOpenJDK/install-jdk@*, \r\n> TobKed/label-when-approved-action@*, actions-cool/issues-helper@*, actions-rs/*, al-cheb/configure-pagefile-action@*, amannn/action-semantic-pull-request@*, apache/*, burrunan/gradle-cache-action@*, \r\n> bytedeco/javacpp-presets/.github/actions/*, chromaui/action@*, codecov/codecov-action@*, \r\n> container-tools/kind-action@*, container-tools/microshift-action@*, \r\n> dawidd6/action-download-artifact@*, delaguardo/setup-graalvm@*, \r\n> docker://jekyll/jekyll:*, docker://pandoc/core:2.9, eps1lon/actions-label-merge-conflict@*, \r\n> gaurav-nelson/github-action-markdown-link-check@*, golangci/*, gr2m/twitter-together@*, \r\n> gradle/wra...\r\n> ```\r\n\r\noh i see :) 

> Created a JIRA issue https://issues.apache.org/jira/browse/INFRA-26262\r\n\r\nnice thank you :)

Hmm. it seems that we might have some hard time (and I kind of agree with it eventually) about using pull_request_target for that one - INFRA (Rightfully) is worried about security of those, so we should likely look for alternative - where we could either user `workflow_dispatch` kind of workflow or maybe even use some **slightly** less automated workflow that we write manually.\r\n

I will revert it for now

Reference https://issues.apache.org/jira/browse/INFRA-26262

> Hmm. it seems that we might have some hard time (and I kind of agree with it eventually) about using pull_request_target for that one - INFRA (Rightfully) is worried about security of those, so we should likely look for alternative - where we could either user `workflow_dispatch` kind of workflow or maybe even use some **slightly** less automated workflow that we write manually.\r\n\r\nYes agree , we can look some alternatives, I have posted another alternative on slack thread which gives ability to use workflow_dispatch, and I feel we can automate this some way. But not entirely sure thats the right path :) please suggest

> > Hmm. it seems that we might have some hard time (and I kind of agree with it eventually) about using pull_request_target for that one - INFRA (Rightfully) is worried about security of those, so we should likely look for alternative - where we could either user `workflow_dispatch` kind of workflow or maybe even use some **slightly** less automated workflow that we write manually.\r\n> \r\n> Yes agree , we can look some alternatives, I have posted another alternative on slack thread which gives ability to use workflow_dispatch, and I feel we can automate this some way. But not entirely sure thats the right path :) please suggest\r\n\r\nhttps://apache-airflow.slack.com/archives/C0808SJPNGM/p1731388388011519

cool :)

Ah .. I wanted to use it to test cherry-picker automation - but I can do it manually actually with cherry-picker cli- now that they released a version that has the new feature that @gopidesupavan also advocated for it in the cherry-picker PR

fyi @pierrejeambrun 

Thanks @kaxil!

Thanks @kaxil 

Note: Kindly review after [commit](https://github.com/apache/airflow/pull/43881/commits/8b6b09ea58b384f96a6cdd07dd6a1ad43fd41856) had to build from  [PR](https://github.com/apache/airflow/pull/43783) as its still not merged. 

Can you rebase the branch and resolve conflicts. The base branch has been merged as well, it should simplify the review, thanks.

> Can you rebase the branch and resolve conflicts. The base branch has been merged as well, it should simplify the review, thanks.\r\n\r\n@pierrejeambrun resolved conflicts

Closing in reference to https://github.com/apache/airflow/pull/43874

Only the last 3 commits are relevant here

The base branch has been merged, branch needs rebasing.\r\n\r\nAlso you can add the `legacy api` tag to allow the CI to build.

OK i got a green CI. Is this one good to merge @pierrejeambrun?

Thanks üéâ 

cc: @enisnazif

A new laptop always helps find bugs in the docs :)

> A new laptop always helps find bugs in the docs :)\r\n\r\nNew laptops and new hires/interns.  Their best first contribution is always verifying and updating docs :+1: 

Hey @jason810496,\r\n\r\nThat‚Äôs a good question, and I‚Äôm glad you raised it! @rawwar and I had a similar discussion, and it seems we all have similar concerns.\r\n\r\nAt first, I thought keeping `"List Jobs"` and `"List Jobs with Filters"` as separate endpoints made sense for clarity. But after thinking it through more and taking a closer look, I believe combining them into a single "List Jobs" endpoint with optional query parameters for filters might be a better approach. This way, the endpoint would return all jobs by default, with filtering options (as a `QueryParameter`) as needed. It would make it both straightforward and easy to maintain.\r\n\r\nThanks for raising this! I am happy to keep the discussion going if others have thoughts on this approach. Otherwise, let‚Äôs go ahead and merge it into one endpoint.\r\n\r\ncc: @josix @rawwar

>I‚Äôve placed the endpoint in airflow/api_fastapi/core_api/routes/public/job.py, but I‚Äôm not certain if this is the most appropriate location‚Äîperhaps airflow/api_fastapi/core_api/routes/cli/job.py would also fine ?\r\nThe query parameters, response data models, and implementation are consistent with the rest of the public API, utilizing common.parameters and paginated_select.\r\n\r\n@jason810496 I missed including this part in my previous answer. I responded from my mind but forgot to put it into text. :) \r\n\r\nI think it makes more sense to keep the endpoint under `Core API`. Since most `CLI` calls will still rely on `Core API`, creating a separate category for just a few endpoints could add unnecessary complexity. By sticking to `Core API`, we stay consistent with the rest of the API and follow the same rules across the board. This way, we avoid extra maintenance overhead and keep things simple as we scale.

I think rebasing went wrong on the genrated file part.\r\n\r\nWhen you have any conflicts on those (openapi spec, or front end generated code). You can just re-run manually the hooks to refresh the files:\r\n```\r\npre-commit run generate-openapi-spec --all-files\r\npre-commit run ts-compile-format-lint-ui --all-files\r\n```\r\n\r\n

Just fix the trailing endpoint naming.\r\nI will work on follow up PR to refactor the test case with manual replace to `datetime_zulu_format` utility function üëç 

Well I just tried the `conflict` resolution via the github interface. (because it was a really small conflict), quite handy for quick  and simple conflicts.

This is due to the recent pydantic release that breaks some stuff. Main lower bound has been added and should be better, you most likely not have the appropriate pydantic version locally. Now it should be `>= 2.10.1`(#44284)

Most likely the CI image used to run the pre-commit hook (`in_container` script) still has an old pydantic version.\r\n\r\nYou can run:\r\n```shell\r\nbreeze ci-image build --python 3.9\r\n```\r\n\r\nTo refresh the image. And you should get the appropriate generated spec locally then.

Upgrade checks failing in canary for hatchling version, I will get that into separate pr

As discussed at the hackathon on Saturday @enisnazif - just run pre-commit, and it will fix all the static checks - then committing it, and should be greeen

Cool!. And one more thing. This is about `breeze\

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

This one needs a unit test

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

> I am not an K8s expert, can init containers run in parallel? Or are they executed serial?\r\nHow about if init containers run for a longer time? Would it be interesting to stream logs while they are running to see progress? And if they run in parallel would it mix logs?\r\n\r\nEach init container runs sequentially, with each one waiting for the previous container to complete. The main containers wait until all init containers are ready before starting.\r\n\r\nRef: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#understanding-init-containers\r\n\r\n> Init containers are exactly like regular containers, except:\r\n> * Init containers always run to completion.\r\n> * Each init container must complete successfully before the next one starts.\r\n

Hi @jscheffl, @hussein-awala, @dstandish, @potiuk, @jedcunningham\r\n\r\nDoes this PR need any additional changes? Are there any blockers we should address? Let me know how I can help to move it forward!

Can you rebase in the meantime @mrk-andreev ?

@potiuk a few seconds tooo fast... I was reminded that the location of providers system tests were also wrong... updated as well...

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

It seems the default filters in cluster activity add 1 hour to current time as `end_date`. I am not sure if the same needs to be done in new UI since it presents preset hours like last 1 hour, 8 hours etc. for now where `end_date` has to be current time in the UI. \r\n\r\nhttps://github.com/apache/airflow/blob/6d85a0466d91d501af87c8904b902ea92cee466d/airflow/www/static/js/cluster-activity/useFilters.tsx#L52-L59

We should take a step back and clarify the intended use case for the `dashboard` (old `historical_metrics_data`). The current lifted and shifted version was designed to show historical data, so it makes sense not to include running jobs in this context. As we transition to the `new UI` and the `dashboard` term for this endpoint, we should ensure the endpoint aligns with the updated requirements.\r\n\r\nIt would be beneficial to get @bbovenzi and @pierrejeambrun‚Äôs input to gain the overall vision of the dashboard‚Äôs requirements.

IMO not showing running dagruns reduces the value of the new dashboard. The running dagruns are present in current cluster activity page too. When I raised the PR I thought it was a bug. It comes to a decision over adding 1hr in UI like cluster activity or changing the API as per the PR to imply dagruns without end_date as running.

I can help with the changes and testing if you‚Äôd like, @tirkarthi. This issue originated from my change, but if you already have a UI set up for testing, it would be much faster to verify the changes through the UI, as it would show exactly what‚Äôs needed. Let me know how you‚Äôd like to proceed!

Closing in favour of https://github.com/apache/airflow/pull/44043 . Thanks everyone for the details.

@potiuk I propose to back-port to 2.10, will make a PR...

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Hi @pierrejeambrun, here is the update:\r\n\r\n### Common `Accept` Header for JSON and Text\r\nI have added `HeaderAcceptJsonOrText` in `common/headers.py`, annotated with `Mimetype` from `common/types.py`. \r\n\r\nI tested it with `application/json; charset=utf-8`, and for headers including `utf-8`, we need to use `startswith` to match the header correctly. Additionally, I included an OpenAPI schema for the `Accept` header.\r\n

Just resolve the conflict  üöÄ

Hello @jason810496 do you mind fixing the formatting errors, it would be great to merge this one :)

<img width="510" alt="Screenshot 2024-11-20 at 11 32 21\u202fAM" src="https://github.com/user-attachments/assets/a060e9a2-553f-4fd5-8820-3a84eeff7c4f">\r\n\r\n\r\nI was just testing this and noticed that we\

Thanks for fixing this! And sorry I was never able to prioritize raising a PR, my local patch of Airflow uses the same method to solve this.\r\n

> Also there is the deprecated `execution_date` difference, maybe the TaskInstanceHistory is missing the `logical_date`/`execution_date` ?\r\n\r\nAdditionally, I noticed that the following columns are missing in `TaskInstanceHistory` when compared with `TaskInstance`:\r\n\r\n- `sla_miss`\r\n- `rendered_map_index`\r\n- `rendered_fields`\r\n- `trigger`\r\n- `triggerer_job`\r\n- `note`

We should be able to use dependabot for these things, change is already in-progress here https://github.com/dependabot/dependabot-core/pull/10899. :) 

@Lee-W @uranusjr When working on it I realized that assets are added in the DB from DAG definition but never removed (or at least I did not see the code). Meaning, as a DAG author if I define an asset in my DAG and then later on remove it, the asset is never removed from the DB. Am I wrong? If not, is it intended?

> @Lee-W @uranusjr When working on it I realized that assets are added in the DB from DAG definition but never removed (or at least I did not see the code). Meaning, as a DAG author if I define an asset in my DAG and then later on remove it, the asset is never removed from the DB. Am I wrong? If not, is it intended?\r\n\r\nYep, this is by design as of now. To keep the asset history.

> > @Lee-W @uranusjr When working on it I realized that assets are added in the DB from DAG definition but never removed (or at least I did not see the code). Meaning, as a DAG author if I define an asset in my DAG and then later on remove it, the asset is never removed from the DB. Am I wrong? If not, is it intended?\r\n> \r\n> Yep, this is by design as of now. To keep the asset history.\r\n\r\nAlright, thank you. I handled it then. I removed the references from asset and triggers if the asset is no longer used

@Lee-W Any chance you can review it? You have some experience around assets that could be interesting to have :)

> @Lee-W Any chance you can review it? You have some experience around assets that could be interesting to have :)\r\n\r\nSure thing :) Will take a look later today 

Is there a plan to add tests separate ? 

@pierrejeambrun the PR has been rebased now. Only has the relevant changes

Thanks for the review @pierrejeambrun! Handled the review comment for tests. Merging it

Exactly the errors I expected.. I guess after we have #43556  (finally) merged and rebase this one, it should fix itself :)

> Exactly the errors I expected.. I guess after we have #43556 (finally) merged and rebase this one, it should fix itself :)\r\n\r\nYes agree :) 

> Nice catch!\r\n\r\nThanks, the "catch" was when I shadowed this code into my org\

cc: @tirkarthi 

cc: @tirkarthi 

all templated fields can already be a callable since airflow 2.10.0

> all templated fields can already be a callable since airflow 2.10.0\r\n\r\nOw damn well that makes it easy then we may close this one.

Maybe will reopen it and keep the tests but without additional code

> all templated fields can already be a callable since airflow 2.10.0\r\n\r\n~~Indeed templated fields are callable, but here I also support callable values of a dict key/value pair which is passed to a templated field, which is not supported by default in templated fields.  So this PR still makes sense.~~

@raphaelauv just consider this PR as an improvement of the integration tests for the MSGraph operator and sensor in which I now also test the usage of lambdas supported since Airflow 2.10.0

Nice!

I see three endpoints still using `async`. Do we want to change those too?\r\n\r\n<img width="271" alt="Screenshot 2024-11-07 at 1 18 14\u202fPM" src="https://github.com/user-attachments/assets/980c1e3e-610d-4f55-bae9-88cfe6b34b24">\r\n

Good catch,, I forgot the private API. \r\n\r\nUpdated thanks Brent.

I agree with @dolfinus, running in a separate thread manually or leveraging FastAPI to do so is more or less the same. (just less work and more code maintainability to let FastAPI handle that).\r\n\r\nLong term we will rewrite that with full async support, in the meantime FastAPI is just `sync` for us.\r\n

Hey @dolfinus, yes, the throughput in this case would be very similar for both snippets, because when calling `asyncio.to_thread()` (executor unspecified), the default thread pool executor will be used and be subject to same 40 thread limit. But unlike sync functions where entire request is processed in a thread, in async we\

>Long term we will rewrite that with full async support, in the meantime FastAPI is just sync for us.\r\n\r\nThat would be lovely, thank you! :)

Yes sure, sounds good! Thanks @pierrejeambrun üëçüèΩ

Nice!

This is indeed way better!

LGTM

Nice set of changes BTW :)

With the latest commits, most of the things should be taken care of.\r\n@pierrejeambrun and @ephraimbuddy addressed your comments

Thanks

nice

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Pending Unit tests and fixing filtering by uri_pattern and dag_ids

Closing in favour of another PR that I will upload soon

## TODO\r\nExtend the following test case to include more asset attributes\r\n\r\n* [x] tests/api_fastapi/core_api/routes/ui/test_assets.py\r\n* [x] tests/dags/test_only_empty_tasks.py\r\n* [x] tests/dags/test_assets.py\r\n* [x] tests/api_connexion/endpoints/test_dag_run_endpoint.py\r\n* [x] tests/api_connexion/schemas/test_dag_schema.py\r\n* [x] tests/api_connexion/schemas/test_asset_schema.py\r\n* [x] tests/timetables/test_assets_timetable.py\r\n* [x] tests/decorators/test_python.py\r\n* [x] tests/serialization/test_serialized_objects.py\r\n* [x] tests/serialization/test_dag_serialization.py\r\n* [x] tests/serialization/test_serde.py\r\n* ~~[ ] tests/io/test_wrapper.py~~ looks like it makes more sense to keep it as it it\r\n* [x] tests/io/test_path.py\r\n* ~d[ ] tests/utils/test_context.py~~ defer to https://github.com/apache/airflow/pull/43959\r\n* [x] tests/utils/test_json.py\r\n* [x] tests/models/test_dag.py\r\n* ~~[ ] tests/models/test_taskinstance.py~~ defer to https://github.com/apache/airflow/pull/43959\r\n* [x] tests/models/test_serialized_dag.py\r\n* [x] tests/www/views/test_views_asset.py\r\n* [x] tests/www/views/test_views_grid.py\r\n* [x] tests/lineage/test_hook.py\r\n* [x] tests/jobs/test_scheduler_job.py\r\n* [x] tests/assets/test_asset.py\r\n* [x] tests/assets/test_manager.py\r\n* [x] tests/listeners/test_asset_listener.py

Makes sense to me. Will take another look when this is ready for review.

Closing & Reopening for `legacy-api` tests

@Lee-W Should I create an issue to do the same migration in fastAPI endpoints as well?

> @Lee-W Should I create an issue to do the same migration in fastAPI endpoints as well?\r\n\r\nI think I already fixed the part in Fast API. Or are there additional steps needed? 

Looks like the CodeQL part goes through now

> Looks like the CodeQL part goes through now\r\n\r\nYep! Thanks so much for helping out!

> I think we need to update fastapi `datamodels` to be able to serialize extra `group` and `name` in responses.\r\n\r\nI just rebased, and TP seems to have fixed it already. Add one missing group column to the asset alias.\r\n\r\nother parts of asset API improvement be tracked in https://github.com/apache/airflow/issues/44412

> This makes sense. I wonder if we can manage the DagDependency values better‚Ä¶ but maybe it‚Äôs not in scope for this PR.\r\n\r\nprobably need to discuss with brent maybe ü§î 

I like this suggestion. I hadn‚Äôt thought of people verifying the release, but I see the value. Thanks for pointing this out. I will implement this suggestion. Probably Wednesday or Thursday PST. \r\n

Thanks for the help, @potiuk 

> Tested this locally and it does allow `breeze ci-image build` to complete successfully. üëç\r\n\r\nStill - it fails when trying to generate constraints using `pypi` packages.... because it cannot build `apache-beam \r\n\r\n```\r\n    √ó Failed to download and build `apache-beam==2.0.0`\r\n```\r\n\r\nWhich is even stranger!. I will have to workaround it a bit more - and add a beam limitation as well it seems. I think that one will be a head-scratcher for the `uv team` and @notatallshaw :)

From: https://github.com/apache/airflow/actions/runs/11713890095/job/32627785553?pr=43768#step:12:545\r\n\r\n```\r\nRunning command: uv pip install --python /usr/local/bin/python3 \

We have `apache-beam==2.60.0` - but uv  tries to download `2.0.0` ????

> Still - it fails when trying to generate constraints using pypi packages.... because it cannot build `apache-beam\r\n\r\nWhat was the exact command that ran into that?     I ran `breeze ci-image build --python 3.9 --upgrade-to-newer-dependencies` to test it; that failed before the change and worked with this change.

Closing that one. The discussion with @notatallshaw and @charliermarsh on https://github.com/astral-sh/uv/issues/8871 - made me realise this is not an issue with `uv` and resolution but. ... cache invalidation issue.

Closing in favour of #43770

Rebased and solved the conflicts.

Extracted from #43556 

Failing tests should be fixed after https://github.com/apache/airflow/pull/43771 is merged.

Some things are failing but dunno why

It seems like installing node to build packages is very unstable recently (connection reset by peer etc.) - we will get it cached soon WIP is here: https://github.com/apache/airflow/pull/43329 cc: @bugraoz93 that should improve stability.

For now rebasing, commit--amend or close/reopening the PR should retry it.

Nice!

> Nice!\r\n\r\nThx @potiuk 

Docs portion can also be removed.\r\n\r\nhttps://github.com/apache/airflow/blob/b89f43ebe1ae020c9808c90b84d0737ab8ac7f5b/docs/apache-airflow/administration-and-deployment/scheduler.rst?plain=1#L386-L389

> Docs portion can also be removed.\n> \n> https://github.com/apache/airflow/blob/b89f43ebe1ae020c9808c90b84d0737ab8ac7f5b/docs/apache-airflow/administration-and-deployment/scheduler.rst?plain=1#L386-L389\n\nThanks - I was grepping based on config option, guess I missed some

In multiple context of high number of concurrent task ( more than 1000 and around 50 new task by sec ) we had problems and by disabling the mini scheduler things are running way more smoothly,\r\n\r\nmaybe we could deprecate the option in 2.10.4 and set it to false in 2.11.0 , wdyt ?

> In multiple context of high number of concurrent task ( more than 1000 and around 50 new task by sec ) we had problems and by disabling the mini scheduler things are running way more smoothly,\r\n> \r\n> maybe we could deprecate the option in 2.10.4 and set it to false in 2.11.0 , wdyt ?\r\n\r\nYeah good point, fancy a PR to 2-10-test?

> > In multiple context of high number of concurrent task ( more than 1000 and around 50 new task by sec ) we had problems and by disabling the mini scheduler things are running way more smoothly,\r\n> > maybe we could deprecate the option in 2.10.4 and set it to false in 2.11.0 , wdyt ?\r\n> \r\n> Yeah good point, fancy a PR to 2-10-test?\r\n\r\nI think deprecating the option as warning would be good. I also see it needs a re-work as with AIP-72 the function can not be "distributed on worker" as in the past.\r\n\r\nNevertheless when scheduling MANY tasks this was really a performance boost comparing Airflow v1 to Airflow v2 when this was added. Waiting for scheduler loop to schedule the next is really slowing down DAGs... so maybe we need a (future) similar mechanism to schedule next tasks immediately after one has finished to have the same low-latency like in the past... on the backend of course... (like a notification queue where it makes most-sense to schedule next because one task just finished...)

Does it mean that previous versions of openlineage provider will fail with 1.24? Is there any mechanism that the users of past openlineage provider will be warned or guided to upgrade to the new provider when their provider will start to fail after upgrading the`openlineage-python` library to 1.24.2 ?

> In 1.24.2 we add possibility to pass config as argument (not only transport as it was in case of from_dict method that we deprecate in this version).\r\n\r\nUnderstood! perfect then! Should you also yank 1.24.0 and 1.24.1 ?

Should not that capacity be a task parameter rather than executor config parameter on DAG level. We have similar concept with `pool_slots` and there they are "per task" - and part of the BaseOperator. It seems to be way more flexible to specify it this way (additionally then this could be renamed as "task_slots"  - to be similar to "pool_slots") or maybe even we should combine the two. This way it will also be potentially usable by other executors.

> Should not that capacity be a task parameter rather than executor config parameter on DAG level. We have similar concept with `pool_slots` and there they are "per task" - and part of the BaseOperator. It seems to be way more flexible to specify it this way (additionally then this could be renamed as "task_slots" - to be similar to "pool_slots") or maybe even we should combine the two. This way it will also be potentially usable by other executors.\r\n\r\nI was thinking in the same way, but during coding I saw that the need_capacity parameter is no easy to get into the executor. We have to tough core code like TaskInstanceKey class to get the info into the Executor. I just used the idea of the KubernetesExecutor to add additional data into the executor and that is the reason why I started using the executor_config parameter. My main idea is to tough only Edge package t and then make a later PR which can add this changes into the core because the Edge package is the only which will support this feature for the moment and it is not released yet.\r\nSo what is your opinion about that? \r\nShall we change this also in this PR or in a separate PR? \r\nDuring writing this lines I have also the feeling to use still the term concurrency instead of capacity. Then it is easier to adapt this to already existing Executor code in the future.

> > Should not that capacity be a task parameter rather than executor config parameter on DAG level. We have similar concept with `pool_slots` and there they are "per task" - and part of the BaseOperator. It seems to be way more flexible to specify it this way (additionally then this could be renamed as "task_slots" - to be similar to "pool_slots") or maybe even we should combine the two. This way it will also be potentially usable by other executors.\r\n> \r\n> I was thinking in the same way, but during coding I saw that the need_capacity parameter is no easy to get into the executor. We have to tough core code like TaskInstanceKey class to get the info into the Executor. I just used the idea of the KubernetesExecutor to add additional data into the executor and that is the reason why I started using the executor_config parameter. My main idea is to tough only Edge package t and then make a later PR which can add this changes into the core because the Edge package is the only which will support this feature for the moment and it is not released yet. So what is your opinion about that? Shall we change this also in this PR or in a separate PR? During writing this lines I have also the feeling to use still the term concurrency instead of capacity. Then it is easier to adapt this to already existing Executor code in the future.\r\n\r\n@potiuk I had also a longer talk to @AutomationDev85 today about this. Reading the docs from https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/kubernetes_executor.html#pod-override this field is used (and is only used there today) to carry a dict with a potential included of `pod_override` element that can define extra details of the POD to run for the task execution. That can be used to add volume mounts, request resources or add sidecars... whatsoever.\r\n\r\nWith this interface generically more parameters can be carried. An additional field in the dict would not harm. I am thinking that maybe instead of "needs capacity" we should name it `pool_slots` according to the task instance parameter. With this PR here, you would need to define this as extra field on the task instance... but with a small additional PR we could bring the `pool_slots` from the task instance per default in there for future leverage... but then this intrinsic is a bit confusing though.\r\n\r\nAs @AutomationDev85 said we could also bring the `pool_slots` field directly from the task instance into the executor, but today the interface in the scheduler in airflow/executors/base_executor.py:execute_async() only carries TaskInstanceKey, Command, queue and the executor_config - adding the full TaskInstance or the pool_slots here would be a breaking change in the interface or the executor would need to query the DB additionally to get the pool_slots (which the scheduler obviously already has because it allocated the pool slots before scheduling... the calling method `_process_tasks()` has the taskinstance object).\r\n\r\n@potiuk Do you think we need/should to make a breaking change in the scheduler/executor interface or add an intrinsic?\r\n@ashb as being the Scheduler expert, would you have an opinion on this?

> As @AutomationDev85 said we could also bring the pool_slots field directly from the task instance into the executor, but today the interface in the scheduler in airflow/executors/base_executor.py:execute_async() only carries TaskInstanceKey, Command, queue and the executor_config - adding the full TaskInstance or the pool_slots here would be a breaking change in the interface or the executor would need to query the DB additionally to get the pool_slots (which the scheduler obviously already has because it allocated the pool slots before scheduling... the calling method _process_tasks() has the taskinstance object).\r\n\r\nAnd comment on that - again, I am not really trying to block/veto it, but I don\

> Maybe we want this, maybe not - maybe it\

> I want to hear other\

Should we turn it into a devlist discussion? It seems that is a decision that shoudd be made about **now** - i.e. what we really want to do with executor and "task properties".\r\n\r\nI think we have two options:\r\n\r\n1) we make any metadata we want to attach to task "executor specific" - like kubernetes executor so far\r\n2) we try to make some common (well defined) "properties" of the task, exposed to executors so that they can adapt on how they are running tasks. Task "weight" seem to be a good candidate for that that we can also use elsewhere (i.e. pool slots).\r\n\r\nAnd indeed, we are at the right time to introduce breaking chnges (or variations) in the executor interface to acommodate to 2) if we choose to go this direction.\r\n

> Should we turn it into a devlist discussion? It seems that is a decision that shoudd be made about **now** - i.e. what we really want to do with executor and "task properties".\r\n> \r\nYeah, would support a devlist discussion... @AutomationDev85 will you take this?\r\n\r\n@potiuk would it be only a discussion, does this then need a formal vote or lazy consensus? I believe we did not vote on breaking changes in the past... discussion would be mainly to attract opinions and feedback on PR?\r\n\r\nI\

@jscheffl Sorry I had to rework again because if oversaw that the function _process_tasks in BaseExecutor deletes the key out of the queued_tasks. See  [_del self.queued_tasks[key]](https://github.com/apache/airflow/blob/main/airflow/executors/base_executor.py#L407). So I added an overload on the _process_tasks  function to store the queued_tasks in an own variable. What is your opinion about this solution? Not happy about this but looking forward to remove this with Airflow 3.

Yeah.. Looks like fairly `reliably flaky` (if we can use such oxymoron :D ).

If we merge this, could you add a PR description of why it was the cause of the side effect?

Closing in favour of #43731 [43731](https://github.com/apache/airflow/pull/43731) that seem to fix the cause of the problem.

Nice :)

Wooohooo!

> Wooohooo!\r\n\r\nThx @potiuk jarek

Please add @ferruzzi and @vincbeck for review

Thanks!

PR rebased and synced with `main` just now ‚úÖ

@potiuk could you please review this when you have time? It failed at first two days due to other problems affecting the PRs. It is now all green :) 

@pierrejeambrun PR rebased and synced with `main` and all comments resolved, please check it out. Thank you!

> Overall looking good. Need rebasing and conflicts resolution. Thanks\r\n\r\nDone! Rebased and conflicts resolved ‚úÖ 

> Needs rebasing again. Ready to merge\r\n\r\n@pierrejeambrun rebasing done, thank you ‚úÖ 

We we decided to combine datetime operators and sensors under operators documentation?

> We we decided to combine datetime operators and sensors under operators documentation?\r\n\r\nSorry I‚Äôm not aware that, Do you think it‚Äôs worth combining everything into a single operator file? My main concern is that the file could become excessively large with all the content, and large file can be hard to read if any comes and see in my opinion.\r\n\r\nBut i am happy to merge, please suggest :) \r\n\r\nI see one good thing sphinax is very smart :) its keeping all in one place when it generated even the content in different files under operator section, sample screenshot for the docs build.\r\n\r\n<img width="1091" alt="image" src="https://github.com/user-attachments/assets/6319b189-ad4d-4551-b459-52185d6edada">\r\n

> > We we decided to combine datetime operators and sensors under operators documentation?\r\n> \r\n> Sorry I‚Äôm not aware that, Do you think it‚Äôs worth combining everything into a single operator file? My main concern is that the file could become excessively large with all the content, and large file can be hard to read if any comes and see in my opinion.\r\n> \r\n> But i am happy to merge, please suggest :)\r\n> \r\n> I see one good thing sphinax is very smart :) its keeping all in one place when it generated even the content in different files under operator section, sample screenshot for the docs build.\r\n> \r\n> <img alt="image" width="1091" src="https://private-user-images.githubusercontent.com/31437079/384268236-6319b189-ad4d-4551-b459-52185d6edada.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzExNTM4MjMsIm5iZiI6MTczMTE1MzUyMywicGF0aCI6Ii8zMTQzNzA3OS8zODQyNjgyMzYtNjMxOWIxODktYWQ0ZC00NTUxLWI0NTktNTIxODVkNmVkYWRhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTA5VDExNTg0M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ4MjdjMDkwYmEzODQ1MjdkMzRkMTFhZDFhMzg2OGUwMWFkMzkzNzk2OGM2MzAzMzlhYWMyM2IxMTIyMjRkODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Hdo0sNpjUzOweramHkYYFtsJVJIWpV_H75PJskdn-Z4">\r\n\r\nWhat I ment is why we combined the sensors and the operators under the operators page? (and not keep a page for the sensors)

> > > We we decided to combine datetime operators and sensors under operators documentation?\r\n> > \r\n> > \r\n> > Sorry I‚Äôm not aware that, Do you think it‚Äôs worth combining everything into a single operator file? My main concern is that the file could become excessively large with all the content, and large file can be hard to read if any comes and see in my opinion.\r\n> > But i am happy to merge, please suggest :)\r\n> > I see one good thing sphinax is very smart :) its keeping all in one place when it generated even the content in different files under operator section, sample screenshot for the docs build.\r\n> > <img alt="image" width="1091" src="https://private-user-images.githubusercontent.com/31437079/384268236-6319b189-ad4d-4551-b459-52185d6edada.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzExNTM4MjMsIm5iZiI6MTczMTE1MzUyMywicGF0aCI6Ii8zMTQzNzA3OS8zODQyNjgyMzYtNjMxOWIxODktYWQ0ZC00NTUxLWI0NTktNTIxODVkNmVkYWRhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTA5VDExNTg0M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ4MjdjMDkwYmEzODQ1MjdkMzRkMTFhZDFhMzg2OGUwMWFkMzkzNzk2OGM2MzAzMzlhYWMyM2IxMTIyMjRkODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Hdo0sNpjUzOweramHkYYFtsJVJIWpV_H75PJskdn-Z4">\r\n> \r\n> What I ment is why we combined the sensors and the operators under the operators page? (and not keep a page for the sensors)\r\n\r\nOh okay sorry i miss understood the comment :). Yeah can do separate sections for sensors and operators? is it okay please confirm.

> > > > We we decided to combine datetime operators and sensors under operators documentation?\r\n> > > \r\n> > > \r\n> > > Sorry I‚Äôm not aware that, Do you think it‚Äôs worth combining everything into a single operator file? My main concern is that the file could become excessively large with all the content, and large file can be hard to read if any comes and see in my opinion.\r\n> > > But i am happy to merge, please suggest :)\r\n> > > I see one good thing sphinax is very smart :) its keeping all in one place when it generated even the content in different files under operator section, sample screenshot for the docs build.\r\n> > > <img alt="image" width="1091" src="https://private-user-images.githubusercontent.com/31437079/384268236-6319b189-ad4d-4551-b459-52185d6edada.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzExNTM4MjMsIm5iZiI6MTczMTE1MzUyMywicGF0aCI6Ii8zMTQzNzA3OS8zODQyNjgyMzYtNjMxOWIxODktYWQ0ZC00NTUxLWI0NTktNTIxODVkNmVkYWRhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTA5VDExNTg0M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ4MjdjMDkwYmEzODQ1MjdkMzRkMTFhZDFhMzg2OGUwMWFkMzkzNzk2OGM2MzAzMzlhYWMyM2IxMTIyMjRkODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Hdo0sNpjUzOweramHkYYFtsJVJIWpV_H75PJskdn-Z4">\r\n> > \r\n> > \r\n> > What I ment is why we combined the sensors and the operators under the operators page? (and not keep a page for the sensors)\r\n> \r\n> Oh okay sorry i miss understood the comment :). Yeah can do separate sections for sensors and operators? is it okay please confirm.\r\n\r\nYes I think it\

> > > > > We we decided to combine datetime operators and sensors under operators documentation?\r\n> > > > \r\n> > > > \r\n> > > > Sorry I‚Äôm not aware that, Do you think it‚Äôs worth combining everything into a single operator file? My main concern is that the file could become excessively large with all the content, and large file can be hard to read if any comes and see in my opinion.\r\n> > > > But i am happy to merge, please suggest :)\r\n> > > > I see one good thing sphinax is very smart :) its keeping all in one place when it generated even the content in different files under operator section, sample screenshot for the docs build.\r\n> > > > <img alt="image" width="1091" src="https://private-user-images.githubusercontent.com/31437079/384268236-6319b189-ad4d-4551-b459-52185d6edada.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzExNTM4MjMsIm5iZiI6MTczMTE1MzUyMywicGF0aCI6Ii8zMTQzNzA3OS8zODQyNjgyMzYtNjMxOWIxODktYWQ0ZC00NTUxLWI0NTktNTIxODVkNmVkYWRhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTA5VDExNTg0M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ4MjdjMDkwYmEzODQ1MjdkMzRkMTFhZDFhMzg2OGUwMWFkMzkzNzk2OGM2MzAzMzlhYWMyM2IxMTIyMjRkODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Hdo0sNpjUzOweramHkYYFtsJVJIWpV_H75PJskdn-Z4">\r\n> > > \r\n> > > \r\n> > > What I ment is why we combined the sensors and the operators under the operators page? (and not keep a page for the sensors)\r\n> > \r\n> > \r\n> > Oh okay sorry i miss understood the comment :). Yeah can do separate sections for sensors and operators? is it okay please confirm.\r\n> \r\n> Yes I think it\

Backporting #43714

Backport #43694

Hmm wait‚Ä¶ is the logic in `existing_warned_dag_ids` also backwards? Or at least it‚Äôs named wrong.

Does this require compat provider along with the current providers?

suppressed by https://github.com/apache/airflow/pull/44011

static checks are failing

Backport #43683

This breaks backwards compatibility, as described in https://github.com/apache/airflow/issues/43638#issuecomment-2457006902

Woohooo nice :) 

Yeah. Build reproducibiliy is cool :) and surprisingly difficult.

Moved back to draft until pytests are fixed

Makes sense to me!

> Still I think we can do it with Airflow 3 and backport to 2.10.. this is pretty minor and easy to do.\r\n\r\nIf @brouberol wishes to do the back-compatiblity code for both issues, we can do it, yes.

OK lets take it by steps:\r\n1. changes to `airflow/security/kerberos.py` should be in a separated PR. Add new function `get_kerberos_principal` with the logic and deprecate `get_kerberos_principle` once this PR is merged to main branch, we can start similar PR for v2.10-test branch.\r\n2. once (1) is completed we can handle the provider changes\r\n\r\nI can handle (1) if you prefer

Cool but actually we need (2) ready before (1)\r\n\r\nAll you need to do here is to remove the changes to `airflow/security/kerberos.py` and `tests/security/test_kerberos.py`.\r\n\r\nKeep the changes you have in providers but change the imports to:\r\n\r\n```\r\n#todo: remove try/exception when min airflow version is 3.0\r\ntry:\r\n    from airflow.security.kerberos import get_kerberos_principal\r\nexcept ModuleNotFoundError:\r\n    from airflow.security.kerberos import get_kerberos_principle\r\n```\r\n\r\nOnce you do that provider is compatible with both name and I can do the change to airflow core.\r\n\r\n

Lets add `43679.misc.rst` in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).\r\nJust need to say in the file that we renamed the function due to misspelling. This will be used to generate the release notes.\r\n

Are we going to remove it first and then move the API ? I was under the impression we wanted to make it new "while" moving it to the new fast_api framework? But maybe there is a good reason to do it first here?

> Are we going to remove it first and then move the API ? I was under the impression we wanted to make it new "while" moving it to the new fast_api framework? But maybe there is a good reason to do it first here?\r\n\r\nAs long as there\

For these pull requests, we will probably need the `legacy ui` and `legacy api` labels to run the CI. Please let me know if you need help adding the labels to the following pull requests. üôÇ

This can be closed rename of `execution_date` is done as part of `logical_date` here: [43902](https://github.com/apache/airflow/pull/43902)\r\n\r\nRemoval of execution_date is being done in [42404](https://github.com/apache/airflow/pull/42404)

Need rebasing as well.

Needs rebasing. We can merge after the small adjustments are done.

Nice! On to the next one @kandharvishnu 

I just realised some doc update missing for moved operators in standard provider. Added to checklist here https://github.com/apache/airflow/issues/43641 to update those.

It looks like the code at https://github.com/apache/airflow/blob/main/docs/exts/operators_and_hooks_ref.py#L430  is fetching the configuration section from the provider. In this particular case, a recent update added a configuration section to the standard provider, but it seems there‚Äôs no reference to it in the index file, which I suspect is causing the failure. :)

The "standard" python issue with docs is caused by unrelated issue, we will fix it separately.\r\n\r\nYeah - it looks like a good band-aid for now. I will fix it during https://github.com/apache/airflow/issues/42632 (I already have a draft). I will also add a separate test to test sytem tests in CI as part of it, so that it will not happen in the future

To most of the reviewers - your being requested was an accident, a rebase-push went bad and brought in a bunch of other changes so a bunch of people got tagged.\r\n

@jedcunningham Done now :) - https://github.com/apache/airflow/pull/43667/commits/c6505d7d5260a4b3e304a995d2bdb54662408379

> Hey, it\

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Nice, one thing off my todo list.

Some static check failures, shoud be easy to fix :) See [documentation](https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst)

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Thank you!\r\n\r\nOn Tue, Nov 5, 2024, at 5:07 PM, boring-cyborg[bot] wrote:\r\n> \r\n> \r\n> Awesome work, congrats on your first merged pull request! You are invited to check our Issue Tracker <https://github.com/apache/airflow/issues> for additional contributions.\r\n> \r\n> \r\n> ‚Äî\r\n> Reply to this email directly, view it on GitHub <https://github.com/apache/airflow/pull/43662#issuecomment-2457587706>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AADVHA7TOAYVZUBJKKX5FJLZ7DUMBAVCNFSM6AAAAABRFDVPQ2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDINJXGU4DONZQGY>.\r\n> You are receiving this because you authored the thread.Message ID: ***@***.***>\r\n> \r\n

@brouberol  do you know why the code in your PR has been removed in the latest Airflow image? say 2.10.3

@brouberol -> see the comment in https://github.com/apache/airflow/issues/44943 -> providers are always released from main. You need to see which provider version it has been released in and have that provider. If it was not released in 2.10.3 - look at 2.10.4rc1 that is just being voted - maybe it contains newer provider version with the fix.\r\n\r\nLook at https://airflow.apache.org/docs/apache-airflow-providers/index.html to learn how providers vs. core work.

Understood, thanks!\r\n\r\nOn Sun, Dec 15, 2024, at 7:31 PM, Jarek Potiuk wrote:\r\n> \r\n> \r\n> @brouberol <https://github.com/brouberol> -> see the comment in #44943 <https://github.com/apache/airflow/issues/44943> -> providers are always released from main. You need to see which provider version it has been released in and have that provider. If it was not released in 2.10.3 - look at 2.10.4rc1 that is just being voted - maybe it contains newer provider version with the fix.\r\n> \r\n> Look at https://airflow.apache.org/docs/apache-airflow-providers/index.html to learn how providers vs. core work.\r\n> \r\n> \r\n> ‚Äî\r\n> Reply to this email directly, view it on GitHub <https://github.com/apache/airflow/pull/43662#issuecomment-2543992373>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AADVHA6H7GS4LNQFJVWOESL2FXDG5AVCNFSM6AAAAABRFDVPQ2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNBTHE4TEMZXGM>.\r\n> You are receiving this because you were mentioned.Message ID: ***@***.***>\r\n> 

@dstandish I think this endpoint is deleted in https://github.com/apache/airflow/pull/43496. Deleting them should fix CI test issues but I wanted to double-check with you. I am not sure how we are going to handle the permissions and tests in the FastAPI so maybe deleting them would be a good idea for now.

I think mine is a duplicate of #43649. I am closing this one.

Thanks a lot for a swift review @jscheffl! :pray: 

Thx @eladkal :)

@Lee-W fixed all the typing/style comments.

Also replaced `execution_date` filter by `logical_date` filter, added tests.

Yeah this is somewhat problematic thing difficult to debug :) \r\n\r\nI looked at their documentation, they some suggestion to use `fromjson`  \r\n\r\nhttps://docs.github.com/en/actions/writing-workflows/choosing-what-your-workflow-does/evaluate-expressions-in-workflows-and-actions#fromjson\r\n

I\

Backport of #43607 

ok. looks like we have now full support for `uv` in breeze / venv also for the v2-10-test branch after we merge it.

Backport of #43597 

Backport of #43587 (depends on #43623

Backport of #43205

Backporting some recent changes - to enable switching to uv for v2-10-test/stable in ordert to speed up next 2.10.* preparation and switching between branches (mostly to help @utkarsharma2 and @ephraimbuddy and anyone who cherry-picks bugfixes)

Just random node issue :)

Some static check failures, please [fix them](https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst)

The GlueJobTrigger serialization test before I fixed the bug was failing as expected.\r\n\r\n![image](https://github.com/user-attachments/assets/40d3e2c2-2181-47cb-a404-8b59bd66eb8a)\r\n\r\nNow it is passing after removing the conversion to str.

Static type checking looks fine after rebase.\r\n<img width="774" alt="image" src="https://github.com/user-attachments/assets/e2edaef6-0598-4dad-8b20-a525d182b44c">\r\n

LGTM :)

@jimwbaldwin Nice one, BTW congrats on your first contribution :)

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

still blocked waiting for https://github.com/apache/airflow/pull/43340

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Found it while working on https://github.com/apache/airflow/pull/43556

Definitely this is a great catch @potiuk :) Thank you..

> I\

> > I\

> Option "AUTO", "PIP", "UV" as Enum\r\n> AUTO=Like now, checks if UV is installed, if not uses pip\r\n\r\nYes. It\

LGTM :) 

just saw it on slack. love this one!

Besides some adjustments because of rework of classes in regards of TeskSDK I think this can also be back-ported to Ariflow 2.10 line to prevent errors in roll-over. Therefore tagging as 2.10.4 milestone... but backport would most likely a re-write of this PR.

should we have a newsfragment (just in case) just explaining what happened?

@ashb If you feel this is not correct :), please suggest any alternatives or if there is more optimised way to workout these.

Added tests to check if imports are working or not :)

Should we somehow check if "all" moved imports are handled? Not sure if we can do it easily though.

> Should we somehow check if "all" moved imports are handled? Not sure if we can do it easily though.\r\n\r\nYeah have ran basic python operator but that is on 3.9 versions. now that MethPathFinder not supported in 3.11 and 3.12, so will check if we can do anything :)

> Yeah have ran basic python operator but that is on 3.9 versions. now that MethPathFinder not supported in 3.11 and 3.12, so will check if we can do anything :)\r\n\r\nJust to document: MetaPath solution suffers from bug introduced in Python 3.11 https://github.com/python/cpython/issues/117860 which is affecting unittest mock imports but also apparently goes beyond just unit tests - for example affecting Apprise 1.8 https://github.com/PrefectHQ/prefect/issues/13314 after they implemented refactors internally.\r\n\r\nMaybe we can come back (@ashb ?) to the old ways of doing it - we had implemented deprecation utils in v1-> v2 migration that we can apply again: https://github.com/apache/airflow/blob/v2-10-test/airflow/operators/__init__.py  - they did the job, the only problem was that the old "package" will have remain there - with an `__init__.py` with all the deprecation dicts. Which is "slightly" annoying, but also has the benefit of it being pretty explicit, as opposed to MetaPath thing. You know where you should look for it at least. And it will not suffer from the 3.11+ bug of importing packages that are loaded from a different path - because those are  "real" packages and real modules not "linked" modules. \r\n\r\nAlso - I think such `__init__.py` file can be easily generated from checked out `v2-10-test` branch - we could have a simple script that will import all the "airflow.operators", "airflow.hooks" etc and produce it in fully automated way, this way we will not have to verify it (and since we will never be changing it, we can generate it now and re-generate just before we release Airflow 3, to acount for potential Airflow 2.11 changes. This was following PEP 562 `__getattr__` https://peps.python.org/pep-0562/ and has been working 100% reliably so far.\r\n\r\nWDYT @gopidesupavan @ashb -> I think while the MetaPath was a nice idea, but it\

After discussion in Slack @gopidesupavan -> maybe you could (at least for now - attempt to follow the PEP-562 approach and redo the change and we can revisit it later). Ideally simple script to generate moved class dicts would be cool to have so that we can re-apply it as we move new classes.

> After discussion in Slack @gopidesupavan -> maybe you could (at least for now - attempt to follow the PEP-562 approach and redo the change and we can revisit it later). Ideally simple script to generate moved class dicts would be cool to have so that we can re-apply it as we move new classes.\r\n\r\nYeah agree, better to avoid metapath. let me get the changes with PEP 562  `__getattr__`

BTW. This is a bit of an assumption that PEP 562 is going to be "ok" - it **should** work as this really creates aliases to objects in a different package, and we did not have problems with Airflow 2 deprecations even on airflow 2.11 and 2.12  - so it\

And BTW. when you create a PR - you can assign "all versions" and "full tests needed" labels to your PR @gopidesupavan before you create it - this way it will run on all versions  of Python/Databases and K8S (and will trigger all tests)

These changes also require to support 2.8 and 2.9, will wait for this https://github.com/apache/airflow/pull/43556 and then rebase on top so that will fix if anything fails for 2.8 and 2.9.\r\n\r\nMeanwhile working on local and verifying to support these changes on 2.8 and 2.9 :)

> These changes also require to support 2.8 and 2.9, will wait for this #43556 and then rebase on top so that will fix if anything fails for 2.8 and 2.9.\r\n> \r\n> Meanwhile working on local and verifying to support these changes on 2.8 and 2.9 :)\r\n\r\nYeah ... Some of #43556  take a bit more  than I originally anticipated :D 

> > These changes also require to support 2.8 and 2.9, will wait for this #43556 and then rebase on top so that will fix if anything fails for 2.8 and 2.9.\r\n> > Meanwhile working on local and verifying to support these changes on 2.8 and 2.9 :)\r\n> \r\n> Yeah ... Some of #43556 take a bit more than I originally anticipated :D\r\n\r\nYeah fine no rush ‚ò∫Ô∏è...

This is green now, one test failed not related to this. would anyone have another look on the latest modifications, mostly those are test fixes to support compatibility :) 

I also tested a few scenarios where we were switching between "breeze static checks" and "git commit" pre-commit as I noticed that they could be using different pre-commit installation, and they could make no use of the "pre-commit-uv" - I fixed it in this PR, and now also users will get warning if their pre-commit is installed without `uv` and they also will get instructions how to install it.

Relates to #41524 and #41797

We are waiting for this feature. Hope it can be released soon.

@romsharon98 \r\nCould you please explain: How I can pass multiple executors to custom values.yaml file?\r\nThese don\

> @romsharon98 \n> \n> Could you please explain: How I can pass multiple executors to custom values.yaml file?\n> \n> These don\

@romsharon98 I try to deploy Airflow 2.10.3 to Azure k8s via helm chart and I want to use multiple executors (Celery and Kubernetes) in my DAGs. As I understand in my values.yaml in section _executor:_ I need to provide list of executors that I want. I cannot understand hot to do it

You can use it like you wrote, define it like this:\r\n```executor: "CeleryExecutor,KubernetesExecutor"```\r\nWhat the error you get?

@romsharon98 \r\nthe error looks like:\r\n‚Äòonly one the following executors can be used: (here is the list of available executors)‚Äô

What chart version are you trying to deploy?

@romsharon98 \r\n1.15.0

@romsharon98 thanks a lot for your support. \r\nOne question: is it possible to configure Airflow Helm chart 1.15.0 to use multiple executors? \r\nI tried to change the executor via `config:` section in `values.yaml ` like:\r\n```\r\nconfig:\r\n    core:\r\n      executor: KubernetesExecutor, CeleryExecutor\r\n```\r\nbut during deployment I get error:\r\n`helm.go:84: [debug] execution error at (airflow/charts/airflow/templates/NOTES.txt:205:6): Please configure the executor with \

> @romsharon98 thanks a lot for your support. One question: is it possible to configure Airflow Helm chart 1.15.0 to use multiple executors? I tried to change the executor via `config:` section in `values.yaml ` like:\r\n> \r\n> ```\r\n> config:\r\n>     core:\r\n>       executor: KubernetesExecutor, CeleryExecutor\r\n> ```\r\n> \r\n> but during deployment I get error: `helm.go:84: [debug] execution error at (airflow/charts/airflow/templates/NOTES.txt:205:6): Please configure the executor with \

It seems the release of version 1.16.0 has been delayed too long ü•≤

Just to copy a bit more context on this change:\r\n\r\nMy - very basic and long term assumption for our local dev environment is that while ``uv`` makes it (and it will only get better) to maintain, upgrade, sync and setup the environment  very easily and the UX is great and complexity is low, we do not want to EXCLUSIVELY have setup that only works with UV\r\n\r\nI think everything that works with uv should also be achievable (even if in a much more convoluted way) with the standard tooling that is managed by PSF-managed tools (pip , hatch etc.) .\r\n\r\nWhile I love Astral, they are VC-backed company that will expect some returns of investment, and while I believe they are great community member and have no evil goals, they might get acquired, or whatever at any time, and we do not want to have a ‚Äústrong‚Äù dependency on their tooling being available forever.\r\n\r\nThis goes in line with ASF‚Äôs ‚Äúplan for 50 years‚Äù tools dependency approach - use what‚Äôs available and makes you more productive, but don‚Äôt rely too much on anything you cannot exchange easily. Sooner or later EVERY vendor we work with will disappear, and we must be there.\r\n\r\nThat‚Äôs why this doc has two chapters ‚Äúrecommended with UV‚Äù and ‚Äúhere is how you can do it with pip‚Äù\r\n\r\nI also removed hatch - for simplicity - we do not want to describe 3 tools, also I removed plyvel as dependency as it is almost impossible to install it MacOs and it blocked --all-extras and --devel extras to be installed for MacOS\r\n\r\nAll the rest might fail when installing - and you might need to have some system dependencies but they should be ‚Äúeasy‚Äù to install with brew and others\r\n\r\nPlyvel has a long time history of breaking MacOS and 10.15 is famously broken now https://github.com/wbolster/plyvel/issues/114\r\n\r\nAlso ``uv.lock`` is excluded for now until we figure out if /how to keep it updated acrooss all ~700 dependencies of ours and how to join it with the constraints we produce. 

Thanks @rawwar for all the work on this one :) !!! 

I have a (maybe/probably) crazy idea that might make our tests nicer, and easier to parallelize in future:\r\n\r\nIf we use dependency overrides, we can make the session inside our routers use the same session https://fastapi.tiangolo.com/advanced/testing-dependencies/#use-the-appdependency_overrides-attribute\r\n\r\nBut for now: lets set up this dep override and change the `commit()` in tests to `flush()` (possibly putting the flush inside the dag_maker fixture itself?)

I see I have some tests failing, setting PR to draft until I fix these.

The tests seem to fail because some of them mock the `name` parameter, and then the `validate_key` function inside `_set_name` fails because it expects a string.\r\n\r\nI need help here - what is the way to solve this in your opinion?

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

CC @kaxil 

Thanks @o-nikolas 

cool, thank you :)

FYI @dabla 

@utkarsharma2 if 2.10.3rc2 is not (yet) cut... this would be a late arrival candidante...

Thanks guys for backporting this fix!

@eladkal @o-nikolas  pls review, linter fixed

@o-nikolas sorry, i am not able to write proper test for this, i‚Äôm not aware of how to write tests for airflow operators üòî. I could only provide queries for tests. 

Just look at the other tests implemented and extend them appropriately. They are pretty logical:\r\n\r\n``providers/tests/amazon/aws/operators/test_athena.py``

Thanks @Lee-W .

looks like some test are failing ü§î

Does email operator/email utils is required to move inside standard provider?

This pr getting more and more changes i feel and some changes require in selective checks to remove `OPERATORS` tests from core area. So i feel better to have split the triggers to separate pr #43608.\r\n\r\nWill update this later.

> This pr getting more and more changes i feel and some changes require in selective checks to remove `OPERATORS` tests from core area. So i feel better to have split the triggers to separate pr #43608.\r\n> \r\n> Will update this later.\r\n\r\nMakes sense

Do we still need this PR or was it break into smaller ones that were already merged?

cool :) 

Also nice! :)

Naaajs!

This is the day of weirdest bugs ever. Flit released 3.10.0 today. Which uncovered problem in reproducibility of package generation:\r\n\r\n![image](https://github.com/user-attachments/assets/0bb0bd69-958d-47d7-af16-9bc0cda91637)\r\n\r\n\r\n\r\n\r\n

> it caused the packages generated to be binary non-reproducible.\n\nWhat does this mean?

> > it caused the packages generated to be binary non-reproducible.\r\n> \r\n> What does this mean?\r\n\r\nAll the packages we release in airflow are  binary reproducible - which means that whoever builds them gets the excat binary identtical packages (or that\

Ha.... so we are NOT good with standard provider for 2.8 and 2.9 YET

> Ha.... so we are NOT good with standard provider for 2.8 and 2.9 YET\r\n\r\nhm sorry my bad üòû , i thought only on airflow 2.10

> > Ha.... so we are NOT good with standard provider for 2.8 and 2.9 YET\r\n> \r\n> hm sorry my bad üòû , i thought only on airflow 2.10\r\n\r\nNo worries :). Nice excercise to fix it for 2.8 and 2.9 :)

There are still some interesting problems to solve :). I already found another teething problem from providers move :)\r\n

The fix here https://github.com/apache/airflow/pull/43617 allows to iterate fast on the "main providers" + "old airflow" case :)

> alright got it where the failures coming from for this providers/tests/standard/operators/test_python.py::TestBranchExternalPythonOperator::test_use_airflow_context_touch_other_variables its acutally difference in the deserialisation between 2.8 and 2.10\r\n\r\nNice. Good job. So now we have to find out how this deserialization happened in 2.8/2.9 PythonVirtualenv. Let me take a look\r\n

AH... Of course. Sending context was added in 3.0.0 :) . So the fact that it actually works on 2.10 is a bonus of new standard providers, but there should be no expectation it should work on 2.8 or 2.9 because it never worked there!

Happy to make changes, I believe only need some test fixes and use_airflow_context  \r\nParameter checks between Versions 

> Happy to make changes, I believe only need some test fixes and use_airflow_context Parameter checks between Versions\r\n\r\nI am fixing it :)

Actually - really the thing is that that it was only working on 2.10 when ENABLE_AIP_44 was used, so we really should only enable it only for Airlfow 3+

> Actually - really the thing is that that it was only working on 2.10 when ENABLE_AIP_44 was used, so we really should only enable it only for Airlfow 3+\r\n\r\nYes your correct. for airflow less thank 3 versions we can disable, currently by default its coming as true for all the versions.\r\n\r\nI ran this in local , \r\n`breeze shell --use-airflow-version 2.8.4 --mount-sources providers-and-tests` and in the tests its coming as true.   \r\n\r\nraised change here: https://github.com/apache/airflow/pull/43818

@gopidesupavan - you can rebase and check yout #43818 now\r\n

> @gopidesupavan - you can rebase and check yout #43818 now\r\n\r\nwoohoo merged thank you @potiuk :) . Let me rebase other pr.

Ok. Look like it works nicely :). Weird, weird stuff.

@gopidesupavan Could you add your name and details on airflow-site too please: https://github.com/apache/airflow-site/blob/main/landing-pages/site/data/committers.json

> @gopidesupavan Could you add your name and details on airflow-site too please: https://github.com/apache/airflow-site/blob/main/landing-pages/site/data/committers.json\r\n\r\nThank you Kaxil for making changes üòÑ . sure will update in airflow-site

I believe changes required here also `BASE_PROVIDERS_COMPATIBILITY_CHECKS `\r\n\r\nhttps://github.com/apache/airflow/blob/main/dev/breeze/src/airflow_breeze/global_constants.py#L563 ?\r\n\r\nremove-providers: standard from 2.8.4 and 2.9.3

> I believe changes required here also `BASE_PROVIDERS_COMPATIBILITY_CHECKS `\r\n> \r\n> https://github.com/apache/airflow/blob/main/dev/breeze/src/airflow_breeze/global_constants.py#L563 ?\r\n> \r\n> remove-providers: standard from 2.8.4 and 2.9.3\r\n\r\nGREAT POINT

I will merge it first to fix main though, and then wil make a follow up PR @gopidesupavan 

Follow up PR in https://github.com/apache/airflow/pull/43556

I think before release I really need to fix https://github.com/apache/airflow/pull/43556 (@eladkal - just in case).

Nice!

Backporting #42886 

Came from here https://github.com/apache/airflow/pull/43526

I have removed the code snippet.\r\n<img width="1184" alt="zombie_docs" src="https://github.com/user-attachments/assets/bc928881-5209-4452-ba47-437bbd7b69ee">

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Any opposition to this @potiuk ?

@uranusjr are you referring to AIP-83 [project](https://github.com/orgs/apache/projects/410)?

Tests are failing it looks like , indirect datetime reference imports.  

> Remove redundant functions in `airflow.utils.dates`\r\n\r\nYeah, looks like they were bad imports :D -- fixed them in https://github.com/apache/airflow/pull/43533/commits/1bf242d104a97d607cb5108724353524810d50e0

I think I\

> I think I\

Okay, I think my new changes make this simpler and shorter. Please review and let me know if you see any other improvements I could make here. Thank you!

Thank you for the help improving this.

> Pierre had a similar issue with the pause/unpause dags. Somehow `rm -rf node_modules && pnpm install` fixed it for him. Let me know if those work for those two issues.\r\n\r\nSorry, too late... but comments were not planned to block merge... so all good!

@michaeljs-c Can you please rebase and solve conflicts, we are good to merge after that :)

> @michaeljs-c Can you please rebase and solve conflicts, we are good to merge after that :)\r\n\r\nAll done. Thanks @pierrejeambrun!

I have tested this PR by doing the following:\r\n\r\n1. Created two airflows in helm, one Celery and one KubernetesExecutor, I lowered the timeout for queued to 10 seconds\r\n2. Ensured that both Airflows ran correctly\r\n3. For the CeleryExecutor I then removed the workers and ensured that the correct logs showed up\r\n4. For KubernetesExecutor I modified the pod_template_File to ensure that the pod would be stuck in pending. In that same fashion the following logs showed up indicating that the scheduler attempted to restart the queued task 3 times before failing\r\n<img width="1387" alt="Screenshot 2024-11-08 at 9 07 13\u202fAM" src="https://github.com/user-attachments/assets/dd5d700a-a526-48b8-999f-f3829c7526e2">\r\n\r\n\r\n

Hi @shervinrad100, Thank you for the contribution.\r\n\r\nFor the Google provider (and also Airflow in general), we are following a depreciation policy which states that before removal (which requires a major version bump), we need to deprecate the `public` parameters/hooks/operators etc. While adding the depreciation, we also need to mention the **removal date** _(if possible alternative things to use)_.\r\n\r\nNormally, we have a hook to use it for depreciation but parameter depreciation is a bit tricky. Here is an example for depreciation for the parameter.\r\n```python\r\nwarnings.warn("Something is deprecated and will be removed after MONTH DD, YYYY. Please use something else instead." AirflowProviderDeprecationWarning, stacklevel=2)\r\n```\r\n> Please check this warning message on the unit tests side as well.\r\n\r\nWe can remove it from the [underlying API call](https://github.com/apache/airflow/blob/755c10b99752b1868eb410a00563eff537300e91/providers/src/airflow/providers/google/ads/hooks/ads.py#L287C68-L287C90) and private methods like [`_search`](https://github.com/apache/airflow/blob/755c10b99752b1868eb410a00563eff537300e91/providers/src/airflow/providers/google/ads/hooks/ads.py#L270) to avoid API error. However the public parts (in your PR, they are the parameter of the hooks) needs to deprecate first and then remove.\r\n\r\nAlso, I wonder about operators; as far as I can see, the [operator](https://github.com/apache/airflow/blob/755c10b99752b1868eb410a00563eff537300e91/providers/src/airflow/providers/google/ads/transfers/ads_to_gcs.py#L86C9-L86C32) also has this `page_size` parameter. Are there any reasons for **not** changing the operators?\r\n\r\n\r\n---\r\n\r\nI saw this is your first PR on this repo but removing something is just hard. I very much appreciated your effort, thank you!

Hi @eladkal, \r\nThe documentation page says after v16 sunset (February 5, 2025), this parameter will be removed. Currently we are trying to use the latest version in our provider but users of the operator can always change the version and use an old version (like `v16` or `v16.1`). \r\n\r\nConsidering we have approx. 3 months before sunset of v16, I think we need to give some kind of depreciation warning to the user first and then remove it. Also, removing something requires the bump of the major version, we are just trying to avoid removing something directly.

> The documentation page says after v16 sunset (February 5, 2025), this parameter will be removed. \r\n\r\nThen the statement of:\r\n\r\n> When executing the GoogleAdsToGcsOperator I received an error that page_size is not accepted parameters by this API.\r\n\r\nby the PR author, is not accurate?

@eladkal, the comment is accurate for `v17`, `v17.1` and `v18` of the Google Ads API. \r\nOn the other hand, `v16` and `v16.1` (which are supported until February 5, 2025) support this variable. \r\nBy default we have the latest version of the API but the users can always specify the API version to use a specific API version. \r\nAfter February, since the support for `v16` and `v16.1` will be ended, then we can remove the parameter. \r\nIMHO, we need to put the deprecation warning for February right now and will remove it on February

Shall we merge it @eladkal ?

@molcay @VladaZakharova ?

Hi @potiuk,\r\n\r\nSince we have ~3 months before sunset for `v16.x`, we have to deprecate first and then after February we can remove it. \r\nSome users might be using the `v16.x` of the API and the `page_size` parameter is available for them.\r\n\r\n@shervinrad100 did you have an update on this topic?

Hi @eladkal,\r\nYes, it is currently working in v16 but not in v17+. \r\nDo we know when it will be the next cut?\r\nI asked @shervinrad100 to clarify the status. According to the response we might take action.

Hi @eladkal,\r\nI created a [follow-up PR](https://github.com/apache/airflow/pull/45239) for this change. Can you have a look at it?

Hi,\r\nThe follow-up PR: #45239 is already merged! We can close this

Closing

`docs-build = true` yet the docs build step in CI seems to be skipped.\r\n\r\nhttps://github.com/apache/airflow/actions/runs/11594095043/job/32279562944?pr=43512#step:8:731

> `docs-build = true` yet the docs build step in CI seems to be skipped.\r\n> \r\n> https://github.com/apache/airflow/actions/runs/11594095043/job/32279562944?pr=43512#step:8:731\r\n\r\nYou also need to add "docs-build" as input of the composite `static-checks-mypy-docs.yml` workflow and pass it in ci.yml.\r\n\r\nUnfortunately GitHub Actions convers unknown variables, inputs etc into empty string :scream:  and in this case you have:\r\n\r\n```\r\nif inputs.docs-build == \

As per my understanding it seems `ci.yml` has `docs-build` . I just added it to `static-checks-mypy-docs.yml` and I am getting email notifications that "PR run failed at startup:" and "no jobs were run". Maybe I missed something else.\r\n\r\nhttps://github.com/tirkarthi/airflow/blob/54f47844e0da031130d97eb234d607554c86cb5f/.github/workflows/ci.yml#L102\r\n\r\nhttps://github.com/tirkarthi/airflow/blob/54f47844e0da031130d97eb234d607554c86cb5f/.github/workflows/static-checks-mypy-docs.yml#L95-L98

Ok, I had to pass `docs-build` in `static-checks-mypy-docs` section in ci.yml from `build-info`. I thought outputs from ci.yml is automatically available in other builds. My bad. It seems to be building now. Thanks.

I like how we need to improve the docs (not the code) in order to improve the bot :) 

This seems to be even simpler than https://github.com/apache/airflow-site/pull/1055 as  we only need to really use the /mnt space for the airflow-site separate checkout .

BTW. I want to get it out "quick-ish" because it causes very long  "spellcheck" checks on some PRs because first pass of spell-check does not local build, so if inventories are out-dated, it will try to spellcheck, fail, and then fall-back to rebuilding everything -  and it can take more than hour.

Example here: https://github.com/apache/airflow/actions/runs/11591221918/job/32271148668?pr=43503

Spellcheck still running in 1h 30 minutes - hopefully last pass of the spellcheck now. Testing this change is a PAIN. I understand @kaxil why it was not high priority :D\r\n\r\nBTW. Might want to optimise that one but when we have regular pushes - this will happen way less frequently. 

So... Not that easy :) 

Right ... Sudo :) 

Almost: \r\n\r\n![image](https://github.com/user-attachments/assets/a0403f9b-adf5-4d0a-b0fc-b975ecbd57c0)\r\n

ü§û 

Finally!

Nice

> Nice\r\n\r\nYeah... ‚ù§Ô∏è  it too :)

Thanks

Also removed @gopidesupavan (committer now :) ) 

cc: @briana-okyere

@pierrejeambrun , as of now get dag runs does not raise 404 not found if dag id is invalid. [Link](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_dag_runs).\r\n\r\nShould we update it?

> @pierrejeambrun , as of now get dag runs does not raise 404 not found if dag id is invalid. [Link](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_dag_runs).\r\n\r\n> Should we update it?\r\n\r\nI think we should update it in the new FastAPI endpoint, I wouldn\

Let mer revert #43280 temporarily as it breaks running pytest from CLI

Cool

Since last run info is also added I guess the tooltip is needed there too with duration instead of next dag run creation time. This PR could be updated to have two components in DagRunInfo.tsx\r\n\r\n*  `NextDagRunInfo` that receives next_data_interval_start, next_data_interval_end and next_dagrun_create_after .\r\n* `LastDagRunInfo` that receives data_interval_start, data_interval_end and duration .\r\n\r\nThoughts?

Thanks @bbovenzi, I tried using a single component but I feel it got overloaded to be of different components. Type definition is as below where dataIntervalStart and dataIntervalEnd are the only common ones between last dag run and next dag run.\r\n\r\n```\r\ntype Props = {\r\n  readonly dataIntervalEnd?: string | null;\r\n  readonly dataIntervalStart?: string | null;\r\n  readonly endDate?: string | null;\r\n  readonly logicalDate?: string | null;\r\n  readonly nextDagrunCreateAfter?: string | null;\r\n  readonly startDate?: string | null;\r\n};\r\n```

> Thanks @bbovenzi, I tried using a single component but I feel it got overloaded to be of different components. Type definition is as below where dataIntervalStart and dataIntervalEnd are the only common ones between last dag run and next dag run.\r\n> \r\n> ```\r\n> type Props = {\r\n>   readonly dataIntervalEnd?: string | null;\r\n>   readonly dataIntervalStart?: string | null;\r\n>   readonly endDate?: string | null;\r\n>   readonly logicalDate?: string | null;\r\n>   readonly nextDagrunCreateAfter?: string | null;\r\n>   readonly startDate?: string | null;\r\n> };\r\n> ```\r\n\r\nI think its fine to still have just a single run timestamps tooltip.

Hey all,\r\nI would generally give my blessing to such a work. This would make\r\ndebugging OTEL instrumentation much easier and does not require extra\r\nthings (like running otel collector with debug exporter), so I would give a\r\nthumbs up on this.\r\n\r\nThanks for the work!\r\n\r\nOn Wed, Oct 30, 2024 at 2:35\u202fPM D. Ferruzzi ***@***.***>\r\nwrote:\r\n\r\n> ***@***.**** commented on this pull request.\r\n> ------------------------------\r\n>\r\n> In airflow/traces/otel_tracer.py\r\n> <https://github.com/apache/airflow/pull/43500#discussion_r1823267912>:\r\n>\r\n> > @@ -57,7 +57,7 @@ class OtelTrace:\r\n>      When OTEL is enabled, the Trace class will be replaced by this class.\r\n>\r\n> This comment is here to block merging while it is open; feel free to\r\n> resolve this after (and only after) @howardyoo\r\n> <https://github.com/howardyoo> gives his blessing.\r\n>\r\n> ‚Äî\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/pull/43500#pullrequestreview-2406096080>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AHZNLLXZA73JPBVK5CVKKPLZ6EYIPAVCNFSM6AAAAABQ3NIWKKVHI2DSMVQWIX3LMV43YUDVNRWFEZLROVSXG5CSMV3GSZLXHMZDIMBWGA4TMMBYGA>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Ah... \r\n\r\n````\r\nFAILED providers/tests/fab/auth_manager/api_endpoints/test_backfill_endpoint.py::TestListBackfills::test_should_respond_200_with_granular_dag_access - assert 404 == 200\r\n +  where 404 = <WrapperTestResponse streamed [404 NOT FOUND]>.status_code\r\nFAILED providers/tests/fab/auth_manager/api_endpoints/test_backfill_endpoint.py::TestGetBackfill::test_should_respond_200_with_granular_dag_access - assert 404 == 200\r\n +  where 404 = <WrapperTestResponse streamed [404 NOT FOUND]>.status_code\r\nFAILED providers/tests/fab/auth_manager/api_endpoints/test_backfill_endpoint.py::TestCreateBackfill::test_create_backfill - assert 404 == 200\r\n +  where 404 = <WrapperTestResponse streamed [404 NOT FOUND]>.status_code\r\nFAILED providers/tests/fab/auth_manager/api_endpoints/test_backfill_endpoint.py::TestPauseBackfill::test_should_respond_200_with_granular_dag_access - assert 404 == 200\r\n +  where 404 = <WrapperTestResponse streamed [404 NOT FOUND]>.status_code\r\nFAILED providers/tests/fab/auth_manager/api_endpoints/test_backfill_endpoint.py::TestCancelBackfill::test_should_respond_200_with_granular_dag_access - assert 404 == 200\r\n +  where 404 = <WrapperTestResponse streamed [404 NOT FOUND]>.status_code\r\n```` \r\n\r\nWill faili now in main - see https://github.com/apache/airflow/actions/runs/11669732900/job/32492756690?pr=43556#step:7:4733\r\n\r\nI guess those tests should never be in FAB in the first place ? Maybe we should move them to API @dstandish @pierrejeambrun 

Yep, we missed that FAB provider tests were relying on these legacy endpoints. Indeed those should not be there in the first place, Daniel opened quickly a PR to fix it, sorry for the disruption.

> Congratulations on your first Pull Request and welcome to the Apache Airflow community! \r\n\r\nThanks **boring-cyborg**

I think you have a space missing and Sphix does not like it when formatting tables.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Hey @eladkal and @dabla, This pull request fixes the test case of powerbi_trigger, that initially took 30 seconds to execute. It is in continuation to PR #41331.\r\n\r\n<img width="1417" alt="Screenshot 2024-10-29 at 3 47 20\u202fPM" src="https://github.com/user-attachments/assets/262a4b34-38d3-4b6e-bc6e-f4bf8eee4351">\r\n

Looks like `mypy` does not like it :)

Thank you @ambika-garg for the fix

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Nevermind, it actually fixed. I was running `uv sync` from a wrong place (diff venv).

NICE :)

Add TaskInstanceCollection serializer to unblock other PRs.

Backported to v2-10-test in PR - https://github.com/apache/airflow/pull/43572

newsfragment please :) 

> newsfragment please :)\r\n\r\nthis is new feature @kaxil was never released

> > newsfragment please :)\r\n> \r\n> this is new feature @kaxil was never released\r\n\r\naah whoops

> Thanks for that improvement.\r\n> \r\n> I assume this super-seeds PR #43435 correct?\r\n\r\nYes, I will close that

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

> lgtm if tests go green :)\r\n\r\ncool, the tests are green, doc build/spell checks are running :) 

ty

Can you please add a unit test covering it?

@potiuk \r\n\r\n> Can you please add a unit test covering it?\r\n\r\nWe will, but first we want to know if you (== anyone responsible for dbt Cloud Operator) are fine with the approach

BTW. There is no-one "responsible" - this is not how open-source work. There are > 3000 mostly volunteers - who similarl y as you - are both contributing and reviewing the code in their free time away from their day jobs and families, so the best course of action to get your PR merged, is to follow reviewer\

@potiuk \r\n\r\n> That was the part of first pass of review. At this stage assesment from me is that the change lacks unit test - so it won\

> I don\

@potiuk \r\n\r\n> Yes. You trade the time of contributors with time of maintainers. And no. It\

And yes. if you want to reach out to DBT maintainers - feel free. As I said we treat contributors seriously, if you need `dbt` maintainers opinion on that - feel free to reach to them bring them here and state their opinion.

> And if you are not able to understand and adjust, maybe simple open source contribution is not for you\r\n\r\n> As I said we treat contributors seriously\r\n\r\nOf course you do - respecting their time and themselves in the way presented above is the best proof of this üòâ Sorry for wasting your time by raising concerns that you are unable to answer without ad personam. But in fact, it is a valuable feedback that we can significantly change the logic of the operator and no one will even wonder about the rationale behind it - as long as there are tests. We will definitely raise that issue during discussion with `dbt` folks so they are aware what can happen with their provider.

> I noticed the branch is in a weird state, most likely due to a bad rebase I assume.\r\n\r\nSolved!

Next week we have a call with `dbt` folks - we will try to confirm with them whether it\

> Next week we have a call with `dbt` folks - we will try to confirm with them whether it\

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

üéâ Congrats @krzysztof-kubis! Keep the contributions coming!

> Nothing wrong with incremental changes. The dbt folks were, at least peripherally, involved in the initial creation of this provider. Continued collaboration is definitely welcomed. This provider has come a long way since it was created with contributions from folks like you @krzysztof-kubis, so thank you!\r\n\r\nAs you wish, just to note it\

> üéâ Congrats @krzysztof-kubis! Keep the contributions coming!\r\n\r\nIndeed

Despite some grumpy maintainers :). But you know guys, I am Polish as both of you @jaklan @krzysztof-kubis , so I had to complain.\r\n\r\nSorry if you were a bit too much put-off inititally, and if I was a bit too harsh, but this OSS world is a bit different than traditional and the "top-bottom" thing  does not work here - much more responsibility is in the hands of contributors. 

>Sorry if you were a bit too much put-off inititally, and if I was a bit too harsh, but this OSS world is a bit different than traditional and the "top-bottom" thing does not work here - much more responsibility is in the hands of contributors.\r\n\r\n@potiuk Sure, we get this point, but that\

> @potiuk Sure, we get this point, but that\

@joellabes thanks for the input! The most confusing part here was whether we should take "UI mental model" or "API mental model":\r\n- UI mental model: e.g. each day we have a new scheduled run, but if some run fails - we can additionally trigger "retry from failure"\r\n- API mental model: `run` and `rerun` are 2 different endpoints, [where `rerun` can also trigger the full run](https://docs.getdbt.com/dbt-cloud/api-v2#/operations/Retry%20Failed%20Job):\r\n  > Use this endpoint to retry a failed run for a job from the point of failure, if the run failed. **Otherwise trigger a new run.** \r\n  \r\nSo one could argue the expected behaviour of the operator option is just to switch the endpoint which is used. We were in favor of the first approach, but we didn\

@ashb This _should_ fix the problem you mentioned in https://github.com/apache/airflow/pull/43227#issuecomment-2441654936 as https://github.com/astral-sh/uv/issues/4668 was fixed

Hi there! Thanks for your PR, sounds like a good feature to add :)\r\n@potiuk Can we please not merge this one until we run system tests for this? Thanks!

> Hi there! Thanks for your PR, sounds like a good feature to add :) @potiuk Can we please not merge this one until we run system tests for this? Thanks!\r\n\r\nsure!

Quick addition; if the concern is CI / Systems test, as @claw89 says, we can probably work to get the system test project allow-listed for system test purposes if that would help with getting consensus toward mergability.\r\n\r\nIt would also allow the use of the discovery document in those system tests, too.

I think the issues are:\r\n1) Creating a new, google provider will need some considerable design work - which providers should we split out? How? We can start with a cloudai provider, but I don\

I am here with @michalmodras -> the example thing to separate from Google provider is `ads` - this had been a pain to keep in the same provider as other google properties and we want to split for quite some time. The problem with keeping "unrelated" APIs together is not only dependencies but also independent maintenance - upgrades when thing change. For example what happens if financial services API changes and drops support for older version, but other google APIs (say GKE) are fine running using older code? What happens if at the same time we have breaking changes in new providers for GKE - for example - and the users would not like to upgrade to new "google provider", but also they are using "google-finanacial" API and the old API in the "google provider" they want to keep, is not working any more?\r\n\r\nThis is precisely what happened with ads provider.\r\n\r\nAnd If Google team wants to introduce and maintain the dashboard - that\

Though.... After looking at the code - there is a bit of difficulty there @michalmodras -> this one is using GoogleBaseHook and I believe the dependency on this common GoogleBaseHook is main reason why we do not want to splitt the provider into  "many" sub-providers. I am not sure @claw89 if GoogleBAseHook as base is necessary there or not though?

@michalmodras - any thoughts on the above? It would be useful to get your thoughts on the best way to proceed.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Ok closing for now\r\n![image](https://github.com/user-attachments/assets/806a786a-782a-4f8e-b6d5-005344cdb710)\r\n

Please, not something less than 100...

99?

This is not an exact backport. Please use cherrypicking to make it exact.

Super-seeded by #43462

Just random failures when installing node/pnpm

I think you should remove those HASH file modifications :)

Looks like they were added by pre-commit while the source for breeze has not been committed :). Making sure that you do not have any modified file and running `pre-commit  run update-breeze-cmd-output --all-files` should fix things.\r\n\r\nJust make sure you have no non-added changes in your workspace.

Maybe try to reinstall your pre-commit/breeze - maybe it is still installed with python 3.8 (though it should not change too much) . I cannot push a fixup to your branch, but I have exactly the same hashes as the one that are on the server- somehow the hashes in your version are calcualated differently.. \r\n\r\n

Let me try reinstalling, looks to me that it installed an "older" breeze when I was playing around with some really "old" branch!

CI is green, merging.

> Let me try reinstalling, looks to me that it installed an "older" breeze when I was playing around with some really "old" branch!\r\n\r\nThat could be - at some point in time i added the change that automatically breeze reinstalls if an older version is used - without even asking the user - it just reinstalls itself, but if you had REALLLY old version of breeze that did not have this mechanism, then yeah - hash calculation changed at some point in time so it could be different.

 Can you pleas add a unit test covering it ?

> Can you pleas add a unit test covering it ?\r\n\r\nShould i create a new file including the unit test code or edit the existing test file?

> Can you pleas add a unit test covering it ?\r\n\r\nI had added unit test covering my PR regarding S3 bucket_name.\r\n![Screenshot from 2024-10-29 02-57-11](https://github.com/user-attachments/assets/b94b6ba9-538d-47c7-9911-d8454e4509ef)

I\

@potiuk @hussein-awala Would be good to get your thoughts (or anyone else who might have insights) on this before progressing further.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Yep - it nicely works in CI as well:\r\n\r\n<img width="1612" alt="Screenshot 2024-10-27 at 12 36 17" src="https://github.com/user-attachments/assets/3207aef2-b8f7-4372-9e8a-894b702a5f98">\r\n\r\n\r\nCompared to (in main):\r\n\r\n<img width="1587" alt="Screenshot 2024-10-27 at 12 37 43" src="https://github.com/user-attachments/assets/04331bac-c856-47b7-964c-3bc43132d8f3">\r\n

The doc failure is unrelated - seems that sqlalchemy doc inventory was temporarily unavailable. Merging - since the changes are only in "doc generation" and it already passed.

There is something quite wrong with commits being removed from the docs.

I might take a look in 2 hours or so.

Fix in https://github.com/apache/airflow/pull/43412

You should be able to rebase @eladkal and regenerate the documentation -  it **should** fix itself automatically when you re-run the `breeze release-management prepare-provider-documentation --answer yes`

Hello @koustreak  - could you please explain how are you going to provide synchronization between multiple repos?\r\n\r\nOne of the functionalities of git-sync is that it performs atomic swap when new commits are synced - it keeps the old repository, while it checks-out the new one and then atomically swaps the new folder with the old one via symbolic link.\r\n\r\nThis is an important feature of git-sync, because it guarantees that DAG parsing across multiple folders inside the repo is consistent - and uses always the same single-consistent version of the DAG files and any related imports.\r\n\r\nThis is one of the reasons why both git-sync and us are reluctant to manage and sync multiple repositories via "external means" - because you can already do it via "submodules" - and then the consistency and atomic swap is maintained - while you get  a good way of keeping references to all the updated repos and even reverting and backing individual repos to previous versions in the single "umbrella repo" using submodules.\r\n\r\nWith the approach you propose, not only you have multiple init containers, and running containers with git-sync, but also they have somewhat unpredictible schedule - and it\

> A dag sync hook or dependence can be created , to make sure both repo sync at same time . I agree to your point there will be a problem with sync staleness , but if we keep the wait parameter value is same for both repo then it will somewhat do the job will less lag .\r\n\r\nBut why not promote submodule (which solves the problem - because in submodule, you can also make sure that you have a single commit, syncing both repos at the same time) .\r\n\r\nSay you have \r\n\r\n* repo a) commit 1 -> commit 2\r\n* repo b) commit 3 -> commit 4\r\n\r\nHaving a single "umbrella" subrebo and single git-sync allows you to do this:\r\n\r\nrepo c) (umbrella over b) and c)\r\n\r\n* commit 4: -> links to commit 1 of repo a) and commit 3 of repo b)\r\n* commit 5: -> links to commit 2 or repo a) and commit 4 of repo b)\r\n\r\nThat allows for atomic sync of repo c) - being umbrella on top of a) and b) where a) and b) repos are always consistent\r\n\r\nIs there any problem with that? Would not it be better to explain (and make a PR documenting it) how you can use submodule "umbrella" repo to synchronize multiple repos? Is there any problem with that? Have you looked at the Jagex presentation explaining it and benefits it brings for managing multi-repos?\r\n\r\nI am afraid that by adding more than one repo in git-sync we are opening up for a number of problems, and basicaly eventually we would have to develop a "submodule" equivalent, because the syncrhronisation problems between several repos will cause various errors - and submodule approach seem to have a solution for that, so it seems better to just describe how you can use it.\r\n\r\nAny problems with that?\r\n

BTW. Have you tried to use submodules for your case? What are your experiences?

Submodules are great and i have tried it ,\nBut the main problem with monorepo is the two different version of airflow dag and its code resides in single repo , which makes it difficult to manage sometime .\n\nThe main purpose i came up with this is something different . \n\nLets say you have two branch,  develop and main , and you want to test your dags and code are ok in develop branch first before merge it to main . \n\nAs per the existing airflow setup , you need to have two seperate airflow deployment in your kubernetes cluster ,  one is pointing to main and another is pointing to develop . \n\nIf you put both the repository to single repo , like the mono-repo approach then sometimes it become difficult to manage . \n\nMany small budget projects end up paying extra to manage two different deployments . \n\nThis is the problem i want to solve. Like i said earlier Cross repo sync wont be a blocker in multirepo gitsync, but it surely open up a thought in users mind to refer cross repo functions in airflow dag code .\n\nAs of now , we can imagine , that all the repos in the multirepo gitsync , should be independent of each other .\n\nMy goal is completely different.\n\nThe multirepo gitsync is a way to deal with multiple version of dags in a same environment. 

> The multirepo gitsync is a way to deal with multiple version of dags in a same environment.\r\n\r\nI am not sure if I get it. How? 

Just to put it in context: in Airflow 2, all the DAGs in DAG folder are in the same "namespace" - they can see each other and import each other. You can potentially bundle things together in a DAG .zip file (and this is the way how you can currently also bundle DAG and it\

Understood , @potiuk , happy to contrubute to the community approach of dag versioning . \nSince the community is going towards a different direction , i would post my work as a fork and continue to use it in my current project as a stopgap . Once airflow 3 dag version is out comepletely i will switch to it .  \n\nThanks for reviewing my work , will close this PR today . 

I reviewed the other PR on which is based this one. I will do an in-depth review here when the other one is updated and merge because it will most likely imply changes here as well.

Fix common issue in #43406 ( should add `create_openapi_http_exception_doc` at router ).

Sure, I will separate `FilterParam` from this PR.

Hi @pierrejeambrun, I believe this PR is ready to be merged. Afterward, I‚Äôll start working on PR to refactor SortParam and FilterParam.

> Hi @pierrejeambrun, I believe this PR is ready to be merged. Afterward, I‚Äôll start working on PR to refactor SortParam and FilterParam.\r\n\r\nFor Range filtering, you can take a look at which implements some helper code, you might need to re-use some of it for your refactoring:\r\nhttps://github.com/apache/airflow/pull/43642

I just rebased the branch, waiting for a green CI to merge.

Thanks for helping resolve the conflict.\r\nI‚Äôll take a look at the implementation of range filtering.

Thanks for the PR. A few suggestions before we can merge.

Thanks for reviewing, @pierrejeambrun. I‚Äôve just resolved the issue.\r\n\r\n

Random issue only ... merging.

Actually - this is not correct/complete - is really a breaking change. \r\n\r\nSo far PythonVirtualenv operator used `virtualenv` package to create the venvs, not the built-in `venv` module.  Also there is a dedicated `[virtualenv]` extra to install the package. The package is here: https://pypi.org/project/virtualenv/\r\n\r\nNote that virtualenv is stil maintained till today, even if venv became available \r\n\r\nThis is mostly due to historical reasons, but also because `virtualenv` created venv behaves slightly differently than the built-in venv one (one of the differences for example is that it cannot be relocatable).\r\n\r\nI do not think we are bound to any specifics of `venv` vs `virtualenv` - however when customers use PythonVirtualenvOperat (and `virtualenv` under the hood) they might rely on some internals and differences of `virtualenv` vs `venv`.\r\n\r\nSo while I sympathise with the change - since `venv` is now built-in in Python since 3.3 - I think a change to use `venv` instead of `virtualenv` needs to be discussed in devlist, ending up as a potentially breaking change in Airflow 3 and newsfragment.\r\n\r\nAlso this change is plainly wrong - creating virtualenv currently actually uses "virtualenv" command line not `venv` module - so if we accept it as it is, it will check for `venv` but actually will use for `virtualenv`.

Thanks for taking a look, I, for some reason though venv and virtualenv were the same. As you say this would be breaking.\r\n\r\nhttps://peps.python.org/pep-0405/ - is relevant for any future readers.

Switching to venv in https://github.com/apache/airflow/pull/43568 - closing that one 

NICE!

Thanks Elad

i am ready to co-operate with all sorts of help , my next phase planning is - \r\n- change in airflow UI to add two columns , repo name and branch name so that users can understand which dag is from which repo or branch .  

> Looks good overall, could you please add some unit tests?\r\n\r\nI am beginner in Apache CodeBase, I had written the code but unable to find the reason why few test cases are failing.

> > Looks good overall, could you please add some unit tests?\r\n> \r\n> I am beginner in Apache CodeBase, I had written the code but unable to find the reason why few test cases are failing.\r\n\r\n1 test is failing:\r\nhttps://github.com/apache/airflow/actions/runs/11531791746/job/32105138377#step:7:2095\r\n\r\n```\r\nproviders/tests/amazon/aws/operators/test_athena.py:245: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nairflow/models/baseoperator.py:417: in wrapper\r\n    return func(self, *args, **kwargs)\r\nproviders/src/airflow/providers/amazon/aws/operators/athena.py:158: in execute\r\n    query_status = self.hook.poll_query_status(\r\nproviders/src/airflow/providers/amazon/aws/hooks/athena.py:276: in poll_query_status\r\n    wait(\r\n_ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\n...\r\n\r\n>                   raise AirflowException(f"{failure_message}: {error}")\r\nE                   airflow.exceptions.AirflowException: Error while waiting for query eac29bf8-daa1-4ffc-b19a-0db31dc3b784 to complete: Waiter query_complete failed: An error occurred (UnrecognizedClientException): The security token included in the request is invalid.\r\n\r\nproviders/src/airflow/providers/amazon/aws/utils/waiter_with_logging.py:86: AirflowException\r\n```

> > > Looks good overall, could you please add some unit tests?\r\n> > \r\n> > \r\n> > I am beginner in Apache CodeBase, I had written the code but unable to find the reason why few test cases are failing.\r\n> \r\n> 1 test is failing: https://github.com/apache/airflow/actions/runs/11531791746/job/32105138377#step:7:2095\r\n> \r\n> ```\r\n> providers/tests/amazon/aws/operators/test_athena.py:245: \r\n> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n> airflow/models/baseoperator.py:417: in wrapper\r\n>     return func(self, *args, **kwargs)\r\n> providers/src/airflow/providers/amazon/aws/operators/athena.py:158: in execute\r\n>     query_status = self.hook.poll_query_status(\r\n> providers/src/airflow/providers/amazon/aws/hooks/athena.py:276: in poll_query_status\r\n>     wait(\r\n> _ _ _ _ _ _ _ _ _ _ _ _ _\r\n> \r\n> ...\r\n> \r\n> >                   raise AirflowException(f"{failure_message}: {error}")\r\n> E                   airflow.exceptions.AirflowException: Error while waiting for query eac29bf8-daa1-4ffc-b19a-0db31dc3b784 to complete: Waiter query_complete failed: An error occurred (UnrecognizedClientException): The security token included in the request is invalid.\r\n> \r\n> providers/src/airflow/providers/amazon/aws/utils/waiter_with_logging.py:86: AirflowException\r\n> ```\r\n\r\nThis error is not from my side It may be due to the AWS Credentials. Kindly Guide me with hints of solution.

> This error is not from my side It may be due to the AWS Credentials. Kindly Guide me with hints of solution.\r\n\r\nYour code changes AthenaHook. The only failing test is with `TestAthenaOperator`, it seems related(?)

> > This error is not from my side It may be due to the AWS Credentials. Kindly Guide me with hints of solution.\r\n> \r\n> Your code changes AthenaHook. The only failing test is with `TestAthenaOperator`, it seems related(?)\r\n\r\nSir its my first contribution I am little bit confused why that test case is failing can you edit the code or give hint and merge the code.\r\n

> > > This error is not from my side It may be due to the AWS Credentials. Kindly Guide me with hints of solution.\r\n> > \r\n> > \r\n> > Your code changes AthenaHook. The only failing test is with `TestAthenaOperator`, it seems related(?)\r\n> \r\n> Sir its my first contribution I am little bit confused why that test case is failing can you edit the code or give hint and merge the code. \r\nPlz Help @potiuk 

> Should we handle the `password` as well ? How do we plan to set the connection password through the rest api ?\r\n> \r\n> (And check that in the response of the POST it is properly redacted).\r\n> \r\n> ATM this is done trough FAB in the `Adming/Connection` form that allow the `password` field. But this will be removed in airflow 3.0 so we should handle it in the rest API. Also the doc states `password` https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/post_connection but it was not handled I believe.\r\n\r\nYes, we should. Indeed, the post and the patch include a password. Awesome one! \r\nLet me make this work with the password as well as for the PATCH one. My mind ignored the password in the API. When I see passwords my mind just makes them disappear :smile: 

This still needs some work. I am looking\r\nEdit: depends on the work on #43102

We need to rebase to solve conflicts, but ready to merge IMO.

Good to hear that! Rebased and included the field into redact from the other PR comment :) 

Something is off with the tests. These should have been caught by the CI/CD already.\r\n`FAILED providers/tests/fab/auth_manager/api_endpoints/test_backfill_endpoint.py`\r\nI think those parts are deleted maybe :thinking: it should be fixed in #43654.

> Very cool! Will this save a few seconds in CI as well?\r\n\r\nShould - in a number of cases - depending on whether cache was used or not. But in some it won\

It is a bit ü§Ø 

Thanks @raphaelauv for pointing. It might be not so flexible, but majority of cases should cover.

Tests failing

@potiuk , I removed duplicate otel_* options from the [metrics] section in airflow.cfg, specifically otel_debugging_on, otel_host, otel_on, otel_port, and otel_ssl_active. Additionally, I improved formatting in the [traces] section for clarity and marked otel_* options as deprecated in configuration.py to guide users toward the updated [traces] section options. However, tests are still failing, potentially indicating deeper dependencies or parsing issues beyond these changes. Could I get feedback on any overlooked dependencies or configuration specifics that may be causing this?\r\nAlso, I am looking into the issue and trying to resolve it.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

As requested, @potiuk!

NICE! Thank you !

Renamed the title to use `uv` instead of `ruff` :) - but when merging it might come back from the commit (just watch-out to update it when merging)

AAAND we have "dependency groups" support in (https://peps.python.org/pep-0735/)

> AAAND we have "dependency groups" support in (https://peps.python.org/pep-0735/)\r\n\r\nYup, lol, the timing of your comment here and [my Slack message](https://apache-airflow.slack.com/archives/C06K9Q5G2UA/p1729889456191669) is almost the same.

Alright this should be good to go.  Think I have it to pretty stable place / sensible behavior.

Nice!

Hello,\r\n\r\nTests for this file are located in `test_dag_endpoint.py`.

Thanks for the improvements! That sounds reasonable to add.

> Nice. Good one. There are few small things though for the future @AutomationDev85 (and @jscheffl ) - as written at the top of provider.yaml - we usually increment versions and add release notes by the relase manager - not in the PR, mainly because release manager can take decision on how it should be bumped. (MAJOR/MINOR/PATCHELEVEL) - based on all the changes between last release and new.\r\n> \r\n> This is less of a concern for this pre-release provider, but would be good to keep it - especially removal of previous version is a bit concerning - we usually don\

> I\

I added the description in the commit @Lee-W -> why we are doing it (also updated it in the PR description for posterity)

> I added the description in the commit @Lee-W -> why we are doing it (also updated it in the PR description for posterity)\r\n\r\nThanks!

> NOTE: This is a **BREAKING CHANGE**.\r\n\r\nIndeed. Can you please add a note on top of the changelog for the provider explaining the breaking change and what the users should do to switch?

Added note. Thanks for letting me know what to do.

Hmmm...the contributing guide says "update the provider.yaml file with the new (breaking) version of the provider", but the provider.yaml says "note that those versions are maintained by release manager - do not update them manually". I assume I don\

Yes. No need to update versions, just update changelog - and any clarifications in our contributing docs are also very good contributions (just saying!) 

Built provider locally and tested end-to-end in flow. Everything worked as expected. 

cc: @uranusjr -> I think it will need your input.

Can you resolve conflicts @shahar1  and back-port it - we are going to have 2.10.3 RC2 so we might include it there\r\n

So it does work if you use static list to expand:\r\n\r\n```python\r\n    @task(trigger_rule="always")\r\n    def hello(input):\r\n        print(f"Hello, {input}")\r\n\r\n    hello.expand(input=["world", "moon"])\r\n```\r\n\r\nGranted, not sure it matters. I question the value of "always" anyway - there is no point in having any upstream if it\

> So it does work if you use static list to expand:\r\n> \r\n> ```python\r\n>     @task(trigger_rule="always")\r\n>     def hello(input):\r\n>         print(f"Hello, {input}")\r\n> \r\n>     hello.expand(input=["world", "moon"])\r\n> ```\r\n> \r\n> Granted, not sure it matters. I question the value of "always" anyway - there is no point in having any upstream if it\

Sorry, I should have spent more time digging into this - I leaned too much on the title/newsfrag/docs changes.\r\n\r\nI will say, however, that the docs/newsfragment on this is misleading/wrong. It does still work for static expansions. This also only blocks task group expansions, expanding bare tasks with dynamic input from another task still parses and ultimately fails when running:\r\n\r\n```python\r\n    @task\r\n    def get_input():\r\n        return ["world", "moon"]\r\n\r\n    @task(trigger_rule="always")\r\n    def hello(input):\r\n        print(f"Hello, {input}")\r\n\r\n    hello.expand(input=get_input())\r\n```\r\n\r\n![Screenshot 2024-12-06 at 11 14 50\u202fAM](https://github.com/user-attachments/assets/8845338e-0134-41e3-982f-f4563f27e5b1)\r\n\r\nI\

@eladkal I\

@jscheffl Tried to cover many points. Still some points left.\r\n1. Reset should be shown only when "show advance option" is true.\r\n2. JSON linters and validation should be part of codemirror.\r\n3. Some style fix for config box.

Oh you already rebased? How was moving your changes to chakra v3?

Ok. Let me know if you want me to pull the branch down and help with the rebase.

@bbovenzi @jscheffl Rebased successfully.\r\nLast few commits just have rebase related changes. Will address the reviews in coming commits.

Looking promising. Since there is a conflict... let me know when it is best to do a re-review.\r\n\r\nI noticed a few details:\r\n- Trigger button on DAG details is still disabled, would be great to add the modal there as well\r\n- Trigger is not really working, correct? I did not see a successful run later\r\n- Still the dialog has three lines, title, DAG name and DAG id. I thing this can really be reduced, e.g. put the DAG ID into the title and the gray DAG ID only needs to be displayed if no title is there.\r\n- If the DAG is disabled - on the legacy UI there was an option added and turned on per default to enable DAG scheduling if the DAG was disabled before opening the trigger dialog\r\n- Default params of the DAG are not loaded per default into the DICT element

> Looking promising. Since there is a conflict... let me know when it is best to do a re-review.\r\n> \r\n> I noticed a few details:\r\n> \r\n> * Trigger button on DAG details is still disabled, would be great to add the modal there as well\r\n> * Trigger is not really working, correct? I did not see a successful run later\r\n> * Still the dialog has three lines, title, DAG name and DAG id. I thing this can really be reduced, e.g. put the DAG ID into the title and the gray DAG ID only needs to be displayed if no title is there.\r\n> * If the DAG is disabled - on the legacy UI there was an option added and turned on per default to enable DAG scheduling if the DAG was disabled before opening the trigger dialog\r\n> * Default params of the DAG are not loaded per default into the DICT element\r\n\r\n1. Added the button on DAG details too.\r\n2. As discussed , using an alert and console, Can be taken in another PR post fast API migration.\r\n3. Did the changes, It will be like "Trigger DAG - <DAG_DISPLAY_NAME>" and if it\

@bbovenzi @jscheffl Should we have the reset button in blue colour? Irrespective of light or dark mode?

> @bbovenzi @jscheffl Should we have the reset button in blue colour? Irrespective of light or dark mode?\r\n\r\nNo, let\

> > Looking promising. Since there is a conflict... let me know when it is best to do a re-review.\r\n> > I noticed a few details:\r\n> > \r\n> > * Trigger button on DAG details is still disabled, would be great to add the modal there as well\r\n> > * Trigger is not really working, correct? I did not see a successful run later\r\n> > * Still the dialog has three lines, title, DAG name and DAG id. I thing this can really be reduced, e.g. put the DAG ID into the title and the gray DAG ID only needs to be displayed if no title is there.\r\n> > * If the DAG is disabled - on the legacy UI there was an option added and turned on per default to enable DAG scheduling if the DAG was disabled before opening the trigger dialog\r\n> > * Default params of the DAG are not loaded per default into the DICT element\r\n> \r\n>     1. Added the button on DAG details too.\r\n> \r\n>     2. As discussed , using an alert and console, Can be taken in another PR post fast API migration.\r\n> \r\n>     3. Did the changes, It will be like "Trigger DAG - <DAG_DISPLAY_NAME>" and if it\

A quick thoughts needed?\r\nFor the type "configJson" should be use it as record or string.\r\nUsing as record will further need to convert for usage of codemirror.\r\nif we use string we can handle that in backend maybe while API Calling or maybe while creating json for API, we can convert.\r\n\r\ncc: @bbovenzi @jscheffl 

> A quick thoughts needed? For the type "configJson" should be use it as record or string. Using as record will further need to convert for usage of codemirror. if we use string we can handle that in backend maybe while API Calling or maybe while creating json for API, we can convert.\r\n> \r\n> cc: @bbovenzi @jscheffl\r\n\r\nThe current public REST API expects an JSON object as parameter for the conf, see https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/post_dag_run\r\nBefore adding a new API just to expect a string... I think it is easier to parse the JSON body from CodeMirror to JSON and use this for the trigger. Also has the benefit that you validate early and don\

> Great work! Thanks for being patient with all the back-and-forth review comments.\r\n\r\n+1 from my side for handling all the comments :-D

After review of previous PR I get a "white page" and the document inspectr in Firefox shows me:\r\n![image](https://github.com/user-attachments/assets/8a22b5a7-9be1-4e3e-857f-d7e6808e675e)\r\n

> Besides the "glitch" of a wrong TimeZone selection, crashing script, this looks good.\r\n> \r\n> (I am not sure if the problem is a generic thing, a side effect from Browser Storage of other PR or introduced here. In a other browser (Chrome) it is not crashing.. but maybe I did not select a Timezone there)\r\n\r\nOh that is from another PR. You can just clear the old localStorage for now

But our very diligent bot is not happy with spelling in the docs ... You can put `executemany` in reverse quotes to skip spell checking BTW.

Woops, apparently it is backward compatible in Airflow 2.

Also needs news fragment as this is a breaking change

Static check failure is the known transient issue\r\n```\r\nTS types generation / ESLint / Prettier against UI files...........................Failed\r\n- hook id: ts-compile-format-lint-www\r\n- exit code: 1\r\n\r\nyarn install v1.22.21\r\n\r\n[1/4] Resolving packages...\r\n\r\n[2/4] Fetching packages...\r\n(node:6675) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.\r\n(Use `node --trace-deprecation ...` to show where the warning was created)\r\n\r\nerror Error: https://registry.yarnpkg.com/v8-compile-cache/-/v8-compile-cache-2.3.0.tgz: Request failed "502 Bad Gateway"\r\n    at ResponseError.ExtendableBuiltin (/home/runner/.cache/pre-commit/repo0q5gvmzk/node_env-22.2.0/lib/node_modules/yarn/lib/cli.js:696:66)\r\n    at new ResponseError (/home/runner/.cache/pre-commit/repo0q5gvmzk/node_env-22.2.0/lib/node_modules/yarn/lib/cli.js:802:124)\r\n    at Request.<anonymous> (/home/runner/.cache/pre-commit/repo0q5gvmzk/node_env-22.2.0/lib/node_modules/yarn/lib/cli.js:66218:16)\r\n    at Request.emit (node:events:520:28)\r\n    at module.exports.Request.onRequestResponse (/home/runner/.cache/pre-commit/repo0q5gvmzk/node_env-22.2.0/lib/node_modules/yarn/lib/cli.js:141751:10)\r\n    at ClientRequest.emit (node:events:520:28)\r\n    at HTTPParser.parserOnIncomingClient [as onIncoming] (node:_http_client:700:27)\r\n    at HTTPParser.parserOnHeadersComplete (node:_http_common:119:17)\r\n    at TLSSocket.socketOnData (node:_http_client:542:22)\r\n    at TLSSocket.emit (node:events:520:28)\r\n```

Thank you @shahar1 , I have updated sort order

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

@ferruzzi, @howardyoo, @o-nikolas  Can you please take a look at it? If anything needs to change please let me know.

> @ferruzzi, @howardyoo, @o-nikolas Can you please take a look at it? If anything needs to change please let me know.\r\n\r\n@ArshiaZr, I am not a reviewer, and thus cannot review this PR, but I believe it would be nice if you can add unit tests for your changes as part of this PR, so that the changes can be properly checked to work properly. cc to @ferruzzi 

> @ArshiaZr, I am not a reviewer, and thus cannot review this PR, but I believe it would be nice if you can add unit tests for your changes as part of this PR, so that the changes can be properly checked to work properly. cc to @ferruzzi\r\n\r\nActually - anyone can review PRs - and we encourage it. You can even approve it @howardyoo  and it might guide other maintainers with their approvals.

No, as far as I know, I don‚Äôt think I have the ability to approve the PR, but if I get a chance I‚Äôll sure do the review. Having the unit test would be very helpful for the testing too.Sent from my iPhoneOn Oct 25, 2024, at 12:11\u202fPM, Jarek Potiuk ***@***.***> wrote:\ufeff\r\n\r\n@ArshiaZr, I am not a reviewer, and thus cannot review this PR, but I believe it would be nice if you can add unit tests for your changes as part of this PR, so that the changes can be properly checked to work properly. cc to @ferruzzi\r\n\r\nActually - anyone can review PRs - and we encourage it. You can even approve it @howardyoo  and it might guide other maintainers with their approvals.\r\n\r\n‚ÄîReply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>

> No, as far as I know, I don‚Äôt think I have the ability to approve the PR \r\n\r\nYou can approve it (many contributors do that). But you need at least one approval from a commiter to merge it - but approvals can be done by absolutely anyone who has a GitHub account.

Ok, I see. Thanks!Sent from my iPhoneOn Oct 25, 2024, at 12:26\u202fPM, Jarek Potiuk ***@***.***> wrote:\ufeff\r\n\r\nNo, as far as I know, I don‚Äôt think I have the ability to approve the PR\r\n\r\nYou can approve it (many contributors do that). But you need at least one approval from a commiter to merge it - but approvals can be done by absolutely anyone who has a GitHub account.\r\n\r\n‚ÄîReply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>

I reviewed the code, and to me, it is well implemented and ready to be\r\nmerged.\r\n\r\nOn Fri, Oct 25, 2024 at 12:30\u202fPM Howard Yoo ***@***.***> wrote:\r\n\r\n> Ok, I see. Thanks!\r\n>\r\n> Sent from my iPhone\r\n>\r\n> On Oct 25, 2024, at 12:26\u202fPM, Jarek Potiuk ***@***.***>\r\n> wrote:\r\n>\r\n> \ufeff\r\n>\r\n> No, as far as I know, I don‚Äôt think I have the ability to approve the PR\r\n>\r\n> You can approve it (many contributors do that). But you need at least one\r\n> approval from a commiter to merge it - but approvals can be done by\r\n> absolutely anyone who has a GitHub account.\r\n>\r\n> ‚Äî\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/pull/43340#issuecomment-2438402280>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AHZNLLU25JM2URXAA4NZZRLZ5J5M7AVCNFSM6AAAAABQQJT7UOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDIMZYGQYDEMRYGA>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n>\r\n

Do we expect any incompatibilities in produced metrics @ArshiaZr ? I would love to understand what was the intention of "cleaner and more consistent implementation." before I get into details.

perhaps it\

Agreed. Thanks for the work, Arshia!\r\n\r\nOn Thu, Oct 31, 2024 at 1:22\u202fPM D. Ferruzzi ***@***.***>\r\nwrote:\r\n\r\n> ***@***.**** approved this pull request.\r\n>\r\n> Nice work.\r\n>\r\n> ‚Äî\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/pull/43340#pullrequestreview-2408792003>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AHZNLLQPLN2CF3R4GCLZMBDZ6JYNRAVCNFSM6AAAAABQQJT7UOVHI2DSMVQWIX3LMV43YUDVNRWFEZLROVSXG5CSMV3GSZLXHMZDIMBYG44TEMBQGM>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n

I generally would not go with using the longer name (even otel has 255\r\nlimits to it), simply because of the following:\r\n1. it introduces flood of \

I agree we should have short names where possible, but lets not artificially limit it in the logger to 32 characters

I can def check whether the name validation works on python otel sdk side, as well as on collector side.Will update sometime this weekend

Yes, we shouldn‚Äôt have the limit to 32 chars - that is way too small and not useful.

## Rationale for Using Decorators Instead of Inheritance with Datadog\r\n\r\nUsing `get_name` with the Datadog protocol is unnecessary since we don‚Äôt combine tags and the stat name when calling `DogStatsd` functions. The functions in `DogStatsd` (e.g., `increment`, `decrement`, `gauge`, `timing`) expect the metric name and tags separately, so there‚Äôs no need to modify or combine them beforehand. \r\n\r\nEven if tags need to be prepared, we already apply the `@prepare_metric_name_with_tags` decorator before `get_name`, which impacts only the values passed to `get_name` and not the final metric name used in `DogStatsd` calls.\r\n\r\nTherefore, making `SafeDogStatsdLogger` a subclass of `StatsLogger` doesn‚Äôt add any functional value here. Instead, adding `@prepare_metric_name_with_tags` to each function in `SafeDogStatsdLogger` keeps the code organized and consistent without requiring inheritance specifically for Datadog. This approach provides clarity while keeping `SafeDogStatsdLogger` focused solely on Datadog-specific logic.\r\nAdditionally, I added the `@prepare_stat_with_tags` decorator for a more consistent way of validating tags, allowing for cleaner code by centralizing tag preparation and validation across all metric functions.\r\n\r\n@ashb @ferruzzi if you have any other suggestions just let me know\r\n

I have also added inheritance for consistency. Just let me know which approach is better. @ashb, @ferruzzi 

@ashb  - Can we get this merged?

Looking at this right now, sorry for the delay

They increased the name length in September of last year :sweat_smile:  https://github.com/open-telemetry/opentelemetry-python/pull/3442 

Needs conflicng resolving/rebase

@potiuk and @ferruzzi  I resolved all of the conflicts and rebased it. Can you now confirm if everything is in place.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Backport of #43335. SHould be merged after #43336 is merged/

Bacport of #43334 

Should be followed by cherry-picking #43335 

Nice find @kaxil !

I am cherry-picking #43334 to (in #43336) - this one will also have to be cherry-picked.

I will debug the warnings separately

Figured out, it was because of the change in https://github.com/apache/airflow/pull/43040\r\n\r\n> Good quick fix - but I think the warning should be eventually removed via `@suppress_logs_and_warnings`\r\n> \r\n> Let me see if I can see why it moght not work.\r\n\r\n

Can you please add/modify a unit test for it to avoid regression?

> Can you please add/modify a unit test for it to avoid regression?\r\n\r\nSure, I will try

> > Can you please add/modify a unit test for it to avoid regression?\r\n> \r\n> Sure, I will try\r\n\r\n@lucafurrer , if you would like some help, I can write the test case for this. Thanks for fixing it.

> > > Can you please add/modify a unit test for it to avoid regression?\r\n> > \r\n> > \r\n> > Sure, I will try\r\n> \r\n> @lucafurrer , if you would like some help, I can write the test case for this. Thanks for fixing it.\r\n\r\n@rawwar Hi, I will not have time before tonight to look into it. If you want go for it.

@potiuk, @rawwar I added an unit test. Comments are welcome. \r\nChallenge is that the problem only appears when logging is enabled and the object which had invalid property is mocked as it is the response from the API.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Strange but prod image verification failed - could be because of some uv problem. I re-run it.

> Strange but prod image verification failed - could be because of some uv problem. I re-run it.\r\n\r\nYeah:\r\n\r\n```\r\n=================================== FAILURES ===================================\r\n  ___________ TestPythonPackages.test_required_providers_are_installed ___________\r\n  [gw2] linux -- Python 3.9.20 /tmp/tmplx3z9mmg/.venv/bin/python\r\n  \r\n  self = <docker_tests.test_prod_image.TestPythonPackages object at 0x7f7ce56bbc70>\r\n  default_docker_image = \

> Strange but prod image verification failed - could be because of some uv problem. I re-run it.\r\n\r\nOh actually, looks like something is broken on main too: https://github.com/apache/airflow/actions/runs/11486815718/job/31971045626 (same error)

> > Strange but prod image verification failed - could be because of some uv problem. I re-run it.\r\n> \r\n> Oh actually, looks like something is broken on main too: https://github.com/apache/airflow/actions/runs/11486815718/job/31971045626 (same error)\r\n\r\nOk, I know now. PR incoming

\r\n> Ok, I know now. PR incoming\r\n\r\nYep:\r\n\r\n` WARNING - section/key [core/asset_manager_kwargs] not found in ...kage_name"`

@potiuk Fix here: https://github.com/apache/airflow/pull/43334

> @potiuk Fix here: #43334\r\n\r\nProper fix at https://github.com/apache/airflow/pull/43335

@potiuk Could I get a review on this :)

In v3 it appears that they are tokens per color palette, so something like\r\n`"blue.muted"` would probably be more inline with how they use them.\r\n\r\nOn Thu, Oct 24, 2024 at 10:24\u202fAM Brent Bovenzi ***@***.***>\r\nwrote:\r\n\r\n> ***@***.**** commented on this pull request.\r\n> ------------------------------\r\n>\r\n> In airflow/ui/src/theme.ts\r\n> <https://github.com/apache/airflow/pull/43330#discussion_r1815450840>:\r\n>\r\n> > @@ -67,8 +72,16 @@ const theme = extendTheme({\r\n>    },\r\n>    semanticTokens: {\r\n>      colors: {\r\n> -      "subtle-bg": { _dark: "gray.900", _light: "blue.50" },\r\n> -      "subtle-text": { _dark: "blue.500", _light: "blue.600" },\r\n> +      /* eslint-disable perfectionist/sort-objects */\r\n> +      contrast: { _dark: "blue.200", _light: "blue.600" },\r\n> +      focusRing: "blue.500",\r\n> +      fg: { _dark: "blue.600", _light: "blue.400" },\r\n> +      emphasized: { _dark: "blue.700", _light: "blue.300" },\r\n> +      solid: { _dark: "blue.800", _light: "blue.200" },\r\n> +      muted: { _dark: "blue.900", _light: "blue.100" },\r\n>\r\n> These are the color names in chakra v3. So I wanted to practice using\r\n> them. Which fg and emphasized are the colors for foreground elements like\r\n> text, borders, icons. This is a bit exploratory too. I might need to adjust\r\n> it later.\r\n>\r\n> ‚Äî\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/pull/43330#discussion_r1815450840>, or\r\n> unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAAAZQ35PWBX77UAV72ZDMDZ5EULNAVCNFSM6AAAAABQPS32KKVHI2DSMVQWIX3LMV43YUDVNRWFEZLROVSXG5CSMV3GSZLXHMZDGOJTGMYTAMJYHE>\r\n> .\r\n> You are receiving this because your review was requested.Message ID:\r\n> ***@***.***>\r\n>\r\n

> In v3 it appears that they are tokens per color palette, so something like `"blue.muted"` would probably be more inline with how they use them.\r\n\r\nGood point. Updated. \r\n\r\n

> I think using generic caches from GitHub actions have also a couple of actions for caching. Some of them specifically to yarn, npm etc... I don\

> Yeah, I found the script itself from pre-commit and ran it multiple times as pre-commit and as a raw Python call. I think this is something in my local setup then. I can sort that out. Thanks!\r\n\r\nOne watchout here - in order for the inlining to work with **NEW** script - you should make sure to add a comment with the right script path just before the script that is "inlined" (and ending comment after) - this is the way how the script finds where to inline it.

Looks good, but we also have to handle UI dependencies (with pnpm) 

While reviewing the logs to ensure everything was running smoothly, I noticed an issue: I had accidentally reversed the -+e option ü§¶\u200d‚ôÇÔ∏è.\r\n\r\nHowever, I encountered the following error:\r\n>#50 2.616 /scripts/docker/install_yarn_dependencies_from_branch_tip.sh: line 21: yarn: command not found\r\n\r\nThis suggests that yarn was either not installed or unavailable in the environment when the script was executed.\r\n\r\nThis made me realize that `npm` and `yarn` should have been installed at the point when the script is executed. I identified the best place to install them using apt and to cache them just before switching to the `airflow` user. Since `yarn` installs `node_modules` to a local directory, I copied the installed dependencies to a cache directory for future use. Once the cache was utilized in the appropriate location, it was deleted during the execution of `compile_www_assets.py` to ensure it was not included in the final package.\r\n\r\n> Looks good, but we also have to handle UI dependencies (with pnpm)\r\n\r\nIndeed, thanks for pointing that out! I wanted to make sure everything was functioning as expected before implementing a similar solution with `pnpm`. That‚Äôs the next step I‚Äôll tackle. Many thanks for the quick review!\r\n

There are too many commits, rebased rather than merge :) Could you please check again when you have time?

Sorry for missing that one. Rebased it. It looks really good.

Closing this PR. If the problem appears again multiple times and bothers us, we can follow up again and discuss possible solutions. :)

Merge after https://github.com/apache/airflow/pull/43316 is merged

Backport of #43251 - should be followed with backport of #43309 when merged

This seems to be fixed in main already

Static checks fail for some reason

> > Hmm not so sure about just renaming constraints. What about folks who are already on this version?\r\n> \r\n> These reflect how the actual model is defined. As no feature is expected to be added to this version, I think these columns won\

Yep. Agree with @kaxil. That should be a separate migration. Also the original cahnge in main should be reverted and re-applied after that because people will go eventually 2.x -> 2.11 -> 3.x and things will break for them.

Got it. makes sense. I will create a fix pr to the main as well

Also I guess it should be back-ported ?

> Also I guess it should be back-ported ?\r\n\r\nThis is for 2.10.x, but I guess what you mean is for main and yes, the PR is here https://github.com/apache/airflow/pull/43373

Backport of #43297 that should fix the DB isolation tests in `v2-10-test`

Backport of #43282  -> Updated here for Python "3.8" - @jscheffl , you will have to update it later as well in #42788

Failed tests fixed already in tip of 2-10-test. merging

This the WTForms fix?

> This the WTForms fix?\r\n\r\nYup

> This the WTForms fix?\r\n\r\n~Actually I will update the WTForms thing too~\r\n\r\nDone in https://github.com/apache/airflow/pull/43309/commits/3459d4f74c9bfbbb2dc41741ca994d85c7f36755

That was quick (from Daniel). But you will have to rebase @kaxil as I just merged 4.5.1 as it also had some "security manager" code changes - so we cannot go straight to 4.5.1 -> and now you have conflicts

> That was quick (from Daniel). But you will have to rebase @kaxil as I just merged 4.5.1 as it also had some "security manager" code changes - so we cannot go straight to 4.5.1 -> and now you have conflicts\r\n\r\nDone

Also we will need to backport this one to 2-10-test because of the wtforms limit removal

> Also we will need to backport this one to 2-10-test because of the wtforms limit removal\r\n\r\nYup, will create a backport PR once this is merged

I think due to our new backport policy, the number of PRs merged in Airflow increased significantly... But I am still increasing the lead @kaxil ;) 

> But I am still increasing the lead @kaxil ;)\r\n\r\nor NOT\r\n\r\n![image](https://github.com/user-attachments/assets/10c4a9f3-2caa-4047-9774-9429ce6a873c)\r\n

> I think due to our new backport policy, the number of PRs merged in Airflow increased significantly... But I am still increasing the lead @kaxil ;)\r\n\r\nI need to change git author email and name on your laptop to mine :D 

> > But I am still increasing the lead @kaxil ;)\r\n> \r\n> or NOT\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/595491/379285675-10c4a9f3-2caa-4047-9774-9429ce6a873c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk2ODk5MDIsIm5iZiI6MTcyOTY4OTYwMiwicGF0aCI6Ii81OTU0OTEvMzc5Mjg1Njc1LTEwYzRhOWYzLTJjYWEtNDA0Ny05Nzc0LTk0MjljZTZhODczYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMDIzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTAyM1QxMzIwMDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1jNzcxOGI3OWRhMTgyMGMzMzQ1YTFhMjQ1OTkxYjRlY2M3NmFiNGVjNjk2YTNiNGJkODk1ZmJjNGI3MjI2NWFiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.7HpLlphuaV98HkHCqZn5CUx73KDBySPRiu5iYY5zu2I)\r\n\r\nLet me be happy with the weekly ones :D 

Creating backport

Backport PR: https://github.com/apache/airflow/pull/43318

Backport of #43273 - should finally get the last remaining "providers" dependence in v2-10-test (or so I hope).

Finally v2-10-test green @utkarsharma2 @ephraimbuddy \r\n

> The list could use a "does the title make sense for an end user" pass or 2, but overall looks good.\r\n\r\n@jedcunningham I made some changes to the PR title in changelogs, PTAL. 

@utkarsharma2 we need to get https://github.com/apache/airflow/pull/43348 in fyi

> @utkarsharma2 we need to get #43348 in fyi\r\n\r\nIncluded it. 

One comment. I think - at least before final realease we need to release all the providers or at least the `apache-airflow-providers-fab` - with the `Flask-Application-Builder == 4.5.2`. We already have a number of issues of people who either do not use constraints or use and update their own constraints (because of WTForms release breaking FAB):\r\n\r\n* https://github.com/apache/airflow/issues/43228\r\n* https://github.com/apache/airflow/issues/43270\r\n* https://github.com/apache/airflow/issues/43356\r\n* https://github.com/apache/airflow/issues/43375\r\n\r\nAnd this is something we can only address by releasing the new FAB provider and making sure constraints are updated to point to it.

cc: @eladkal  ^^

This is backport of #42582 \r\n\r\nSince we are closing to 2.10.3 I merged #42582 as I believe it solves often occuring "serialization" issues (with partial_subset) that happen in "mini-scheduler" and causes failures of celery workers - and tests had shown that partial_subset are also wrong when it comes to determining eligibility of dependent tasks in some cases.\r\n\r\n@uranusjr and @ashb -> if you have any concerns about it this is the time to raise it before we merge - I think you were mostly those who knew most about using `partial_subset` and `mini-scheduler` so if you have any doubts whether we should release it in 2.10.3 - please comment. Details and discussion in #42582  and in the comments in this PR.

The failure here should be fixed by #43307

Failing tests fixed in tip of `2-10-test` merging.

Backport of #43276

Both failures should already be fixed in tip of v2-10-test - merging

NIce!\r\n

@ashb since you were the author of https://github.com/apache/airflow/pull/12108 , could you review this PR and see if it makes sense

Should be fine. The plugins from Airlfow 2 will stop working anyway and people will have to modify them. This is pretty much a "given" incompatibility and we need to bite the bullet.  But the  test needs to be fixed and I think we should add a newsfragment (just one liner now and we can expand it later).\r\n

> Should be fine. The plugins from Airlfow 2 will stop working anyway and people will have to modify them. This is pretty much a "given" incompatibility and we need to bite the bullet. But the test needs to be fixed and I think we should add a newsfragment (just one liner now and we can expand it later).\r\n\r\nThat was already part of the docs though, check PR description -- should we still add a newsfragment?\r\n\r\n\r\n[Our docs](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/plugins.html#plugins) already mark this as removed in 2.0:\r\n\r\n\r\n<img width="1099" alt="image" src="https://github.com/user-attachments/assets/c1aa9fbb-c0d0-4947-8ab9-7712fad8c5b1">\r\n\r\nhttps://github.com/apache/airflow/blob/156878806618145430629a4069911d425d6bf77a/docs/apache-airflow/authoring-and-scheduling/plugins.rst#L33-L37\r\n\r\n

Added in https://github.com/apache/airflow/pull/43291/commits/3f92bf7da18a423545bb49d7282a061b1acf3760

> I think we have a still ongoing discussion, not voted towards future scope of provders package. I think this discussion should settle first.\r\n> \r\n> It would also mean that if this PR is merged, that the executor descriptions in provider packages need to be removed - or at least be deprecated until Airflow 2.x support is generated.\r\n\r\n@jscheffl The change in this PR just impact importing executors "via plugins": https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/plugins.html#plugins\r\n\r\nIt is not removing executors from Providers

Where did I get the `[0]` from? I have no idea. Sorry for that ...

> Looks like we need to wait for #43287, but overall looks good\r\n\r\nSince the only failure is the one fixed in #43287, we are good

Mypy check fixed in main.

Nice. I was wondering why we have warnings (yellow tests) back ... I think too much now happens in helm tests simply 

> Actually, when I run the tests locally on main it also fails. Which is a relief but also frustrating. How is main not more broken in the CI of all other PRs and Canary builds?\r\n\r\nAs discussed in slack - and for posterity.  This is another teeting problem after #42505 and later #43173.\r\n\r\nIt is the result of accidentally triggering database initialization for xdist (parallel) tests - in your PR by plugin initialization, in main runnnig  when BACKEND != none.\r\n\r\nA solution to that would be to force BACKEND to "none" and _AIRLFOW_SKIP_DB_TESTS to "true" in order to avoid all the DB autouse fixtures. But this is quite tricky as plugins passed with `-p` wil be initialized before conftest.py that could be used for it, also conftest.py cannot be put at top-level apparently because pytest has problems when several top-level tests folders have conftest.py - it will refuse to load them.\r\n\r\nWe need to find a better solution - likely use different mechanism of Pytest to do imports - likely https://docs.pytest.org/en/7.1.x/explanation/pythonpath.html or maybe find some other creative ways of loading plugins from common place. Or maybe figure out a better way of sharing common test code.

cc: @ashb -> maybe you will have some ideas?

Cool :) 

I think together with the uv fix today, it might be about the last "teething" issue I am aware of with moving providers out of "airlfow" (cc: @ashb @kaxil ) 

> I think together with the uv fix today, it might be about the last "teething" issue I am aware of with moving providers out of "airlfow" (cc: @ashb @kaxil )\r\n\r\nYeah, let\

> pytest plugins cannot be installed by a non-top-level conftest file. Now that provider tests are moved to a nested directory we need to plumb the plugin in a different way.\r\n\r\nYes, but when was this actually a problem?

> Yes, but when was this actually a problem?\r\n\r\nThe problem was that the plugin did not work - really after the move . It did not catch the "forbidden warnings" in both provider and non-provider tests. That resulted in our tests becoming "yellow" in CI instead of "green" as they used to be before the move - the idea with "forbidden_warnings.py" was that we keep on removing them and go down to 0 eventually but no new warnings should be added. \r\n\r\nSo this one fixes the setup that no new warnings should be added when tests are running.

This breaks running Pytest in a Breeze shell. I‚Äôll try to find a solution for it. In the mean time, add `-p tests_common.pytest_plugin` manually to your call.

> This breaks running Pytest in a Breeze shell. I‚Äôll try to find a solution for it. In the mean time, add `-p tests_common.pytest_plugin` manually to your call.\r\n\r\nLet me take a look as well

Yes. Reverting it now. This something we have to handle differently.

And when I recall that (coming back from a meetup now) - I believe this code must have been there  before the move only it has been disabled or removed when the providers were moved . I will have to check when I am back.

I thought it has not been needed BTW and iindownplayed importance of it but I see now that it has been indeed used for system tests so we must bring it back 

@ashb @o-nikolas  \r\n\r\nI was really scratching my head why this happen and could not figure it out, but I know what happens. I recalled some of the history, I checked out the version before the providers move and it turned out that the "tests" folder has been added there even then.\r\n\r\nBUT (And here comes surprise from pytest) the "tests" were never collected when we added any provider\

@ashb @o-nikolas -> https://github.com/apache/airflow/pull/43529 fix here.

Actually it is not yet final fix - due to the multiple conftest.py at top-level, and moving providers the "All" test types are not exactly working as expected - they only run "core" tests. But this is not an issue for CI at least - becasue there in most cases (except Quarantine tests" we are not  using "All" test suite, but individual tests folders (also for Non-DB tests) - so it has no impact on running all tests. I will fix it as a follow up and likely change  approach for "All-" test suites

> The thing that if you run: pytest tests tests/providers/something - the "tests" pytest will NOT be collected, because as of Pytest 8, if there is a parent folder before sub-path, ONLY the subpath tests are collected (!!!!).\r\n\r\nI feel so much better now. I was also really scratching my head trying to figure out how this worked beforehand with passing a provider path, since I knew the `tests` was also there before. Thanks for solving that one!  

Uuups, after some loops, was able to still have it failing locally, need to provide a better fix :-(

> Uuups, after some loops, was able to still have it failing locally, need to provide a better fix :-(\r\n\r\nNow should be stable, removed dependency of logs in caplog as much as possible.

Thank you @jscheffl for your effort here. I was not aware that caplog has concurrency issues. I will not use it in future PRs.

Backport of #43123 to v2-10-test.

The errors will be handled separately PR is coming.

You committed a lot of files in this PR that should not be committed.

please update the files in the PR only to what is relevant to the change

provider checks are good locally\r\n```\r\nhatch run pre-commit run mypy-providers --all-files\r\n```\r\n![Screenshot 2024-10-28 at 1 47 07 PM](https://github.com/user-attachments/assets/14b769b9-cada-4bd6-b8fc-7924f9649e99)\r\n

reran with manual mypy-providers check\r\n```\r\nhatch run pre-commit run --color always --verbose --hook-stage manual  mypy-providers --all-files\r\n```\r\n<img width="1835" alt="Screenshot 2024-10-30 at 11 31 15 AM" src="https://github.com/user-attachments/assets/564416c4-3a72-4372-a6c3-fd59f88bff14">\r\n

just syncing my fork, but the manual checks were passing before this so I think this should be ready for review?

Approved the workflow: üëÄ 

sorry I think I needed to rebase again, there were quite a few test failures in the test_backfill_endpoint module, but I think not related to my changes, I do see it was removed in this PR: https://github.com/apache/airflow/pull/43649/files . Could you please help me trigger checks whenever you get chance again üôá \r\n\r\nCI run test failures: https://github.com/apache/airflow/actions/runs/11671655535/job/32506671351?pr=43272\r\n<img width="1336" alt="Screenshot 2024-11-05 at 11 48 25 AM" src="https://github.com/user-attachments/assets/3d91c09d-83aa-487b-a567-02ef4f10ca40">\r\n

Still there seem to be databricks imports in snowflake ?

You need to rebase, rebuild image and it should work then.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

Cool. @ashb you should be unblocked with this sqlalchemy 2 related dependency. One less for our "Airflow Beach Cleaning" to analyse as well.

@romsharon98 Could you please update the PR title? The current one is a bit vague and doesn‚Äôt clearly communicate the purpose of the change. I recommend reading [this blog post](https://cbea.ms/git-commit/) for guidance.\r\n\r\nIn particular, it would be helpful to follow this principle:\r\n\r\n> **_A properly formed Git commit subject line should always be able to complete the following sentence: ‚ÄúIf applied, this commit will [your subject line here].‚Äù_**\r\n\r\nSince this entry will appear in the release notes, it‚Äôs important that the PR title and description clearly explain the changes and their impact.

> @romsharon98 Could you please update the PR title? The current one is a bit vague and doesn‚Äôt clearly communicate the purpose of the change. I recommend reading [this blog post](https://cbea.ms/git-commit/) for guidance.\n> \n> \n> \n> In particular, it would be helpful to follow this principle:\n> \n> \n> \n> > **_A properly formed Git commit subject line should always be able to complete the following sentence: ‚ÄúIf applied, this commit will [your subject line here].‚Äù_**\n> \n> \n> \n> Since this entry will appear in the release notes, it‚Äôs important that the PR title and description clearly explain the changes and their impact.\n\nThanks @kaxil really appreciate the informative comment.\nI changed the PR title accordingly

> Can you add unit test for this?\r\n\r\nI added them here: https://github.com/apache/airflow/pull/43264/commits/2b70391f92bd1b5f44709b4a60c97958e342e971

It was a really quick solution, great one! @tobimichael96

Any updates on this here? From my perspective it could be merged.

> Any updates on this here? From my perspective it could be merged.\r\n\r\nLooks good to me.\r\n@jedcunningham 

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

@jedcunningham is this PR waiting for chart 2.0 or can we merge it?

Backport of #43260 - should help with getting v2-10-test PRs green - excluding provider examples from importability checks

Hmm. For some reason it does not work :) 

OK. I know why it did not work (and actully found a teething problem with provider folder move). Proper fix is in https://github.com/apache/airflow/pull/43273 - but it will be easier to merge that one and the apply #43273 once merged to avoid conflicts.

hi @shahar1 can you please check the changes? c:

Backport of #43250 to v2-10-test

Failing test fixed in tip of `2-10-test`. Merging

@pierrejeambrun All comments resolved, please have a look when possible. Thank you!

> We are missing the `@mark_fastapi_migration_done` decorator\r\n\r\nDone, added it. And have added `legacy api` label to PR.

@pierrejeambrun all conversations and conflicts resolved, PR ready to review ‚úÖ \r\n\r\nAlso kindly see this thread, let me know if any changes needed there: https://github.com/apache/airflow/pull/43255#discussion_r1819328653

Need rebasing but can be merged after that.

> Need rebasing but can be merged after that.\r\n\r\n@pierrejeambrun done, PR rebased and synced with `main` ‚úÖ

Thanks @omkar-foss 

OK. I know why it failed - seems like old change of mine (adding constraints) did not take into account that sometimes constraints might be conflicting with local deps (like in this case where we pin FAB). Fix in #43276

Merging it - there is an openlineage mypy issue that will be addressed separately.

Looks like we had another flaky test occuring in this one - I added flaky marker for that one too and updated the description

The tests looks good.\r\nOne small thing, in my opinion, when you want to test a function that calls a function inside it you will want to mock the inner call and just test the call has been done like you expected (and not really call the inner function).\r\nLet me know what you think.

> The tests looks good. One small thing, in my opinion, when you want to test a function that calls a function inside it you will want to mock the inner call and just test the call has been done like you expected (and not really call the inner function). Let me know what you think.\r\n\r\nYeah, couldn\

> > The tests looks good. One small thing, in my opinion, when you want to test a function that calls a function inside it you will want to mock the inner call and just test the call has been done like you expected (and not really call the inner function). Let me know what you think.\r\n> \r\n> Yeah, couldn\

> Yes but I think we need to add that we call the Mock function correctly for example in this one we should add:\r\n> \r\n> ```python\r\n> mock_convert_kube_model_object.assert_called_with(...)\r\n> ```\r\n> \r\n> to really verify the logic of the function\r\n\r\n@romsharon98, I have updated for that, will appreciate it if you could review again.\r\n

Nice!

Why is it `dataset_alias` and not `asset_alias`?\r\n\r\n\r\nAlso running `airflow db migrate` locally, I run into `constraint "dsdar_dag_id_fkey" of relation "dag_schedule_asset_alias_reference" does not exist`. Not sure if it might be related?

> Why is it `dataset_alias` and not `asset_alias`?\r\n\r\nThose are fks from Airflow 2.10 that we need to rename.\r\n\r\n> Also running `airflow db migrate` locally, I run into `constraint "dsdar_dag_id_fkey" of relation "dag_schedule_asset_alias_reference" does not exist`. Not sure if it might be related?\r\n\r\nThanks for reminding me! There\

> This should be backported, right?\r\n\r\n0026 yes, 0040 no

backport pr https://github.com/apache/airflow/pull/43314

## Issues identified\r\n\r\nAfter updating the primary key on the `TaskInstance` model to a UUID7 `id` field (from the composite primary key of `["dag_id", "task_id", "run_id", "map_index"]`), I have now run into issues  in testing and session management:\r\n\r\n1. **`session.merge()` Compatibility**  \r\n\r\n    The `session.merge()` function operates strictly on primary keys as mentioned in [this SQLAlchemy docs](https://docs.sqlalchemy.org/en/20/orm/session_state_management.html#merging), which means it no longer recognizes the unique constraint on `["dag_id", "task_id", "run_id", "map_index"]` to identify existing `TaskInstance` records. This leads to issues in cases where `session.merge()` was previously used to either update or insert TaskInstance records, as it now fails to locate the intended record by the unique constraint.\r\n\r\n    Example:\r\n \r\n     https://github.com/apache/airflow/blob/b2ecb6c9caacd2ca7a2c1519ca892b19b2bb35fb/tests/models/test_dag.py#L2013-L2026\r\n\r\n    A possible workaround would be to follow `session.merge()` calls with `refresh_from_db()` to ensure the instance has the latest state:  \r\n   ```python\r\n   task_instance_1 = session.merge(TaskInstance(task=t_1, run_id=dagrun_1.run_id, state=ti_state_begin))\r\n   task_instance_1.refresh_from_db()  # Update instance with current DB state\r\n   ```\r\n\r\n\r\n2. **`session.get()` usage with `TaskInstanceKey`**  \r\n\r\n    Similarly, `session.get()` and other primary key-based session operations now rely solely on the `id` UUID rather than the composite fields. This results in errors when accessing or sorting records based on the previous composite fields, as `session.get()` cannot match identifiers to `TaskInstance` records by the unique constraint.\r\n\r\n    **Current code in main**:\r\n\r\n    https://github.com/apache/airflow/blob/78ff0a99700125121b7f0647023503750f14a11b/tests/models/test_cleartasks.py#L742\r\n\r\n    I have been changing those references from `TaskInstanceKey` in Database calls with `ti.id`, example:\r\n\r\n   https://github.com/apache/airflow/blob/cca7ce0fd66b01293e21221d1dd9332312a4cb66/tests/models/test_cleartasks.py#L742\r\n\r\n3. **Airflow REST API calls which filters TIs**  \r\n    \r\n    Similar to (2) above, some of our REST API call filters TI records based on current primary key as below:\r\n     https://github.com/apache/airflow/blob/78ff0a99700125121b7f0647023503750f14a11b/airflow/api_connexion/endpoints/task_instance_endpoint.py#L536-L537\r\n\r\n     I am changing that to `session.scalars(select(TI).where(...))..one_or_none()` queries. For example, the above code is changed to the following in this PR.\r\n     https://github.com/apache/airflow/blob/3c61ba7157bce0dd9d7456670e7ba553be046d6e/airflow/api_connexion/endpoints/task_instance_endpoint.py#L536-L540

I do think long-term/by release of 3.0 TaskInstanceHistory should also have a uuid.

> Yeah, our use of `session.merge()` in Airflow is v. bad :(\r\n\r\nAgree. This is one of the reasons I wanted to avoid all those db-schema changes for the initial proposal of multi-team and proposed the team-id prefix, becaue I was afraid we are going to have similar ripple-effects that will result in rewriting big part of our database code. Which seem that it was a pretty justified worry.

One problem with generating the unique hash with other fields, is that it will be even **more** difficult to add new fields, so probably getting rid of session.merge() everywhere and replacing it with our own way of merging objects and database entries by retrieving the unique id based on the unique fields from the DB before merging is likely a long-term better approach.

> Yeah, our use of `session.merge()` in Airflow is v. bad :(\r\n\r\nIndeed

> UUID v7 is explicitly designed to support distributed databases with high insert rates due to its temporal ordering.\r\n\r\nYep. that settles it as well. We would have to implement our hashing in similar way (if possible at all)

@potiuk Let me know what you think of this, before I take it out of Draft.\r\n

This looks great. I only have one comment about using same "postgres" service for keycloack and metadata-db. Those two should be separate postgres instances IMHO.

Also it does not yet work when I try to run it due to "keyclok" typo in constants :)

> Also it does not yet work when I try to run it due to "keyclok" typo in constants :)\r\n\r\nUgh, let me fix :sweat_smile: 

@potiuk \r\n\r\nThis one is ready for review as well. Fixed the typo and added a check to ensure the keycloak integration is used in conjunction with the postgres backend.

Maybe also we should extract all other backend constants for consistency @o-nikolas ? It looks weird when only one POSTGRES is constant.

Backport of #43236 

We have some kiota related tests failing "importability" - we need to cherry-pick another fix from main to fix it (on it).

> Would be good to back-port to v2-10-test as well\r\n\r\nOh absolutely. On it.

https://github.com/apache/airflow/pull/43233 is more complete :)

Close in favor of #43233

<img width="319" alt="Screenshot 2024-11-11 at 11 19 19\u202fAM" src="https://github.com/user-attachments/assets/f9d1d717-291c-4e96-9379-eded8453d8c0">\r\n \r\n \r\nLet\

@ashb -- This _should_ be fixed in the next uv release, I just tested it.

See: https://github.com/astral-sh/uv/pull/8628

Working around a bug in UV. It "shouldn\

Unrelated CI failure, merging.

Relates to #43200

Just failing with the known WTForms issue. Merging

Can someone test if this lets `uv sync` work for them now.\r\n\r\nIf not then add  ` - \

> Can someone test if this lets `uv sync` work for them now.\r\n> \r\n> If not then add ` - \

Nice!

Added those.

@romsharon98 Could you update the PR title please? "Fix return issue" doesn\

Cool

The errrors are due to the WTforms issue being fixed in https://github.com/pallets-eco/wtforms/pull/862

Merging

\r\n### Refactor:\r\nUsed `DAGRunResponse`, `DAGCollectionResponse`, and `DAGResponse` from the public serializers for the UI batch of recent runs. However, the subclasses that inherit from the public serializers should also inherit from `OptionalModel`, as most of the fields should be optional.\r\n

\r\n**Resolved:**\r\n\r\nMade this UI endpoint more consistent with the public endpoint by removing `OptionalModel` and using `DAGRunResponse` directly from the public endpoint instead.

@jason810496 Nice, I just added a small commit on top to make a few adjustment, let me know what you think, I believe we can merge then.

Hi @pierrejeambrun, the changes look good to me, but I have another question:\r\n\r\nWhen should I utilize a parameter class (the classes declared in `common/parameters.py` with `to_orm` and `depends` methods) instead of adding query parameters directly (like `dag_runs_limit: int = 10` in this case)?\r\n\r\nFor example, in the Event Log endpoints, the query should leverage `paginated_select`, but the `paginated_select` function depends on a parameter class rather than query parameters. In this case, should I declare all filter classes in `common/parameters.py`, or would it be better to add a `FilterParam` with `dynamic_depends`, similar to `SortParam`, that could be used across all endpoints?\r\n

>When should I utilize a parameter class (the classes declared in common/parameters.py with to_orm and depends methods) instead of adding query parameters directly (like dag_runs_limit: int = 10 in this case)?\r\nFor example, in the Event Log endpoints, the query should leverage paginated_select, but the paginated_select function depends on a parameter class rather than query parameters. In this case, should I declare all filter classes in common/parameters.py, or would it be better to add a FilterParam with dynamic_depends, similar to SortParam, that could be used across all endpoints?\r\n\r\n\r\nThe idea is if the parameter (query or path) is meant to be re-used in other endpoints and implement a common db operation (filtering, sorting, matching, anything), then it should go into `parameters` with our common parameters.\r\n\r\nIf like here it is a very specific one, in terms of parameters (a secondary limit index operating on nested ressources) in terms of naming "dag_runs_limit", and in terms of implementation (`.where(recent_runs_subquery.c.rank <= dag_runs_limit)` which is really specific to the subquery), then it\

Tests failing

This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

closed by https://github.com/apache/airflow/pull/43509

I updated the "update-installers" pre-commit to handle all the places of uv and pip - including all scripts and docs modified in here and #43135 

closed by https://github.com/apache/airflow/pull/43509

Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n

More fixes needed, but current PR fixes some parts, will follow-up with another PR...

Changes were added in #42896. cc  @luyangliuable 

I tried to test this with below patch. For some reason the click is triggered and DagsFilter is setting the searchParams but the subsequent re-render still has empty SearchParams thus making the API call without the filter.\r\n\r\nhttps://testing-library.com/docs/user-event/intro/\r\n\r\n```patch\r\ndiff --git a/airflow/ui/src/App.test.tsx b/airflow/ui/src/App.test.tsx\r\nindex 38b90d1c49..75b06c9b4e 100644\r\n--- a/airflow/ui/src/App.test.tsx\r\n+++ b/airflow/ui/src/App.test.tsx\r\n@@ -17,7 +17,8 @@\r\n  * under the License.\r\n  */\r\n import type { QueryObserverSuccessResult } from "@tanstack/react-query";\r\n-import { render } from "@testing-library/react";\r\n+import { render, waitFor, fireEvent } from "@testing-library/react";\r\n+import { userEvent } from "@testing-library/user-event";\r\n import { afterEach, beforeEach, describe, it, vi } from "vitest";\r\n \r\n import * as openapiQueriesModule from "openapi/queries";\r\n@@ -121,4 +122,21 @@ describe("App", () => {\r\n   it("App component should render", () => {\r\n     render(<App />, { wrapper: Wrapper });\r\n   });\r\n+\r\n+  it("App component should render DAGs page", async () => {\r\n+    const { getByText, getByRole, queryByText } = render(<App />, {\r\n+      wrapper: Wrapper,\r\n+    });\r\n+    const user = userEvent.setup();\r\n+\r\n+    expect(queryByText("Running")).not.toBeInTheDocument();\r\n+    expect(getByText("DAGs")).toBeInTheDocument();\r\n+\r\n+    await userEvent.click(getByText("DAGs"));\r\n+\r\n+    expect(getByText("Running")).toBeInTheDocument();\r\n+    await user.click(getByRole("button", { name: "Running" }));\r\n+\r\n+    console.log(openapiQueriesModule.useDagServiceGetDags.mock.calls);\r\n+  });\r\n });\r\ndiff --git a/airflow/ui/src/pages/DagsList/DagsFilters.tsx b/airflow/ui/src/pages/DagsList/DagsFilters.tsx\r\nindex 3d507ace36..838862c32f 100644\r\n--- a/airflow/ui/src/pages/DagsList/DagsFilters.tsx\r\n+++ b/airflow/ui/src/pages/DagsList/DagsFilters.tsx\r\n@@ -66,6 +66,7 @@ export const DagsFilters = () => {\r\n   const handleStateChange: React.MouseEventHandler<HTMLButtonElement> =\r\n     useCallback(\r\n       ({ currentTarget: { value } }) => {\r\n+        console.log("on click ", value);\r\n         if (value === "all") {\r\n           searchParams.delete(LAST_DAG_RUN_STATE_PARAM);\r\n         } else {\r\n@@ -76,6 +77,11 @@ export const DagsFilters = () => {\r\n           pagination: { ...pagination, pageIndex: 0 },\r\n           sorting,\r\n         });\r\n+        console.log("on click after ", value);\r\n+        console.log(\r\n+          "on click confirm ",\r\n+          searchParams.get(LAST_DAG_RUN_STATE_PARAM),\r\n+        );\r\n       },\r\n       [pagination, searchParams, setSearchParams, setTableURLState, sorting],\r\n     );\r\ndiff --git a/airflow/ui/src/pages/DagsList/DagsList.tsx b/airflow/ui/src/pages/DagsList/DagsList.tsx\r\nindex ad1b08fe87..5d5bbfe0dd 100644\r\n--- a/airflow/ui/src/pages/DagsList/DagsList.tsx\r\n+++ b/airflow/ui/src/pages/DagsList/DagsList.tsx\r\n@@ -121,6 +121,7 @@ export const DagsList = () => {\r\n   const [searchParams, setSearchParams] = useSearchParams();\r\n   const [display, setDisplay] = useState<"card" | "table">("card");\r\n \r\n+  console.log("search params", searchParams);\r\n   const showPaused = searchParams.get(PAUSED_PARAM);\r\n   const lastDagRunState = searchParams.get(\r\n     LAST_DAG_RUN_STATE_PARAM,\r\n@@ -152,6 +153,8 @@ export const DagsList = () => {\r\n     setDagDisplayNamePattern(value);\r\n   };\r\n \r\n+  console.log("last dag run state render", lastDagRunState, searchParams);\r\n+\r\n   const { data, error, isFetching, isLoading } = useDagServiceGetDags(\r\n     {\r\n       dagDisplayNamePattern: Boolean(dagDisplayNamePattern)\r\n\r\n```

Thanks @bbovenzi for the link. I will try to take the test in another PR to unblock this fix since I might need sometime to fully process the testing and mocking flow to test this properly.

Rebased with latest main branch. CI is green now.

Ah it is not that easy indeed :( 

Closing for now until #42632 is fixed - I forgot about it.