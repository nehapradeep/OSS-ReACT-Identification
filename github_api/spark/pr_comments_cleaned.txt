Merging to master. Thank you, @dongjoon-hyun @HyukjinKwon for review.

cc @agubichev

cc @cloud-fan @gengliangwang 

Related Jira : https://issues.apache.org/jira/browse/SPARK-51515

The related PR: https://github.com/apache/spark/pull/45817\r\nI couldn\

cc @cloud-fan 

ping @dongjoon-hyun @LuciferYang @HyukjinKwon 

Merged to master. Thank you, @beliefer .

Could we investigate the reason lead to the null value?

> Could we investigate the reason lead to the null value?\r\n\r\nI manually cancelled the running query, the registered running shuffle had no mapStatus for the running tasks. \r\nThe shuffle cleaner throws this exception when it cleans up these empty mapStatus  asynchronously.\r\n\r\n

I think we can advance the check at report map status.

@pan3793 \r\nWhen setting the [option](https://github.com/apache/spark/blob/master/sql/api/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala#L87), Spark doesn\

Could you review this PR, @yaooqinn ? It would be great if we can have your nice feature in `Spark Master` too.

Thank you, @yaooqinn and @ShreyeshArangath 

Merged to master for Apache Spark 4.1.0.

@maryannxue @cloud-fan 

nits: There seem to be more places that could remove the unnecessary `close()` impl in test file: e.g. https://github.com/apache/spark/blob/ccfc0a9dcea8fa9d6aab4dbb233f8135a3947441/python/pyspark/sql/tests/pandas/test_pandas_transform_with_state.py#L1479

> nits: There seem to be more places that could remove the unnecessary `close()` impl in test file: e.g.\r\n> \r\n> https://github.com/apache/spark/blob/ccfc0a9dcea8fa9d6aab4dbb233f8135a3947441/python/pyspark/sql/tests/pandas/test_pandas_transform_with_state.py#L1479\r\n\r\nYeah I tend to keep both in the test. Removing some `close()` should be able to verify. Do you think we should remove all the `close()` instead?

It\

Thanks! Merging to master/4.0.

Forgot to tell, I have got approval on merging this bugfix in branch-4.0 from @cloud-fan beforehand.

cc @huaxingao 

@MaxGekk this is ready for review

This test failure will be fixed by https://github.com/apache/spark/pull/50282\r\n```\r\n[info] - check outputs of expression examples *** FAILED *** (9 seconds, 272 milliseconds)\r\n...\r\n[info]   Cause: scala.MatchError: (06:30:45.887,TimeType(6)) (of class scala.Tuple2)\r\n[info]   at org.apache.spark.sql.execution.HiveResult$.toHiveString(HiveResult.scala:111)\r\n```

@MaxGekk can you take a look at this when you get the chance?

+1, LGTM. Merging to master/4.0.\r\nThank you, @stefankandic.

@cloud-fan please take a look when you get the chance.

thanks, merging to master/4.0!

late LGTM, Thank you @MaxGekk 

LGTM Thank you @wForget 

@wForget @viirya Thanks\r\nMerged into branch-3.5

Merged into branch-4.0. Thanks @pan3793 

I appreciate it. Thank you, @dongjoon-hyun !

Fixed. It was a difference between  Scala 2.12 vs 2.13. \r\nThe `Option#zip` gives back an `Iterable` on Scala 2.12 instead of an `Option`:\r\n```\r\n# scala\r\nWelcome to Scala 2.12.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_442).\r\nType in expressions for evaluation. Or try :help.\r\n\r\nscala> val option = Some(1)\r\noption: Some[Int] = Some(1)\r\n\r\nscala> option.zip(option)\r\nres1: Iterable[(Int, Int)] = List((1,1))\r\n\r\nscala> None.zip(None)\r\nres2: Iterable[(Nothing, Nothing)] = List()\r\n\r\nscala> res1.headOption\r\nres3: Option[(Int, Int)] = Some((1,1))\r\n\r\nscala> res2.headOption\r\nres4: Option[(Nothing, Nothing)] = None\r\n```\r\n\r\nAlthough the documentation does not says differently: https://www.scala-lang.org/api/2.12.8/scala/Option.html#zip[B](that:scala.collection.GenIterable[B]):Option[(A,B)]

Merged to branch-3.5.

merged to master

\r\nWill `dev/create-release/spark-rm/Dockerfile` be fixed in a separate pull request?\r\n\r\n

> Will `dev/create-release/spark-rm/Dockerfile` be fixed in a separate pull request?\r\n\r\nI think we can update/sync dev/create-release/spark-rm/Dockerfile just before each release.\r\nWDYT?

Thanks, I am going to merge it only in master, in order to check whether it also updates the dependencies in branch-4.0.\r\nIf branch-4.0 is not touched at all, that is great; otherwise, pyarrow=19 should also be compatibile with branch-4.0

merged to master

cc @LuciferYang 

Merged to master.

![image](https://github.com/user-attachments/assets/c86b11c4-1994-4e66-be06-1481a1419e97)\r\nall passed

Merged into master. Thanks @MaxGekk 

Merged to master. Thank you, @MaxGekk .

Late LGTM!

Merged to master. Thank you, @LuciferYang and @beliefer .

Thank you @dongjoon-hyun and @beliefer 

@viirya @dongjoon-hyun @kazuyukitanimura could you please take a look? 

@wForget @viirya @dongjoon-hyun Thank you!\r\nMerged into 4.0/master

@wForget Could you create backport PR for branch-3.5

> @wForget Could you create backport PR for branch-3.5\r\n\r\nSure, I will create later

Thank you @wForget late LGTM\r\n

Merged to master.

Merged to master.

Thank you @dongjoon-hyun 

Thanks @HyukjinKwon 

Thank you @dongjoon-hyun 

@yaooqinn @LuciferYang @dongjoon-hyun @HyukjinKwon Could you take a look at the PR when you have time, please.

Merging to master. Thank you, @LuciferYang @beliefer for review.

ping @dongjoon-hyun @yaooqinn @LuciferYang @MaxGekk 

@MaxGekk can you take a look?

+1, LGTM. Merging to master/4.0.\r\nThank you, @stefankandic.

Let me build and verify manually too~

Strangely, I was unable to verify this new changes. I tried in both Safari and Chrome with two normal and private(Incognito) modes. Is there something for me to do see this new change, @yaooqinn ?

To show the filter input text box, we need to click the table head, and be sure to see this file loaded @dongjoon-hyun  \r\n![image](https://github.com/user-attachments/assets/294876bc-36e3-43d6-9670-42bcaea8a28b)\r\n\r\n

\r\nhttps://github.com/user-attachments/assets/388bde03-b18a-4525-af94-c1d1701fa52e\r\n\r\n

It\

Maybe, am I hitting a timing issue?

Oh, indeed. BTW, I realized that the method `click` has a side-effect.\r\n\r\nWhenever I clicked the header, the table sorting order is flipped like the following.\r\n<img width="223" alt="Screenshot 2025-03-11 at 20 56 00" src="https://github.com/user-attachments/assets/c25cea8b-97ca-4dee-b36c-e85d7b7a2ad2" />\r\n\r\n<img width="217" alt="Screenshot 2025-03-11 at 20 56 04" src="https://github.com/user-attachments/assets/c25171c0-648b-4c97-ac5f-7da15f660982" />\r\n\r\nIt\

Thank you, @pan3793 and @LuciferYang . Merged to master.\r\n\r\nCould you make a backporting PR to branch-4.0 to pass CI there once more, @pan3793 ?

@dongjoon-hyun I opend https://github.com/apache/spark/pull/50264 for 4.0 backport

Hey, why are any manual tests listed in the PR desc not included in UTs?

> Hey, why are any manual tests listed in the PR desc not included in UTs?\r\n\r\nGiven that we have excluded `hive-llap-*` deps from STS modules, the existing STS SQL tests should cover all my manual test cases.

late LGTM

tests files are not in the releasee. Why do we need to remove them?

They are. Please see https://lists.apache.org/thread/0ro5yn6lbbpmvmqp2px3s2pf7cwljlc4

Do you mind explicitly explaining the background, the discussion links, etc in PR description?

> Do you mind explicitly explaining the background, the discussion links, etc in PR description?\r\n\r\nI can provide link to the discussion. Replicating everything that were discussed on the e-mail thread IMO is an overkill. 

> Please put the link there, and fill the PR description with summary as guided in the template\r\n\r\nPlease see:\r\n\r\n> Why are the changes needed?\r\n> Apache source releases must not contained jar files.

-1 if we disable the tests. We are introducing a set of technical debt to remove the other.

@HyukjinKwon removing jars from Apache source release is a must otherwise ASF may pull out release. By voting -1 on the PR I assume that you will provide another PR shortly with removed jars and tests being enabled. cc: @dongjoon-hyun

No, I agree with removing jars but disagree with removing tests. \r\n\r\n>  I assume that you will provide another PR shortly with removed jars and tests being enabled\r\n\r\nNo, I casted my veto with a technical justification. It does not mean that I will start working on it.

> No, I agree with removing jars but disagree with removing tests.\r\n> \r\n> > I assume that you will provide another PR shortly with removed jars and tests being enabled\r\n> \r\n> No, I casted my veto with a technical justification. It does not mean that I will start working on it.\r\n\r\nIf you agree that jars must be removed and they needs to be preferably removed **before** the next release goes out, somebody needs to provide code changes. As you are blocking this change even though others voted for it, the assumption is that you will provide a better solution as keeping jars is not actually even a technical debt, it is ASF legal issue.\r\n\r\nNote that the PR does not remove tests from the source code, it keeps the code, so test can be enabled back whenever there is a solution that avoids having jars in the source code.

Thank you, @HyukjinKwon .

Merged to master for Apache Spark 4.1.0.

Merged to master for Apache Spark 4.1.0.

late LGTM

Late LGTM!

cc: @viirya 

All the checks have passed. @viirya can you please help in merging it?

Thanks @Surbhi-Vijay @HyukjinKwon 

LGTM if CI passes

Hey @cloud-fan, @dongjoon-hyun it seems that all the relevant CIs passed. Could we merge it? Thanks in advance

If you don\

@dongjoon-hyun could you please take a look at this PR in an hour and merge it if everything is fine? Thanks in advance

all tests passed (the notify job is somehow still red), thanks, merging to master!

I also built and verified manually this PR.\r\nMerged to master for Apache Spark 4.1.0.

Thank you @dongjoon-hyun 

late LGTM

Missed out on this PR. Late LGTM!

@beliefer - pls take a look

> Hello @jjayadeep06 . I saw how #50020 on master switched to a daemon thread. Do you think we should construct the `Timer` here on branch-3.5 with a daemon thread too for extra safety?\r\n\r\nAck\r\n\r\n

At that time, https://issues.apache.org/jira/browse/SPARK-46895 is an improvement, not a bug fix.\r\nYou means JVM will not exit even if the branch-3.5 uses `Timer`?

> At that time, https://issues.apache.org/jira/browse/SPARK-46895 is an improvement, not a bug fix. You means JVM will not exit even if the branch-3.5 uses `Timer`?\r\n\r\nyes, and if you look at the changes, it is fixing the existing `Timer` implementation. 

cc @srowen 

@jjayadeep06 @srowen @cnauroth Thank you !\r\nMerged into branch-3.5

Thank you for the reviews and commit everyone.

I personally would treat this as a blocker for the 4.0.0 release.\r\ncc @cloud-fan @dongjoon-hyun @LuciferYang @wangyum @yaooqinn

+1 for restore

Close and in favor SPARK-51466 (https://github.com/apache/spark/pull/50232)

ping @dongjoon-hyun @yaooqinn @LuciferYang 

Merged into master.\r\n@LuciferYang Thank you!

@itholic @dongjoon-hyun @yaooqinn Could you review this PR, please. 

It seems that `SparkSessionE2ESuite` hangs. Could you re-trigger the test pipeline, @MaxGekk ?\r\n```\r\n[info] *** Test still running after 40 minutes, 27 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n```

Merging to master. All GAs passed. Thank you, @yaooqinn @dongjoon-hyun for review.

> Are these all instances, @LuciferYang ?\r\n\r\nSupplemented the cases in the test code path, these are all instances now

Thank you @yaooqinn 

Merged into master for Spark 4.1.0. Thanks @yaooqinn @dongjoon-hyun and @beliefer ~

Thanks @dongjoon-hyun and @amoghantarkar 

@cloud-fan could you help review? Thanks!

thanks, merging to master/4.0!

my concern is that this is a breaking change ... We will at least have to update the migration guide

> int96 support in parquet apache/iceberg#1138\r\n\r\n@HyukjinKwon Yes, this will be a breaking change for applications that depend on INT96 and use future versions of Spark. We are changing the default timestamp type for Parquet files in a future version of Spark.\r\nApplications using future versions of Spark to read or write Parquet files will need to configure Spark to handle INT96, as it will no longer be supported by default. We will need to document this change and highlight it in our release notes.\r\n

Thanks! Merging to master.

Could you further point out where in the current Spark code the `hashCode`/`equals` methods of `CatalystDataToProtobuf ` and `ProtobufDataToCatalyst ` are being used?

Ah, sure, @LuciferYang . This is important for the single-pass Analyzer project, to compare the logical plans between fixed-point and single-pass. Otherwise two objects would be incomparable because of different pointer addresses.

Added this to PR description.

Thank you for your explanation @vladimirg-db 

thanks, merging to master!

ping @srowen @dongjoon-hyun @LuciferYang 

I make a mistake.

Please config github action.

> Please config github action.\r\n\r\ndone

Merged to master for Apache Spark 4.1.0. Thank you, @jinkachy and all.

ping @srowen @dongjoon-hyun @LuciferYang 

thanks, merging to master/4.0!

The PR title and description will be updated after the finalization of the plan.\r\n\r\n

> +1, LGTM. Thank you, @LuciferYang .\r\n> \r\n> Could you revert the change on `build_and_test.yml` file?\r\n\r\ndone

Thank you, @LuciferYang . Merged to master/4.0 for Apache Spark 4.0.0.

Thanks @dongjoon-hyun 

start https://github.com/apache/spark/actions/runs/13755976255 for double check

https://github.com/apache/spark/actions/runs/13797191525\r\n\r\n![image](https://github.com/user-attachments/assets/b00b3f6e-c293-4e36-a048-59082190d41c)\r\n\r\nThe latest macOS daily test has succeeded.\r\n\r\n

Merged to master and branch-4.0.

Oops, I mistakenly removed your comment. Yes, it has to be != and fixed

Merged to master.

Merged to master.

(since this is a clean revert)

cc @HyukjinKwon @itholic 

Merged to master and branch-4.0.

@HeartSaVioR can you take a quick look at this? Thanks ðŸ™‚ \r\n\r\nI understand that you have a lot of other tasks that are more pressing, just wondering if this is on your radar.

Thanks! Merging to master.

+1, LGTM. Merging to master.\r\nThank you, @calilisantos.

@calilisantos Congratulations with your first contribution to Apache Spark!

> @calilisantos Congratulations with your first contribution to Apache Spark!\r\n\r\nThanks @MaxGekk, see you next time!

Putting this on hold ATM as some unexpected complications popped up (particularly the interactions with response indices and response caching)

Will reopen later as required

Thanks for the review! CI is green after some lint changes :) 

@LuciferYang @gengliangwang Could you review this PR, please.

Merging to master. Thank you, @LuciferYang for review.

All tests passed.\r\n\r\n<img width="739" alt="Screenshot 2025-03-06 at 11 18 15" src="https://github.com/user-attachments/assets/068e298e-14e0-4b00-83c3-d8c4e2d2733d" />\r\n

Could you review this PR, @huaxingao ?

Thank you so much, @huaxingao !

Merged to master/4.0.

@HyukjinKwon can you take a look?

Can the examples module simply point to SNAPSHOT versions like everything else in the build? the main branch code is always pointing at unreleased code, but on release, those SNAPSHOT versions are changed to the current release version.

merged to master

Merged to master and 4.0, thank you again @dongjoon-hyun 

Merged to master and branch-4.0.

Merged to master and branch-4.0.

@HeartSaVioR - PTAL, thanks !

Thanks! Merging to master.

Thank you! Merged to master/4.0.

This is a retry of the previous commit. Currently, this PR is a draft status to investigate the CI Python failures.

@pan3793 and @LuciferYang , this relates to code you added/reviewed in #46611 . Would you please review this test fix? Thank you.

Merged into master and branch-4.0. Thanks @cnauroth @pan3793 and @dongjoon-hyun 

Thank you, @LuciferYang !

FYI, apache/hadoop#7478 has a follow-up to keep in sync with the changes here to randomize the fake host name.

Thank you, @allisonwang-db .

Merged to branch-3.5.

Thank you all!

Thank you, @allisonwang-db .

Merged to master/4.0 for Apache Spark 4.0.0.

LGTM

@HeartSaVioR - PTAL, thx !

CI has passed. Thanks! Merging to master.

Merged to master.

thanks, merge into master

Thanks! Merging to 4.0.

Sorry but could you rebase once more because Docker and SparkR CI also failed before, @yaooqinn ?\r\n\r\n<img width="292" alt="Screenshot 2025-03-05 at 18 45 01" src="https://github.com/user-attachments/assets/5e092a64-baea-4e93-9725-b490cbebe2d7" />\r\n

Thank you @dongjoon-hyun 

Merged to branch 3.5, thank you again @dongjoon-hyun 

Merged to master and branch-4.0.

Gonna merge to recover the CI,

Merged to master and branch-4.0.

Merged to master and branch-4.0.

The comment is addressed, @HyukjinKwon . Thank you.

Could you review this once more, @HyukjinKwon ?

Could you review this `Spark Connect` PR, @huaxingao ?

Could you this PR when you have some time, @viirya ?

Looking at this.

Thank you, @viirya and @HyukjinKwon .\r\nMerged to master/4.0.

Merged to master/4.0, thank you @cnauroth 

@yaooqinn , I appreciate the review and merge. Thank you!

@cloud-fan I was recommended to get your advice on this change, as this is related to https://github.com/apache/spark/pull/49816 and was merged in recently

Although this is reported as a bug, I believe this as an improvement.\r\n> This may waste time \r\n\r\nIf you need this in other branches, please let me know again~

Thank you for the confirmation.

Thank you @dongjoon-hyun ~

Merged to master.

thanks, merging to master/4.0!

Merging to master. Thank you, @yaooqinn @dongjoon-hyun for review.

Thank you @dongjoon-hyun 

Thank you, @LuciferYang .

The downloading failure is irrelevant to this PR. Let me merge this.\r\n```\r\n[error] lmcoursier.internal.shaded.coursier.error.FetchError$DownloadingArtifacts: Error fetching artifacts:\r\n[error] https://repo1.maven.org/maven2/org/jline/jline/3.27.1/jline-3.27.1.jar: download error: Caught java.net.SocketException (Network is unreachable) while downloading https://repo1.maven.org/maven2/org/jline/jline/3.27.1/jline-3.27.1.jar\r\n```

Merged to master/4.0.

I reverted this from `branch-4.0`.

Merged to master. Thank you, @pan3793 and all.

thanks, merging to master!

Actually I have a better approach. Let me push some more changes

Merged to master and branch-4.0.

Thank you, @HyukjinKwon !

Thank you, @LuciferYang !

Merged to master for Apache Spark 4.1.0.

ping @cloud-fan @MaxGekk 

@cloud-fan Please take a review again.

thanks, merging to master!

@cloud-fan Thank you!

merged to master

Merged to master and branch-4.0.

Thank you, @HyukjinKwon . \r\n\r\nMerged to master/4.0.

@cnauroth @dongjoon-hyun Thanks for the review.\r\nThe test failure is not relevant.  I am merging this one to master and branch-4.0 

LGTM late.

 > I think we just need to have an interface to hold all table information, and let createTable/replaceTable take it instead of many parameters.\r\n\r\nThanks for the feedback. Your suggestion seems simpler. \r\n\r\n@aokolnychyi What do you think?\r\n\r\n> ```\r\n> interface TableInfo {\r\n>   Column[] columns;\r\n>   Transform[] partitions;\r\n>   ...\r\n> }\r\n\r\nThis could just be an inheritable POJO instead?\r\n\r\n> \r\n> interface TableCatalog ... {\r\n>   Table createTable(Identifier ident, TableInfo t);\r\n> }\r\n> ```\r\n\r\nWould we deprecate the existing `create/replace Table()`  methods (In `TableCatalog` and `StagingTableCatalog`) for a new one that takes in `TableInfo` instead?\r\n

OK so the requirement is:\r\n- adding a new item to table metadata should not require a new overload of `def createTable`\r\n- the existing `def createTable` implementation should fail if a new table feature is added, but should still work if the new feature is not used\r\n\r\nI think the builder should not be an interface, but a class that can have member variables and methods:\r\n```\r\nclass CreateTableBuilder {\r\n  protected Column[] columns;\r\n  ...\r\n  CreateTableBuilder withColumns ...\r\n  ...\r\n  Table create...\r\n}\r\n```\r\n\r\nThen we add a new method in `TableCatalog` to create a builder, with the default implementation to return the builtin builder.\r\n\r\nThe workflow is Spark getting the builder from `TableCatalog`, calling `withXXX` methods to set up fields, and calling `create` at the end.\r\n\r\nWhen we add a new item (e.g. constraints), we add a new `withXXX` method and `supportsXXX` method in the builder. By default `supportsXXX` returns false, and `withXXX` throws an exception if `supportsXXX` is false. Users need to explicitly override `supportsXXX` to return true to adopt the new feature.

@cloud-fan Could you PTAL when you have time? Should we add `(AUTO_GENERATED_ALIAS, "true")` metadata when creating alias because they are implicitly added (or it was intentionally left out)?\r\ncc @srielau

Strictly speaking, it\

@cloud-fan it passed all the relevant tests. Could we merge it? Thanks

The spark connect test failure is unrelated, thanks, merging to master!

Thanks, merging to master

Merged to master and 4.0, thank you @tomscut 

cc @yaooqinn @LuciferYang 

I was a bit purposeful to mute the warning because several Hive releases use BONECP as the default value, IIRC.

Thank you for closing, @pan3793 .

cc @cloud-fan @dusantism-db @miland-db 

thanks, merging to master/4.0!

Merged to master.

ping @cloud-fan @yaooqinn 

This looks like not valuable to me but complicates the API.

> This looks like not valuable to me but complicates the API.\r\n\r\nOK. Let me close this one.

cc @HeartSaVioR @jingz-db 

> cc @HeartSaVioR @jingz-db\r\n\r\n~~Hey @HyukjinKwon , unfortunately this suite is flaky only on CI but I am not able to reproduce the failure locally. Could you run CI one more time and unblock yourself from merging your PR? I filed a JIRA to keep track of the fix (but I probably need sometime to investigate into it as I am not able to reproduce locally): https://issues.apache.org/jira/browse/SPARK-51368~~\r\n\r\n~~If you think this may impact the overall testing health of Spark tests, feel free to merge this PR for temporarily disabling the flaky schema evolution test case for spark connect suite here: https://github.com/apache/spark/pull/50130~~

This is for a scheduled build that tests PySpark installed via PyPI. I think this PR fixes something different from what you mentioned.

> This is for a scheduled build that tests PySpark installed via PyPI. I think this PR fixes something different from what you mentioned.\r\n\r\nOh I thought you were tagging me for fixing the failed flaky suite on CI. If not, then please ignore my thread above.

cc @wayneguow @yaooqinn 

Thanks @yaooqinn 

ping @cloud-fan @yaooqinn @MaxGekk 

cc @dongjoon-hyun 

@dongjoon-hyun @LuciferYang Thank you!

thanks, merging to master/4.0!

Change the description a bit, hope it makes more sense.

@beliefer \r\nI agree that this is not a latency sensitive change - I can imagine some extreme case where this could still help, but I couldn\

If you read through my explanation, you would know it\

@beliefer Thanks for understanding and bearing with me :)

Thanks! Merging to master.

cc @mridulm, @Ngone51

cc @gengliangwang

Thank you, @attilapiros and @Ngone51 .\r\n\r\nBTW, could you make two backporting PRs to branch-4.0 and branch-3.5 in order to make it sure to pass all CIs there, @attilapiros ?

Also, cc @mridulm (once more since this is merged).

Backport to branch-4.0: https://github.com/apache/spark/pull/50259

Backport to branch-3.5: https://github.com/apache/spark/pull/50260

@cloud-fan could you help review? Thanks!

thanks, merging to master/4.0!

Merged to master and branch-4.0.

@HeartSaVioR Can you PTAL at this change

Thanks! Merging to master/4.0.

@cloud-fan Wenchen, can you please take a look?

thanks, merging to master/4.0!

Merged to master/4.0. Thank you, @HyukjinKwon and @allisonwang-db .

Merged to master.

Thank you @dongjoon-hyun 

Merged to master/4.0/3.5\r\n\r\nThank you @LuciferYang 

I revert this as I found that the asc files for legacy versions might still be available from the mirrors 

Thank you for reverting this from master/4.0/3.5, @yaooqinn .

cc @dongjoon-hyun @yaooqinn 

Thank you, @pan3793 , @yaooqinn , @beliefer .\r\nMerged to master/4.0/3.5.

ping @cloud-fan 

> So, is this a new correctness issue due to the original implementation of [SPARK-49488](https://issues.apache.org/jira/browse/SPARK-49488), @beliefer ?\r\n\r\nYes. The original PR brings the bug that the behavior of Spark not match MySQL very well.

thanks, merging to master/4.0!

@cloud-fan @dongjoon-hyun Thank you!

Merged to master and branch-4.0.

Also updated the PR description to explain how I figured out the issue and how I verified the fix.

cc. @HyukjinKwon Would you mind taking a look? Thanks!

Thanks! Merging to master/4.0.

@cloud-fan @aokolnychyi do you want to look when you have a chance, thanks!

Addressed review (thanks @allisonwang-db ).  \r\n\r\nAlso had an offline chat with @cloud-fan , we will not support LIKE in the first cut due to avoid consistency problems as different catalog implementations may treat the patterns different (the problem already exists in other SHOW statements, but we should not spread it).  So I removed that in latest update.  We will explore system tables for filtering.

ping @mridulm @dongjoon-hyun  @LuciferYang @srowen  cc @jjayadeep06

Merged into master.\r\n@srowen @the-sakthi Thank you!

I created a new PR in https://github.com/apache/spark/pull/50278

LGTM\r\n

Merged into master and branch-4.0. Thanks @HyukjinKwon and @the-sakthi 

Thank you @dongjoon-hyun 

thanks, merging to master!

@srielau Please, have a look at the PR when you have time.

LGTM

@LuciferYang @yaooqinn @HeartSaVioR Could you review the PR, please.

@dongjoon-hyun @HyukjinKwon @cloud-fan @gengliangwang Please, have a look at the PR.

> There should be no case of auto conversion from TimeType and TimestampType, correct?\r\n\r\n@HeartSaVioR Auto conversion, probably no, but the SQL standard allows explicit cast using `current_date`:\r\n<img width="859" alt="Screenshot 2025-03-04 at 09 56 05" src="https://github.com/user-attachments/assets/acdb7a05-743c-4a56-9ed2-2b171b39aad6" />\r\n\r\nWe will discuss this when be implementing such castings.

Merging to master. Thank you, @LuciferYang @HeartSaVioR @yaooqinn @gengliangwang @the-sakthi for your review.

I think we could add some tests here to verify the change in the response? 

@the-sakthi do you know where i can write them? im not seeing a relevant test suite that already includes tests for analyze plan handler

@dongjoon-hyun please take another pass thanks!

cc @beliefer @MaxGekk 

and also cc @yaooqinn 

the streaming test failure is unrelated, thanks for the review, merging to master/4.0!

LGTM

Merged to master and branch-4.0.

Let me add a legacy conf ...  to be safer ..

also cc @wbo4958 

LGTM. Thx

thank you all, merged to master

ping @cloud-fan @dongjoon-hyun 

cc @allisonwang-db 

Made a draft PR for SPARK-44856 https://github.com/apache/spark/pull/50099

let me close this for now. I think behaviour diff is too much.

merged to master

The remaining test failures are not related to this PR.

Thanks! merging to branch-4.0.

As "Protobuf breaking change detection and Python CodeGen check" failed with:\r\n\r\n```\r\nError: Previously present message "LazyExpression" was deleted from file.\r\nError: Previously present field "22" with name "subquery_expression" on message "Expression" was deleted.\r\nError: Field "21" with name "subquery_expression" on message "Expression" changed option "json_name" from "lazyExpression" to "subqueryExpression".\r\nError: Field "21" with name "subquery_expression" on message "Expression" changed type from "spark.connect.LazyExpression" to "spark.connect.SubqueryExpression".\r\nError: Field "21" on message "Expression" changed name from "lazy_expression" to "subquery_expression".\r\nError: buf found 5 breaking changes.\r\n```\r\n\r\n#50094 should show it\

I reran the compatibility test after #50094 was merged and it passed now. \r\n- https://github.com/ueshin/apache-spark/actions/runs/13554631961/job/37946210197

Thanks! merging to master.

@dongjoon-hyun @vrozov Would a change like this where the source code is included be acceptable in keeping the JAR?

Thank you for pining me. However, I would recommend to participate the dev mailing list before taking any actions on code, @vicennial .

Understood. Closing this PR and holding the code changes until we have a direction based on the dev list discussion

LGTM (barring the builds pass) :) 

Merged to master

thanks, merging to master/4.0!

LGTM (also could you please add a screenshot of the fixed issue as you did in the Jira desc if possible)? 

Merged to master and branch-4.0.

cc @HeartSaVioR @viirya 

KafkaSourceStressForDontFailOnDataLossSuite failed but looks like unrelated.\r\n\r\n```\r\n[info]   Cause: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: Failed to fetch metadata for partition failOnDataLoss-9-0 because metadata for topic `failOnDataLoss-9` could not be found\r\n```

LGTM

thanks for the review, merging to master/4.0!

The mentioned JIRA is in "resolved" state. We either ought to reopen it or open a new jira?

> The mentioned JIRA is in "resolved" state. We either ought to reopen it or open a new jira?\r\n\r\nIt doesn\

Merged into master/branch-4.0. Thanks @beliefer and @the-sakthi 

@LuciferYang @the-sakthi Thank you!

LGTM

I noticed a very minor grammatical nit here, apologies for the oversight. Have created a PR to quickly address that: https://github.com/apache/spark/pull/50196 \r\n\r\n@itholic 

Also, cc @yaooqinn and @LuciferYang , too

![image](https://github.com/user-attachments/assets/763aae0f-c852-4299-bd49-d33ae9a0e98b)\r\n

These API looks not used anymore! +1 for this change.

thanks, merging to master!

Merged to master and branch-4.0.

Merged to master and branch-4.0.

@cloud-fan, @stefankandic, @stevomitric, please take a look when you find some time, thanks!

thanks, merging to master/4.0!

Thanks! merging to master/4.0.

cc @viirya

Merged to master and branch-4.0.

@cloud-fan @MaxGekk @HyukjinKwon could you please review this?

+1, LGTM. Merging to master/4.0.\r\nThank you, @mihailom-db and @hvanhovell @cloud-fan for review.

cc @dongjoon-hyun @LuciferYang @cnauroth

should we update LICENSE at the same time 

Thank you, @pan3793 , @LuciferYang , @cnauroth .\r\nMerged to master/4.0.

Related benchmark results:\r\n\r\n- jdk17: https://github.com/wayneguow/spark/actions/runs/13513028574\r\n- jdk21: https://github.com/wayneguow/spark/actions/runs/13513032754

Merged into master. Thanks @wayneguow and @yaooqinn 

cc @HyukjinKwon @hvanhovell FYI

@LuciferYang can you make shading for both the client and the server use the `org.sparkproject.connect` prefix? This was still on my to do list. The reason is that the connect and classic dataset implementations can live in the same classpath now, accidentally serialized classes from standalone client could cause a weird serialization error now.

> @LuciferYang can you make shading for both the client and the server use the `org.sparkproject.connect` prefix? This was still on my to do list. The reason is that the connect and classic dataset implementations can live in the same classpath now, accidentally serialized classes from standalone client could cause a weird serialization error now.\r\n\r\nhmm... Is this supposed to be sent to  @wayneguow ï¼Ÿ\r\n\r\n

cc @cloud-fan @dongjoon-hyun @HyukjinKwon, thank you

Merged to master.\r\n\r\nSince this is reported as a `Bug`, could you make backporting PRs, @yaooqinn ?

Thank you @dongjoon-hyun \r\n\r\nI have cherrypicked this to 4.0 and will send a backport PR for 3.5.6

cc @MaxGekk  @allisonwang-db 

also cc @cloud-fan 

Merged to branch-4.0.

Merged to master.

https://github.com/LuciferYang/spark/actions/runs/13502647699/job/37724729211

@MaxGekk Could you PTAL when you have time? Thanks

Please address this: https://github.com/apache/spark/pull/50069#discussion_r1970270797

This is a test only PR and other test failures are definitely unrelated. Thanks, merging to master/4.0!

Oh, did you aim to use this as a follow-up, @beliefer ?

> Oh, did you aim to use this as a follow-up, @beliefer ?\r\n\r\nUh, I forgot it. I want it to be a follow-up.

@dongjoon-hyun Thank you!

thanks @LuciferYang \r\nmerged to master/4.0

Merged to master/4.0.

@dongjoon-hyun Thank you !

Merged to master/4.0, thank you @dongjoon-hyun @asl3 @HyukjinKwon 

Thank you, @yaooqinn and all.

Merged to master and brnach-4.0.

Merged to master and branch-4.0.

Merged into master and branch-4.0. Thanks @itholic @HyukjinKwon @beliefer 

Thanks all for the review!

Thank you @dongjoon-hyun @LuciferYang, SPARK-51306 is attached.

Thank you for adding JIRA issue ID, @yaooqinn .

Please create an issue for track down.

> Please create an issue for track down.\r\n\r\nDone SPARK-51321

Please add test cases.

@beliefer - added tests cases

cc @dongjoon-hyun @MaxGekk 

thanks, merging to master!

ping @HyukjinKwon @zhengruifeng @LuciferYang 

> Do you think you can add some test cases, @beliefer , to be clear what was the problem and to prevent a future regression?\r\n\r\nSpark Connect already have the test cases. This improvement is just to unify the code path and improve the maintenance.

Merged into branch-4.0/master.\r\n@LuciferYang @dongjoon-hyun Thank you!

cc @dongjoon-hyun This should fix the scheduled build and make it green ðŸ‘ 

Thank you, @HyukjinKwon !

Merged to master and branch-4.0.

cc @dongjoon-hyun @LuciferYang 

Merged to master and branch-4.0.

@dongjoon-hyun @cloud-fan could you let me know your thoughts about this fix? Thanks.

I\

@cloud-fan Thanks a lot for the response and the context in https://github.com/apache/spark/pull/47721. It\

Merged to master and branch-4.0.

@cloud-fan @dejankrak-db please take a look, thanks!

thanks, merging to master/4.0!

Hello @chenhao-db thanks for the changes, could we please get some description in the jira and the response to the PR template quetsions above? Helps a lot with understanding the context for reviewing the PR. Thanks.

ping @zhengruifeng @HyukjinKwon  @LuciferYang 

Merged to master and branch-4.0.

@HyukjinKwon Thank you very much!

Merged into master. Thanks @dongjoon-hyun and @beliefer 

Please config your GA.\r\nhttps://github.com/apache/spark/pull/50048/checks?check_run_id=37647515971

> Please config your GA. https://github.com/apache/spark/pull/50048/checks?check_run_id=37647515971\r\n\r\nI enabled GA and triggered GA with an empty commit, is that correct?\r\n

Merged into branch-4.0/master\r\n@llphxd @HyukjinKwon @yaooqinn Thank you all!

ping @LuciferYang cc @cloud-fan 

Merged into branch-4.0/master\r\n@LuciferYang Thank you!

ping @LuciferYang @MaxGekk  @cloud-fan 

Merged into branch-4.0/master\r\n@LuciferYang Thank you!

cc @cloud-fan @szehon-ho @amaliujia @gengliangwang @dongjoon-hyun @viirya @huaxingao 

@viirya @cloud-fan, could you take another look?

thanks, merging to master!

Thanks for reviewing, @cloud-fan @gengliangwang @dongjoon-hyun @viirya!

ping @dongjoon-hyun @LuciferYang 

Thank you, @beliefer and @LuciferYang .\r\nMerged to master/4.0.

@dongjoon-hyun @LuciferYang Thank you!

Nice. LGTM

thanks, merged to master

@dongjoon-hyun shall we include it in 3.5.5? Also cc @aokolnychyi 

We only have the API doc: https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameWriterV2.html

thanks for the review, merging to master/4.0/3.5!

Hi, https://github.com/apache/spark/actions/runs/13567662565/job/37924455859\r\n\r\nThe doc build CI for 3.5 seems broken after this PR merged

I verified that the reverting recovers `unidoc`.\r\n```\r\n$ git log --oneline -n1\r\nc8c0f1feb0f (HEAD -> branch-3.5) Revert "[SPARK-51281][SQL] DataFrameWriterV2 should respect the path option"\r\n\r\n$ ./build/sbt unidoc\r\n...\r\n[info] Main Scala API documentation successful.\r\n[success] Total time: 242 s (04:02), completed Mar 5, 2025 6:12:13 PM\r\n```\r\n\r\nLet me revert this from `branch-3.5` to recover `branch-3.5` CI and unblock other PRs. 

Now, this is reverted from branch-3.5 via https://github.com/apache/spark/commit/c8c0f1feb0f2680ac7437eae926c17724af0d94e due to the CI failure.\r\n\r\nPlease make a backporting PR to `branch-3.5` once more, @cloud-fan .

For the record, `branch-3.5` is recovered.\r\n\r\n<img width="662" alt="Screenshot 2025-03-05 at 20 15 03" src="https://github.com/user-attachments/assets/d88ab2ae-d817-4552-b7c4-edbf5bf41740" />\r\n

cc @cloud-fan 

Merged to master and branch-4.0.

Merged to master and branch-4.0.\r\nThanks @ueshin @HyukjinKwon for the review!

thanks for the review, merging to master/4.0!

cc @cloud-fan since this is targeting Apache Spark 4.0.0.

Merged to master and branch-4.0.

Actually let me revert this ... there are too many subtle behaivour differences ... I will improve here more, and enable it back.

thanks, merged to master

Merged to master and branch-4.0.

I will try to write a unit test which demonstrates race condition, which can be part of the checkin..

> The provided unittest is invalid as it uses multiple threads to process events meanwhile the production code has a single dedicated thread so a race condition is not possible. ( apart from creating the right env. for FetchFailedException, the source code attempts to stop executor on the host in case of Exception, so without source code changes, end to end reproduction in single VM is very difficult for me).\r\n> \r\n> If you still think it is a race condition and you can reproduce the problem with a standalone test I suggest to add log lines two those places where you think the two threads are competing and use the "%t" formatter in log4j2 to include the thread names in the log. In this case please attach the reproduction code without any production code change (only the new logging lines should be added but it is fine if the reproduction should be retried 1000 times as race conditions are flaky in nature but I prefer the original production code) and attach the section of the logs where you think the race occurs.\r\n\r\nThe end to end  bug reproduction without product code change is not feasible for me ( atleast at this point), in a single VM unit test.\r\n\r\nThe race condition is possible ( and happens) because in the DagScheduler ::handleTaskCompletion method, there is asynchronicity introduced due to following snippet of code\r\n             ```\r\n messageScheduler.schedule(\r\n                new Runnable {\r\n                  override def run(): Unit = eventProcessLoop.post(ResubmitFailedStages)\r\n                },\r\n                DAGScheduler.RESUBMIT_TIMEOUT,\r\n                TimeUnit.MILLISECONDS\r\n              )\r\n```\r\n

Let me see if I can modify the test and expose the race, without using multiple threads at top level..

\r\n\r\n\r\n> > The provided unittest is invalid as it uses multiple threads to process events meanwhile the production code has a single dedicated thread so a race condition is not possible. ( apart from creating the right env. for FetchFailedException, the source code attempts to stop executor on the host in case of Exception, so without source code changes, end to end reproduction in single VM is very difficult for me).\r\n> > If you still think it is a race condition and you can reproduce the problem with a standalone test I suggest to add log lines two those places where you think the two threads are competing and use the "%t" formatter in log4j2 to include the thread names in the log. In this case please attach the reproduction code without any production code change (only the new logging lines should be added but it is fine if the reproduction should be retried 1000 times as race conditions are flaky in nature but I prefer the original production code) and attach the section of the logs where you think the race occurs.\r\n> \r\n> The end to end bug reproduction without product code change is not feasible for me ( atleast at this point), in a single VM unit test.\r\n> \r\n> The race condition is possible ( and happens) because in the DagScheduler ::handleTaskCompletion method, there is asynchronicity introduced due to following snippet of code \r\n``` \r\nmessageScheduler.schedule(\r\n    new Runnable {\r\n      override def run(): Unit = eventProcessLoop.post(ResubmitFailedStages)\r\n    },\r\n    DAGScheduler.RESUBMIT_TIMEOUT,\r\n    TimeUnit.MILLISECONDS\r\n  )\r\n```\r\n\r\nHere what we have is producer-consumer pattern.\r\n\r\nThe `messageScheduler` is a queue connecting the producer(s) to the consumer. Regarding the producer side we are fine you can post from any number of threads (you can have multiple producers) but the processing of those event (the consumer side) is in question, if you assume the race condition is in one or multiple handle method calls where you introduced the locking (such as `handleTaskCompletion` which is called from https://github.com/apache/spark/blob/1da65be8921813a28472678cad170e19576fb173/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L3158 method via https://github.com/apache/spark/blob/1da65be8921813a28472678cad170e19576fb173/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L3149)  but that is single threaded because `DAGSchedulerEventProcessLoop` extends the `EventLoop` where the `onReceive()` method is called from a single thread:\r\n\r\nhttps://github.com/apache/spark/blob/1da65be8921813a28472678cad170e19576fb173/core/src/main/scala/org/apache/spark/util/EventLoop.scala#L42-L66\r\n\r\nThe locking implementation itself also contains some bugs but first let\

> > > The provided unittest is invalid as it uses multiple threads to process events meanwhile the production code has a single dedicated thread so a race condition is not possible. ( apart from creating the right env. for FetchFailedException, the source code attempts to stop executor on the host in case of Exception, so without source code changes, end to end reproduction in single VM is very difficult for me).\r\n> > > If you still think it is a race condition and you can reproduce the problem with a standalone test I suggest to add log lines two those places where you think the two threads are competing and use the "%t" formatter in log4j2 to include the thread names in the log. In this case please attach the reproduction code without any production code change (only the new logging lines should be added but it is fine if the reproduction should be retried 1000 times as race conditions are flaky in nature but I prefer the original production code) and attach the section of the logs where you think the race occurs.\r\n> > \r\n> > \r\n> > The end to end bug reproduction without product code change is not feasible for me ( atleast at this point), in a single VM unit test.\r\n> > The race condition is possible ( and happens) because in the DagScheduler ::handleTaskCompletion method, there is asynchronicity introduced due to following snippet of code\r\n> \r\n> ```\r\n> messageScheduler.schedule(\r\n>     new Runnable {\r\n>       override def run(): Unit = eventProcessLoop.post(ResubmitFailedStages)\r\n>     },\r\n>     DAGScheduler.RESUBMIT_TIMEOUT,\r\n>     TimeUnit.MILLISECONDS\r\n>   )\r\n> ```\r\n> \r\n> Here what we have is producer-consumer pattern.\r\n> \r\n> The `messageScheduler` is a queue connecting the producer(s) to the consumer. Regarding the producer side we are fine you can post from any number of threads (you can have multiple producers) but the processing of those event (the consumer side) is in question, if you assume the race condition is in one or multiple handle method calls where you introduced the locking (such as `handleTaskCompletion` which is called from\r\n> \r\n> https://github.com/apache/spark/blob/1da65be8921813a28472678cad170e19576fb173/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L3158\r\n> \r\n> method via\r\n> https://github.com/apache/spark/blob/1da65be8921813a28472678cad170e19576fb173/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L3149\r\n> \r\n> ) but that is single threaded because `DAGSchedulerEventProcessLoop` extends the `EventLoop` where the `onReceive()` method is called from a single thread:\r\n> https://github.com/apache/spark/blob/1da65be8921813a28472678cad170e19576fb173/core/src/main/scala/org/apache/spark/util/EventLoop.scala#L42-L66\r\n> \r\n> The locking implementation itself also contains some bugs but first let\

@attilapiros @squito ,\r\nGiven that there is a guarantee that DagScheduler::onReceive(event: DAGSchedulerEvent) is always going to be invoked in single thread of EventLoop and in NO SITUATION can there be conurrency in invocation of onReceive(event: DAGSchedulerEvent) ,  then I suppose the race can be fixed with the existing changes of PR **sans** the locks.\r\nI will modify the PR to remove locks altogether.\r\n

@ahshahid as the solution became very different please update its description too in the PR description.\r\n

> @ahshahid as the solution became very different please update its description too in the PR description.\r\n\r\n@attilapiros .Sincere thanks for the detailed review.

Thanks! merging to master/4.0.

@aokolnychyi @cloud-fan do you guys want to take a look, thanks!

Also wanted to give credit to @raveeram-db to identify the issue and the idea for the fix!

thanks, merging to master/4.0!

There is some overlapping logic between the two types of tests that I could see if we can simplify, but HDFS state store do their snapshot upload checks a little bit differently [(see this comment)](https://github.com/apache/spark/pull/50030/files#diff-25907d502d153fb199ac2da9f8138ec0afa8b34721ec116cde0791d119c6b847R328) that makes this more difficult to combine in a clean manner.

@HeartSaVioR There are two non-test differences compared to the RocksDB PR:\r\n\r\n- Add tracking of `lastUploadedSnapshotVersion` to HDFS state stores, which is updated in the state store object instead of RocksDB. Also updates at the end of `writeUpdates` instead of `uploadSnapshot`.\r\n- Metric creation is done in the state store provider class instead of object\r\n\r\nThe fixes for SparkUI does not affect this PR, since that was an issue with the underlying instance metrics collection process.\r\n\r\nAs for the test changes:\r\n\r\n- Modified the RocksDB tests slightly to process some additional data in the queries, because the version difference check used in HDFS and RocksDB state stores uses `>` and `>=` respectively. The general structure of the tests remain the same.

It is unclear to me why the changes to spark core are required - marking the RDD with the appropriate `DeterministicLevel` should be sufficient.

> It is unclear to me why the changes to spark core are required - marking the RDD with the appropriate `DeterministicLevel` should be sufficient.\r\nIn case of ShuffleMap stage, the base RDD itself might be deterministic but the Partitioner may be not.\r\nIf any of the changes present are removed, tests will fail.\r\n

@mridulm  @squito ,\r\nI am unsure as to what you mean by marking the RDD inDeterministic, without modifying the RDD code....\r\n1) There is no concrete field in the RDD which marks it inDeterministic ( The root RDD is always considered inDeterministic)\r\n2) Based on existing code, there is a function in RDD and overridden in MapPartitionsRDD which identifies whether RDD is deterministic or not.  And that code relies on Dependency and the RDD contained in dependencies.\r\n3) It does not take into account anywhere, the indeterministic nature of PartitionEvaluator  of the RDD.\r\n\r\nConsider the test "SPARK-51016: ShuffleMapStage using indeterministic join keys should be INDETERMINATE", in newly added file ShuffleMapStageTest\r\nIn this case, the  ShuffleMapStage contains ShuffleDependency and the corresponding RDD (MapPartitionsRDD).  And MapPartitionsRDD \

thanks, merging to master/4.0!

thanks, merging to master/4.0!

thanks, merging to master/4.0!

thanks, merging to master/4.0!

cc @cloud-fan @MaxGekk 

thanks, merging to master/4.0!

replaces https://github.com/apache/spark/pull/48477 with TransformingEncoder fixes.\r\n\r\nThis allows all of Frameless tests to pass when used either with the backwards compat AgnosticExpressionPathEncoder root _and_ all tests to work with the frameless AgnosticEncoder based [encoder derivation branch](https://github.com/chris-twiner/frameless/tree/temp/spark4_agnosticEncoder_reformated).\r\n\r\nOne ExpressionEncoderSuite test "transforming encoders as value class - Frameless value class as parameter use case" does not work when using a TransformingEncoder over the string field.  I\

@hvanhovell - per our convo

@chris-twiner can you fix the style issue?

> @chris-twiner can you fix the style issue?\r\n\r\n@hvanhovell - yeah done

Merging to master/4.0. Thanks!

@shrprasa @wangyum @Madhukar525722 I just made it to work, would be great if you could have a test with your internal cases, I will polish the code according to the GHA result and your feedback

Thanks @pan3793. We will test and update

@shrprasa Does it work after applying the patch?

HI @pan3793 , while testing we are facing a warning\r\nI am trying to backport this change to spark3.4, with builtin hive 2.3.10\r\n\r\n`[spark3-client]$ spark-sql --master yarn --deploy-mode client --driver-memory 4g --executor-memory 4g --conf spark.hadoop.hive.thrift.client.max.message.size=1gb`\r\n\r\n`Setting default log level to "WARN".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n25/02/24 07:45:46 WARN HiveConf: HiveConf of name hive.thrift.client.max.message.size does not exist\r\n25/02/24 07:45:49 WARN HiveConf: HiveConf of name hive.thrift.client.max.message.size does not exist`\r\n\r\nWe have already defined the conf in hive-site.xml\r\n`    <property>\r\n      <name>hive.thrift.client.max.message.size</name>\r\n      <value>1gb</value>\r\n    </property>`\r\n\r\nSo, the error message for table persists.

It didnt worked @pan3793 , I am suspecting that the config setup didnt happend. Thats why it resulted in the same old behaviour. Apart from that other error logs are still same\r\n

HI @pan3793 . The flow was able to reach the case msc:\r\nBut there I added the debug log - \r\n```\r\nmsc.getTTransport match {\r\n                  case t: TEndpointTransport =>\r\n                    val currentMaxMessageSize = t.getConfiguration.getMaxMessageSize\r\n                   ...\r\n                  case _ => logDebug(s"The metastore client transport is not TEndpointTransport, but: ${msc.getTTransport.getClass.getName}")\r\n                }\r\n```\r\n\r\nand found corresponding value to be\r\nThe metastore client transport is not TEndpointTransport, but: org.apache.hadoop.hive.thrift.client.**TUGIAssumingTransport**\r\n\r\nDoes it require, more unwrapping before, getting the exact thrift TEndpointTransport\r\n\r\nAs when I tried it without unwrapping\r\n```\r\nmsc.getTTransport match {\r\n                  case t: TUGIAssumingTransport =>\r\n                    val currentMaxMessageSize = t.getConfiguration.getMaxMessageSize\r\n                   ...\r\n                  case _ => logDebug(s"The metastore client transport is not TEndpointTransport, but: ${msc.getTTransport.getClass.getName}")\r\n                }\r\n```\r\nGives error-\r\nHiveClientImpl: Failed to configure max thrift message size\r\njava.lang.NullPointerException: Cannot invoke "org.apache.thrift.TConfiguration.getMaxMessageSize()" because the return value of "org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.getConfiguration()" is null\r\n\r\nWanted to understand as well, why this might be coming null - org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.getConfiguration()

@Madhukar525722 thanks for testing, I updated the code and verified on both kerberized and simple hadoop clusters, and I confirm those two DEBUG logs are printed. \r\n\r\n```\r\n2025-02-25 15:56:20 DEBUG HiveClientImpl: Trying to set metastore client thrift max message to 1024\r\n2025-02-25 15:56:20 DEBUG HiveClientImpl: Change the current metastore client thrift max message size from 104857600 to 1024\r\n```

Thanks @pan3793 for the implementation. I validated my queries, it went through

@wangyum Does it work after applying the patch? This may block the release of version 4.0.\r\n\r\n

It seems not work. Hive metastore version is 3.1. \r\n[error.log](https://github.com/user-attachments/files/19048917/error.log)\r\n

> It seems not work. Hive metastore version is 3.1. [error.log](https://github.com/user-attachments/files/19048917/error.log)\r\n\r\n@wangyum thank you for testing, the change should only affect client side, it does not matter which version of HMS you used. you can enable the debug log to see if these logs are printed\r\n\r\n```\r\n2025-02-25 15:56:20 DEBUG HiveClientImpl: Trying to set metastore client thrift max message to 1024\r\n2025-02-25 15:56:20 DEBUG HiveClientImpl: Change the current metastore client thrift max message size from 104857600 to 1024\r\n```

It works after adding one config: `--conf spark.hive.thrift.client.max.message.size=1gb`.

Merged into master and branch-4.0. Thanks @pan3793 @wangyum and @Madhukar525722 

ping @yaooqinn cc @dongjoon-hyun @LuciferYang 

> Could you elaborate on this a little more, @beliefer ?\r\n\r\nUpdated.\r\n\r\n

Shouldnâ€˜t the PR scope be `Simplify AvroCompressionCodec by removing defaultCompressionLevel`ï¼Ÿ

> Shouldnâ€˜t the PR scope be `Simplify AvroCompressionCodec by removing defaultCompressionLevel`ï¼Ÿ\r\n\r\nSounds good to me.

@yaooqinn @dongjoon-hyun Thank you!\r\nMerged into 4.0/master.

@mridulm / @LuciferYang - Can you please review when you get a chance ? 

@HyukjinKwon / @dongjoon-hyun - can you take a look at this PR, this is a follow up to https://github.com/apache/spark/pull/49934\r\n\r\n

cc @srowen @dongjoon-hyun 

Merged into branch-4.0/master\r\n@jjayadeep06 @srowen @jayadeep-jayaraman Thank you!

@jjayadeep06 Could you create a backport PR for branch-3.5 ?

> Merged into branch-4.0/master @jjayadeep06 @srowen @jayadeep-jayaraman Thank you!\r\n\r\nThank you @srowen / @beliefer  for a very thorough review!

cc @JoshRosen and @cloud-fan 

> spark submit command builder will set some system variables for MASTER/REMOTE url, connect enablement, etc.\r\n\r\nThis is internal thing so they will have to be documented separately .. I will take a look separately.\r\n\r\n> spark session (both python and scala) will start a temp connect server for the connect api molde with local master.\r\n\r\nThis is documented here https://github.com/apache/spark/blob/master/docs/app-dev-spark-connect.md#spark-api-mode-spark-client-and-spark-classic\r\n\r\n

Yea I know we have a user-facing doc, but I was asking for developer-facing code comments for people to understand how this api mode is implemented.

Let me open a PR tmr separately

Merged to master and branch-4.0.

Thanks @beliefer  ~

Thanks @dongjoon-hyun 

cc. @cloud-fan @viirya Would you mind taking a look? Thanks!

@viirya \r\nWe have a TODO comment to apply DSv2 write to streaming query as well. e.g. df.writeTo(tblName).append() uses AppendData node. Not sure whether we will address this in near future though (this TODO comment seems to be there for years).

Depends on how we address it - here, we assert when executing command, and AppendData is AFAIK a command. So if we just make V2Command work for streaming, valid streaming queries will be routed to the assertion path and we will incorrectly fail the query.\r\n\r\nThough I think we may need a bigger change to address TODO comment, so probably yet to be worried.

I see. Yea, maybe it is too early to worry about it.

I think the root cause is command execution mode set incorrectly in `IncrementalExecution`, see https://github.com/apache/spark/pull/50037

Closing via #50037 - much simpler change and both of PRs do not address the origin report which @cloud-fan will address later.

currently, we have a pre-training model size check, if the model to be trained seems larger, the ml server will directly fail the training.\r\n\r\nregarding model loading, it first load the model into memory, then throw error if its size is large. Probably we should also use a pre-loading model size check.

cc @dongjoon-hyun Could you please take a look at this pr if you have time? Thanks ~\r\n\r\n

Thank you @beliefer  ~

Merging to master

thanks, merging to master!

thanks, merging to master!

thanks, merging to 3.5/4.0/master

@MaxGekk PTAL at this PR when you have time. Thanks

+1, LGTM. Merging to master.\r\nThank you, @mihailoale-db.

nice!

will take a closer look tmr :-)

Yep working on some tests and figuring out the scala client still

Merged to master and branch-4.0.\r\n\r\nThanks @Kimahriman for taking this over!

ping @cloud-fan @MaxGekk @dongjoon-hyun 

Merged into branch-4.0/master\r\n@LuciferYang @cloud-fan Thank you!

cc @sunchao @cloud-fan 

```scala\r\nspark.sql("set spark.sql.autoBroadcastJoinThreshold=-1")\r\nspark.range(10000000).selectExpr("id", "id + 1 as new_id").write.saveAsTable("t1")\r\nspark.range(10).selectExpr("id").write.bucketBy(1, "id").saveAsTable("t2")\r\nspark.sql("select * from t1 join t2 on t1.id = t2.id").explain("cost")\r\n```\r\nSpark 3.2:\r\n```\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- SortMergeJoin [id#21L], [id#23L], Inner\r\n   :- Sort [id#21L ASC NULLS FIRST], false, 0\r\n   :  +- Exchange hashpartitioning(id#21L, 200), ENSURE_REQUIREMENTS, [plan_id=50]\r\n   :     +- Filter isnotnull(id#21L)\r\n   :        +- FileScan parquet default.t1[id#21L,new_id#22L] Batched: true, DataFilters: [isnotnull(id#21L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/Downloads/spark-3.2.4-bin-hadoop3.2/spark-warehous..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint,new_id:bigint>\r\n   +- Sort [id#23L ASC NULLS FIRST], false, 0\r\n      +- Exchange hashpartitioning(id#23L, 200), ENSURE_REQUIREMENTS, [plan_id=57]\r\n         +- Filter isnotnull(id#23L)\r\n            +- FileScan parquet default.t2[id#23L] Batched: true, DataFilters: [isnotnull(id#23L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/Downloads/spark-3.2.4-bin-hadoop3.2/spark-warehous..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>\r\n```\r\n\r\nAfter Spark 3.3:\r\n```\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- SortMergeJoin [id#21L], [id#23L], Inner\r\n   :- Sort [id#21L ASC NULLS FIRST], false, 0\r\n   :  +- Exchange hashpartitioning(id#21L, 1), ENSURE_REQUIREMENTS, [plan_id=51]\r\n   :     +- Filter isnotnull(id#21L)\r\n   :        +- FileScan parquet default.t1[id#21L,new_id#22L] Batched: true, DataFilters: [isnotnull(id#21L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/Downloads/spark-3.3.3-bin-hadoop3/spark-warehouse/t1], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint,new_id:bigint>\r\n   +- Sort [id#23L ASC NULLS FIRST], false, 0\r\n      +- Filter isnotnull(id#23L)\r\n         +- FileScan parquet default.t2[id#23L] Batched: true, Bucketed: true, DataFilters: [isnotnull(id#23L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/Downloads/spark-3.3.3-bin-hadoop3/spark-warehouse/t2], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 1 out of 1\r\n```\r\n

Merged to master.

Merged to master and branch-4.0.

Merged to master and branch-4.0.

Merged to master and branch-4.0.

@xupefei PTAL

Bump :P

This will need an SPIP. Please refer to https://spark.apache.org/improvement-proposals.html

@HeartSaVioR PTAL when you get a chance!

@ericm-db \r\nTest failure looks relevant. Could you please take a look?

I see the PR description is updated. Thanks! Merging to master/4.0.

cc @cloud-fan @dongjoon-hyun 

Compilation failed, probably due to 3.5 using scala 2.12

Hmm, looks like the build failures are in `ShowTablesExec`:\r\n```\r\n[error] /home/runner/work/spark/spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowTablesExec.scala:28:30: object ArrayImplicits is not a member of package org.apache.spark.util\r\n[error] import org.apache.spark.util.ArrayImplicits._\r\n[error]                              ^\r\n[info] done compiling\r\n[info] compiling 11 Scala sources to /home/runner/work/spark/spark/mllib-local/target/scala-2.12/test-classes ...\r\n[error] /home/runner/work/spark/spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowTablesExec.scala:54:57: value toImmutableArraySeq is not a member of Array[String]\r\n[error]         .isTempView((ident.namespace() :+ ident.name()).toImmutableArraySeq)\r\n[error]\r\n```\r\nwhich this PR is not touching. I just rebased and checks are running again.\r\n

@LukasRupprecht can you rebase your PR and try again? The issue should have been resolved.

Thank you for making a PR, @LukasRupprecht .\r\n\r\nIs this target for Apache Spark 3.5.5, @HyukjinKwon and @cloud-fan ?

@dongjoon-hyun yes it is, let me merge it now, thanks all!

Got it. Thank you, @cloud-fan .

+1, LGTM. Merging to master/4.0.\r\nThank you, @vladimirg-db.

@vladimirg-db Is SPARK-48114 correct JIRA?

@MaxGekk updated the jira (SPARK-51248)

Please add this PR to the description of the local variables one, and also edit the description there to better explain the new approach.

thanks, merging to master/4.0!

cc @pan3793 @LuciferYang @dongjoon-hyun \r\n\r\nBTW @dongjoon-hyun do you know how to trigger the CI job to regenerate benchmark results?

LGTM

@cloud-fan  https://github.com/cloud-fan/spark/actions/workflows/benchmark.yml\r\n\r\nManually trigger the benchmark like:\r\n\r\n![image](https://github.com/user-attachments/assets/8f7f8a1e-db02-4a29-881f-d5d09392ba28)\r\n\r\nBoth Java 17 and 21 need to be executed once. Then download the results, and upload them to current pr.

+1, LGTM. Merging to master/4.0.\r\nThank you, @cloud-fan and @LuciferYang @panbingkun for review.

Thank you, @cloud-fan and all!

+1, LGTM. Merging to master/4.0.\r\nThank you, @vladimirg-db.

+1, LGTM. Merging to master/4.0.\r\nThank you, @beliefer and @cloud-fan for review.

@MaxGekk @cloud-fan Thank you!

Merged to master and branch-4.0.

cc @LuciferYang , because there were more changes on branch 3.5 than the master branch, I used a new jira ID.

ping @MaxGekk @dongjoon-hyun @HyukjinKwon @LuciferYang  cc @yaooqinn 

@dongjoon-hyun Thank you!

the current approach works with `spark-submit`\r\n```\r\nspark-submit --conf spark.ml.allowNativeBlas=false ...\r\n```\r\n\r\nuser should modify their Java command options for cases that create embedded `SparkContext` in the Java app\r\n```\r\njava -Dnetlib.allowNativeBlas=false ...\r\n```\r\n

cc @zhengruifeng @panbingkun, could you please take a look? and do you have a better idea of how to implement the configuration?

I think this PR needs reviews from @srowen @WeichenXu123 and @luhenry \r\n\r\n

Thank you for making this PR, @HeartSaVioR .\r\n\r\nI sent an email for further discussion.\r\n- https://lists.apache.org/thread/qwxb21g5xjl7xfp4rozqmg1g0ndfw2jd

Thank you! Sure, @HeartSaVioR .

The very previous commit passed the CI and the last commit is only to change the version number string to `3.5.5` which is verified by manually.\r\n\r\nLet me merge this.

Merged to branch-3.5.

Thank you, @HeartSaVioR .

I sent an email for further discussion\r\n- https://lists.apache.org/thread/qwxb21g5xjl7xfp4rozqmg1g0ndfw2jd

cc. @dongjoon-hyun @HyukjinKwon Please take a look. Thanks!\r\ncc. @cloud-fan for visibility of the fix for blocker issue

* #49984 (4.0)\r\n* #49985 (3.5)

I sent an email for further discussion\r\n- https://lists.apache.org/thread/qwxb21g5xjl7xfp4rozqmg1g0ndfw2jd

@dongjoon-hyun \r\n\r\nLet me clarify a bit.\r\n\r\n1. I have claimed that the config is not something user (even admin) would understand what it is and try to flip. That said, removing this config does not matter to me at all and I\

have we merged this graceful deprecation in branch 3.5?

@cloud-fan \r\n\r\n> have we merged this graceful deprecation in branch 3.5?\r\n\r\nYes, that is merged. It\

Thanks, posted to dev@. - https://lists.apache.org/thread/xzk9729lsmo397crdtk14f74g8cyv4sr

Merged to master and branch-4.0.

ping @MaxGekk @cloud-fan  cc @vitaliili-db 

If this PR works, it needs to be applied to all active branches.

![image](https://github.com/user-attachments/assets/a4df8473-a5d0-4345-8754-473003554bba)\r\nSeems ok.

Merged into master and branch-4.0. Thanks @wayneguow @HyukjinKwon and @beliefer 

Test first

Merged into master. Thanks @HyukjinKwon 

@HeartSaVioR - PTAL, thx !

https://github.com/anishshri-db/spark/actions/runs/13364971025/job/37322247149\r\nFailure is unrelated.

Thanks! Merging to master/4.0.

Thank you, @yaooqinn ! ðŸ˜„ 

Merged to master/4.0.

Thank you, @yaooqinn .\r\nMerged to master/4.0.

LGTM

thanks, merged to master/4.0

Could you review this PR when you have some time, please, @panbingkun ?

Thank you, @yaooqinn !\r\n\r\nMerged to master/4.0.

cc @HyukjinKwon 

Could you review this PR, @itholic ?

Thank you, @zhengruifeng !\r\n\r\nMerged to master for Apache Spark 4.1.0.

Late LGTM. Thanks for addressing this!

Thank you, @itholic .

All tests passed.\r\n\r\n<img width="894" alt="Screenshot 2025-02-16 at 16 12 45" src="https://github.com/user-attachments/assets/2ce6afcf-279b-4dd7-856f-bdcc01069acc" />\r\n

Could you review this PR when you have some time, @LuciferYang ?

Oh, Thank you, @yaooqinn !\r\nMerged to master.

late LGTM

Thank you, @LuciferYang .

@vicennial please see my response

@vrozov I am going to merge this, as it is needed for QA testing. Can you still answer my questions? 

Merging to master/4.0.

thanks, merging to master!

cc @cloud-fan 

Thank you, @cloud-fan . Merged to master/4.0.

cc @cloud-fan 

Thank you, @cloud-fan . Merged to master/4.0.

Merged into master. Thank you @dongjoon-hyun 

Thank you, @LuciferYang !

For the record, `Maven on Java 17` CI passed all tests.\r\n- Java 17: https://github.com/apache/spark/actions/runs/13351893254\r\n\r\n<img width="330" alt="Screenshot 2025-02-15 at 22 44 37" src="https://github.com/user-attachments/assets/cfc41c08-63d6-44d4-8580-72e6d39a362b" />\r\n

and I also think about using Unix Domain Socket instead - I have a draft here: https://github.com/apache/spark/compare/master...HyukjinKwon:spark:SPARK-51156-2?expand=1 but this will likely happen in 4.1

oh if you have another approach, please go ahead and open a PR ðŸ‘ 

Merged into branch-4.0/master\r\n@szehon-ho @dongjoon-hyun Thank you!

Thank you, @beliefer !

Thank you @beliefer and @dongjoon-hyun !

I re-ran the failing test and its green on the build link, but somehow still shows red on the PR.

thanks, merging to master/4.0!

cc @cloud-fan @allisonwang-db 

@HyukjinKwon Please review

Merged to master and branch-4.0.

Related issue in Delta: https://github.com/delta-io/delta/issues/2610\r\ncc: @cloud-fan 

Hey @cloud-fan. Do you think we can merge this PR?

thanks, merging ton master/4.0/3.5!

scala 2.12\r\n![image](https://github.com/user-attachments/assets/e82a7b38-bad0-40a0-95f4-d701f42e0f98)\r\n

thanks, merging to master!

ping @cloud-fan cc @andrej-db 

Merged into branch-4.0/master!\r\n@cloud-fan Thank you!

Late LGTM

merged to master/4.0

In talks with @cloud-fan we decided to leave the pruning and separate dealing with simple queries for a follow-up PR which will aim to optimize Recursive CTEs.

LGTM

Merged to master and branch-4.0.

cc @pan3793 @dongjoon-hyun 

It looks like the assembly module has not been deleted in parent pom?

@yaooqinn `assembly` module is required, just `bigtop-dist`` profile is not necessary.

Merged into master. Thanks @dongjoon-hyun @yaooqinn and @pan3793 

thanks, merged to master/4.0

Could you re-trigger the failed test pipelines? Both failures look like flaky ones.

Merged to master, thank you @dongjoon-hyun 

cc @yaooqinn 

Thank you so much for this suggestion. This becomes much better.

`Document generations` passed at the last commit.\r\nMerged to master.

merged to master/4.0

thanks, merging to master/4.0!

Thanks @dongjoon-hyun , @cloud-fan !

Note, I realize I made a typo in this PR, it should have been:  [SPARK-51208][SQL][FOLLOW-UP]

+1 to put the main implementation idea in the PR description.

Oh, my bad. I was looking old thread only and forgot the recent one. :( 

@zhengruifeng do you know if these spark connect ml test failures are legimate or not?

It seems the run was just cancelled. My suggestion would be to simply re-run.

cool, we can merge it then, thanks!

We are yet to finalize the discussion, sorry, let me make this draft.

Could you review this `Java option` PR when you have some time, @viirya ?

Thank you for review and approval, @viirya !

Merged to master for Apache Spark 4.1.0.

late LGTM

All tests passed.

Could you review this PR when you have some time, @huaxingao ?

Thank you, @huaxingao !

Merged to master for Apache Spark 4.1.0.

Thank you @dongjoon-hyun !

Merged to master/4.0.

With this change, I think we can simplify `CatalogV2Utils.v2ColumnToStructField` to `ColumnDefinition.fromV2Column(col).toV1Column`

https://github.com/apache/spark/pull/49947 is follow up

cc @HyukjinKwon 

Merged to master and branch-4.0.

All tests passed.

Could you review this test-dependency PR when you have some time, @huaxingao ?

Thank you, @huaxingao . Merged to master.

According to the release notes, it is better to go 4.0 too.\r\n\r\n> - Fixes assemblyOutputPath\r\n> - Fixes assemblyExcludedJars\r\n\r\nSpark uses these features, but I am not sure if it is affected, cc @LuciferYang you may have more context.

> According to the release notes, it is better to go 4.0 too.\r\n> \r\n> > * Fixes assemblyOutputPath\r\n> > * Fixes assemblyExcludedJars\r\n> \r\n> Spark uses these features, but I am not sure if it is affected, cc @LuciferYang you may have more context.\r\n\r\nI manually inspected the packaging results of version 4.0, and it seems that the behavior of `assemblyExcludedJars ` is correct, so it appears that there is no need to backport it to branch-4.0.

Thank you for sharing your concerns, @pan3793 .\r\nAnd, Thank you for confirming `branch-4.0` status, @LuciferYang . 

Benchmark jdk17: https://github.com/wayneguow/spark/actions/runs/13310975547\r\nBenchmark jdk21: https://github.com/wayneguow/spark/actions/runs/13310979208

cc @dongjoon-hyun , Could you take a look when you have some time? maybe we can have some discussion.

cc @panbingkun , @cloud-fan , @LuciferYang \r\n\r\nIt seems that we need to make a decision. Are we good with this codeine perf regression of `from_json`?

BTW, Thank you for your active contributions, @wayneguow !

> BTW, Thank you for your active contributions, @wayneguow !\r\n\r\nHappy to contributeï¼ðŸ˜€ Keep goingï¼

> cc @panbingkun , @cloud-fan , @LuciferYang\r\n> \r\n> It seems that we need to make a decision. Are we good with this codeine perf regression of `from_json`?\r\n\r\n@panbingkun Can https://github.com/apache/spark/pull/49573 be completed before the 4.0 release? If so, we can wait until the optimization is finished before refreshing this result. Additionally, if  https://github.com/apache/spark/pull/49573 is completed, will we still need to change `numCols` from 500 to 330?

Nice catch!

@dongjoon-hyun tests passed

@MaxGekk @cloud-fan ptal when you have time. Thanks

thanks, merging to master!

Yes, let me close this to prevent accidental merging. We need to discuss from `master` branch as mentioned in the above.

Please open a PR to `master` branch, @bcheena .

Merged to master and branch-4.0, thank you @dongjoon-hyun 

This PR is for master-only (4.1)

merged to master

cc @HyukjinKwon 

Merged to master and branch-4.0, thank you @dongjoon-hyun 

Thank you, @yaooqinn !

cc @HyukjinKwon 

Merged to master, thank you @dongjoon-hyun 

Thank you, @yaooqinn !

@HeartSaVioR Please review.

@cloud-fan Please review

@HeartSaVioR can you please suggest another committer to review this small PR?

The change LGTM, can we add a test in `DataFrameReaderWriterSuite`?

@cloud-fan Added tests, please check.

@cloud-fan any action on my side before the PR is merged?

@vrozov can you remove the java test? https://github.com/apache/spark/pull/49928#discussion_r1981021185

@cloud-fan @LuciferYang I prefer to keep the test in the java as it does not hurt and \r\n\r\n1. There is similar test in R even though it is not R specific\r\n2. Other tests in `JavaDataFrameReaderWriterSuite.java` are not java specific either

all tests passed.\r\nmerged to master/4.0

LGTM

Merged to master and brnach-4.0.

All K8s tests passed. Merged to master for Apache Spark 4.1.0.

After more testing, I backported this to branch-4.0.

cc @LuciferYang 

Could you review this PR, @LuciferYang ? Technically, there are two CVE patches here.

Thank you always, @LuciferYang !

Thank you, @yaooqinn !

Merged to master/4.0.

cc @dongjoon-hyun @cloud-fan

Merged to master/4.0. Thank you again, @pan3793 .

All tests passed. Merged to master/4.0.

thank you @dongjoon-hyun 

merged to master/4.0

Merged to master and branch-4.0

Merged to master and branch-4.0\r\nThanks @HyukjinKwon @zhengruifeng for the review.

cc @gengliangwang from \r\n- #41632\r\n\r\nAlso, cc @cloud-fan as a release manager of Apache Spark 4.0.0. (Although this PR aims for all live branches, master/branch-4.0/branch-3.5).

thanks, merging to master/4.0!

it conflicts with 3.5, @LukasRupprecht can you open a new 3.5 PR? thanks!

Late +1

I resolved the issue with the Fix Version, 4.0.0, for now.\r\n- https://issues.apache.org/jira/browse/SPARK-51185\r\n\r\n<img width="877" alt="Screenshot 2025-02-13 at 12 47 03" src="https://github.com/user-attachments/assets/8b845e5e-7a48-4be7-9927-f1150eb21b02" />\r\n

Thanks @cloud-fan for merging this! Will prepare a separate PR for 3.5.

@cloud-fan @dongjoon-hyun Here is the 3.5 version of this PR: https://github.com/apache/spark/pull/49995.

Could you review this PR when you have some time, @viirya ?

Looks good to me. Thanks @dongjoon-hyun 

Thank you so much, @viirya !

Merged to master for Apache Spark 4.1.0.

Merged to branch-3.5. Thank you, @jonathan-albrecht-ibm and @MaxGekk .

Probably not needed to go to 3.5..

BTW, are you sure about this, @WweiL ? For me, this looks like a wrong claim.\r\n\r\n![Screenshot 2025-02-12 at 11 38 12](https://github.com/user-attachments/assets/0f857fb9-d27d-4abb-95b2-a8a63e9c44ae)\r\n

@dongjoon-hyun I checked again the original PR and everything else seems fine. I also searched if there were some other occurrence of "class   " and didn\

cc @cloud-fan , @pan3793, @gene-db 

I\

> I\

The Spark community had agreed to move the spec over to the parquet project, so we are currently in that process. It takes time for the spec to be formally finalized in Parquet, but it is definitely getting closer. The implementation in Spark is following the updated specs, and since the specs will be living in the parquet repo, it doesn\

What are the possible paths forward? We are currently in the transition period, so the spec is not fully finalized in parquet. Therefore, the options are:\r\n- point to the (not finalized) Variant spec in Parquet (what this PR is doing)\r\n- keep the Spark README as is (it is currently out-of-date)\r\n- copy the text of the Variant spec in Parquet into the Spark README\r\n\r\nWe thought it would be best to link to the parquet docs, since it is going to be the source-of-truth moving forward, it reflects the current state of the implementation and will avoid confusion.\r\n\r\nWhat do others think?

Hi @dongjoon-hyun, I updated the text to clarify the current status of the project. Let me know what you think.

Is anything else required on this PR? If not, @dongjoon-hyun or @cloud-fan, can one of you please merge it?

thanks, merging to master/4.0!

For some reason, the workflow gets forbidden (403) when uploading the image.

@dongjoon-hyun: I will look the settings for the fork.\r\n\r\nI will update the PR and the ticket on JIRA, and probably hold off until Arrow 19.0.1 since 19.0.0 is not on central maven, and has a small issue which will be corrected on 19.0.1.

cc @LuciferYang who is also working on arrow upgrade

Starting from 18.2.0, the Java version of Arrow has been moved to a separate repository and is now released independently\r\n\r\n- https://github.com/apache/arrow/issues/45364\r\n- https://github.com/apache/arrow-java\r\n- https://github.com/apache/arrow-java/releases/tag/v18.2.0\r\n\r\n19.0.0 of Arrow Java has not yet been released now ...

Just a question. Is there any update because we have 19.0.1 already, @aimtsou and @LuciferYang ?\r\n\r\nhttps://arrow.apache.org/release/19.0.1.html

> Just a question. Is there any update because we have 19.0.1 already, @aimtsou and @LuciferYang ?\r\n> \r\n> https://arrow.apache.org/release/19.0.1.html\r\n\r\nNo, the latest `arrow-java` is still at version 18.2.0\r\n\r\nhttps://github.com/apache/arrow-java/releases

ping @cloud-fan cc @mikhailnik-db 

@cloud-fan Could you take a review again?

Merged into branch-4.0/master\r\n@cloud-fan @dongjoon-hyun @mikhailnik-db Thanks!

cc @dongjoon-hyun, and thanks for reminding 

What do we get from that deprecation procedural when we no longer have that config in very next (technically) minor release? If you think we need to follow that deprecation, this has to be there for a couple minor releases, otherwise it sounds to me just adding more work without outcome. Users do not upgrade every single version release.

> this has to be there for a couple minor releases, otherwise it sounds to me just adding more work without outcome.\r\n\r\nAre you aware of the meaning of `.withAlternative("spark.databricks.sql.optimizer.pruneFiltersCanPruneStreamingSubplan")`, @HeartSaVioR ?

What I meant is, don\

Apache Spark 4.0.0 RC1 is scheduled in three days (2025-02-15). So, we are on the same page on `master` and `branch-4.0`.\r\n> What I meant is, don\

cc @cloud-fan 

Technically, I aim not to block 4.0.0 release and do 3.5.5 after 4.0.0 release immediately. That was my best compromised idea. But, you are right, @HeartSaVioR .

I actually also prefer to just remove this .... 

Thank you for closing this.

Is this ready, @LuciferYang ?

Oh, is it marked as `Improvement`? Then, merged to master only~

Thank you @dongjoon-hyun  ~

Merged to master.

Merged to master/4.0. Thank you, @zhengruifeng and @HyukjinKwon .

thanks, merged to master/4.0

cc @HyukjinKwon and @LuciferYang 

Thank you, @HyukjinKwon .

Let me rebase this PR.

Thank you, @LuciferYang . Scala Linter passed.\r\n\r\n<img width="182" alt="Screenshot 2025-02-11 at 18 43 58" src="https://github.com/user-attachments/assets/4cebd329-ad64-44fa-af47-c48c032e009c" />\r\n\r\nLet me merge this.

While preparing Apache Spark 3.5.5, I backported this rule to branch-3.5 as a release manager in order to prevent a future regression.

Merged to master. Thank you, @wayneguow and all!

Merged to master/4.0/3.5.

Thank you again, @cnauroth , and sorry for the trouble on Maven testing. :)

@dongjoon-hyun , thanks for the quick approval and merge. No worries!

Could you file a new JIRA issue for this because this should have a fix version `3.5.5`?

uhoh

sure, will do 

Hmmmm .. @HeartSaVioR WDYT? should we only make this change in master branch alone?

Merged to master and branch-4.0.

There are two cases:\r\n\r\n1. The streaming query has started from Spark 3.5.4\r\n2. The streaming query has started before Spark 3.5.4, and had migrated to Spark 3.5.4\r\n\r\n1>\r\nWhen they start the new query in Spark 3.5.4, there is no offset log to read the static config back from, so the value of config `spark.databricks.sql.optimizer.pruneFiltersCanPruneStreamingSubplan` will follow the default value, `true`.\r\n\r\nThis will be written back to offset log to ensure this value to be kept on streaming query lifecycle.\r\n\r\nWhen they upgrade to the Spark version which we renamed the config, there is offset log to read the static config back, and there is no entry for `spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan`, hence we enable backward compatibility mode and put the value `false` for the config `spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan`.\r\n\r\nThis could break the query if the rule impacts the query, because the effectiveness of the fix is flipped.\r\n\r\n2>\r\nWhen they upgrade their existing streaming query from older version to Spark 3.5.4, there is offset log to read the static config back from, and there is no entry for `spark.databricks.sql.optimizer.pruneFiltersCanPruneStreamingSubplan`, hence we enable backward compatibility mode and put the value `false` for the config `spark.databricks.sql.optimizer.pruneFiltersCanPruneStreamingSubplan`. So the fix is disabled.\r\n\r\nWhen they further upgrade this query to the Spark version which we renamed the config, same thing applies and the fix is disabled. So no change.\r\n\r\nI feel like this is a sort of "dead end" because if we don\

Also a tiny fix if folks can merge \r\n@HeartSaVioR @HyukjinKwon @dongjoon-hyun \r\nhttps://github.com/apache/spark/pull/49911

Merged to master and branch-4.0.

Merged to master. Thank you, @ueshin and @HyukjinKwon .

Could you review this PR, @LuciferYang ?

Thank you so much, @LuciferYang !\r\n\r\nMerged to master for Apache Spark 4.1.0.

Thank you, @cnauroth !

Could you review this when you have some time, @sunchao ?

Thank you so much, @sunchao !

Merged to master/4.0/3.5.\r\n\r\nThank you again, @cnauroth and @sunchao .

@dongjoon-hyun and @sunchao , FYI, this might need a small follow-up: #49898 . Thank you.

Ack, @cnauroth .

cc @HyukjinKwon @LuciferYang 

Also, cc @hvanhovell , too.

@dongjoon-hyun Thanks for checking the GHA result. The UT classpath is different from the packaged classpath, I tuned the POM to ensure both are fine. (test steps are updated in PR desc)

Thank you for updating the PR, @pan3793 ! ðŸ‘ðŸ» 

@dongjoon-hyun the desc is up to date except for `spark-connect-client-jvm_2.13-4.1.0-SNAPSHOT.jar` because I compared `sql/connect/client/jvm/target/connect-repl` previously. I updated the desc to compare `assembly/target/scala-2.13/jars/connect-repl` now.\r\n\r\nBTW, I use `build/mvn -DskipTests clean package -pl assembly -am` to make the package.

Thank you for the confirmation, @pan3793 ! \r\nMerged to master/4.0.

What about the Windows scripts? (I may not be able to help with that because I am not familiar with Windows scripts nor have Windows env)

thanks for the review, merging to master/4.0!

@antban Thanks for contributing. Could you create a JIRA ticket and add it to the PR title?

Merged into master and branch-4.0. Thanks @beliefer and @HyukjinKwon 

@LuciferYang @HyukjinKwon Thank you!

cc @cloud-fan @LuciferYang 

Does awk fix the issue? https://github.com/apache/spark/pull/49891

close and in favor SPARK-51146

thanks, merging to master!

cc @cloud-fan @HyukjinKwon @dongjoon-hyun @LuciferYang 

LGTM

thank you all\r\n\r\nmerged to master/4.0

cc @HyukjinKwon @dongjoon-hyun 

thanks for the review, merging to master/4.0

cc @HyukjinKwon 

Merged to master/4.0.

Thanks @dongjoon-hyun 

Merged to master/4.0.0\r\n\r\nThank you @dongjoon-hyun @beliefer @HyukjinKwon @zhengruifeng 

To @yaooqinn , do we need this in `branch-3.5`?

@dongjoon-hyun shall we get this `[ML][CONNECT]` one in 4.0? I should only affect ml connect

Merged to master/4.0. Thank you all!

cc @cloud-fan 

The failed tests are unrelated, thanks, merging to master/4.0!

(and backward compat will be tested in the scheduled build )

Can `AccessTokenCallCredentials` added in SPARK-42533 protect the spark local connect server?

This can secure individual connections themselves if I am not wrong .. but the problem is that any user can make a connection to the running server

@HyukjinKwon can you make sure we are use SSL/TLS in this case. Otherwise it will be kind of easy to intercept the token.

let me take a look

Some tests might fail ... need to go sleep .. but should be able to review this. I addressed most of major comments.

I am back. I am going back to work on this.

https://github.com/apache/spark/pull/50006 replaces this. Thanks @Kimahriman 

cc @cloud-fan and @HyukjinKwon 

There is a confusion in the usage of `string` in many functions, some string inputs are treated as literal strings, some are treated as column names.\r\n\r\nWith a column input, it is easy to clearly resolve such confusion, sf.col("a") vs sf.lit("a")

I disagree with using string in a method for different meanings can cause confusion when it has clear declaration/signatures

https://github.com/apache/spark/pull/49879#issuecomment-2650528939\r\n\r\n@yaooqinn The problem is that spark doesn\

Thank you, @yaooqinn .

Merged to master/4.0. Thank you again.

Thank you for the review and commit, @dongjoon-hyun !

It comes from https://github.com/apache/spark/pull/22081/files#r209707448, but after #49854 , it seems that nowhere depends on this.

`OracleIntegrationSuite` failure is irrelevant to this PR.\r\nMerged to master/4.0

Please help review it when you have free time, thanks! @panbingkun 

Are there any other examples of `wide characters` that need to be shown? \r\nAdditionally, the `PySpark example` also needs to be updated.

No, I have added a more comprehensive example. The PySpark example has been updated.

Welcome to the Apache Spark community,\xa0@fusheng9399 !\r\nI added you to the Apache Spark contributor group and assigned\xa0[SPARK-51152](https://issues.apache.org/jira/browse/SPARK-51152)\xa0to you.\r\n

Thanks for the review @HyukjinKwon @zhengruifeng.\r\nMerging to master!

Thanks all @panbingkun @HyukjinKwon @zhengruifeng

cc @chenhao-db, @cashmand , @gene-db , @cloud-fan 

Merged to master/4.0. Thank you, @pan3793 .

The implementation currently merged into Spark master and Spark 4.0 is already following the [updated Variant spec](https://github.com/apache/parquet-format/pull/461). We have been working closely with the community for the spec and Spark implementation.

@pan3793 can you elaborate on the incompatibilities between the spark variant implementation and the new parquet spec?

@gene-db @cloud-fan\r\nI read the following docs to learn the new added variant feature, and haven\

Closing this PR as we found naming of `None` brings in naming collisions and we should keep TimeMode.None() as notime internally.

thanks, merging to master/4.0/3.5

The python side is kind of complicated (mixture of sc, connect session, classic session), I will resolve it in separate PRs

thanks, merged to master for 4.1

Seems you can use `spark-submit --verbose ...` to rerun the failed cases to know the classpath.\r\n\r\nI think verbose error messages should help administrators diagnose the issue, but they might also scare users. Keeping the default error message clear and short, and providing a `--verbose` might be a good balance.

@dongjoon-hyun Please review

@dongjoon-hyun Please review

@dongjoon-hyun Please review or advise who may review the PR?

Still trying to find a reviewer for this PR. @hvanhovell can you suggest one?

@hvanhovell ?

BTW, you also can revert directly from `branch-4.0` and push it in this case, @zhengruifeng ~

merged to 4.0

Thank you for your swift action!

Thank you, @HyukjinKwon !

Merged to master~

Thanks! Merging to master/4.0.

Merged to branch-3.5 for Apache Spark 3.5.5. Thank you again, @jonathan-albrecht-ibm .

cc @dongjoon-hyun @HyukjinKwon 

thanks for the review, merging to master/4.0!

also cc @cloud-fan we probably need to pin `plotly` in the release docker image

update in the docker file trigger the refresh of the cache, and `torch` is also upgraded and caused \r\n\r\n```\r\n======================================================================\r\nERROR [42.116s]: test_save_load (pyspark.ml.tests.connect.test_connect_classification.ClassificationTestsOnConnect.test_save_load)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File "/__w/spark/spark/python/pyspark/ml/tests/connect/test_legacy_mode_classification.py", line 185, in test_save_load\r\n    lor_torch_model = torch.load(\r\n                      ^^^^^^^^^^^\r\n  File "/usr/local/lib/python3.11/dist-packages/torch/serialization.py", line 1470, in load\r\n    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\r\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint. \r\n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\r\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\r\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.nn.modules.container.Sequential was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Sequential])` or the `torch.serialization.safe_globals([Sequential])` context manager to allowlist this global if you trust this class/function.\r\n\r\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\r\n```

set to draft first to avoid accidental merging due to the existence of build failures

Merged to master and branch-4.0.

cc @grundprinzip and @wbo4958 

The failed protobuf breaking change test is expected, we will merge this also in 4.0

I am going to merge this into 4.0, otherwise the protobuf compatibility between 4.0 and 4.1 will be broken

merged to master/4.0

thanks, merged to master/4.0

Hi, @zhengruifeng . May I ask why you backport this improvement?\r\n\r\n<img width="611" alt="Screenshot 2025-02-10 at 16 15 45" src="https://github.com/user-attachments/assets/775dcdd9-8bc2-4425-a961-fbef1dfacefb" />\r\n\r\nIf you don\

@dongjoon-hyun ah, sorry, shall we revet it in 4.0?

thanks, merged to master/4.0

cc @xinrong-meng mind taking a look please? I think the plot tests fail with dependencies with different versions.

cc @zhengruifeng too

Merged to master and branch-4.0.

This failure also happens in https://github.com/apache/spark/actions/workflows/build_python_3.11_macos.yml\r\n\r\nafter check the history, I think it is due to `plotly` upgrade\r\n\r\n@LuciferYang and @HyukjinKwon 

If approved, can this also go into branch-3.5 please? The cherry-pick would need a minor merge conflict resolution in `FsHistoryProvider` import statements, or I can send a separate pull request.

Could you rebase this PR to the `master` branch, @cnauroth ?

@dongjoon-hyun , I rebased to current master and removed my test helper. Thank you.

All `core` module tests passed. Merged to master for Apache Spark 4.1.0.

Great, really appreciate your efforts on testing strategy for this change! Thank you, @dongjoon-hyun .

thanks, merging to master/4.0!

Merged into master. Thanks @wayneguow 

It depends on upgrade of JPMML.

Manually exported PMML models([pmml_export_models.zip](https://github.com/user-attachments/files/18716895/pmml_export_models.zip)) between Spark 3.5.4 and the current PR:\r\n\r\n1. Kmeans\r\nSpark 3.5.4 vs Current master branch with this PR:\r\n![image](https://github.com/user-attachments/assets/96d4d196-e12e-404c-8013-52e754b020c4)\r\n\r\n2. LinearRegression\r\nSpark 3.5.4 vs Current master branch with this PR:\r\n![image](https://github.com/user-attachments/assets/2705c8bf-547c-44cf-ae44-f980dbe29b74)\r\n\r\n

@zhengruifeng  For comment form https://github.com/apache/spark/pull/48611#issuecomment-2638819267\r\n\r\n> adding additional tests to make sure pmml-model 1.7.1 can successfully load previous models\r\n\r\nAfter my confirmation, currently `Spark ML` can only export models in PMML format, but currently does not support reloading models from PMML format.\r\n\r\nAlthough there is `PMMLUtils` tool class in Spark that can load models from PMML format (https://github.com/apache/spark/blob/master/mllib/src/test/scala/org/apache/spark/ml/util/PMMLUtils.scala#L36), but I think it is meaningless to use `JPMML` to verify the load of PMML, it is more about verifying the compatibility of `JPMML`. In fact, what we need to verify is that the PMML model exported by Spark can be reloaded on other downstream platforms or systems.\r\n\r\nSo, here I did some manual checks, which is what the previous comment mentioned.

Also cc @vruusmann , if you have time, would you like to take a look at the version upgrade of JPMML for Spark 4.0.0? Does it have some potential compatibility risks?

A quick memory dump on JPMML-Model evolution:\r\n\r\n- `1.5.X`. Migrating from PMML schema 4.3 to 4.4. JDK 8 compatible.\r\n- `1.6.X`. API upgrades. Specifically, replacing `org.dmg.pmml.FieldName` (pseudo-enum for representing field names) with plain `java.lang.String`. Also, migrating from `javax.xml.bind.*` API to `jakarta.xml.bind.*` API. JDK 8 compatible.\r\n- `1.7.X`. Upgrading from JDK 8 to JDK 11. The codebase is effectively unchanged. Just picking up up-to-date dependencies.\r\n\r\nThe `org.glassfish.jaxb:jaxb-*` dependencies are chosen based on their JDK compaibility. For example, `org.glassfish.jaxb:jaxb-runtime:3.X` versions are targeting JDK 8, whereas `org.glassfish.jaxb:jaxb-runtime:4.X` are targeting JDK 11. Apache Spark 4.X is targeting JDK 17, no?\r\n\r\nAs for encoding PMML schema versions and XML namespaces, then there\

> Apache Spark 4.X is targeting JDK 17, no?\r\n\r\nYes, Spark 4.x is targeting JDK 17.\r\n\r\nhttps://github.com/apache/spark/blob/301b666a1fcbd4c59d96c53fe3a547ea1512f397/pom.xml#L117

@vruusmann  Thank you for sharing these views, much appreciated.

The JPMML upgrade and related code changes LGTM.\r\n\r\nalso ping @LuciferYang and @dongjoon-hyun

Should the change be mentioned in the migration guide? Otherwise LGTM

> Should the change be mentioned in the migration guide? Otherwise LGTM\r\n\r\nAdd a change description in `ml-migration-guide.md`.

> cc @zhengruifeng\r\n\r\nI have approved https://github.com/apache/spark/pull/49854#issuecomment-2646737700, not sure why I am not in the `Reviewers` list :)

Oh, ya. Sorry, I missed that, @zhengruifeng .  :) 

Merged into master and branch-4.0. \r\nThanks @wayneguow @zhengruifeng @dongjoon-hyun @pan3793 @vruusmann

Thank you all! Also, those two annoying Jersey warning logs are now gone.

The code tab switching is now working https://spark.apache.org/docs/latest/quick-start.html after the changes of this PR. Merging to master/4.0/3.5

Adding @LuciferYang, @cloud-fan

+1, LGTM. Merging to master/4.0.\r\nThank you, @miland-db and @dusantism-db for review.

cc @dongjoon-hyun @HyukjinKwon 

The streaming test failure is unrelated, thanks for the review, merging to master/4.0!

Adding reviewers @cloud-fan, @davidm-db, @dejankrak-db, @milastdbx, @MaxGekk 

+1, LGTM. Merging to master/4.0.\r\nThank you, @miland-db and @cloud-fan for review.

cc @wbo4958 and @HyukjinKwon 

merged to master/4.0

cc @HyukjinKwon 

Merged into master and branch-4.0. Thanks @HyukjinKwon \r\n\r\nI will re-enable them after finding a better local testing method.

cc @HyukjinKwon and @wbo4958 

LGTM, can we add an example (like below) to user-facing change in pr description? Thank you!\r\n```\r\n        data = [\r\n            [Vectors.dense([1, 0, 0, -2]), 1.0],\r\n            [Vectors.dense([4, 5, 0, 3]), 2.0],\r\n            [Vectors.dense([6, 7, 0, 8]), 1.0],\r\n            [Vectors.dense([9, 0, 0, 1]), 1.0],\r\n        ]\r\n        df = spark.createDataFrame(data, ["features", "weight"])\r\n\r\n        summarizer = Summarizer.metrics("mean", "count")\r\n        res1 = df.select(summarizer.summary(df.features))\r\n```\r\n

thanks, merged to master/4.0

Merged to master and branch-4.0.

This is not the right fix?

@LuciferYang made a PR first :-) https://github.com/apache/spark/pull/49827

merging to master/4.0/3.5

The remaining test failures are not related to this PR.

Thanks! merging to master/4.0.

@dongjoon-hyun Oops, my bad. I thought we still had time.\r\nLet me revert it from 4.0, and submit a follow-up PR to update the `version` for the config.

If you think this could be a bug fix, you can update JIRA issue type too, @ueshin .

Thank you! Feel free to choose what is required for you.

Got it. Thank you so much!

Reverted https://github.com/apache/spark/commit/a94faa738fcb602df6d641a79bf2a168224a52f5. 

Also the follow-up https://github.com/apache/spark/pull/49895.

linter and doc gen passed\r\n\r\nmeregd to master/4.0

Had a chat offline with @cloud-fan who suggest simplifying the analyzeExistence to be just the following bare bones code to resolve functions.\r\n\r\n```\r\n  def fromSQL(sql: String): Expression = {\r\n    CatalystSqlParser.parseExpression(sql).transformUp {\r\n      case u: UnresolvedFunction =>\r\n          assert(u.nameParts.length == 1)\r\n          assert(!u.isDistinct)\r\n          FunctionRegistry.builtin.lookupFunction(FunctionIdentifier(u.nameParts.head), u.arguments)\r\n    }\r\n  }\r\n  ```\r\n  \r\n  Thanks for the suggestion, this simplifies it a lot.

According to the affected version, I merged this to master/4.0.

Thanks @cloud-fan @dtenedor @dongjoon-hyun !

Synced with @cloud-fan offline, the current code should work for this case.  Made follow up https://github.com/apache/spark/pull/49881 to do some cleanup to put the logic in the right place.

LGTM thank you!

The remaining test failures are not related to this PR.

Thanks! merging to master/4.0.

@viirya thanks for the reviews. I am merging this one to master/branch-4.0/branch-3.5

Please specify that this is for single-pass Analyzer in the PR description.

Hi @MaxGekk could you please review when you have time? Thanks

LGTM

The AQE test failure is unrelated, thanks, merging to master!

Merging to master/branch-4.0

@dtenedor @cloud-fan any ideas why the SQL Parser behaves like that in the first place? I mean, I found a fix (it kinda works the same as what we do for regular queries), but not sure what causes the original problem.

I will add more tests for `EXECUTE IMMEDIATE`.

`spark.conf.set("spark.sql.ansi.enforceReservedKeywords", "true")` mitigates the problem, since UNION becomes a reserved keyword.

> `spark.conf.set("spark.sql.ansi.enforceReservedKeywords", "true")` mitigates the problem, since UNION becomes a reserved keyword.\r\n\r\nTrue, a rather hefty mitigation, though. Lots of collateral damage.

@dongjoon-hyun I cannot repro it. Let me rebase to the latest master.\r\n\r\n![image](https://github.com/user-attachments/assets/f16913ee-74ac-417d-abac-9dffb5eefa31)\r\n

Thank you, @vladimirg-db .

thanks, merging to master/4.0!

To @cloud-fan and @vladimirg-db ,  both the `master` and `branch-4.0` branch are broken due to the same test failure which I reported in the above. Let me revert them to recover the CI.\r\n\r\n- `master`: https://github.com/apache/spark/actions/runs/13289860447/job/37107724664\r\n- `branch-4.0`: https://github.com/apache/spark/actions/runs/13289866609/job/37107705375\r\n```\r\n[info] *** 1 TEST FAILED ***\r\n[error] Failed tests:\r\n[error] \torg.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite\r\n```

This is reverted to recover the CIs and to unblock other preparation tasks for Apache Spark 4.0.0.\r\n- `master`:  https://github.com/apache/spark/commit/6440bdec8af64f0d6b55d53eacb2591eaf8eb859\r\n- `branch-4.0`: https://github.com/apache/spark/commit/478c33b29cda02751b1a26f4e8c3c5143a537893

@dongjoon-hyun sorry for that.. Thanks for the revert. Let me fix this.

According to the CI result, not only `hive` module, but also `sql` module also failed. You may want to see `sql` module first because it\

oh sorry somehow I thought the CI passed and merged it...

Merged to master and branch-4.0.

Could you please help me review the code again? @MaxGekk 

Please help review it when you have free time, thanks! @LuciferYang\r\ncc @cloud-fan

It can also reproduce locally (my computer has 12 cores , so I create create 13 DirectStream)\r\ndemo:\r\n[KafkaJobTest.txt](https://github.com/user-attachments/files/18693310/KafkaJobTest.txt)\r\n\r\n

@tdas Hi~ could you help review this, thanks.

cc @peter-toth @gengliangwang 

thanks for the review! merging to master/4.0!

merged to 4.0

ohh okay you made a followup here

Merged to master and branch-4.0.

Thanks @HyukjinKwon 

https://github.com/apache/spark/actions/runs/13200420613\r\n\r\n![image](https://github.com/user-attachments/assets/860ff6b5-d122-4d8b-930e-7437cd7bf284)\r\n\r\nThe latest Maven daily test has been restored to normal

Although the failure should not be related to the current pr, could you please retrigger the two failed GA tasks? @RocMarshal  thanks

> Although the failure should not be related to the current pr, could you please retrigger the two failed GA tasks? @RocMarshal thanks\r\n\r\n\r\nhi, @LuciferYang \r\nI retriggered its. It seems to have run successfully~\r\n\r\n

Merged into master. Thanks @RocMarshal 

Thank you @dongjoon-hyun 

merged to master

Note: after this PR, there is still one error:\r\n```\r\nRefused to load the script \

Not to distract from the main purpose of this PR, but are you able to build the PySpark API docs locally without hitting memory errors? (Mentioning this only because it was part of my email thread reporting the broken CSS link.)

@LuciferYang @HyukjinKwon @dongjoon-hyun @nchammas thanks for the reviews. I am merging this one to master/branch-4.0/branch-3.5

LGTM late.

Merged to master and branch-4.0 thanks!

cc @HyukjinKwon would you mind taking another look?

LGTM thank you!

thanks @xinrong-meng and @HyukjinKwon \r\n\r\nmerged to master

thank you for the fix! @HyukjinKwon 

Merged to master and branch-4.0.

cc @wbo4958 @HyukjinKwon 

thanks, merged to master/4.0

Tests seem passed but showing `Error: The operation was canceled.`

The remaining failures are not related to this PR.

Thanks! merging to master/4.0.

Merged to master and branch-4.0.

I thought we were creating a new metric for the snapshot delta lag, not the last uploaded version? Did I miss something, why did we change this? Now we need to manually compute the lag right?

This was more of an offline discussion, but I think the delta is implied through oldest snapshots as well, so we can also use this to identify problematic partitions

@zecookiez Looks good. Thanks for the work here.

Thanks! Merging to master/4.0.

Good catch, just updated the PR description to match what we have now. Thanks!

@ericm-db \r\nhttps://github.com/ericm-db/spark/actions/runs/13167171290/job/36750002304\r\n\r\nLooks like failure happens from refactored suite?

The CI failure is not relevant - it only failed from pyspark-pandas and this PR only touched "Scala tests".

Thanks! Merging to master/4.0.

This must be:\r\nhttps://github.com/sririshindra/spark/actions/runs/13162571866/job/36749883818#step:13:5783\r\n\r\n```\r\n[error] /__w/spark/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala:53:0: org.apache.spark.internal.config.APP_CALLER_CONTEXT is in wrong order relative to org.apache.spark.internal.config.Tests.IS_TESTING.\r\n```

\r\n\r\n\r\n> Note that due to recent changes in #49893 and #49898 , it should be possible to write a reliable unit test for this. Otherwise, I am +1 (non-binding).\r\n\r\nThanks @cnauroth . I now added a unit test as well. \r\n\r\n

@attilapiros , @dongjoon-hyun, @sunchao  Can you please review this PR when you get a chance. Thanks.

Hi @dongjoon-hyun, just wanted to check if you have some time to look at my comments above

+1, LGTM. Merging to master/4.0.\r\nThank you, @jonathan-albrecht-ibm.

@MaxGekk Thanks for reviewing and merging!

@HeartSaVioR, I have updated this PR to just skip the failing v1 tests on big endian platforms

Thanks! Merging to master/4.0.

@HeartSaVioR, thanks for reviewing and merging!

cc @wbo4958 @HyukjinKwon 

A minor comment. Overall, LGTM.\r\n

merged to master/4.0

LGTM

Merged to master and branch-4.0.

merged to master/4.0

Merged to master and branch-4.0.

merged to master/4.0

merged to master/4.0

merged to master/4.0

Merging to master/4.0

Merging to 4.0/master.

merged to master/4.0

I forgot to enable the parity tests, will fix it in https://github.com/apache/spark/pull/49802

Merged to master and branch-4.0.

Good point, how do you like the current approach? @ueshin 

Thank you @ueshin ! Now it raises a warning when the client/driver does not have memory-profiler and there is no result profile

Merged to master and branch-4.0 thanks!

@neilramaswamy - is the PR status still WIP ? if not - lets remove that from the title ?

Addressed, thanks @anishshri-db!

thanks, merging to master/4.0!

@MaxGekk There are some cases in `column-resolution-aggregate.sql` related to group by alias but I feel uncomfortable with deleting them as they are also `column resolution` related. Should we just copy them (without deleting) to the `group-by-alias.sql` as there are not many of them?

+1, LGTM. Merging to master/4.0.\r\nThank you, @mihailoale-db and @beliefer for review.

thanks, merging to master/4.0!

merged to master/4.0\r\n\r\nwe can fix the save/load later

thanks, merged to master/4.0

@Kimahriman FYI

Seems like panads <> Arrow conversion is fixed in their releases

cc @ueshin @zhengruifeng too

Should be ready to go once it passes

Merged to master and branch-4.0.

merged to master/4.0

Merged to master and branch-4.0.

Merged to master and branch-4.0.

I am going to fix it in https://github.com/apache/spark/pull/49789

cc @wbo4958 @HyukjinKwon 

merged to master/4.0

@HyukjinKwon @ueshin would you please review? Thank you!

```\r\norg.apache.spark.sql.streaming.FlatMapGroupsWithStateWithInitialStateSuite\r\n\r\nflatMapGroupsWithState - initial state and initial batch have same keys and skipEmittingInitialStateKeys=true\r\n```\r\nfailed in the PR builder but passed locally\r\n```\r\n15:47:42.778 WARN org.apache.spark.sql.streaming.FlatMapGroupsWithStateWithInitialStateSuite: \r\n...\r\n[info] Total number of tests run: 31\r\n[info] Suites: completed 1, aborted 0\r\n[info] Tests: succeeded 31, failed 0, canceled 0, ignored 0, pending 0\r\n[info] All tests passed.\r\n[success] Total time: 168 s (02:48), completed Feb 4, 2025, 3:47:43 PM\r\nsbt:spark-parent> \r\n```

Merged to master and branch-4.0, thank you!

Merging to master/4.0

Merged to master.

Thanks! merging to master/4.0.

Thank you!!

Hi, @yaooqinn . WDYT about this?

Thank you, @yaooqinn and @HyukjinKwon .\r\n\r\nMerged to master for Apache Spark 4.1.0.

@cnauroth well, you should -if you can get it anywhere into your logs, possibly as a new http header. s3afs attaches as an http referrer as it is the sole entry other than UA which goes into the standard S3 logs -and other things like to set that UA field.

Got it. Please let me know when the PR is ready, @steveloughran .

thanks, merging to master

Could you review this PR, @viirya , when you have some time ?

Thanks for recovering CI.

Thank you so much, @viirya !\r\n\r\nMerged to branch-3.5.

cc @LuciferYang , too

> cc @LuciferYang , too\r\n\r\nThanks for pinging me @dongjoon-hyun \r\n\r\nlate LGTM, thanks to @cryeo  for the fix.

Requesting feedback from @maropu  @HyukjinKwon @imback82 @Ngone51 \r\nI will appreciate your thoughts and feedback.\r\n

cc @cloud-fan as a release manager.

Merging to master/4.0.

@cloud-fan, @stefankandic, please take a look - this is just a revert of PR https://github.com/apache/spark/pull/48962, as we decided not to proceed with session level collations for now, and will do a follow up to apply object level collations for queries.

> For the other audience, could you provide a link for this decision, @dejankrak-db ?\r\n> \r\n> > The decision has since been made not to ship this functionality for now,\r\n\r\n@dongjoon-hyun , there are 2 main reasons for this decision:\r\n1) There were some unresolved technical issues when attempting to merge the original PR functionality on Delta side, due to its effect on DML queries when changing the underlying collation in this way.\r\n2) As per customer feedback gathered so far, object level collation is much more requested functionality, whereas there were no explicit requests for default session level collation so far, hence the focus has shifted to introducing the resolution of object level collation for DDL queries instead, allowing the collation to be specified per table or view on their creation or modification, with propagating the default collation specified to subsequent queries on top of those entities.\r\n\r\nTherefore, it was decided to pause session level collation functionality for now, thus partially reverting unused parts of the original PR for maintaining a cleaner code moving forward, while still keeping other parts required to support object level collation resolution. Hope this clarifies the reasoning well! I have also updated the PR description with this info, thanks!

@stefankandic, when you find some time please take a look at the latest logic for DDL collation resolution as well as removing the DML collation resolution entirely, as discussed.

thanks, merging to master/4.0!

Overall, LGTM.

merged to master/4.0

merged to master/4.0

@neilramaswamy @HeartSaVioR - any thoughts on this before we finalize for 4.0 ?

> Could you fix the CI failures ?\r\n\r\nYup done - should be good now

Thanks! Merging to master/4.0.

merged to master/4.0

merged to master/4.0

What do you think about this change, @sunchao and @szehon-ho ?

Thank you, @sunchao and @viirya .\r\nMerged to master for Apache Spark 4.1.0.

Merged to master and branch-4.0.

Thank you, @HyukjinKwon !

merged to master/4.0

cc @wbo4958 @grundprinzip @HyukjinKwon 

merged to master/4.0

Could you review this when you have some time, @LuciferYang , please?

Could you review this PR when you have some time, @HyukjinKwon ?

Could you review this security PR, @huaxingao ?

Thank you, @huaxingao !

Merged to master/4.0/3.5.

late lgtm, thanks @dongjoon-hyun 

@grundprinzip @wbo4958 @HyukjinKwon 

merged to master/4.0

Let me merge this and see how it goes. The test failure should be not the real one.

Merged to master and branch-4.0.

Merging to master/4.0.

The remaining tests are not related to this PR.

Thanks! merging to master/4.0.

@WweiL Would you mind looking at the CI build? The failure seems to be relevant.

@HyukjinKwon can we merge this : ) TY!

Merged to master and branch-4.0.

Could you review this PR when you have some time, @LuciferYang ?

All tests passed.

Could you review this dependency change PR when you have some time, @ueshin ?

Merged into master and branch-4.0. Thanks @dongjoon-hyun 

Thank you so much, @LuciferYang !

@zhengruifeng : Tagging you for review as the original author of this change. \r\n@HyukjinKwon : Tagging you for review as the reviewer of the original PR. ðŸ™‚

Build github workflows failing above due to deprecation of v3 actions/upload-artifact. \r\nhttps://issues.apache.org/jira/browse/SPARK-46474 had upgraded to v4 in master branch. Do we want to backport it to branch-3.5? I can take that task up if we are going via that route.\r\n

Looks like this issue got resolved after https://github.com/apache/spark/pull/49777.\r\nLet me rebase with the latest changes.

Now that all the checks pass, trying pinging a few folks for a review on this PR.\r\n@dongjoon-hyun  @HyukjinKwon can I some reviews here, please? 

Let me close this PR to prevent any accidental merging. We can continue our discussion on this PR.

It is an improvement from the perspective of spark connect,\r\nit is also a bug fix if users migrate from spark classic to spark connect.\r\n\r\nI think there are many PRs like this.\r\n\r\nIf you @the-sakthi feel strong about this one, I am personally fine to backport this one.  

Well, I disagree with the idea of backporting. Especially, we should be more conservative because Apache Spark 3.5.x is a long-term support version. \r\n\r\nTo @zhengruifeng , please see the our policy which is the result of our discussion on the same perspective and trade-off .\r\n\r\n<img width="991" alt="Screenshot 2025-02-11 at 22 26 49" src="https://github.com/user-attachments/assets/cc0c56b4-0a91-41eb-bbb3-058e54806334" />\r\n

I fully understand how you feel and thank you for saying like that.\r\n\r\nI\

Thank you, all!

@hvanhovell, @HyukjinKwon, and @EnricoMi, I am looking for possible reviewers for this PR. Thank you all for your time!

@hvanhovell, @HyukjinKwon, and @EnricoMi, just a reminder about this outstanding PR. I know its a busy time working on Spark 4.0, but would appreciate a review when you have time. Thanks.

@hvanhovell, @HyukjinKwon, and @EnricoMi, sorry to keep bothering you. I am hoping to find out if this is something that might be considered in the near future or if I should consider this as a "soft no". I believe this is a miss between the Dataset and RDD APIs as RDD currently allows cogroups of up to 4 RDDs while Dataset only allows cogroups of 2 datasets. There are definitely some performance implications if you are forced to use multiple joins/cogroups to achieve the same thing that could be done with a single cogroup earlier.

Could you review this PR when you have some time, @viirya ?

Thank you always, @viirya !

Thank you @dongjoon-hyun for updating this.

Merged to master/4.0.

Could you review this PR, too, @huaxingao ?

Could you review this too, @viirya , please?

Thank you, @viirya ! ðŸ˜„ 

late LGTM

Thank you, @huaxingao !

late LGTM.

@ericm-db - can u add the SPARK ticket in the PR title ?

@ericm-db - also, is test failure related to the change ?

@HeartSaVioR Can you PTAL when you get a chance?

@ericm-db Looks like CI failure is relevant (I only checked with core module and there is another failure in other module). Could you please take a look and fix it? Thanks!

@HeartSaVioR Can you PTAL now?

Thanks! Merging to master/4.0.

Why you add these test cases here?

@MaxGekk ptal when you have time. Thanks

@beliefer Are you ok with placing new tests into the dedicated `group-by-alias.sql`?

@MaxGekk Done. I also did it in the Jira issue.

+1, LGTM. Merging to master/4.0.\r\nThank you, @mihailoale-db and @vladimirg-db @beliefer for review.

cc @zhengruifeng 

Failed tests dont seem relevant

> Failed tests dont seem relevant\r\n\r\nplease rebase this PR to latest master, to make sure CI is green

merged to master/4.0

Thank you, @cnauroth .

Could you review this PR when you have some time, @huaxingao ?

Thank you, @huaxingao . All tests passed.\r\n\r\n![Screenshot 2025-01-31 at 10 20 19](https://github.com/user-attachments/assets/0c2e73dd-98a0-46e4-a927-39edea55d9fc)\r\n

Merged to master/4.0.

cc - @HeartSaVioR - PTAL, thx !

Thanks! Merging to master/4.0.

Hi @dongjoon-hyun could you please take a look, thank you !

Hi @HyukjinKwon Kindly merge the PR if all looks good.

Merged to master.

Hi @zhengruifeng , Please take a look at this PR. Thx

merged to master/4.0

This PR is ready.

Could you review this PR when you have some time, @huaxingao ?

All tests passed.\r\n\r\n![Screenshot 2025-01-31 at 10 19 20](https://github.com/user-attachments/assets/8e20c54e-eb7d-4598-a07a-fe86b57e069b)\r\n\r\n\r\n

Thank you, @huaxingao !\r\nMerged to master.

Hey @zhengruifeng, Could you help review this PR, thx very much.

thanks, merged to master/4.0

cc - @HeartSaVioR - PTAL, thx !

Thanks! Merging to master/4.0.

There are a few places to spot, but in general, when we cross check both Java 17/21. There seems to be no regression in Scala 2.13.16.

Could you review this PR when you have some time, @huaxingao ?

Thank you for spending your time here to help, @huaxingao !

Merged to branch-4.0.

test

test

@HyukjinKwon mind merging it : ) 

@dongjoon-hyun Ah yes in the jira ticket I put 4.1.0, may I know if there is anything else I need to do?

+1, LGTM. Merging to master/4.0.\r\nThank you, @jonathan-albrecht-ibm.

@jonathan-albrecht-ibm Congratulations with your first contribution to Apache Spark!

@MaxGekk Thanks for reviewing and merging!

> Would it be possible to merge this to branch-3.5 as well?\r\n\r\nI think so. In the public doc we declare:\r\n```\r\nSpark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS), and it should run on any platform that runs a supported version of Java.\r\n```\r\nsee https://spark.apache.org/docs/latest/\r\n\r\n> Fixes big endian platforms where CalendarIntervals are not read or written correctly in Unsafe* classes\r\n\r\nApparently, we can consider the changes as a bug fix. cc @dongjoon-hyun @cloud-fan WDYT?

+1 to backport

@MaxGekk, @cloud-fan Thanks for considering this to be backported. Should I open a new PR against branch-3.5? The cherry-pick of a79ba486766bd47c26879231cd4292ca79c926fa applies cleanly on branch-3.5

+1 for backporting. Thank you for pinging me, @MaxGekk .\r\n\r\nTo @jonathan-albrecht-ibm , yes, we need a new PR in order to make it sure that it passes in `branch-3.5`.

Thank you so much, @LuciferYang . I also looking at those failure.

1. `GenericUDTFGetSplits` imports `org.apache.hadoop.hive.llap.security.LlapSigner`, which comes from llap-common. \r\n\r\nhttps://github.com/apache/hive/blob/5160d3af392248255f68e41e1e0557eae4d95273/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java#L44-L58\r\n\r\n![image](https://github.com/user-attachments/assets/da9e0e5e-0651-4176-875b-3e7c93979cba)\r\n\r\n\r\n1. `FunctionRegistry` will load `GenericUDTFGetSplits` in a static block.\r\n\r\nhttps://github.com/apache/hive/blob/5160d3af392248255f68e41e1e0557eae4d95273/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java#L500\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/0410db6a-267c-4ab6-a839-7b7be1dd04dd)\r\n\r\n\r\nAt the same time, I saw the following content in the log:\r\n\r\n```\r\nException in thread "HiveServer2-Handler-Pool: Thread-66" java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/llap/security/LlapSigner$Signable\r\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:76)\r\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.lambda$invoke$0(HiveSessionProxy.java:58)\r\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\r\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\r\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:58)\r\n\tat jdk.proxy2/jdk.proxy2.$Proxy46.close(Unknown Source)\r\n\tat org.apache.hive.service.cli.session.SessionManager.closeSession(SessionManager.java:304)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLSessionManager.closeSession(SparkSQLSessionManager.scala:92)\r\n\tat org.apache.hive.service.cli.CLIService.closeSession(CLIService.java:244)\r\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService$1.deleteContext(ThriftCLIService.java:120)\r\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:264)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/llap/security/LlapSigner$Signable\r\n\tat java.base/java.lang.Class.getDeclaredConstructors0(Native Method)\r\n\tat java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3373)\r\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3578)\r\n\tat java.base/java.lang.Class.getDeclaredConstructor(Class.java:2754)\r\n\tat org.apache.hive.common.util.ReflectionUtil.newInstance(ReflectionUtil.java:79)\r\n\tat org.apache.hadoop.hive.ql.exec.Registry.registerGenericUDTF(Registry.java:208)\r\n\tat org.apache.hadoop.hive.ql.exec.Registry.registerGenericUDTF(Registry.java:201)\r\n\tat org.apache.hadoop.hive.ql.exec.FunctionRegistry.<clinit>(FunctionRegistry.java:500)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:247)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:231)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:395)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:339)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:319)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:288)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.unCacheDataNucleusClassLoaders(SessionState.java:1596)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.close(SessionState.java:1586)\r\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.close(HiveSessionImpl.java:676)\r\n\tat org.apache.hive.service.cli.session.HiveSessionImplwithUGI.close(HiveSessionImplwithUGI.java:87)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:71)\r\n\t... 14 more\r\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.llap.security.LlapSigner$Signable\r\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\r\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\t... 37 more\r\n```\r\n\r\nIt seems that this caused the `FunctionRegistry` initialization to fail, so it looks like we can\

I verified this manually.\r\n```\r\n$ build/mvn -Phive-thriftserver install -DskipTests\r\n$ build/mvn -pl sql/hive-thriftserver -Phive-thriftserver install -fae\r\n...\r\nHiveThriftBinaryServerSuite:\r\n- SPARK-17819: Support default database in connection URIs\r\n- GetInfo Thrift API\r\n- SPARK-16563 ThriftCLIService FetchResults repeat fetching result\r\n- Support beeline --hiveconf and --hivevar\r\n- JDBC query execution\r\n- Checks Hive version\r\n- SPARK-3004 regression: result set containing NULL\r\n- SPARK-4292 regression: result set iterator issue\r\n- SPARK-4309 regression: Date type support\r\n- SPARK-4407 regression: Complex type support\r\n- SPARK-12143 regression: Binary type support\r\n- test multiple session\r\n- test jdbc cancel !!! IGNORED !!!\r\n- test add jar\r\n- Checks Hive version via SET -v\r\n- Checks Hive version via SET\r\n- SPARK-11595 ADD JAR with input path having URL scheme\r\n- SPARK-11043 check operation log root directory\r\n- SPARK-23547 Cleanup the .pipeout file when the Hive Session closed\r\n- SPARK-24829 Checks cast as float\r\n- SPARK-28463: Thriftserver throws BigDecimal incompatible with HiveDecimal\r\n- Support interval type\r\n- Query Intervals in VIEWs through thrift server\r\n- ThriftCLIService FetchResults FETCH_FIRST, FETCH_NEXT, FETCH_PRIOR\r\n- SPARK-29492: use add jar in sync mode\r\n- SPARK-31859 Thriftserver works with spark.sql.datetime.java8API.enabled=true\r\n- SPARK-31861 Thriftserver respects spark.sql.session.timeZone\r\n- SPARK-31863 Session conf should persist between Thriftserver worker threads\r\n- SPARK-30808: use Java 8 time API and Proleptic Gregorian calendar by default\r\n- SPARK-26533: Support query auto timeout cancel on thriftserver - setQueryTimeout\r\n- SPARK-26533: Support query auto timeout cancel on thriftserver - SQLConf\r\nCliSuite:\r\n- load warehouse dir from hive-site.xml\r\n- ...\r\n```

Thanks @dongjoon-hyun 

Thank you, @LuciferYang !

The one failure is irrelevant and flaky one.\r\n```\r\n[info] *** 1 TEST FAILED ***\r\n[error] Failed tests:\r\n[error] \torg.apache.spark.sql.kafka010.KafkaMicroBatchV2SourceWithConsumerSuite\r\n```\r\n\r\nMerged to master/4.0/3.5.

Merged into master. Thanks @dongjoon-hyun 

Triggered a new `Publish Snapshot`: https://github.com/apache/spark/actions/runs/13047290104\r\n\r\n![image](https://github.com/user-attachments/assets/38cf7e0c-d63f-4d18-85b0-2be7d8ac6e0c)\r\n

Thank you, @LuciferYang !

![image](https://github.com/user-attachments/assets/8d07ed1c-e6cc-47f1-b417-3232aa128454)\r\n\r\n![image](https://github.com/user-attachments/assets/4e64bcbf-b505-47bb-b904-ac80584164e2)\r\n\r\nPublished successfully

Thank you for the checking!

@dongjoon-hyun Here is the `branch-4.0` PR that you requested. PTAL.

Merged to branch-4.0.\r\n\r\nThank you again, @viktorluc-db .

Could you review this test dependency update PR when you have some time, @huaxingao ?

Thank you, @huaxingao . All tests passed. Merged to master/4.0.

@dependabot ignore this dependency

thanks, merging to master/4.0!

Thank you, @LuciferYang !

Merged to master/4.0/3.5.

Adding reviewers: @cloud-fan, @davidm-db, @dejankrak-db, @dusantism-db, @MaxGekk

Thank you so much, @LuciferYang .

Merged to master/4.0.

Due to the lack of these test dependencies of hive-llap-client and hive-llap-common, testing hive-thriftserver using Maven will hang at \r\n\r\n```\r\nDiscovery starting.\r\n2025-01-29T19:23:53.496214833Z ScalaTest-main ERROR Filters contains invalid attributes "onMatch", "onMismatch"\r\n2025-01-29T19:23:53.510863632Z ScalaTest-main ERROR Filters contains invalid attributes "onMatch", "onMismatch"\r\nDiscovery completed in 1 second, 258 milliseconds.\r\nRun starting. Expected test count is: 634\r\nHiveThriftBinaryServerSuite:\r\n- SPARK-17819: Support default database in connection URIs\r\n- GetInfo Thrift API\r\n```\r\n\r\nShould we add them as test dependencies for `hive-thriftserver` or revert?

Based on my analysis at https://github.com/apache/spark/pull/49736#issuecomment-2625039106, it seems that the compile-scope dependency on llap-common cannot be removed.

Yes, as I wrote in the PR description, UDF parts are affected, @LuciferYang .

The purpose of this PR is to eliminate the risk from Apache Spark side and to give a full freedom to users to take it or deploy with the patched `hive-llap-common`.\r\n\r\nFor example, Apache Spark 3.5.4, we can expect to delete `hive-llap-common.jar` and run like the following.\r\n```\r\n$ sbin/start-thriftserver.sh --packages org.apache.hive:hive-llap-common:2.3.9\r\n```\r\n\r\nWDTY, @LuciferYang ?

For example, this kind of risk issue.\r\n- https://github.com/apache/spark/security/dependabot/112

Understood, fine to me. Thank you @dongjoon-hyun 

Thank you. We can discuss more during the QA and RC period in order to get the final decision~

Sorry, I can\

Thank you for the comments, @pan3793 .

I was expecting your first point since it\

Merged to master/4.0. Thank you, @LuciferYang .

Thank you @dongjoon-hyun 

Could you review this PR when you have some time, @LuciferYang ? This is related to the Spark mailing list discussion about Hive usage.

Thank you, @LuciferYang . Merged to master/4.0.

Let me backport this to branch-3.5 because this provides more safe environment.

After validating in branch-3.5 like the following, I pushed.\r\n```\r\n$ bin/spark-shell\r\nSetting default log level to "WARN".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n[SPARK-51027][SQL] Prevent `HiveClient.runSqlHive` invocation in non-testing environment\r\n25/01/29 16:18:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n25/01/29 16:18:39 WARN Utils: Service \

Thank you @dongjoon-hyun 

Merged into master and branch-4.0. Thanks @dongjoon-hyun 

Merged to master/4.0.

@dependabot ignore this dependency

Hey @HyukjinKwon @dongjoon-hyun \r\nwhen you have a moment, could you take a look at the PR?\r\nThank you!

Could you review this K8s PR when you have some time, @viirya ?

Looks good. ðŸ‘ 

Thank you so much, @viirya !

All tests passed. Merged to master/4.0.

Could you review this PR when you have some time, @huaxingao ?

Thank you, @huaxingao !

All tests passed. Merged to master/4.0.

@cloud-fan 

cc @ulysses-you 

@ulysses-you yes, after this PR, we can implement the proposed idea in https://github.com/apache/spark/pull/44013#issuecomment-2421167393 and keep contexts in the AQE query stage.

shall we ignore `ResultQeryStage` in spark ui like other query stage ?

I think we already did it for all query stages. @liuzqt how did you see result query stage in the UI?

> I think we already did it for all query stages. @liuzqt how did you see result query stage in the UI?\r\n\r\nI think we need to explicitly match the name to ignore it (updated in [this commit](https://github.com/apache/spark/pull/49715/commits/138c46c3068ad2baf2ca68106e859b56f10990bb))

thanks, merging to master/4.0!

cc @beliefer 

Merged into master/branch-4.0 Thank you all! @wayneguow @dongjoon-hyun 

Thank you, @wayneguow and @beliefer .

cc @dongjoon-hyun @cloud-fan @LuciferYang @HyukjinKwon 

Merged into branch-4.0. Thanks @hvanhovell and @dongjoon-hyun 

ping @dongjoon-hyun cc @wayneguow 

The failed `sparkr` is irrelevant to this PR. Let me merge this.

@dongjoon-hyun @wayneguow Thank you!

Here is a discussion about republishing to Maven Centralï¼š\r\n- https://github.com/RoaringBitmap/RoaringBitmap/issues/749\r\n\r\nbut it seems unclear when it will be republished to Maven Central instead of just JitPack and GitHub packages. \r\n\r\nWe also have the option to wait for the republish of `RoaringBitmap` on Maven Central before proceeding with the upgrade.

@wbo4958 @grundprinzip 

merged to master/4.0

Let me merge this because CI result is irrelevant to this PR.\r\nMerged to master.

For the record, I triggered the job manually and verified that it works with `branch-4.0` and `pypy3.10` correctly.\r\n- https://github.com/apache/spark/actions/workflows/build_branch40_python_pypy3.10.yml\r\n\r\n```\r\n  Switched to a new branch \

Hi @zhengruifeng please help review, thx very much.

merged to master/4.0

@cloud-fan could you take a look at this?

good catch! merging to master/4.0

The last commit just revert the previous one, and CI passed in https://github.com/zhengruifeng/spark/actions/runs/13002433261\r\n\r\nMerging to master/4.0

LGTM.

merged to master/4.0

merged to master/4.0

cc @dongjoon-hyun @LuciferYang @HyukjinKwon 

@dongjoon-hyun should be good now...

@LuciferYang it is a unstable developer API. We are allowed to remove those.

Merging this.

Thank you for the fix, @hvanhovell .

Thank you, @wayneguow .

Merged to master and branch-4.0.

Thank you, @HyukjinKwon . Merged to master.

For the record, I triggered the CI manually and verified that it checked out `branch-4.0` and downloaded Java 21.\r\n- https://github.com/apache/spark/actions/workflows/build_branch40_maven_java21.yml\r\n\r\n```\r\n...\r\n/usr/bin/git log -1 --format=%H\r\n68615c4275c678f7cc72f5caa0fa1eb8e39e89fb\r\n...\r\nDownloading Java 21.0.6+7 (Zulu) from https://cdn.azul.com/zulu/bin/zulu21.40.17-ca-jdk21.0.6-linux_x64.tar.gz ...\r\n```

Could you review this PR when you have some time, @huaxingao ?

Thank you so much, @huaxingao .

Thank you, @xinrong-meng !

Merged to master/4.0 for Apache Spark 4.0.

cc @hvanhovell , @cloud-fan , @LuciferYang , @viirya 

Thank you, @viirya . Yes, we need SPARK-49700 back soon.

Merged to branch-4.0.

For the record, Maven CI is recovered finally after this commit.\r\n- https://github.com/apache/spark/actions/workflows/build_branch40_maven.yml\r\n\r\n![Screenshot 2025-01-27 at 13 16 54](https://github.com/user-attachments/assets/5eacafb9-b2c1-4d2c-88c5-883303ed7542)\r\n

https://github.com/apache/spark/blob/aa24a9a235b1e33adf39f67b661852298b3d3bdf/.github/workflows/maven_test.yml#L197\r\n\r\n![image](https://github.com/user-attachments/assets/452d7ac9-a313-4541-b21e-00f02186a9ee)\r\n\r\n\r\nWe need to correct some parts of the maven_test.yml file, and at the same time, consider how to ensure compatibility if there are differences in code structure between the master and branch-4.0 now, as this yml file will also be used for the Maven daily test in branch-4.0.

https://github.com/apache/spark/blob/04f862aaf94d688cd18267e04c2ab61748e3a616/assembly/pom.xml#L195\r\n\r\nhttps://github.com/apache/spark/blob/04f862aaf94d688cd18267e04c2ab61748e3a616/assembly/pom.xml#L209\r\n\r\nThe configuration in the `assembly/pom.xml` also needs to be fixed, otherwise the following error will occur during the build process:\r\n\r\n```\r\n[INFO] --- exec:3.2.0:exec (copy-connect-client-repl-jars) @ spark-assembly_2.13 ---\r\ncp: /Users/yangjie01/SourceCode/git/spark-mine-13/assembly/../connector/connect/client/jvm/target/connect-repl: No such file or directory\r\n[ERROR] Command execution failed.\r\norg.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1)\r\n    at org.apache.commons.exec.DefaultExecutor.executeInternal (DefaultExecutor.java:355)\r\n    at org.apache.commons.exec.DefaultExecutor.execute (DefaultExecutor.java:253)\r\n    at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:884)\r\n    at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:844)\r\n    at org.codehaus.mojo.exec.ExecMojo.execute (ExecMojo.java:450)\r\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:328)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.DirectMethodHandleAccessor.invoke (DirectMethodHandleAccessor.java:103)\r\n    at java.lang.reflect.Method.invoke (Method.java:580)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:255)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:201)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:361)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:314)\r\n\r\n```

@LuciferYang yeah I was afraid of that. Thanks for the heads-up!

@LuciferYang do you think this is good to go? I will port this to 4.0 as well.

Let me double-check that

Merging to master/4.0.

Adding @cloud-fan @MaxGekk @srielau 

Hi @MaxGekk, Please take a look.

@MaxGekk done.

+1, LGTM. Merging to master/4.0.\r\nThank you, @miland-db and @srielau @dejankrak-db @beliefer @davidm-db @dusantism-db for review.

Merged to master and branch-4.0.

It seems there are still some lint failure, I am preparing a fix for it

oops

Thank you for reverting this. I was also tracking the linter failures.

thanks all, I will send another PR

Merged to master and branch-4.0.

Merged to master and branch-4.0.

merged to master/4.0

merged to master/4.0

merged to master/4.0

merged to master/4.0

merged to master/4.0

Merged to master/4.0

Thank you, @HyukjinKwon ! Merged to master/4.0.

Thank you, @HyukjinKwon . Merged to master/4.0/3.5.

Thank you, @HyukjinKwon . Merged to master/4.0.

Thank you, @HyukjinKwon !  Merged to master/4.0.

thanks, merged to master/4.0

thanks, merged to master/4.0

Marking WIP, this would require some more work around event listeners and observable  due to exposure of RDD stages.

Ready for view

@dongjoon-hyun / @HyukjinKwon seeking your attention. 

thanks, merging to master/4.0!

ping @MaxGekk cc @dongjoon-hyun 

+1, LGTM. Merging to master/4.0.\r\nThank you, @beliefer.

@MaxGekk Thank you!

Hi @zhengruifeng, Please help review it. Thx very much.

merged to master/4.0

merged to master/4.0

thanks @LuciferYang \r\n\r\nmerged to master/4.0

@the-sakthi just linking the PR that addresses some issues around this error https://github.com/apache/spark/pull/48242. Due to long time it being open, github closed it. We need to make sure suggestions are aligned with this.

TLDR; I would say it is better if we first fix the java specific message and only then remove the ANSI suggestion.

Ack, let me take a look at the linked PR @mihailom-db 

@LuciferYang thanks for the concern, but there a couple of reasons why we would want to do this, apart from abandoning non-ANSI behaviour. \r\n\r\nIn spark 4.0.0 ANSI is turned on by default. Because of this, we need to make sure we do not suggest turning off ANSI config that easily. Previously this suggestion made sense, as users had to had set ANSI config explicitly, so suggestion to turn it off was a suggestion to revert an explicit set to the default state. Now we would suggest turning off (switching to non-default value) on a config that ensures spark queries return proper result, without returning unexpected nulls on erroneous inputs. \r\n\r\nAdditionally, once user sets a config to specific value, they would usually stick with it, without considering it until they run into some problems. Switching off ANSI would make many different expressions return nulls, which is really hard to catch without inspecting data, which might not be something user wants to do, when the default behaviour of spark now is with ANSI on. Also, getting to the phase where the query already run, sometimes leads to the state where it is almost impossible to go back and revert the change in data without a big pain.\r\n\r\nSo IMO we need to keep clear the difference between the change that is coming, the switch of default value, and the newly proposed thing of abandoning non-ANSI behaviour.

+1, LGTM. Merging to master/4.0.\r\nThank you, @vladimirg-db.

merged to master/4.0

merged to master/4.0

thanks, merged to master/4.0\r\n\r\nhas manually test locally

Thank you for this stabilization effort.

Thank you, @HyukjinKwon . Merged to master/4.0/3.5.

@zhengruifeng I guess my repo is hitting [SPARK-50864](https://issues.apache.org/jira/browse/SPARK-50864)?\r\nWhen I disabled `TorchDistributorDataLoaderUnitTests` and `TorchDistributorBaselineUnitTestsOnConnect`, tests passed.

Thanks! merging to master.

https://github.com/micheal-o/spark/actions/runs/13127546346/job/36626660666\r\n\r\nCI failure is unrelated.

Thanks! Merging to master/4.0.

Could you review this PR when you have some time, @huaxingao ?

Thank you, @huaxingao !

All tests passed. Merged to master/4.0/3.5.

cc: @agubichev for review

WIP:\r\n- [ ] tests\r\n- [ ] subError class marking unresolvedOuterAttrs are not within OuterAttrs\r\n- [ ] check if these changes affect other operators which will turn into Subqueries, eg: UDF, CTE...\r\n- [ ] currently the feature cannot be turned on as optimizer is not ready yet, may need to add another config to check if optimizer supports nested correlated subqueries

Closing, will open a new PR with logic for cogroup.

Thanks @mihailotim-db and @mihailoale-db for working on this!

Thanks @gotocoding-DB for UNIONs!

@MaxGekk thanks! resolved.

+1, LGTM. Merging to master/4.0.\r\nThank you, @vladimirg-db and @cloud-fan for review.

Based on failed test, there may be user facing change as plan changes.

Oh, ya. The plan becomes different.\r\n```\r\n[info] - hint *** FAILED *** (11 milliseconds)\r\n[info]   Expected and actual plans do not match:\r\n[info]   \r\n[info]   === Expected Plan (with excess fields trimmed) ===\r\n[info]   common {\r\n[info]     plan_id: 1\r\n[info]   }\r\n[info]   hint {\r\n[info]     input {\r\n[info]       common {\r\n[info]         plan_id: 0\r\n[info]       }\r\n[info]       local_relation {\r\n[info]         schema: "struct<id:bigint,a:int,b:double>"\r\n[info]       }\r\n[info]     }\r\n[info]     name: "coalesce"\r\n[info]     parameters {\r\n[info]       literal {\r\n[info]         integer: 100\r\n[info]       }\r\n[info]       common {\r\n[info]         origin {\r\n[info]           jvm_origin {\r\n[info]             stack_trace {\r\n[info]               class_loader_name: "app"\r\n[info]               declaring_class: "org.apache.spark.sql.connect.Dataset"\r\n[info]               method_name: "~~trimmed~anonfun~~"\r\n[info]               file_name: "Dataset.scala"\r\n[info]             }\r\n[info]             stack_trace {\r\n[info]               class_loader_name: "app"\r\n[info]               declaring_class: "org.apache.spark.sql.connect.SparkSession"\r\n[info]               method_name: "newDataset"\r\n[info]               file_name: "SparkSession.scala"\r\n[info]             }\r\n[info]           }\r\n[info]         }\r\n[info]       }\r\n[info]     }\r\n[info]   }\r\n[info]   \r\n[info]   \r\n[info]   === Actual Plan (with excess fields trimmed) ===\r\n[info]   common {\r\n[info]     plan_id: 1\r\n[info]   }\r\n[info]   hint {\r\n[info]     input {\r\n[info]       common {\r\n[info]         plan_id: 0\r\n[info]       }\r\n[info]       local_relation {\r\n[info]         schema: "struct<id:bigint,a:int,b:double>"\r\n[info]       }\r\n[info]     }\r\n[info]     name: "coalesce"\r\n[info]     parameters {\r\n[info]       literal {\r\n[info]         integer: 100\r\n[info]       }\r\n[info]       common {\r\n[info]         origin {\r\n[info]           jvm_origin {\r\n[info]             stack_trace {\r\n[info]               class_loader_name: "app"\r\n[info]               declaring_class: "org.apache.spark.sql.functions$"\r\n[info]               method_name: "lit"\r\n[info]               file_name: "functions.scala"\r\n[info]             }\r\n[info]             stack_trace {\r\n[info]               class_loader_name: "app"\r\n[info]               declaring_class: "org.apache.spark.sql.connect.ColumnNodeToProtoConverter$"\r\n[info]               method_name: "toLiteral"\r\n[info]               file_name: "columnNodeSupport.scala"\r\n[info]             }\r\n[info]           }\r\n[info]         }\r\n[info]       }\r\n[info]     }\r\n[info]   } (PlanGenerationTestSuite.scala:160)\r\n```

Please update "Does this PR introduce any user-facing change?"

@vrozov I does not really introduce a user facing change. I actually improved the origin for these cases.

Merging.

Thank you, @hvanhovell .

Thanks! Merging to master/4.0.

Let me know if we could suggest something else in the error message.

+1, LGTM. Merging to master/4.0.\r\nThank you, @the-sakthi.

Thanks for the help with the review & merge @MaxGekk !

Late LGTM, thanks for picking this task up @the-sakthi.

@dongjoon-hyun @HeartSaVioR Please review

@dongjoon-hyun Please review

While I strongly suspect this is purely based on the setup of the cluster, I\

> Though I don\

I\

@HeartSaVioR  please take a look

Thanks! Merging to master/4.0.

No problem at all~ Thank you for update, @ChenMichael .

merged to master/4.0

cc @MaxGekk 

+1, LGTM. Merging to master/4.0.\r\nThank you, @olaky.

cc @srowen and @rebo16v

```\r\nfrom pyspark.ml.feature import *\r\n\r\ndf = spark.createDataFrame(\r\n[\r\n      (0, 3, 5.0, 0.0),\r\n      (1, 4, 5.0, 1.0),\r\n      (2, 3, 5.0, 0.0),\r\n      (0, 4, 6.0, 1.0),\r\n      (1, 3, 6.0, 0.0),\r\n      (2, 4, 6.0, 1.0),\r\n      (0, 3, 7.0, 0.0),\r\n      (1, 4, 8.0, 1.0),\r\n      (2, 3, 9.0, 0.0),\r\n],\r\nschema="input1 short, input2 int, input3 double, label double",\r\n)\r\nencoder = TargetEncoder(\r\ninputCols=["input1", "input2", "input3"],\r\noutputCols=["output", "output2", "output3"],\r\nlabelCol="label",\r\ntargetType="binary",\r\n)\r\nmodel = encoder.fit(df)\r\nmodel.write().overwrite().save("/tmp/ta")\r\nTargetEncoderModel.load("/tmp/ta")\r\n```\r\n\r\n`TargetEncoderModel.load("/tmp/ta")` fails with\r\n```\r\njava.lang.NullPointerException\r\n\tat org.apache.spark.connect.proto.FetchErrorDetailsResponse$Error$Builder.setMessage(FetchErrorDetailsResponse.java:5654)\r\n\tat org.apache.spark.sql.connect.utils.ErrorUtils$.throwableToFetchErrorDetailsResponse(ErrorUtils.scala:89)\r\n\tat org.apache.spark.sql.connect.service.SparkConnectFetchErrorDetailsHandler.$anonfun$handle$1(SparkConnectFetchErrorDetailsHandler.scala:51)\r\n\tat scala.Option.map(Option.scala:242)\r\n\tat org.apache.spark.sql.connect.service.SparkConnectFetchErrorDetailsHandler.handle(SparkConnectFetchErrorDetailsHandler.scala:44)\r\n\tat org.apache.spark.sql.connect.service.SparkConnectService.fetchErrorDetails(SparkConnectService.scala:224)\r\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:935)\r\n\tat org.sparkproject.connect.grpc.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\r\n\tat org.sparkproject.connect.grpc.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\r\n\tat org.sparkproject.connect.grpc.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\r\n\tat org.sparkproject.connect.grpc.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\r\n\tat org.sparkproject.connect.grpc.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n\r\n```

merged to master/4.0

merged to master/4.0

merged to master and 4.0

@grundprinzip yes, the new test is in `FeatureTestsMixin` which is reused in `test_parity_feature`

Thanks for the review @LuciferYang ! Will check and do the needful.

Closing this in favor of the above comment.

Hi @zhengruifeng, Please help review it again. Thx very much\r\n

For now, the foldCol is not supported yet, I filed a Jira to track it https://issues.apache.org/jira/browse/SPARK-50974

merged to master/4.0

cc @wbo4958 

merged to master/4.0

merged to master/4.0

merged to master/4.0

cc @dongjoon-hyun 

Thanks @dongjoon-hyun 

Thanks @dongjoon-hyun 

merged to master/4.0

https://github.com/bogao007/spark/actions/runs/12942132674/job/36099374376\r\nIt seems to be failing on linter.

https://github.com/bogao007/spark/runs/36104154381\r\nLooks like CI is passed.

Thanks! Merging to master/4.0.

Oh, according to the log, `pyspark.ml.tests.connect.test_parity_torch_data_loader` seems to hang consequtively twice on this change. Is there any difference from `master` branch?\r\n- https://github.com/ueshin/apache-spark/actions/runs/12941687725/job/36098223383\r\n- https://github.com/ueshin/apache-spark/actions/runs/12943954553/job/36134941177\r\n```\r\nFinished test(python3.11): pyspark.ml.tests.connect.test_parity_regression (26s)\r\nStarting test(python3.11): pyspark.ml.tests.connect.test_parity_torch_data_loader (temp output: /__w/apache-spark/apache-spark/python/target/f869134c-ed13-471f-aa7e-37562cf80415/python3.11__pyspark.ml.tests.connect.test_parity_torch_data_loader__nrhuie5k.log)\r\nError: The operation was canceled.\r\n```

Okay we can skip it for now

Merged to master and branch-4.0.

@HyukjinKwon @wbo4958 

merged to master/4.0

cc - @HeartSaVioR - PTAL, thx !

Thanks! Merging to master/4.0.

The remaining test failures are not related to this PR.

Thanks! merging to master/4.0.

Discussed offline and we will close this in favor of this PR - https://github.com/apache/spark/pull/49632 Thx

@vladimirg-db Can you take a look?

@HyukjinKwon Could you please take a look?

@vladimirg-db Addressed the comments. PTAL.

Thanks @MaxGekk! Addressed your comment.

@viktorluc-db Could you resolve conflicts, please.

@MaxGekk Done.

@dongjoon-hyun Addressed your comments. Thanks!

There was a conflict on `branch-4.0`. Could you make a backporting PR, @viktorluc-db ?

cc @cloud-fan @chenhao-db @harshmotw-db @HyukjinKwon 

thanks\r\nmerged to master/4.0

cc @HyukjinKwon and @wbo4958 

merged to master/4.0

thanks. merged to master/4.0

The pyspark failure is unrelated, thanks, merging to master/4.0!

LGTM

I have checked the failed test locally:\r\n```\r\n[info] *** 1 TEST FAILED ***\r\n[error] Failed tests:\r\n[error] \torg.apache.spark.sql.connect.planner.SparkConnectServiceSuite\r\n```\r\nSo, it passed.\r\n\r\n+1, LGTM. Merging to master/4.0/3.5.\r\nThank you, @yaooqinn and @the-sakthi for review.

@yaooqinn The changes conflict with the branch `branch-3.5`. Please, open a PR with a backport.

thx!

Merging to master/4.0. Thank you, @HyukjinKwon @zhengruifeng for review.

Thank you for the fix, @MaxGekk, @HyukjinKwon , @zhengruifeng .

merged to master/4.0

Merged to master and branch-4.0.

cc @cloud-fan @dongjoon-hyun @HyukjinKwon, thank you!

Actually here is Core examples (https://github.com/apache/spark/tree/7f1158badeccfaaa763a3f39e5fc63a66029b52e/examples/src/main/scala/org/apache/spark/examples). SQL examples should be in https://github.com/apache/spark/tree/7f1158badeccfaaa763a3f39e5fc63a66029b52e/examples/src/main/scala/org/apache/spark/examples/sql

> Actually here is Core examples (https://github.com/apache/spark/tree/7f1158badeccfaaa763a3f39e5fc63a66029b52e/examples/src/main/scala/org/apache/spark/examples). SQL examples should be in https://github.com/apache/spark/tree/7f1158badeccfaaa763a3f39e5fc63a66029b52e/examples/src/main/scala/org/apache/spark/examples/sql\r\n\r\nmake sense

Thank you. Please revise the PR title and description accordingly too.

Updated. thank you @dongjoon-hyun

+1 for the naming change (SparkConnectPi -> SparkDataFramePi).

Thank you @cloud-fan and @dongjoon-hyun, SparkDataFramePi sounds good to me.

Merged to master/4.0.\r\n\r\nThank you, @yaooqinn , @cloud-fan , @HyukjinKwon .

Thank you very much, @dongjoon-hyun , @cloud-fan , @HyukjinKwon .

Merged to master and branch-4.0.

Hi @HyukjinKwon , @zhengruifeng @WeichenXu123 , please help review it. Thx a lot.

merged to master/4.0

@HyukjinKwon @wbo4958 

merged to master/4.0

Thanks! Merging to master/4.0.

Merged to master and branch-4.0

The changes cause a test failure. Please, have a look at the fix: https://github.com/apache/spark/pull/49621

Hi, @HyukjinKwon and @zhengruifeng .\r\n\r\nSince we are close to Spark 4 release phase, please use JIRA ID always for trace-ability next time~

Yup, will do

merged to master/4.0

Thanks, merged to master/4.0

@HyukjinKwon Did you get a chance to look at the Spark Connect issue?

thanks, merging to master/4.0!

Could you review this when you have some time, @viirya ?

Thank you so much, @viirya .

Merged to master/4.0.

cc @LuciferYang and @MaxGekk 

Thank you for review, @LuciferYang . I fixed it by moving `toInt` later.

Here is the manual test.\r\n```\r\n$ java -version\r\nopenjdk version "1.8.0_312"\r\nOpenJDK Runtime Environment AppleJDK-8.0.312.7.1 (build 1.8.0_312-b07)\r\nOpenJDK 64-Bit Server VM AppleJDK-8.0.312.7.1 (build 25.312-b07, mixed mode)\r\n```\r\n\r\n```\r\n[info] UtilsSuite:\r\n[info] - SPARK-35907: createDirectory (68 milliseconds)\r\n[info] Run completed in 2 seconds, 273 milliseconds.\r\n[info] Total number of tests run: 1\r\n[info] Suites: completed 1, aborted 0\r\n[info] Tests: succeeded 1, failed 0, canceled 0, ignored 0, pending 0\r\n[info] All tests passed.\r\n[success] Total time: 80 s (01:20), completed Jan 22, 2025 10:23:59 AM\r\n```

Thank you, @LuciferYang !

Thank you, @MaxGekk 

Merged to branch-3.5.

Thank you @dongjoon-hyun 

Merging master/4.0

Since the failure looks like a flaky one, could you simply re-trigger the failed test pipeline, @stefankandic ?\r\n```\r\n[info] SparkSessionE2ESuite:\r\n[info] - interrupt all - background queries, foreground interrupt (219 milliseconds)\r\n[info] - interrupt all - foreground queries, background interrupt (311 milliseconds)\r\n[info] - interrupt all - streaming queries (390 milliseconds)\r\n[info] - interrupt tag !!! IGNORED !!!\r\n[info] - interrupt tag - streaming query (714 milliseconds)\r\n[info] - progress is available for the spark result (3 seconds, 866 milliseconds)\r\n[info] *** Test still running after 5 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 10 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 15 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 20 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 25 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 30 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 35 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 40 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 45 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 50 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 55 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 1 hour, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 1 hour, 5 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 1 hour, 10 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 1 hour, 15 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n[info] *** Test still running after 1 hour, 20 minutes, 49 seconds: suite name: SparkSessionE2ESuite, test name: interrupt operation. \r\n```

+1, LGTM. Merging to 4.0.\r\nThank you, @stefankandic and @dongjoon-hyun for review.

I used a little bit diff approach :-). Thanks for comments.

Merged to master and branch-4.0.

will add more tests if this approach is in general fine

how about changing the title to "refactor the way handling ALLOWED_ATTRIBUTES"

overall, LGTM.

thanks, merged to master/4.0

merged to master/4.0

cc @HyukjinKwon @dongjoon-hyun FYI\r\n

+1, LGTM. Merging to master/4.0.\r\nThank you, @LuciferYang.

Thanks @MaxGekk 

Thank you, @LuciferYang and @MaxGekk .

I backported this to `branch-3.5` too.

Oops. I tested with Java 17 only when I backport to `branch-3.5`. Let me check with Java 8.

I made a backporting PR to branch-3.5 by replacing `Runtime.Version` (Java 9+)\r\n- https://github.com/apache/spark/pull/49606

Also cc @MaxGekk fyi

The test failure is not related to the changes:\r\n```\r\n[info] - SPARK-29442 Set `default` mode should override the existing mode *** FAILED *** (9 milliseconds)\r\n[info]   java.lang.NoSuchFieldException: mode\r\n[info]   at java.base/java.lang.Class.getDeclaredField(Class.java:2610)\r\n[info]   at org.apache.spark.sql.DataFrameSuite.$anonfun$new$476(DataFrameSuite.scala:2061)\r\n[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r\n```

+1, LGTM. Merging to master/4.0.\r\nThank you, @itholic.

merged to master/4.0

thanks. merged to master/4.0

Merged to master and branch-4.0.

Merged to master and branch-4.0.\r\n\r\nThanks @HyukjinKwon for the review!

Merged to master and branch-4.0.

@LuciferYang Thanks for the fix! Let me merge it and rerun tests.

The remaining test failures are not related to this PR.

Thanks! merging to master.

As a side note, it seems that we need to check `branch-4.0` like `master` branch in order to cover PyPy in Spark 4.0.0.\r\n- https://github.com/apache/spark/pull/49707

Thanks for the report. I submitted the fix #49720.

Thank you, @ueshin !

Merged to master and branch-4.0.

Thanks, merging to master!

I hope to backport this fix to branch-3.5, as I encountered similar test failures in the daily tests of branch-3.5:\r\n- https://github.com/apache/spark/actions/runs/12885594112/job/35924424737\r\n\r\n![image](https://github.com/user-attachments/assets/87eb8337-b848-4baf-ab95-a0de196772c1)\r\n\r\n\r\ncc @dongjoon-hyun @yaooqinn 

Merged to branch-3.5.

Thanks @dongjoon-hyun 

BTW, please check the email, @LuciferYang .

merged to master/4.0

Could you review this PR, @LuciferYang ?

Thank you, @MaxGekk !

`SparkSessionExtensionSuite` passed in the CI. Merged to master/4.0.

late LGTM

+1, LGTM. Merging to master/4.0.\r\nThank you, @stefankandic.

thanks, merging to master/4.0!

@xi-db FYI\r\n@hvanhovell\r\nHi, can you review this PR when you have time?\r\n* Plans are *not* cached during transformation.\r\n* Plans are cached once after data frames are created (after analysis).\r\n* The number of cache get/put calls stays the same; only the timing was adjusted.\r\nThanks!

cc @HyukjinKwon 

Merged to branch-4.0.

Could you review this PR, @yaooqinn ?

Thank you, @yaooqinn !

A failure occurred in https://github.com/dongjoon-hyun/spark/actions/runs/12881008656/job/35910916844\r\n, causing the `KubernetesLocalDiskShuffleDataIOSuite` to not be executed. Although I have verified it locally, it would be best to have it pass through CI.

Thank you! Now all tests passed. Merged to master/4.0.\r\n\r\n![Screenshot 2025-01-21 at 08 00 57](https://github.com/user-attachments/assets/d74967d6-ea5f-47fd-b241-0781257445c1)\r\n

cc @wbo4958 @HyukjinKwon 

thanks, merged to master/4.0

thanks, merged to master/4.0

cc @wbo4958 

thanks, merged to master/4.0

Thank you, @zhengruifeng . Merged to master.

Hi @grundprinzip, I created a jira for this PR, https://issues.apache.org/jira/browse/SPARK-50897, if possible, please change the title  to [SPARK-50897]

merging to master and branch-4.0

@MaxGekk I have created a separate PR to unblock the test that is failing in the collation expression walker suite #49586.

+1, LGTM. Merging to master.\r\nThank you, @stefankandic and @stevomitric for review.

@stefankandic Could you open a PR with backport to `branch-4.0`, please.

The failed yarn test is unrelated, merging to master/4.0!

Merged to master and branch-4.0.

- A bad case\r\nhttps://github.com/apache/spark/blob/620f55262cab75485f4e1ee5d85dd24ab8a4c1aa/sql/core/src/test/scala/org/apache/spark/sql/DataFrameTimeWindowingSuite.scala#L165-L196\r\n\r\n- codegen code (when it triggers the `split` expression)\r\n```scala\r\n/* 001 */ public Object generate(Object[] references) {\r\n/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\r\n/* 003 */ }\r\n/* 004 */\r\n/* 005 */ // codegenStageId=1\r\n/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\r\n/* 007 */   private Object[] references;\r\n/* 008 */   private scala.collection.Iterator[] inputs;\r\n/* 009 */   private boolean hashAgg_initAgg_0;\r\n/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\r\n/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\r\n/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\r\n/* 013 */   private scala.collection.Iterator localtablescan_input_0;\r\n/* 014 */   private boolean expand_resultIsNull_0;\r\n/* 015 */   private long filter_subExprValue_0;\r\n/* 016 */   private boolean filter_subExprIsNull_0;\r\n/* 017 */   private long filter_subExprValue_1;\r\n/* 018 */   private boolean filter_subExprIsNull_1;\r\n/* 019 */   private long filter_subExprValue_2;\r\n/* 020 */   private boolean filter_subExprIsNull_2;\r\n/* 021 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[15];\r\n/* 022 */   private InternalRow[] expand_mutableStateArray_0 = new InternalRow[1];\r\n/* 023 */\r\n/* 024 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\r\n/* 025 */     this.references = references;\r\n/* 026 */   }\r\n/* 027 */\r\n/* 028 */   public void init(int index, scala.collection.Iterator[] inputs) {\r\n/* 029 */     partitionIndex = index;\r\n/* 030 */     this.inputs = inputs;\r\n/* 031 */     wholestagecodegen_init_0_0();\r\n/* 032 */     wholestagecodegen_init_0_1();\r\n/* 033 */\r\n/* 034 */   }\r\n/* 035 */\r\n/* 036 */   private void wholestagecodegen_init_0_1() {\r\n/* 037 */     filter_mutableStateArray_0[8] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(filter_mutableStateArray_0[7], 2);\r\n/* 038 */     filter_mutableStateArray_0[9] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\r\n/* 039 */     filter_mutableStateArray_0[10] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(filter_mutableStateArray_0[9], 2);\r\n/* 040 */     filter_mutableStateArray_0[11] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\r\n/* 041 */     filter_mutableStateArray_0[12] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(filter_mutableStateArray_0[11], 2);\r\n/* 042 */     filter_mutableStateArray_0[13] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\r\n/* 043 */     filter_mutableStateArray_0[14] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(filter_mutableStateArray_0[13], 2);\r\n/* 044 */\r\n/* 045 */   }\r\n/* 046 */\r\n/* 047 */   private void wholestagecodegen_init_0_0() {\r\n/* 048 */     localtablescan_input_0 = inputs[0];\r\n/* 049 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 64);\r\n/* 050 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\r\n/* 051 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\r\n/* 052 */     expand_resultIsNull_0 = true;\r\n/* 053 */     expand_mutableStateArray_0[0] = null;\r\n/* 054 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);\r\n/* 055 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(filter_mutableStateArray_0[3], 2);\r\n/* 056 */     filter_mutableStateArray_0[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);\r\n/* 057 */     filter_mutableStateArray_0[6] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(filter_mutableStateArray_0[5], 2);\r\n/* 058 */     filter_mutableStateArray_0[7] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\r\n/* 059 */\r\n/* 060 */   }\r\n/* 061 */\r\n/* 062 */   private void filter_subExpr_0(boolean expand_resultIsNull_0, org.apache.spark.sql.catalyst.InternalRow expand_mutableStateArray_0[0]) {\r\n/* 063 */     // 1...\r\n/* 064 */     boolean filter_isNull_11 = expand_resultIsNull_0;\r\n/* 065 */     long filter_value_12 = -1L;\r\n/* 066 */\r\n/* 067 */     if (!expand_resultIsNull_0) {\r\n/* 068 */       if (expand_mutableStateArray_0[0].isNullAt(0)) {\r\n/* 069 */         filter_isNull_11 = true;\r\n/* 070 */       } else {\r\n/* 071 */         filter_value_12 = expand_mutableStateArray_0[0].getLong(0);\r\n/* 072 */       }\r\n/* 073 */\r\n/* 074 */     }\r\n/* 075 */     // 2...\r\n/* 076 */     filter_subExprIsNull_0 = filter_isNull_11;\r\n/* 077 */     // 3...\r\n/* 078 */     filter_subExprValue_0 = filter_value_12;\r\n/* 079 */   }\r\n/* 080 */\r\n/* 081 */   protected void processNext() throws java.io.IOException {\r\n/* 082 */     if (!hashAgg_initAgg_0) {\r\n/* 083 */       hashAgg_initAgg_0 = true;\r\n\r\n```\r\n\r\n- CodeGenerator##getLocalInputVariableValues\r\n(It seems that there is no good support for array type variables.)\r\n<img width="910" alt="image" src="https://github.com/user-attachments/assets/e5d54cfe-9c55-4340-aa7d-111ee6c91e54" />\r\n\r\n- If I adjust the `spark.sql.codegen.methodSplitThreshold` value to be larger, this case can pass (I understand that it directly uses `global variables` instead of `splitting multiple functions`)\r\nval needSplit = nonSplitCode.map(_.eval.code.length).sum > SQLConf.get.methodSplitThreshold\r\n\r\n

interesting

Any updates?

Merged to master and created separate PR for branch-4.0: https://github.com/apache/spark/pull/49583.\r\n\r\nThanks @HyukjinKwon for the review.

I went through the outputs of all golden files tests and compared them with the outputs of the same (or syntactically slightly adapted) queries in Snowflake and PostgreSQL engines. All the outputs match.

merged to master/4.0

Hi @grundprinzip, Could you help review this PR, so with PR, all the spark ml operators will be "loaded" by ServiceLoader.

Hi @grundprinzip, \r\n\r\nI\

@wbo4958 cf https://github.com/apache/spark/pull/49577

Merged to master/4.0

LGTM

hi @srowen , @Yikun , will you have look at this pr? thanks.

I think I might make a JIRA for this, just because it is touching an important method. Just for completeness.\r\nBut I buy the change, and that is an interesting analysis

Nevermind I got it

thanks\r\n\r\nmerged to master/4.0

cc @wbo4958 @HyukjinKwon 

thanks, merged to master/4.0

Merged to master and branch-4.0.

cherry-picked to branch-3.5 as well to recover https://github.com/apache/spark/actions/workflows/build_python_connect.yml

cc @wangyum 

also cc @beliefer 

Also, cc @yaooqinn because this could be a nice improvement as a part of the umbrella JIRA, SPARK-47361 (\r\nImprove JDBC data sources).

> Member\r\n\r\n@dongjoon-hyun As I mentioned above, `INDEX(test idx1)` is a hint, `/*+`, `*/` are not parts of it. We still use a regrex to validate the `/*+`, `*/` ?

cc @LuciferYang 

Could you review this PR, @panbingkun ?

```\r\n[error] \r\n[error]   bad constant pool index: 0 at pos: 3287\r\n[error]      while compiling: /home/runner/work/spark/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/IsolatedClientLoader.scala\r\n[error]         during phase: globalPhase=typer, enteringPhase=namer\r\n[error]      library version: version 2.13.8\r\n[error]     compiler version: version 2.13.8\r\n[error]   reconstructed args: -bootclasspath /usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/classes:/home/runner/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/scala-lang/scala-library/2.13.8/scala-library-2.13.8.jar -sourcepath /home/runner/work/spark/spark -explaintypes -feature -unchecked -Wconf:cat=unused-imports&src=org\\/apache\\/spark\\/graphx\\/impl\\/VertexPartitionBaseOps.scala:s -Wconf:cat=unused-imports&src=org\\/apache\\/spark\\/graphx\\/impl\\/VertexPartitionBase.scala:s -Wconf:msg=^(?=.*?a value of type)(?=.*?cannot also be).+$:s -Wconf:cat=unchecked&msg=eliminated by erasure:s -Wconf:cat=unchecked&msg=outer reference:s -Wconf:cat=deprecation&msg=procedure syntax is deprecated:e -Wconf:msg=method without a parameter list overrides a method with a single empty one:s -Wconf:msg=method with a single empty parameter list overrides method without any parameter list:s -Wconf:msg=Auto-application to \\`\\(\\)\\` is deprecated:s -Wconf:msg=^(?=.*?Widening conversion from)(?=.*?is deprecated because it loses precision).+$:s -Wconf:msg=^(?=.*?method|value|type|object|trait|inheritance)(?=.*?deprecated)(?=.*?since 2.13).+$:s -Wconf:cat=other-pure-statement&site=org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite.testPositiveCase.\\$anonfun:wv -Wconf:cat=other-pure-statement&site=org.apache.spark.scheduler.OutputCommitCoordinatorSuite.<local OutputCommitCoordinatorSuite>.futureAction:wv -Wconf:cat=other-pure-statement&site=org.apache.spark.streaming.util.FileBasedWriteAheadLog.readAll.readFile:wv -Wconf:cat=other-match-\r\n```\r\n\r\nThe compilation for Scala 2.13 failed. Can we re-trigger it to confirm if it was just a random occurrence?

I tested it locally and encountered the same error.\r\n![image](https://github.com/user-attachments/assets/551a9213-e68d-4cea-8998-0f6dd6a63253)\r\n\r\n![image](https://github.com/user-attachments/assets/dd5734d4-1f73-4b3b-96a1-a4b3ef637336)\r\n

Oh.. Let me check.

I tried it out and found that 1.11.3 can be compiled successfully. The issue likely stems from 1.11.4.

Oh, surprising. \r\n\r\nActually, 1.11.4 was requested last year.\r\n- https://lists.apache.org/thread/1w9nmrhy716hl6fz6w3pw9w37tv6o0nl\r\n\r\nLet me dig more.

BTW, this patch and the pinning is only for `branch-3.5` because `branch-3.5` has old Scala version, `2.13.8`.

It seems so. I tried compiling with Maven, and it worked fine at commit `86dfeae`. I also tried compiling the latest commit using sbt, and it worked as well.

Thank you always for your deep review and co-investigation!

All tests passed. Merged to branch-3.5.

+1, late LGTM

+1, LGTM. Merging to master/4.0.\r\nThank you, @camilesing and @HyukjinKwon for review.

@hvanhovell Please take a look at this PR if you have time. I think this is simpler than Scala PR and many things are following the existing practice with applyInPandasWithState. Thanks in advance!

All green CI run: https://github.com/jingz-db/spark/actions/runs/13528908675/job/37813528825

@MaxGekk I updated the PR

thanks, merging to master/4.0!

Hi, @nikolamand-db , @cloud-fan . \r\nAlthough I know that this is a valuable test suite for Collation feature, can we exclude this long-running query (14+ minutes) from the CIs?

Could you review this PR, @LuciferYang ?

Could you review this PR, @panbingkun ?

I added the TODO comment, @LuciferYang .

Thank you, @LuciferYang . Merged to master/4.0.

+1, Late LGTM.

cc @nemanjapetr-db  @MaxGekk 

Thank you, @cloud-fan and @LuciferYang .

ping @cloud-fan cc @sunxiaoguang 

thanks, merging to master/4.0!

@cloud-fan Thank you!

ping @cloud-fan cc @sunxiaoguang 

+1, LGTM. Merging to master/4.0.\r\nThank you, @beliefer and @cloud-fan @HyukjinKwon for review.

@MaxGekk @cloud-fan @HyukjinKwon Thank you for all!

cc @stefankandic and @dejankrak-db to take a look.

thanks, merging to master/4.0!\r\n

@wbo4958 @HyukjinKwon @WeichenXu123 

LGTM if tests can pass

The python linter passed in the last run\r\n\r\nmerged to master/4.0

Merged to master/4.0. Thank you, @LuciferYang .

Thanks @dongjoon-hyun 

Merged to master/4.0. Thank you, @yaooqinn .

Merged to master and branch-4.0.

Thanks @HyukjinKwon 

@cloud-fan FYI

> This query fails to execute because the injected cast expression lacks the timezone information\r\n\r\nBTW, @changgyoopark-db can you add a test for the case?

Test first

Thanks @dongjoon-hyun  ~

Merged to master/4.0

Merged into master for Spark 4.1.0. Thanks @dongjoon-hyun and @MaxGekk 

I have checked locally and it is safe to merge this PR at the moment.\r\n\r\n

Merged into master/branch-4.0. Thanks @panbingkun \r\n\r\nMerging this into branch-4.0 is to avoid formatting conflicts with recent backports related to connect.

I wonder if 2 hours is too short ....

Thank you!

Thank you all. I just rebased this to the master to bring SPARK-50863 .

All tests passed.\r\n<img width="354" alt="Screenshot 2025-01-16 at 23 04 47" src="https://github.com/user-attachments/assets/62b6ec4a-3def-4aa3-ba19-21fba2a44103" />\r\n

Merged to master/4.0.\r\n\r\nThank you again, @HyukjinKwon , @zhengruifeng , @LuciferYang .

merged to master/4.0

cc @dongjoon-hyun 

I disable two tests based on my local test, if the remaining ones are still slow in GA, I will disable more

```\r\nFinished test(python3.11): pyspark.ml.tests.connect.test_parity_torch_distributor (1145s) ... 9 tests were skipped\r\nFinished test(python3.11): pyspark.ml.torch.tests.test_distributor (1149s) ... 9 tests were skipped\r\n```\r\n\r\nlet me skip more

```\r\nFinished test(python3.11): pyspark.ml.tests.connect.test_parity_torch_distributor (11s) ... 14 tests were skipped\r\nFinished test(python3.11): pyspark.ml.torch.tests.test_distributor (10s) ... 14 tests were skipped\r\n```\r\n\r\nThe whole pyspark-ml takes less than 1hour now

Ya. All tests passed in time. Merged to master/4.0.

Thank you!

cc: @agubichev 

cc @cloud-fan for review and merging

@AveryQi115 mind reading https://github.com/apache/spark/pull/49536/checks?check_run_id=35749405174, configuring the CI, and runing it?

> Does this PR introduce any user-facing change?\r\n\r\nIt does add the user-facing configuration, so please describe that

Can you link to the original PR that did the change?

thanks, merging to master/4.0!

@HyukjinKwon I agree, and this PR is just to make it configurable (with the default value set to **false** - show stack trace by default). There are many user-friendly errors on the Python side, but they are often buried in a long Python-side stack trace. This change is intended to optionally hide these stack traces to improve the user experience.

@allisonwang-db @ueshin Could you review this PR that adds configuration to hide Python stack trace from analyze_udtf?

> Could you show the example of the error messages, between this is enabled and disabled, in the PR description?\r\n\r\n@ueshin \r\n\r\nIt\

Thanks! Merging to master and branch-4.0.

Hi, @allisonwang-db , @ueshin , @HyukjinKwon . Although this passed CIs, it seems that this causes Python linter failures at both master/branch-4.0 branches. Could you check them please?\r\n\r\n- `master`: https://github.com/apache/spark/actions/runs/13001188421/job/36260099936\r\n- `branch-4.0`: https://github.com/apache/spark/actions/runs/13001191589/job/36260101973\r\n\r\n```\r\nstarting mypy annotations test...\r\nannotations failed mypy checks:\r\npython/pyspark/ml/classification.py:3578: error: Function is missing a return type annotation  [no-untyped-def]\r\npython/pyspark/ml/classification.py:3598: error: Incompatible return value type (got "tuple[int, Any]", expected "CM")  [return-value]\r\npython/pyspark/ml/classification.py:3616: error: Argument "models" to "OneVsRestModel" has incompatible type "list[None]"; expected "list[ClassificationModel]"  [arg-type]\r\npython/pyspark/ml/connect/readwrite.py:198: error: Incompatible types in assignment (expression has type "OneVsRestWriter", variable has type "Write")  [assignment]\r\npython/pyspark/ml/connect/readwrite.py:199: error: "Write" has no attribute "session"  [attr-defined]\r\npython/pyspark/ml/connect/readwrite.py:200: error: "Write" has no attribute "save"  [attr-defined]\r\npython/pyspark/ml/connect/readwrite.py:209: error: Incompatible types in assignment (expression has type "OneVsRestModelWriter", variable has type "Write")  [assignment]\r\npython/pyspark/ml/connect/readwrite.py:210: error: "Write" has no attribute "session"  [attr-defined]\r\npython/pyspark/ml/connect/readwrite.py:211: error: "Write" has no attribute "save"  [attr-defined]\r\nFound 9 errors in 2 files (checked 1083 source files)\r\n1\r\nError: Process completed with exit code 1.\r\n```

@dongjoon-hyun Thanks for letting us know. cc @wengh could you take a look?

Thank you for the reply anyway, @allisonwang-db .

sorry it was my bad ðŸ˜¢ 

Merged to master and branch-4.0.

Hi, @huaxingao . Could you review this PR?

Thank you, @huaxingao .

All tests passed. Merged to master/4.0.

cc @HyukjinKwon 

Merged to master and branch-4.0.

cc @mridulm 

It seems that this code path does not have sufficient test coverage. I added a log statement in the `closeAndRead` method and then conducted tests on the `network-shuffle` and `core` modules locally. I found that: \r\n\r\n\r\n1. The tests in the `network-shuffle` module do not cover this code path\r\n2. There are a few test cases in the `core` module that indirectly cover this path (such as some test cases in `DistributedSuite`),  but it is not easy to add new assertions. \r\n\r\nSo, could you add a targeted test case in this pull request? @ChenMichael  Thanks ~

Thank you for updating. Could you make CI happy @ChenMichael 

Merged to master/4.0.\r\n\r\nPlease make a backporting PR to branch-3.5, @ChenMichael .

Opened https://github.com/apache/spark/pull/49653 for branch-3.5

Could you review this PR to help Apache Spark 4.0 Java 21 test coverage, @huaxingao ?

Thank you, @huaxingao . Merged to master for enabling Apache Spark 4.0.0 Java 21 test coverage.

cc @wbo4958 @HyukjinKwon @WeichenXu123 

![image](https://github.com/user-attachments/assets/f3be2bc8-9208-4f29-981e-9616dc4a0c3a)\r\n

> ![image](https://private-user-images.githubusercontent.com/1475305/404103187-f3be2bc8-9208-4f29-981e-9616dc4a0c3a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzcwODIzNDIsIm5iZiI6MTczNzA4MjA0MiwicGF0aCI6Ii8xNDc1MzA1LzQwNDEwMzE4Ny1mM2JlMmJjOC05MjA4LTRmMjktOTgxZS05NjE2ZGM0YTBjM2EucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDExNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTAxMTdUMDI0NzIyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NjliZWM1OTNkMWI3ODAwOThkYzFiZDU2ODQ3YmU2Y2RhZTdkN2NmNTM3MzIxMzVjMmQzNjdkM2I5NDNmMzBkOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.ubd-wAhrEVgkoFdK2HYIn-PyHIDvOAcVzbwI7s1eEx8)\r\n\r\nThis is expected, we will merge this PR in 4.0

thanks, merged to master/4.0

@MaxGekk thanks for the comments, all addressed in [a03345c0](https://github.com/G-Research/spark/commit/a03345c001f3e58ae80e21abd85b3806e251017d).

cc @beliefer and @yaooqinn FYI 

Contrast to https://github.com/apache/spark/pull/49507, it is a relatively clean PR.

cc @dongjoon-hyun @HyukjinKwon @zhengruifeng @LuciferYang\r\nThanks!

Thank you for keeping tracking this. I hope this could help the situation become better.\r\n\r\nThank you, @panbingkun and @LuciferYang .\r\n\r\nMerged to master for Apache Spark 4.1.0.

When we are sure that everything is clean, you can do backport.

@HyukjinKwon @zhengruifeng @dongjoon-hyun @LuciferYang \r\nJust now I manually triggered the workflow `build_coverage.yml`, and after stopping updating for 11 months, `the codecov of spark` has been restored, as follows:\r\nhttps://app.codecov.io/github/apache/spark\r\nhttps://github.com/apache/spark/actions/runs/12819922613/job/35748629504\r\n<img width="995" alt="image" src="https://github.com/user-attachments/assets/63d7bf30-c7c4-4a85-b6ae-02a0f9d3db86" />\r\n<img width="1405" alt="image" src="https://github.com/user-attachments/assets/5ae52af5-01a0-4e88-87f6-3c987977621e" />\r\n\r\nThanks all!

Nice! 

Then, please backport the relevant patches including this, @panbingkun ~ Thank you so much!

> Then, please backport the relevant patches including this, @panbingkun ~ Thank you so much!\r\n\r\nOkay! â¤ï¸

> BTW, could you double-check this doesn\

CIs finished. @MaxGekk @cloud-fan could we merge it?

The test failure:\r\n```\r\n[info] SparkSessionE2ESuite:\r\n[info] - interrupt all - background queries, foreground interrupt *** FAILED *** (20 seconds, 68 milliseconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 30 times over 20.060643578000004 seconds. Last failure message: q2Interrupted was false. (SparkSessionE2ESuite.scala:71)\r\n[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:\r\n```\r\nis not related to changes.\r\n\r\n+1, LGTM. Merging to master/4.0.\r\nThank you, @mihailoale-db and @cloud-fan @mihailom-db @srielau for review.

cc @wbo4958 

LGTM. Thx @zhengruifeng 

thanks, merged to master/4.0

Merged to master and branch-4.0

nice refactor, btw, is it also merged in 4.0?

brilliant solution. \r\n

yeah it is merged in 4.0 too

cc @HeartSaVioR @WweiL @dongjoon-hyun @HyukjinKwon 

Thanks! I was going to make a PR, but great to see you fixed quickly.

I verified manually.\r\n\r\n**BEFORE**\r\n```\r\n$ build/sbt "sql/testOnly *RocksDBStateStoreCheckpointFormatV2Suite"\r\n...\r\n[error] /Users/dongjoon/APACHE/spark-merge/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBStateStoreCheckpointFormatV2Suite.scala:626:41: Option[Map[Long,Array[Array[String]]]] does not take parameters\r\n[error]       val res2 = metadata.stateUniqueIds(0).map { uniqueIds =>\r\n[error]                                         ^\r\n[error] /Users/dongjoon/APACHE/spark-merge/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBStateStoreCheckpointFormatV2Suite.scala:634:85: value sorted is not a member of Nothing\r\n[error]         if (!versionToUniqueIdFromStateStore(version).sorted.sameElements(uniqueIds.sorted)) {\r\n[error]                                                                                     ^\r\n[error] two errors found\r\n[error] (sql / Test / compileIncremental) Compilation failed\r\n[error] Total time: 101 s (01:41), completed Jan 15, 2025, 7:44:17\u202fPM\r\n```\r\n\r\n**THIS PR**\r\n```\r\n$ build/sbt "sql/testOnly *RocksDBStateStoreCheckpointFormatV2Suite"\r\n...\r\nWARNING: Using incubator modules: jdk.incubator.vector\r\n[info] RocksDBStateStoreCheckpointFormatV2Suite:\r\n...\r\n```\r\n\r\nLet me merge this to recover master branch.

Thank you for the quick fix and sorry for the trouble! @LuciferYang @HeartSaVioR @dongjoon-hyun 

It didn\

We can work around. I created a new PR: https://github.com/apache/spark/pull/49524

Merged to master and branch-4.0.

late LGTM

Merged to master and branch-4.0.

Late LGTM, thanks

The whole checkRecursion logic is now rewritten and placed in ResolveWithCTE as discussed offline. \r\nOne note here: I am still not sure if we should keep datatype check since it throws an error already before coming to this part of the code in case data types of anchor and recursive part are different. Also, still not sure which equality check between data types should be used

As discussed offline, `checkIfSelfReferenceIsPlacedCorrectly` and `checkDataTypesAnchorAndRecursiveTerm` definitions left in `resolveWithCTE` singleton, but invoked in `checkAnalysis` to be invoked only once during analysis and not multiple times as if it would have been if we had invoked them in `resolveWithCTE` as well. \r\n\r\n`checkNumberOfSelfReferences` is moved earlier to CTESubstitution stage following the "fail early" approach

thanks, merging to master/4.0!

Thank you, @LuciferYang . Merged to master.

Could you review this follow-up, @huaxingao , please?

Thank you, @huaxingao .\r\n\r\nMerged to master.

Thank you, @huaxingao .\r\n\r\nMerged to master.

Thank you, @huaxingao !

For the record, this is applied successfully.\r\n- https://github.com/apache/spark/actions/workflows/build_branch40.yml\r\n\r\n![Screenshot 2025-01-15 at 10 44 38](https://github.com/user-attachments/assets/58497f65-2097-4cb9-8653-6a9ad72bad5b)\r\n

thanks, merging to master/4.0!

Thank you, @LuciferYang .\r\n\r\nMerged to master.

Thank you, @huaxingao !\r\n\r\nMerged to branch-4.0

cc @stefankandic and @dejankrak-db 

> LGTM, I would also like a confirmation from @jovanpavl-db just to double check that there are no blockers for enabling this by default\r\nYes, ready to go.\r\n

Gentle ping, @stevomitric .

+1, LGTM. Merging to master.\r\nThank you, @stevomitric and @dongjoon-hyun @jovanpavl-db @stefankandic for review.

Thank you all!

LGTM.

The Spark Connect failure is unrelated, thanks, merging to master/4.0!

Merged to master/4.0. Thank you, @Ngone51 and all.

cc @dongjoon-hyun @HyukjinKwon @zhengruifeng @LuciferYang \r\nCould you take a look? I want to try this approach.\r\nThanks!

> Just want to check. According to the commit log and CI result, this isn\

- I have verified it in my own mock repo:\r\nhttps://github.com/panbingkun/github-actions/pull/1\r\n<img width="1340" alt="image" src="https://github.com/user-attachments/assets/00cc0006-ee1a-4713-8dfd-229b8b0270b8" />\r\nhttps://github.com/panbingkun/github-actions/actions/runs/12804808017/job/35700210844\r\n<img width="1012" alt="image" src="https://github.com/user-attachments/assets/d1b69056-35c8-4921-80bb-9dcbbd3741a3" />\r\n<img width="985" alt="image" src="https://github.com/user-attachments/assets/fb525ba3-41ed-4589-8f9e-64c4ec76ffbf" />\r\n\r\n- Meanwhile, I am conducting further verification on `my own Spark repo`.\r\nhttps://github.com/apache/spark/pull/49499\r\nhttps://github.com/panbingkun/spark/actions/runs/12805018154/job/35700637278\r\n<img width="859" alt="image" src="https://github.com/user-attachments/assets/1ec57ec0-d179-4c73-a0c5-c5495196d719" />\r\n<img width="992" alt="image" src="https://github.com/user-attachments/assets/072c3d18-6c6c-4b40-8bdb-335450818ae7" />\r\n

hmm... can we make a clean one?

> hmm... can we make a clean one?\r\n\r\nOkay\r\n\r\nhttps://github.com/apache/spark/pull/49527.

closing in favor of\xa0https://github.com/apache/spark/pull/49527

cc @cloud-fan 

Can we also update the comment in `HiveClientImpl#getSparkSQLDataType`?

> Can we also update the comment in HiveClientImpl#getSparkSQLDataType?\r\n\r\nI checked the comment and found them still suitable\r\n\r\n\r\n

> I checked the comment and found them still suitable\r\n\r\nThe comment should only focus on native Hive tables, as Spark created tables will never hit it. Anyway, not a big deal.

Thank you, @yaooqinn and @cloud-fan .\r\nMerged to master/4.0.

Thanks a lot @dongjoon-hyun @cloud-fan 

@MaxGekk please take a look when you get the chance

@stefankandic Thanks for the ping. Will look at it soon.

+1, LGTM. Merging to master/4.0.\r\nThank you, @stefankandic.

merged to master

Hi, @zhengruifeng and @cloud-fan .\r\n\r\nSurprisingly, from this commit, `pyspark.ml.connect` test pipeline becomes very slower. The increase is `1h 30m`. Could you take a look at the slowness, please?\r\n- https://github.com/apache/spark/actions/runs/12787606555/job/35647230303 (2h 48m 50s)\r\n- https://github.com/apache/spark/actions/runs/12784129315/job/35636611457 (1h 17m 40s, Previous Commit)\r\n\r\n<img width="645" alt="Screenshot 2025-01-16 at 17 13 10" src="https://github.com/user-attachments/assets/f3049c1d-cb3e-418b-9925-a3cbf2514809" />\r\n\r\nAlso, cc @HyukjinKwon , FYI

@dongjoon-hyun yes, this PR is doc-only

sure, I will take look

Thank you so much. \r\n\r\nFor the record, the situation identically happened in both `master` and `branch-4.0`.

Will merge as soon as the CI passed.

Merging to master and branch-4.0

cc @dongjoon-hyun and @LuciferYang, sorry for missing that.

@LuciferYang thanks for reviewing and suggestion, I updated the manual test steps in the PR description.

Merged into master. Thanks @pan3793 

@Ngone51 the new test failed

@cloud-fan is there a way to configure the parallelize for `Range`?

Gentle ping, @Ngone51 ~

Maybe, could you rebase this PR to `master` branch once more, @Ngone51 ? The failure is a fixed one in the master branch.\r\n- https://github.com/apache/spark/pull/49621\r\n\r\n```\r\n[info] - SPARK-29442 Set `default` mode should override the existing mode *** FAILED *** (6 milliseconds)\r\n[info]   java.lang.NoSuchFieldException: mode\r\n```

Oh. Got it.

@cloud-fan @dongjoon-hyun I have updated the PR. Could you take another look? Thanks!

let me trigger the core test ..

https://github.com/HyukjinKwon/spark/actions/runs/12782790053

Merged to master.

hardcode + empty file: https://github.com/panbingkun/spark/actions/runs/12782585262/job/35632373115\r\nhardcode(success): https://github.com/panbingkun/spark/runs/35636937617\r\nenv(fail): https://github.com/panbingkun/spark/runs/35636874980

Thank you, @LuciferYang .

Merged to master for Apache Spark 4.0.0.

Merged to master.

cc @cloud-fan and @HyukjinKwon 

All tests passed. Just waiting for the branch cut.\r\n\r\n![Screenshot 2025-01-15 at 00 00 49](https://github.com/user-attachments/assets/6700a064-a701-4e63-85b5-599f4be55010)\r\n

Yep, I replied on your last email. Thanks.

4.0 branch has been cut, thanks, merging to master!

Thank you, @HyukjinKwon , @cloud-fan ,  @LuciferYang , @panbingkun .

Merged to master.

cc @viirya @huaxingao @cloud-fan @allisonwang-db @rdblue @dongjoon-hyun

thanks, merging to master/4.0!

Thank you, @cloud-fan @dongjoon-hyun!

@dongjoon-hyun @parthchandra @LuciferYang would you please take a look at this PR?

Merged to master/4.0 for Apache Spark 4.0.0.\r\n\r\nThank you again, @pan3793 and @parthchandra .

This is a follow-up for https://github.com/apache/spark/pull/49399.

> Could you re-trigger those failed test pipeline by rebasing to `master` branch, @bozhang2820 ?\r\n\r\nThanks @dongjoon-hyun! Rebased to master branch.

@JiexingLi, @HyukjinKwon, could you review again?

Merged to master and branch-4.0.

thanks, merged to master

Is the CI failure related - https://github.com/jingz-db/spark/actions/runs/12837471455/job/35801692179 ?

> Is the CI failure related - https://github.com/jingz-db/spark/actions/runs/12837471455/job/35801692179 ?\r\n\r\nYes it is related to proto file changes. I just rebased on latest master and generate py files from proto file.

cc @hvanhovell 

@jingz-db - lets update the PR description to mention that this only covers support in Scala. Thx

> @jingz-db - lets update the PR description to mention that this only covers support in Scala. Thx\r\n\r\nJust a side question. Do you have some Spark committers in your mind to get reviews for this PR?

> Do you have some Spark committers in your mind to get reviews for this PR?\r\n\r\nYes - cc - @HeartSaVioR - PTAL also, thx !

LGTM pending green CI

@jingz-db - test failures seem related ?

> @jingz-db - test failures seem related ?\r\n\r\nFixed now. It is failing because global spark session should not be used in the FEB sink. Fix it by explicitly throw exception and ending the query.

Just a question. Is this ready for 4.0.0, @HeartSaVioR ?

cc @cloud-fan 

https://lists.apache.org/thread/zxdox7hgzk1ol7d1d0zsjrrvyck18y0o\r\nThe thread for seeking consensus.

@HyukjinKwon @cloud-fan Can you look at this?

@gene-db I have now added support for variants in createDataFrame in Spark Connect as well. Can you review again since it modifies one of the code paths that you worked on?

thanks, merging to master!

cc @MaxGekk as I believe you have recently reviewed similar doc PRs.

+1, LGTM. Merging to master.\r\nThank you, @nchammas.

Adding @cloud-fan @srielau @davidm-db @dusantism-db 

thanks, merging to master!

Merged to master.

SPARK-46094 added the JVM profiler but limited the scope to executor-only, and all classes and configurations are located under the "executor" namespace, it looks a little bit weird if we accept this PR to extend the scope also cover the driver.\r\n\r\nAdditionally, Spark already has the following configurations for profiling Python process\r\n\r\n- `spark.python.profile`\r\n- `spark.python.profile.memory`\r\n- `spark.python.profile.dump`\r\n\r\nGiven that, I propose the following changes:\r\n\r\n- rename `ExecutorProfilerPlugin` to `ProfilerPlugin`(or `JVMProfilerPlugin`)\r\n- move classes to `org.apache.spark.profile`(`profile` or `profiler`) package\r\n- move configuration to `spark.profile.`(or `spark.jvm.profile.`) namespace\r\n\r\nthen the configurations might like\r\n\r\n```\r\nspark.profile.driver.enabled\r\nspark.profile.executor.enabled\r\nspark.profile.executor.fraction\r\nspark.profile.dfsDir\r\nspark.profile.local\r\nspark.profile.options\r\nspark.profile.writeInterval\r\n```\r\n\r\nsome thoughts:\r\n\r\n- `spark.profile.` vs `spark.jvm.profile.`, I prefer the former because 1) JVM is the main process of Spark that we don\

cc @dongjoon-hyun @HyukjinKwon @mridulm would you please give some suggestions for the proposed feature and refactoring idea? thanks in advance

Thank you, @pan3793 .

cc @parthchandra , too.

BTW, @pan3793 . For the CI failures, I reverted the root cause from the master branch via https://github.com/apache/spark/commit/2d498d51ab8333238e77e5e7de952ff3b0276b3b .  Could you rebase once more?\r\n- https://github.com/pan3793/spark/actions/runs/12764571679/job/35580992855

One thing to keep in mind for refactoring is that at some point we also want to integrate this with [SPARK-45209](https://issues.apache.org/jira/browse/SPARK-45209) which essentially means collecting stack traces via async profiler and shipping them to the liveUI for display. 

I made a refactor PR https://github.com/apache/spark/pull/49492 first

If this is ready again, please let us know, @pan3793 .

Gentle ping, @xinrong-meng .\r\n\r\nIf this is targeting Apache Spark 4.0, we had better have this before February 1st.\r\n- https://spark.apache.org/versioning-policy.html

Thanks @dongjoon-hyun I just got some free cycles for that and will resolve it ASAP.

A quick update: the PR is blocked by UDT support in Arrow Python UDF, which Iâ€™m currently working on

Thank you for the updated context.

Thanks @dongjoon-hyun for attention! The current proposal is to fall back to the existing (non-Arrow-optimized) Python UDF when UDT is involved. My understanding is that no further testing is needed and the code change is minimal (just an if-else), but I respect the communityâ€™s decision.

I marked it as WIP because I wanted to file a separate PR for the fallback mechanism with tests. Once that PR is in, this PR will be unblocked immediately.

The Arrow fallback PR is in so this PR should be unblocked. Iâ€™ll keep an eye on testing and make it ready ASAP!\r\n\r\nThank you @HyukjinKwon @dongjoon-hyun 

can we address https://github.com/apache/spark/pull/49482#discussion_r1914465635 @xinrong-meng ?

Can you take a look at the test failure to make sure? I think those failures look related.

Failed tests seem irrelevant:\r\n```\r\n[info] MySQLNamespaceSuite:\r\n[info] org.apache.spark.sql.jdbc.v2.MySQLNamespaceSuite *** ABORTED *** (10 seconds, 369 milliseconds)\r\n[info]   com.github.dockerjava.api.exception.InternalServerErrorException: Status 500: {"message":"driver failed programming external connectivity on endpoint condescending_lumiere (7a051f139d30436d8a1e231e3f4aeb991784c60ddc1f391ee8c1fc587c8ce2ca): Error starting userland proxy: listen tcp4 0.0.0.0:39901: bind: address already in use"}\r\n```\r\nRetriggering tests https://github.com/xinrong-meng/spark/runs/36869403425

Thank you @dongjoon-hyun !

There are some subtle diff found. I will revert this for now, and enable it back later after improving this more.

same with SPARK-50793, closed

maybe I should convert the purpose of this PR to "Enable SBT CI for profiler module"

Yes, +1 for re-write the scope. Please describe the problem exactly. Then, I can help you easily.

https://github.com/apache/spark/blob/2d498d51ab8333238e77e5e7de952ff3b0276b3b/project/SparkBuild.scala#L1468-L1478\r\n\r\nPerhaps there is a potential requirement to define the `profiler` in `SparkBuild.scala` to exclude this module from the `unidoc` build. However, even if such a requirement exists, it should be addressed in a separate PR.

+CC @HeartSaVioR (who reviewed #29149), thanks !

Thanks @HeartSaVioR !\r\nMerged to master/branch-4.0

FYI, @LuciferYang and @panbingkun 

`genjavadoc` is ready and this is blocked by `Ammonite` currently.

For the record, the root cause and workaround is on the way in Scala and Ammonite community.\r\n- https://github.com/scala/scala/pull/10868#issuecomment-2613022940

Finally, this PR is ready for review. Could you review this PR when you have some time, @LuciferYang ?

Merged into master and branch-4.0. Thanks @dongjoon-hyun 

Thank you, @LuciferYang !

@zhengruifeng @LuciferYang @dongjoon-hyun @HyukjinKwon \r\nCould you take a quick look? I want to merge it quickly and verify that this approach is feasible.\r\nThanks!

> +1 for the try, @panbingkun .\r\n\r\nThanks! Let me try it out.

Merging to master!\r\nLet me trigger a new job now.

Merged to master for Apache Spark 4.0.0.\r\nI manually tested. \r\n\r\n```\r\n$ build/sbt "core/testOnly org.apache.spark.deploy.history.*"\r\n...\r\n[info] RollingEventLogFilesWriterSuite:\r\n[info] - create EventLogFileWriter with enable/disable rolling (184 milliseconds)\r\n[info] - initialize, write, stop - with codec None (73 milliseconds)\r\n[info] - initialize, write, stop - with codec Some(lz4) (176 milliseconds)\r\n[info] - initialize, write, stop - with codec Some(lzf) (39 milliseconds)\r\n[info] - initialize, write, stop - with codec Some(snappy) (157 milliseconds)\r\n[info] - initialize, write, stop - with codec Some(zstd) (44 milliseconds)\r\n[info] - Use the default value of spark.eventLog.compression.codec (5 milliseconds)\r\n[info] - Event log names (1 millisecond)\r\n[info] - Log overwriting (38 milliseconds)\r\n[info] - rolling event log files - codec None (177 milliseconds)\r\n[info] - rolling event log files - codec Some(lz4) (95 milliseconds)\r\n[info] - rolling event log files - codec Some(lzf) (92 milliseconds)\r\n[info] - rolling event log files - codec Some(snappy) (87 milliseconds)\r\n[info] - rolling event log files - codec Some(zstd) (91 milliseconds)\r\n[info] - rolling event log files - the max size of event log file size less than lower limit (6 milliseconds)\r\n[info] RocksDBBackendChromeUIHistoryServerSuite:\r\n[info] LevelDBBackendChromeUIHistoryServerSuite:\r\n[info] LevelDBBackendFsHistoryProviderSuite:\r\n[info] RocksDBBackendWithProtobufSerializerSuite:\r\n[info] - Parse application logs (inMemory = true) (196 milliseconds)\r\n[info] - Parse application logs (inMemory = false) (779 milliseconds)\r\n[info] - SPARK-31608: parse application logs with HybridStore (297 milliseconds)\r\n[info] - SPARK-41685: Verify the configurable serializer for history server (2 milliseconds)\r\n[info] - SPARK-3697: ignore files that cannot be read. (79 milliseconds)\r\n[info] - history file is renamed from inprogress to completed (70 milliseconds)\r\n[info] - SPARK-39439: Check final file if in-progress event log file does not exist (62 milliseconds)\r\n[info] - Parse logs that application is not started (57 milliseconds)\r\n[info] - SPARK-5582: empty log directory (59 milliseconds)\r\n[info] - apps with multiple attempts with order (426 milliseconds)\r\n[info] - log urls without customization (225 milliseconds)\r\n[info] - custom log urls, including FILE_NAME (151 milliseconds)\r\n[info] - custom log urls, excluding FILE_NAME (157 milliseconds)\r\n[info] - custom log urls with invalid attribute (161 milliseconds)\r\n[info] - custom log urls, LOG_FILES not available while FILE_NAME is specified (164 milliseconds)\r\n[info] - custom log urls, app not finished, applyIncompleteApplication: true (176 milliseconds)\r\n[info] - custom log urls, app not finished, applyIncompleteApplication: false (188 milliseconds)\r\n[info] - log cleaner (73 milliseconds)\r\n[info] - should not clean inprogress application with lastUpdated time less than maxTime (69 milliseconds)\r\n[info] - log cleaner for inProgress files (62 milliseconds)\r\n[info] - Event log copy (67 milliseconds)\r\n[info] - driver log cleaner (9 milliseconds)\r\n[info] - SPARK-8372: new logs with no app ID are ignored (56 milliseconds)\r\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\r\n[info] - provider correctly checks whether fs is in safe mode (724 milliseconds)\r\n[info] - provider waits for safe mode to finish before initializing (88 milliseconds)\r\n[info] - provider reports error after FS leaves safe mode (106 milliseconds)\r\n[info] - ignore hidden files (89 milliseconds)\r\n[info] - support history server ui admin acls (482 milliseconds)\r\n[info] - mismatched version discards old listing (149 milliseconds)\r\n[info] - invalidate cached UI (245 milliseconds)\r\n[info] - clean up stale app information (181 milliseconds)\r\n[info] - SPARK-21571: clean up removes invalid history files (63 milliseconds)\r\n[info] - always find end event for finished apps (66 milliseconds)\r\n[info] - parse event logs with optimizations off (60 milliseconds)\r\n[info] - SPARK-24948: ignore files we don\

`sql` module passed. Merged to master.

Merged to master.

@HyukjinKwon I change the way the bug is fixed based on your comment.

Hi, @siying , @HyukjinKwon , @cloud-fan , unfortunately, this streaming failure hid the real `connect` module failure during the review. The following is a failure in `master` branch.\r\n- https://github.com/apache/spark/actions/runs/12764965488/job/35578198872\r\n- https://github.com/apache/spark/actions/runs/12764990775/job/35578281436\r\n- https://github.com/apache/spark/actions/runs/12765679479/job/35580462079\r\n- https://github.com/apache/spark/actions/runs/12766705780/job/35583611269\r\n- https://github.com/apache/spark/actions/runs/12768518891/job/35589102781\r\n- https://github.com/apache/spark/actions/runs/12769341480/job/35591745660\r\n\r\nI also verified manually.\r\n```\r\n$ build/sbt "connect/testOnly org.apache.spark.sql.connect.ProtoToParsedPlanTestSuite"\r\n...\r\n[info] *** 4 TESTS FAILED ***\r\n[error] Failed tests:\r\n[error] \torg.apache.spark.sql.connect.ProtoToParsedPlanTestSuite\r\n[error] (connect / Test / testOnly) sbt.TestsFailedException: Tests unsuccessful\r\n[error] Total time: 152 s (02:32), completed Jan 14, 2025, 6:47:51\u202fAM\r\nM3-Max ~s:master $ build/sbt "connect/testOnly org.apache.spark.sql.connect.ProtoToParsedPlanTestSuite"\r\n```\r\n\r\nThe error message is clear, `The first argument of the TO_PROTOBUF SQL function must be a struct type`.\r\n```\r\n[info] - to_protobuf_messageClassName_descFilePath_options *** FAILED *** (3 milliseconds)\r\n[info]   org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DATATYPE_MISMATCH.TYPE_CHECK_FAILURE_WITH_HINT] Cannot resolve "to_protobuf(bytes, StorageLevel, X\

For the record, `master` branch and PR builders became healthy back.\r\n- https://github.com/apache/spark/actions/runs/12770190322/job/35594495933

To @siying , please make a new PR after testing `ProtoToParsedPlanTestSuite` from your side locally.

```\r\n2025-01-14T06:34:36.3394415Z ========================================================================\r\n2025-01-14T06:34:36.3395038Z Running Spark unit tests\r\n2025-01-14T06:34:36.3395523Z ========================================================================\r\n2025-01-14T06:34:36.3396528Z [info] Running Spark tests using SBT with these arguments:  -Phadoop-3 sql-kafka-0-10/test protobuf/test connect/test connect-client-jvm/test\r\n2025-01-14T06:34:36.3519799Z Using /opt/hostedtoolcache/Java_Zulu_jdk/17.0.13-11/x64 as default JAVA_HOME.\r\n2025-01-14T06:34:36.3520828Z Note, this will be overridden by -java-home if it is set.\r\n2025-01-14T06:34:36.3524276Z Using SPARK_LOCAL_IP=localhost\r\n\r\n...\r\n\r\n2025-01-14T07:20:58.0914533Z \x1b[0m[\x1b[0m\x1b[0minfo\x1b[0m] \x1b[0m\x1b[0m\x1b[36mRun completed in 45 minutes, 48 seconds.\x1b[0m\x1b[0m\r\n2025-01-14T07:20:58.0915915Z \x1b[0m[\x1b[0m\x1b[0minfo\x1b[0m] \x1b[0m\x1b[0m\x1b[36mTotal number of tests run: 513\x1b[0m\x1b[0m\r\n2025-01-14T07:20:58.0916964Z \x1b[0m[\x1b[0m\x1b[0minfo\x1b[0m] \x1b[0m\x1b[0m\x1b[36mSuites: completed 30, aborted 0\x1b[0m\x1b[0m\r\n2025-01-14T07:20:58.0918191Z \x1b[0m[\x1b[0m\x1b[0minfo\x1b[0m] \x1b[0m\x1b[0m\x1b[36mTests: succeeded 512, failed 1, canceled 0, ignored 0, pending 0\x1b[0m\x1b[0m\r\n2025-01-14T07:20:58.0919376Z \x1b[0m[\x1b[0m\x1b[0minfo\x1b[0m] \x1b[0m\x1b[0m\x1b[31m*** 1 TEST FAILED ***\x1b[0m\x1b[0m\r\n2025-01-14T07:20:58.0930204Z \x1b[0m[\x1b[0m\x1b[31merror\x1b[0m] \x1b[0m\x1b[0mFailed tests:\x1b[0m\r\n2025-01-14T07:20:58.0931430Z \x1b[0m[\x1b[0m\x1b[31merror\x1b[0m] \x1b[0m\x1b[0m\torg.apache.spark.sql.kafka010.KafkaMicroBatchV1SourceWithConsumerSuite\x1b[0m\r\n2025-01-14T07:20:59.4326024Z \x1b[0m[\x1b[0m\x1b[31merror\x1b[0m] \x1b[0m\x1b[0m(sql-kafka-0-10 / Test / \x1b[31mtest\x1b[0m) sbt.TestsFailedException: Tests unsuccessful\x1b[0m\r\n2025-01-14T07:20:59.4392106Z \x1b[0m[\x1b[0m\x1b[31merror\x1b[0m] \x1b[0m\x1b[0mTotal time: 2774 s (46:14), completed Jan 14, 2025, 7:20:59 AM\x1b[0m\r\n2025-01-14T07:20:59.9214006Z \x1b[0J[error] running /home/runner/work/spark/spark/build/sbt -Phadoop-3 sql-kafka-0-10/test protobuf/test connect/test connect-client-jvm/test ; received return code 1\r\n2025-01-14T07:20:59.9431289Z ##[error]Process completed with exit code 18.\r\n```\r\n\r\nBased on the test [logs](https://productionresultssa7.blob.core.windows.net/actions-results/4ac510af-2050-47af-86e4-0721a670d2e4/workflow-job-run-c61d38dd-6dd7-596f-9b5b-92b6dd0e3d75/logs/job/job-logs.txt?rsct=text%2Fplain&se=2025-01-14T16%3A37%3A13Z&sig=SmlovgbhvJSLcpd6zvJ1J443ftatt3mtzlw%2Fh0ElTm4%3D&ske=2025-01-15T02%3A20%3A14Z&skoid=ca7593d4-ee42-46cd-af88-8b886a2f84eb&sks=b&skt=2025-01-14T14%3A20%3A14Z&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skv=2024-11-04&sp=r&spr=https&sr=b&st=2025-01-14T16%3A27%3A08Z&sv=2024-11-04), it was expected to test four modules: `-Phadoop-3 sql-kafka-0-10/test protobuf/test connect/test connect-client-jvm/test`. However, an early termination occurred after the `sql-kafka-0-10` test failed, and the subsequent modules were not tested

Thanks for reverting this out.

Gentle ping, @siying .

let me followup

Merged to master. Thank you!

Thank you @ueshin for fixing that!

cc @cloud-fan @srielau 

thanks, merging to master/4.0! (This is the last piece of the SQL UDF feature)

The last similar change was made on SPARK-37256 | https://issues.apache.org/jira/browse/SPARK-37256

Thanks @dongjoon-hyun 

CIs passed. We can merge it.

thanks, merging to master!

gentle ping @yaooqinn @wangyum 

Today, I revisited this PR and after testing, I found that in client mode, both the hadoop conf and krb5.conf cannot be mounted to the executor pod correctly. Is this the expected behavior? \r\nOf course, the executor pod can still start normally, executor pod log shows `UserGroupInformation: Hadoop UGI authentication : SIMPLE`. @pan3793 @turboFei 

> Today, I revisited this PR and after testing, I found that in client mode, both the hadoop conf and krb5.conf cannot be mounted to the executor pod correctly. Is this the expected behavior? Of course, the executor pod can still start normally, executor pod log shows `UserGroupInformation: Hadoop UGI authentication : SIMPLE`. @pan3793 @turboFei\r\n\r\nHi @maomaodev in my use case, the spark jobs are submitted via kyuubi gateway with cluster mode. So I did not aware this kind of issue. 

> Is this the expected behavior?\r\n\r\nCan it pass the kerberos authentication in client mode?

> Can it pass the kerberos authentication in client mode?\r\n\r\n@turboFei  Yes, in client mode, since the Hadoop conf is not mounted correctly, it is also acceptable not to mount krb5.conf, although this not be the expected behavior).\r\nIn my opinion, if we want to be compatible with client mode, then the executor pod should not share the same config map with the driver pod. Perhaps we can refer to the implementation here: `org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend#setUpExecutorConfigMap`.

@asl3 @dongjoon-hyun 

thanks @panbingkun so much!

Merged to master.

Thanks all.

I think we should spend some time to pick which versions to use in the CI.

@dongjoon-hyun @HyukjinKwon \r\nIt is a good question, I think probably we should test it against all supported versions, WDYT?

We actually have the schedule the build for all supported versions ... If we need to test one specific thing with the one specific Python version, then we should think about which version to use

thanks, let me do more investigation, I am currently confused by the python versions in the test

thanks, merged to master

closing in favor of https://github.com/apache/spark/pull/49527

Could you review this too, @LuciferYang ?

+1, LGTM

Thank you!

Merged to branch-3.5.

The test failure is unrelated, thanks, merging to master!

Thank you, @LuciferYang . Merged to master.

cc @jovanm-db and @cloud-fan 

Thank you for review, @mihailom-db .

Merged into master. Thanks @dongjoon-hyun @cloud-fan @mihailom-db 

Thank you, @LuciferYang and @cloud-fan .

To @LuciferYang and other reviewers, we need two follow-up patches (SPARK-50525 and SPARK-50707) to recover NON-ANSI CI completely.\r\n\r\nI trigger another one (the following) because [the above one](https://github.com/apache/spark/pull/49458#issuecomment-2586066056) only has SPARK-50707.\r\n- https://github.com/apache/spark/actions/runs/12739621896\r\n```\r\n$ git log --oneline -n3\r\n4cecde625df (HEAD -> master, apache/master, apache/HEAD) [SPARK-50797][SQL][TESTS] Move `HiveCharVarcharTestSuite` from `o/a/s/sql` to `o/a/s/sql/hive`\r\ne284b2ce1ec [SPARK-50525][SQL][TESTS][FOLLOWUP] Fix `DataFrameSuite.repartition by MapType` test assumption\r\n898f7afc5dd [SPARK-50707][SQL][TESTS][FOLLOWUP] Fix `CharVarcharTestSuite` test case assumption\r\n```

cc @ostronaut and @cloud-fan 

Thank you, @cloud-fan . Merged to master.

Thank you @dongjoon-hyun for fixing it! I was not aware that `SPARK_ANSI_SQL_MODE` might cause some issues.

Merged into master for Spark 4.0. Thanks @dongjoon-hyun 

Thank you, @LuciferYang !

thanks, merging to master!

All maven test passed: https://github.com/LuciferYang/spark/runs/35474430380\r\n\r\n![image](https://github.com/user-attachments/assets/079eed98-82c3-4bfe-b42f-358182b7551d)\r\n

cc @yaooqinn 

Thank you @sunxiaoguang, the changes almost look good to me, except for two minor comments.

@sunxiaoguang\r\nI have an idea split the support for StringType into another PR. So this PR is good to review and merge.

> @sunxiaoguang I have an idea split the support for StringType into another PR. So this PR is good to review and merge.\r\n\r\nUnfortunately, that would be problematic for functions that accepts both binary and string as parameter, e.g. MD5. Without cast support, those functions work partially only which can surprise users. Since we need to support string eventually, the questions about collation have to be resolved somehow.

> Thanks for the hint. Another question, do we need to write a complete design document and going through a review process about the design before actually writing the implementation?\r\n\r\nI think it is not complicate enough, you just need a try and commit a PR.

Can you provide your take on this:\r\n> My understanding is that this never produced any kind of results that customers could have adapted to - hence flag is not required. Is this your understanding as well ?

> Question: Prior to these changes is there any scenario where some data sources might have returned something, although incorrect ? Is there a chance that someone has adapted to this "wrong" behaviour ?\r\n> \r\n> Should we protect this change behind a flag ?\r\n> \r\n> My understanding is that this never produced any kind of results that customers could have adapted to - hence flag is not required. Is this your understanding as well ?\r\n> \r\n> Also. please lets check that pushdown is happening, to make sure we are testing what we want\r\n\r\nYes your understanding is correct. Before this PR, queries using binary literal would generates SQL with invalid binary literal and causes SQL to fail.\r\n\r\n<img width="1180" alt="image" src="https://github.com/user-attachments/assets/92800c55-5400-46b0-b3f1-d95b85d89cb5" />\r\n

thanks, merging to master/4.0!

```\r\n[info] - get, put, remove, commit, and all data iterator - with codec lz4 - with colFamiliesEnabled=true *** FAILED *** (157 milliseconds)\r\n[info]   "[CANNOT_LOAD_STATE_STORE.UNCATEGORIZED] An error occurred during loading state.  SQLSTATE: 58030" did not contain "does not exist" (StateStoreSuite.scala:1124)\r\n[info]   org.scalatest.exceptions.TestFailedException:\r\n[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n[info]   at org.apache.spark.sql.execution.streaming.state.StateStoreSuiteBase.$anonfun$new$100(StateStoreSuite.scala:1124)\r\n[info]   at org.apache.spark.sql.execution.streaming.state.StateStoreSuiteBase.tryWithProviderResource(StateStoreSuite.scala:1764)\r\n[info]   at org.apache.spark.sql.execution.streaming.state.StateStoreSuiteBase.$anonfun$new$99(StateStoreSuite.scala:1092)\r\n[info]   at org.apache.spark.sql.execution.streaming.state.StateStoreSuiteBase.$anonfun$new$99$adapted(StateStoreSuite.scala:1091)\r\n[info]   at org.apache.spark.sql.execution.streaming.state.StateStoreCodecsTest.$anonfun$testWithAllCodec$4(StateStoreCompatibilitySuite.scala:72)\r\n[info]   at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:56)\r\n[info]   at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)\r\n[info]   at org.apache.spark.sql.execution.streaming.state.StateStoreSuiteBase.withSQLConf(StateStoreSuite.scala:1082)\r\n[info]   at org.apache.spark.sql.execution.streaming.state.StateStoreCodecsTest.$anonfun$testWithAllCodec$3(StateStoreCompatibilitySuite.scala:72)\r\n[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r\n[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r\n[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)\r\n[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)\r\n```\r\n\r\nIt seems that the failed case is related to this pr?

> ```\r\n> [info] - get, put, remove, commit, and all data iterator - with codec lz4 - with colFamiliesEnabled=true *** FAILED *** (157 milliseconds)\r\n> [info]   "[CANNOT_LOAD_STATE_STORE.UNCATEGORIZED] An error occurred during loading state.  SQLSTATE: 58030" did not contain "does not exist" (StateStoreSuite.scala:1124)\r\n> [info]   org.scalatest.exceptions.TestFailedException:\r\n> [info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n> [info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n> [info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n> [info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n> [info]   at org.apache.spark.sql.execution.streaming.state.StateStoreSuiteBase.$anonfun$new$100(StateStoreSuite.scala:1124)\r\n> [info]   at org.apache.spark.sql.execution.streaming.state.StateStoreSuiteBase.tryWithProviderResource(StateStoreSuite.scala:1764)\r\n> [info]   at org.apache.spark.sql.execution.streaming.state.StateStoreSuiteBase.$anonfun$new$99(StateStoreSuite.scala:1092)\r\n> [info]   at org.apache.spark.sql.execution.streaming.state.StateStoreSuiteBase.$anonfun$new$99$adapted(StateStoreSuite.scala:1091)\r\n> [info]   at org.apache.spark.sql.execution.streaming.state.StateStoreCodecsTest.$anonfun$testWithAllCodec$4(StateStoreCompatibilitySuite.scala:72)\r\n> [info]   at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:56)\r\n> [info]   at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)\r\n> [info]   at org.apache.spark.sql.execution.streaming.state.StateStoreSuiteBase.withSQLConf(StateStoreSuite.scala:1082)\r\n> [info]   at org.apache.spark.sql.execution.streaming.state.StateStoreCodecsTest.$anonfun$testWithAllCodec$3(StateStoreCompatibilitySuite.scala:72)\r\n> [info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r\n> [info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r\n> [info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r\n> [info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r\n> [info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)\r\n> [info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)\r\n> ```\r\n> \r\n> It seems that the failed case is related to this pr?\r\n\r\nYes, I added the error handling incorrectly. Modified it to Option(e.getCondition).exists(_.contains("CANNOT_LOAD_STATE_STORE")) instead, should be correct now. Thanks for catching.

Merged into master for Spark 4.0. Thanks @liviazhu-db and @WweiL 

Merged to master.

The remaining test failures are not related to this PR.

Thanks! merging to master.

Ahh gotcha @dongjoon-hyun \r\nThanks for the information. Will keep in mind for the future. \r\nAppreciate the approval ! 

@dongjoon-hyun could you help with the merge of the PR as well, please?

+1, LGTM. Merging to master.\r\nThank you, @the-sakthi and @dongjoon-hyun for review.

@the-sakthi Congratulations with your first contribution to Apache Spark!

Thanks for all the help here, @MaxGekk and @dongjoon-hyun !\r\n

Thank YOU, @the-sakthi and @MaxGekk ! :)

@gengliangwang

Thanks for the contribution, @yhuang-db. This will be valuable for future performance regression checks.

Thanks, merging to master

Closing to fix the build workflow not running issue.

@cloud-fan @MaxGekk I resolved all comments, could you take a look?

We still need to know that the label is not user generated. So we do not expose the name in error messages.\r\n\r\nSent from my iPhone\r\n\r\nOn Jan 27, 2025, at 4:24\u202fAM, DuÅ¡an TiÅ¡ma ***@***.***> wrote:\r\n\r\n\ufeff\r\n\r\n@dusantism-db commented on this pull request.\r\n\r\n________________________________\r\n\r\nIn sql/core/src/main/scala/org/apache/spark/sql/scripting/SqlScriptingExecutionContext.scala<https://github.com/apache/spark/pull/49445#discussion_r1930444091>:\r\n\r\n> @@ -89,4 +96,6 @@ class SqlScriptingExecutionFrame(\r\n  * @param label\r\n  *   Label of the scope.\r\n  */\r\n-class SqlScriptingExecutionScope(val label: String)\r\n+class SqlScriptingExecutionScope(val label: String) {\r\n\r\n\r\nEach scope has a random UUID which is assigned as the label name, it is different for every scope and the users do not know its value.\r\n\r\nâ€”\r\nReply to this email directly, view it on GitHub<https://github.com/apache/spark/pull/49445#discussion_r1930444091>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AA22CFGDJXK27P7A4ST3NY32MYQOVAVCNFSM6AAAAABU64U3PCVHI2DSMVQWIX3LMV43YUDVNRWFEZLROVSXG5CSMV3GSZLXHMZDKNZVGI4TKMRTGU>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n

@dusantism-db we have the same problem in column resolution: is `x.x` a column `x` of table `x`, or an inner field `x` inside column `x`? The solution is to define priority: prefer the longest qualifier. Which means "column `x` of table `x`" is preferred. See https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala#L311-L313\r\n\r\nFor variables: if `x` is a label name, then `x.x` means variable `x` under label `x`. Otherwise (no such label found), it\

> @dusantism-db we have the same problem in column resolution: is `x.x` a column `x` of table `x`, or an inner field `x` inside column `x`? The solution is to define priority: prefer the longest qualifier. Which means "column `x` of table `x`" is preferred. See https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala#L311-L313\r\n> \r\n> For variables: if `x` is a label name, then `x.x` means variable `x` under label `x`. Otherwise (no such label found), it\

Considering the size of this PR, and its complexity, I propose that we split some further improvements into followup PRs.  Handling `session` and `system` variable names could be done in a followup PR, either by forbidding them or determining their behavior. Also, hiding generated label names in error messages could be done in a followup PR as well.

thanks, merging to master/4.0!

Thank you, @dusantism-db and @cloud-fan .\r\n\r\n+1 for the backporting decision for Apache Spark 4.0.0.

cc @cnauroth @dongjoon-hyun 

closing in favor of https://github.com/apache/spark/pull/49443.

Merged to master.

cc @grundprinzip, @HyukjinKwon 

I\

> I\

Please note that `scalastyle` is the following which uses `SBT`.\r\nhttps://github.com/apache/spark/blob/9547a476df1f13fb0c6cd19665247a3fef79bf27/dev/scalastyle#L20-L29

Could you review this, @panbingkun ? The CI starts to fail at compilation due to this.

Also, cc @huaxingao and @viirya . When you have some time, could you review this PR?

Let me check my PR once more~ 

This PR is ready back for review.\r\n\r\n<img width="296" alt="Screenshot 2025-01-10 at 09 55 02" src="https://github.com/user-attachments/assets/238716dd-36d4-4c70-baea-0171a14b93d6" />\r\n

Thank you, @grundprinzip , @cnauroth , @viirya .\r\n\r\nMerged to master.\r\n\r\nThere is no previous violation because technically Spark SBT build enforces this if we explicitly skip it by the env variable, `NOLINT_ON_COMPILE`.

@srielau @cloud-fan Could you take a look at the PR, please.

General comment: ensure that the parameter markers inside the execute immediate string are NOT bound by anything except the USING clause.\nSo spark.sql("execute immediate \

> General comment: ensure that the parameter markers inside the execute immediate string are NOT bound by anything except the USING clause.\r\n\r\n@srielau Is it restricted by the SQL standard?\r\n\r\n> So spark.sql("execute immediate \

Because itâ€™s a question mark in a string.\r\nIt is the job of execute immediate using to bind parameter markers to the plan generated by that string. The outside client has no business binding in except to the USING clause.\r\n\r\nOur current implementation (macro expansion) is semantically very suspect. If we did a proper nesting this â€œfirewallâ€ between the EXECUTE IMMEDIATE and the payload statement would be more obvious.\r\n\r\n\r\nSent from my iPhone\r\n\r\nOn Jan 11, 2025, at 8:32\u202fAM, Maxim Gekk ***@***.***> wrote:\r\n\r\n\ufeff\r\n\r\nGeneral comment: ensure that the parameter markers inside the execute immediate string are NOT bound by anything except the USING clause.\r\n\r\n@srielau<https://github.com/srielau> Is it restricted by the SQL standard?\r\n\r\nSo spark.sql("execute immediate \

Let me add some more color:\r\n`"EXECUTE IMMEDIATE \

@srielau I implemented such restriction. PTAL at tests and impl.

Merging to master. Thank you, @srielau @cloud-fan for review.

cc @zhengruifeng @HyukjinKwon 

thanks, it seems the python packaging test is restored:\r\n```\r\nConstructing virtual env for testing\r\nUsing conda virtual environments\r\nTesting pip installation with python 3.9\r\nUsing /tmp/tmp.vmdr3oeOpJ for virtualenv\r\n```

> thanks, it seems the python packaging test is restored:\r\n> \r\n> ```\r\n> Constructing virtual env for testing\r\n> Using conda virtual environments\r\n> Testing pip installation with python 3.9\r\n> Using /tmp/tmp.vmdr3oeOpJ for virtualenv\r\n> ```\r\n\r\nYeah

merged to master

cc @dongjoon-hyun @parthchandra @mridulm @LuciferYang 

@dongjoon-hyun thanks for the suggestion, addressed in e5aa6600e47, I re-verified and updated PR description

Please ping me on your spin-off PR. I can merge your new PR swiftly.

I merged the spin-offed PR, @pan3793 . Could you rebase this to the master?\r\n- https://github.com/apache/spark/pull/49476

@dongjoon-hyun thanks, rebased

Thank you!

Since this is a subset of previous status, I manually tested the compilation.\r\n\r\nMerged to master for Apache Spark 4.0.0.\r\n\r\nThank you, @pan3793 and @parthchandra .

thanks @MaxGekk \r\nmerged to master

Late LGTM.\r\n@zhengruifeng Thanks for fixing it!

cc @HyukjinKwon  @dongjoon-hyun @LuciferYang @zhengruifeng 

https://issues.apache.org/jira/browse/INFRA-26413

Thank you @LuciferYang , I will quickly merge it to identify this issue.

This is the configuration of another project (`nifi`)\r\nhttps://github.com/apache/nifi/blob/main/.github/workflows/code-coverage.yml#L65-L70\r\n<img width="755" alt="image" src="https://github.com/user-attachments/assets/93da70e4-1ec9-4e04-87a1-18c925b92590" />\r\n

I have submitted an issue to the `codecov/codecov-action` community to find a solution to the problem\r\nhttps://github.com/codecov/codecov-action/issues/1738

Merging to master.

@cloud-fan could you take a look please?

thanks, merging to master/3.5!

cc @dongjoon-hyun Could you help review this pr if you have time? Thank you ~\r\n\r\n

Oh, sure.

Thanks @dongjoon-hyun 

Merged into master for Spark 4.0. 

thanks, merged to master

Merged to master for Apache Spark 4.0.0.

Thanks @dongjoon-hyun 

Thank you @dongjoon-hyun! Yes, the PR should be ready -- it restores the golden file to its state before this PR: https://github.com/apache/spark/pull/49139/files#diff-b6f30759017988fd0963ce840918f541caf610a1793fa73f17e61b03a2acb797

Thank you for the confirmation. Let me merge this in AS-IS status.

@cloud-fan , please take a look and help merge this PR, to add a feature flag for disabling object-level collations while the feature is still in development, thanks!\r\nCC @stefankandic, with whom I agreed on adding this earlier today.

thanks, merging to master!

the test failure should be unrelated.\r\nmerged to master

Merged into master for Spark 4.0. Thanks @dongjoon-hyun 

Thank you always, @LuciferYang !

Thank you for making a PR, @cnauroth !

Oh, the CI seems to succeed even though the Scala linter fails.\r\n\r\n- https://github.com/cnauroth/spark/actions/runs/12698857446/job/35398309914\r\n\r\n\r\n![Screenshot 2025-01-10 at 07 14 31](https://github.com/user-attachments/assets/8decb8bc-1e0b-4a07-bd4a-7add1e20e25b)\r\n\r\n

Let me check and do the follow-up.

Thank you, @dongjoon-hyun !

Adding @davidm-db @dejankrak-db @dusantism-db @cloud-fan @srielau  

Could we add a test in which a handler is declared but an error is thrown anyway, because it is a different condition? For example a handler is declared for divide by zero but unresolved column is thrown.

@cloud-fan Thank you for review(s)!

- Basically, we are doing uniqueness check for condition names in `ExceptionHandlerTriggers.addUnique*` methods where case sensitivity is centralized and strings are converted to the same casing .\r\n- When creating `ErrorCondition` object, we pass upper-case arguments.\r\n- We use upper-case when we check if condition is builtin Spark condition.\r\n\r\nI agree more centralized approach should be used, but with current tests, I think we covered all possible checks in code to be case-insensitive. As a follow up, I will try to think of some more clever way of ensuring **case-insensitivity** because this can be a problem if somebody change or add new code without case-sensitivity in mind. Thanks for the comment!

thanks, merging to master/4.0!

> +1, the proposal sounds reasonable and this PR handles all instances in non-test code.\r\n> \r\n> > To be able to have this centralized and not have to create new string literals with "UTF8_BINARY" over and over again.\r\n> \r\n> BTW, I\

Oh, if that is only a single file (containing most of them), could you include `CollationSupportSuite` into this PR?

> Oh, if that is only a single file (containing most of them), could you include `CollationSupportSuite` into this PR?\r\n\r\nDone

Could you run `dev/lint-java` and fix all `LineLength` errors? Otherwise, the CI will fail due to this.\r\n\r\n```\r\n$ dev/lint-java\r\nUsing `mvn` from path: /Users/dongjoon/APACHE/spark-merge/build/apache-maven-3.9.9/bin/mvn\r\nUsing SPARK_LOCAL_IP=localhost\r\nCheckstyle checks failed at following occurrences:\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[162] (sizes) LineLength: Line is longer than 100 characters (found 103).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[163] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[169] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[170] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1132] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1133] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1135] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1139] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1142] (sizes) LineLength: Line is longer than 100 characters (found 104).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1143] (sizes) LineLength: Line is longer than 100 characters (found 107).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1523] (sizes) LineLength: Line is longer than 100 characters (found 107).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1784] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1785] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1787] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1788] (sizes) LineLength: Line is longer than 100 characters (found 103).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1789] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1791] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1792] (sizes) LineLength: Line is longer than 100 characters (found 107).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1793] (sizes) LineLength: Line is longer than 100 characters (found 106).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1794] (sizes) LineLength: Line is longer than 100 characters (found 103).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1795] (sizes) LineLength: Line is longer than 100 characters (found 106).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1857] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1858] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[1860] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[2075] (sizes) LineLength: Line is longer than 100 characters (found 103).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[2076] (sizes) LineLength: Line is longer than 100 characters (found 103).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[2622] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[2624] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[2630] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[2632] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[2634] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[2636] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[2642] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[2644] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[2647] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[2649] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[3734] (sizes) LineLength: Line is longer than 100 characters (found 102).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[3735] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[3737] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[3750] (sizes) LineLength: Line is longer than 100 characters (found 103).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[3751] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[3753] (sizes) LineLength: Line is longer than 100 characters (found 101).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[3754] (sizes) LineLength: Line is longer than 100 characters (found 104).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[3755] (sizes) LineLength: Line is longer than 100 characters (found 103).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[3757] (sizes) LineLength: Line is longer than 100 characters (found 103).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[3800] (sizes) LineLength: Line is longer than 100 characters (found 112).\r\n[ERROR] src/test/java/org/apache/spark/unsafe/types/CollationSupportSuite.java:[3801] (sizes) LineLength: Line is longer than 100 characters (found 105).\r\n```\r\n\r\nIn addition, shall we reuse the static constants by `import` instead of defining new variables like `private final String`?\r\n\r\n```\r\n+import static org.apache.spark.sql.catalyst.util.CollationFactory.UNICODE_CI_COLLATION_NAME;\r\n+import static org.apache.spark.sql.catalyst.util.CollationFactory.UNICODE_COLLATION_NAME;\r\n+import static org.apache.spark.sql.catalyst.util.CollationFactory.UTF8_BINARY_COLLATION_NAME;\r\n+import static org.apache.spark.sql.catalyst.util.CollationFactory.UTF8_LCASE_COLLATION_NAME;\r\n```\r\n\r\n```\r\n-  private final String UTF8_BINARY_COLLATION_NAME = CollationFactory.UTF8_BINARY_COLLATION_NAME;\r\n-  private final String UTF8_LCASE_COLLATION_NAME = CollationFactory.UTF8_LCASE_COLLATION_NAME;\r\n-  private final String UNICODE_COLLATION_NAME = CollationFactory.UNICODE_COLLATION_NAME;\r\n-  private final String UNICODE_CI_COLLATION_NAME = CollationFactory.UNICODE_CI_COLLATION_NAME;\r\n```

Merged to master.

how is it going?

@cloud-fan Thanks for checking in! Itâ€™s ready for review now.

Merged to master and branch-4.0 thank you!

merged to master

Merged to master.

Thanks @HyukjinKwon 

Oh, yeah +1 for vote.

Thank you, @HyukjinKwon and @gengliangwang .

Since the vote was started, could you make a CI pass, @gengliangwang ?

For the record, `StructuredSparkLoggerSuite` seems to fail still.\r\n```\r\n[error] Failed: Total 66, Failed 6, Errors 0, Passed 60\r\n[error] Failed tests:\r\n[error] \torg.apache.spark.util.StructuredSparkLoggerSuite\r\n[error] (common-utils / Test / test) sbt.TestsFailedException: Tests unsuccessful\r\n```

@dongjoon-hyun Thank you for following up on this PR!

The last failure seems to be outdated result after merging \r\n- #49369 \r\n\r\n<img width="430" alt="Screenshot 2025-01-13 at 16 45 04" src="https://github.com/user-attachments/assets/d66b5436-948e-4e70-9613-593bc7fbfc4a" />\r\n\r\nLet me merge this.

For the record, the vote passed already.\r\n- https://lists.apache.org/thread/jhxrkqs3d1f32d6jyw86kd9qomjdjtzv

cc @cloud-fan @gengliangwang here is one last PR to improve behavior for SQL pipe syntax.

thanks, merging to master!

cc @cloud-fan 

thanks, merging to master/3.5!

Gentle ping, @ericm-db . Please address the above review comments.

**NOTE: PLEASE DO NOT MERGE THIS PR. WE HAVE TO ADDRESS SPARK CONNECT WHICH WILL BE COMING SOON.**

@ericm-db Could you please change the title to add `[DO-NOT-MERGE]` in prefix? 

> this depends on Spark Connect integration work. Before that, do not merge.\r\n\r\nWe discussed this offline and we are not going to treat connect work as a blocker for 4.0. We have the Scala PR for connect support in review - https://github.com/apache/spark/pull/49488 and we will work on the Python PR too. But we will treat them as best effort for 4.0.

@HeartSaVioR - PTAL when u get a chance, thx !

@HeartSaVioR - CI is green. Not updating above. Just fyi

Thanks! Merging to master/4.0.

Thank you all for this!

cc @JoshRosen @viirya @dongjoon-hyun @peter-toth 

Thanks for the review, merging to master!

+1, late LGTM, @cloud-fan .

thanks, merging to master!

@Ngone51 Does branch-3.5 also need this bug fix?

@LuciferYang Thanks. Yes, I think so. Do I need send a separate PR? 

Merged into master for Spark 4.0. Thanks @Ngone51 @xuanyuanking and @HyukjinKwon .\r\n\r\n@Ngone51 Due to code conflicts, it cannot be directly merged into branch-3.5. If needed, please submit a separate pr. Thanks ~

For the record, after this PR, surprisingly, 3 commits fail consecutively at the same PySpark pipeline.\r\n\r\n![Screenshot 2025-01-14 at 20 48 10](https://github.com/user-attachments/assets/24145176-eaea-42ae-b9db-50392b10d8c5)\r\n

im taking a look now for the failure

Seems unrelated, I tested the result of reverting the current one, but `pyspark.conf` still failed:\r\n\r\n- https://github.com/apache/spark/pull/49498\r\n- https://github.com/LuciferYang/spark/actions/runs/12781866942/job/35630526718\r\n\r\n![image](https://github.com/user-attachments/assets/45d31343-168e-43b3-bdf2-fc609c70ad97)\r\n\r\n\r\n

found the cuase. working on the fix

no tihs pr is fine

@HyukjinKwon Are you saying this pr caused the test to fail?\r\n\r\n

nono this PR is fine. I will make a PR soon with a PR description with 20 mins

here: https://github.com/apache/spark/pull/49500

FYI I created a followup PR (https://github.com/apache/spark/pull/49508) to use `TaskContext.createResourceUninterruptibly()` where it applies. 

To further optimize Py4J calls, does it make sense to cache the result? e.g.\r\n\r\n```\r\n@functools.lru_cache(maxsize=128)\r\ndef get_jvm_attr(jvm: "JVMView", name: str) -> Any:\r\n    return getattr(jvm, name)\r\n```

Yeah but we should think about how it will affect GC in Python and JVM. 

Merged to master.

Could you update the test results for `SubExprEliminationBenchmark` in this pr to ensure that the changes are as expected? @panbingkun 

> Could you update the test results for `SubExprEliminationBenchmark` in this pr to ensure that the changes are as expected? @panbingkun\r\n\r\nWell, okay.\r\n\r\nJDK17: https://github.com/panbingkun/spark/actions/runs/12666974432\r\nJDK21: https://github.com/panbingkun/spark/actions/runs/12666979089

+1 for including `SubExprEliminationBenchmark` here. All the others are covered in the following.\r\n- #49409 

I found the following content in the log:\r\n```\r\n19:22:37.212 main INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext is 58762 bytes\r\n```\r\n\r\n

- case 1 (3 filtering conditions with a data size of 1000000)\r\n```scala\r\nobject FromJsonBenchmark extends SqlBasedBenchmark {\r\n  import spark.implicits._\r\n\r\n  def withFilter(rowsNum: Int, numIters: Int): Unit = {\r\n    val benchmark = new Benchmark("from_json in Filter", rowsNum, output = output)\r\n\r\n    withTempPath { path =>\r\n      prepareDataInfo(benchmark)\r\n      val numCols = 500\r\n      val schema = writeWideRow(path.getAbsolutePath, rowsNum, numCols)\r\n\r\n      val jsonValue = from_json($"value", schema)\r\n      val predicate = jsonValue.getField(s"col0") >= lit(100000) ||\r\n        jsonValue.getField(s"col50") >= lit(100000) ||\r\n        jsonValue.getField(s"col123") >= lit(100000)\r\n\r\n      val caseName = s"from_object, codegen: no"\r\n      benchmark.addCase(caseName, numIters) { _ =>\r\n        val df = spark.read\r\n          .text(path.getAbsolutePath)\r\n          .where(predicate)\r\n        df.write.mode("overwrite").format("noop").save()\r\n      }\r\n      benchmark.run()\r\n    }\r\n  }\r\n\r\n  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\r\n    val numIters = 3\r\n    runBenchmark("Benchmark for performance of from_json codegen") {\r\n      withFilter(1_000_000, numIters)\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n- codegen for `from_json`\r\n```shell\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.2\r\nApple M2\r\nfrom_json in Filter:                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nfrom_object, codegen: yes                          61029          62195        1781          0.0       61028.8       1.0X\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.2\r\nApple M2\r\nfrom_json in Filter:                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nfrom_object, codegen: yes                          61391          66201        7157          0.0       61391.2       1.0X\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.2\r\nApple M2\r\nfrom_json in Filter:                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nfrom_object, codegen: yes                          60653          61195         481          0.0       60652.7       1.0X\r\n```\r\n\r\n- non-codegen for `from_json`\r\n```shell\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.2\r\nApple M2\r\nfrom_json in Filter:                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nfrom_object, codegen: no                         61289          62508        1155          0.0       61288.6       1.0X\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.2\r\nApple M2\r\nfrom_json in Filter:                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nfrom_object, codegen: no                          61219          61663         386          0.0       61218.9       1.0X\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.2\r\nApple M2\r\nfrom_json in Filter:                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nfrom_object, codegen: no                          61056          61362         287          0.0       61055.9       1.0X\r\n```

- case 2 (1 filtering conditions with a data size of 100000)\r\n```\r\nobject FromJsonBenchmark extends SqlBasedBenchmark {\r\n  import spark.implicits._\r\n\r\n  def withFilter(rowsNum: Int, numIters: Int): Unit = {\r\n    val benchmark = new Benchmark("from_json in Filter", rowsNum, output = output)\r\n\r\n    withTempPath { path =>\r\n      prepareDataInfo(benchmark)\r\n      val numCols = 500\r\n      val schema = writeWideRow(path.getAbsolutePath, rowsNum, numCols)\r\n\r\n      val jsonValue = from_json($"value", schema)\r\n      val predicate = jsonValue.getField(s"col0") >= lit(100000)\r\n\r\n      val caseName = s"from_object, codegen: no"\r\n      benchmark.addCase(caseName, numIters) { _ =>\r\n        val df = spark.read\r\n          .text(path.getAbsolutePath)\r\n          .where(predicate)\r\n        df.write.mode("overwrite").format("noop").save()\r\n      }\r\n      benchmark.run()\r\n    }\r\n  }\r\n\r\n  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\r\n    val numIters = 3\r\n    runBenchmark("Benchmark for performance of from_json codegen") {\r\n      withFilter(1_000_00, numIters)\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n- codegen for `from_json`\r\n```shell\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.2\r\nApple M2\r\nfrom_json in Filter:                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nfrom_object, codegen: yes                          2325           2341          26          0.0       23249.6       1.0X\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.2\r\nApple M2\r\nfrom_json in Filter:                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nfrom_object, codegen: yes                          2237           2266          36          0.0       22373.5       1.0X\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.2\r\nApple M2\r\nfrom_json in Filter:                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nfrom_object, codegen: yes                          2317           2403          74          0.0       23172.3       1.0X\r\n```\r\n\r\n- non-codegen for `from_json`\r\n```shell\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.2\r\nApple M2\r\nfrom_json in Filter:                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nfrom_object, codegen: no                           2264           2286          20          0.0       22639.3       1.0X\r\n\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.2\r\nApple M2\r\nfrom_json in Filter:                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nfrom_object, codegen: no                           2475           3010         554          0.0       24752.2       1.0X\r\n\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.2\r\nApple M2\r\nfrom_json in Filter:                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nfrom_object, codegen: no                           2315           2780         480          0.0       23150.6       1.0X\r\n```

From the above scenario, it seems that there is no a performance regression, and I am investigating other reasons.

before: [code-gen-before.txt](https://github.com/user-attachments/files/18347006/code-gen-before.txt)\r\nafter:[after-code-gen.txt](https://github.com/user-attachments/files/18347019/after-code-gen.txt)\r\n\r\nIt can be seen that after `from_json ` enabling codegen, an huge `processNext` method is generated for filter. I suspect that this method is the reason why JIT cannot optimize it.\r\n\r\n\r\n\r\n\r\n

@cloud-fan @dongjoon-hyun @panbingkun How do we proceed with this issue?\r\n\r\nI think the risk of generating huge `filter` methods has always existed, but it was hidden in this benchmark because `from_json` did not support code generation previously. So I believe the support of code generation in from_json is not the root cause.\r\n\r\nAs more functions come to support code generation, the probability of generating huge methods will increase, It should apply to more than just filters, right?.  \r\n\r\nPerhaps we need to find a more universal approach to split the generated methods in order to avoid this risk?

Give me some time to look at the root cause.

In the `withFilter` scenario of `SubExprEliminationBenchmark`, the root cause as follows:\r\n```scala\r\n  val df = spark.read\r\n              .text(path.getAbsolutePath)\r\n              .where(predicate)\r\n  df.write.mode("overwrite").format("noop").save()\r\n```\r\n\r\n- When `from_json` does not implement codegen\r\nFilterExec.doExecute -> Predicate.create -> CodeGeneratorWithInterpretedFallback.createObject -> Predicate.createCodeGeneratedObject -> CodegenContext.subexpressionElimination\r\nhttps://github.com/apache/spark/blob/0123a5ecbe6d4075b0738e9d2faac354f2cbd008/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala#L281\r\nhttps://github.com/apache/spark/blob/0123a5ecbe6d4075b0738e9d2faac354f2cbd008/sql/core/src/main/scala/org/apache/spark/sql/execution/FilterEvaluatorFactory.scala#L39\r\nhttps://github.com/apache/spark/blob/0123a5ecbe6d4075b0738e9d2faac354f2cbd008/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/CodeGeneratorWithInterpretedFallback.scala#L45\r\nhttps://github.com/apache/spark/blob/0123a5ecbe6d4075b0738e9d2faac354f2cbd008/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GeneratePredicate.scala#L41\r\nhttps://github.com/apache/spark/blob/0123a5ecbe6d4075b0738e9d2faac354f2cbd008/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala#L1270\r\n\r\n## Ultimately, optimize the 500 calls to `from_json` to only 1 call ##\r\n\r\n- When `from_json` implement codegen\r\nFilterExec.doConsume -> GeneratePredicateHelper.generatePredicateCode\r\nhttps://github.com/apache/spark/blob/0123a5ecbe6d4075b0738e9d2faac354f2cbd008/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala#L252\r\n\r\n## there is no `subexpressionElimination` optimization here, 500 calls will ultimately be applied to `JsonToStructs`.

If we can implement subexpressionElimination optimization in the method `FilterExec.doConsume`, like `ProjectExec.doConsume`, that would be great.\r\nhttps://github.com/apache/spark/blob/0123a5ecbe6d4075b0738e9d2faac354f2cbd008/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala#L68-L80\r\ncc @cloud-fan 

@panbingkun great investigation! +1 to implement subexpression elimination for `FilterExec`

any progress? @panbingkun 

> any progress? @panbingkun\r\n\r\nIt is being implemented and will take some time.

- Based on that the root cause for the performance regression of `SubExprEliminationBenchmark` is not due to the codegen implementation of `from_json`(the root cause is that the operator `FilterExEc` did not implement `SubExprElimination` optimization in `WSC`), this PR will close here. \r\n\r\n- The operator `FilterExec` implementing `SubExprElimination` optimization in `WSC` is still being explored for implementation: https://github.com/apache/spark/pull/49573

Hi, @panbingkun , @cloud-fan , @LuciferYang . I ran the benchmark again as a part of regression check Today and I hit this still.\r\n \r\n- branch-4.0, Java 17: https://github.com/dongjoon-hyun/spark/actions/runs/13062008857\r\n- branch-4.0, Java 21: https://github.com/dongjoon-hyun/spark/actions/runs/13062012809\r\n\r\nWhen I do the bisect, it also reached the original PR. So, although we did some follow-ups, is this still required?\r\n```\r\n2a1301133138ba0d5e2d969fc6428153903ffff1 is the first bad commit\r\ncommit 2a1301133138ba0d5e2d969fc6428153903ffff1\r\nAuthor: panbingkun <panbingkun@baidu.com>\r\nDate:   Wed Oct 16 09:10:06 2024 +0200\r\n\r\n    [SPARK-49966][SQL] Codegen Support for JsonToStructs(`from_json`)\r\n```

Got it. Thank you for the info, @LuciferYang ~

cc @LuciferYang and @panbingkun 

All the other benchmarks seem to work.

Merge into master for Spark 4.0. Thanks @dongjoon-hyun 

Thank you so much, @LuciferYang !

Merged to master.

Thank you, @HyukjinKwon . Merged to master.

Thank you, @HyukjinKwon . Merged to master.

cc @asl3 and @cloud-fan 

Could you review this PR, @LuciferYang ?

Thank you so much, @LuciferYang !

Merged to master.

thank you for the fix!! @dongjoon-hyun @LuciferYang 

Thank you for review, @asl3 .

cc @LuciferYang @ulysses-you 

Merged to master for Apache Spark 4.0.0.\r\n\r\nThank you!

late LGTM

Merged to master.

LGTM. Thanks @panbingkun 

Gentle ping, @panbingkun .

> Gentle ping, @panbingkun .\r\n\r\nSorry, I have been investigating the issue with `from_json` all day today.

Merged to master for Apache Spark 4.0.0.

cc. @HyukjinKwon Please take a look, thanks!

Merged to master.

@HyukjinKwon Thanks for reviewing and merging!

+1, LGTM. Merging to master/4.0.\r\nThank you, @itholic.

Thanks @MaxGekk for the review!

Could you review this a minor comment fix, @yaooqinn ?

Merged into master. Thanks @dongjoon-hyun 

Late LGTM

Thank you, @LuciferYang and @yaooqinn !

Could you review this PR, @yaooqinn ?

Thank you, @HyukjinKwon !

How about also wrapping the wait periodï¼Ÿ

The containerâ€˜s bind ports are constantsï¼ŒI donâ€™t think weâ€™re able to recover from this error 

Do you mean `44477` is fixed by `MySQLOverMariaConnectorIntegrationSuite` or docker images?\r\n> The containerâ€˜s bind ports are constantsï¼ŒI donâ€™t think weâ€™re able to recover from this error

I thought only the internal ports (inside containers) are fixed like the following. (In the above error message).\r\n- https://github.com/docker-library/mysql/blob/b7333451d7be9f066e43f9612e6bbe3751e548f1/innovation/Dockerfile.oracle#L122C1-L122C18\r\n> EXPOSE 3306 33060

Let me close this for now. Thank you for review, Hyukjin and Kent.

thanks, merging to master!

merged to master

@michaelzhan-db could you re-run the failed tests?

Thanks, merging to master

cc @cloud-fan too

`SparkConnectPlanner` has an issue. Waiting for #49449.

The remaining test failures are not related to this PR.

Thanks! merging to master.

The test failure is unrelated, thanks, merging to master!

Thank you, Hyukjin. Merged to master.

thanks, merging to master!

Thank you, Hyukjin. Merged to master.

Thank you, Hyukjin. Merged to master.

Yea, this is a simpler approach. Although it still unnecessarily tracks the `outgoingRefs` for the "contains" relationship if `WithCTE` is not duplicated, but it\

That `Protobuf breaking change detection and Python CodeGen check` failure seems unrelated. Let me update this PR from `master`...

For future reviewers:\r\n\r\nThe main problem is when we see\r\n```\r\nWITH\r\nt0 AS (SELECT 1),\r\nt1 AS (\r\n  WITH t2 (...)\r\n  SELECT * FROM t2\r\n)\r\n```\r\nShall we say that `t1` references `t2`? It depends on whether `t2` references `t0` or not. If not, the inner `WITH` is self-contained and `t1` doesn\

thanks, merging to master!

Thanks for the review @cloud-fan and @dongjoon-hyun!

thanks, merging to master!

@cloud-fan great idea. Done.

thanks, merging to master!

Thank you, Hyukjin. Merged to master.

Thank you, @HyukjinKwon . Merged to master.

cc @xupefei and @HyukjinKwon 

Thank you, @HyukjinKwon ! Merged to master.

Merged to master and branch-4.0.

Adding @cloud-fan @davidm-db @dusantism-db  @dejankrak-db @MaxGekk 

thanks, merging to master!

Spooky...

@HyukjinKwon Hi, can you please merge this PR? Thanks!

Merged to master and branch-4.0.

Merged to master.

Merged to master.

BTW, just a question: Do we need to backport Py4J 0.10.9.8 to branch-3.5? For example, to deliver the following?\r\n- https://github.com/py4j/py4j/pull/538

Got it~ Thank you for the answer. :) 

thanks, merged to master

cc @HyukjinKwon 

Merged to master.

Thank you, @HyukjinKwon !

K8s IT passed. Merged to master.

Thank you, @LuciferYang .

Thank you, @xinrong-meng .

Thank you, @HyukjinKwon .

All SQL tests passed. Merged to master.

```\r\n[info] org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatV1Suite *** ABORTED *** (11 seconds, 911 milliseconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 15 times over 10.007530677999998 seconds. Last failure message: There are 1 possibly leaked file streams.. (SharedSparkSession.scala:167)\r\n[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite.eventually(ParquetFileFormatSuite.scala:31)\r\n[info]   at org.apache.spark.sql.test.SharedSparkSessionBase.afterEach(SharedSparkSession.scala:167)\r\n[info]   at org.apache.spark.sql.test.SharedSparkSessionBase.afterEach$(SharedSparkSession.scala:161)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite.afterEach(ParquetFileFormatSuite.scala:31)\r\n[info]   at org.scalatest.BeforeAndAfterEach.$anonfun$runTest$1(BeforeAndAfterEach.scala:247)\r\n[info]   at org.scalatest.Status.$anonfun$withAfterEffect$1(Status.scala:377)\r\n[info]   at org.scalatest.Status.$anonfun$withAfterEffect$1$adapted(Status.scala:373)\r\n[info]   at org.scalatest.SucceededStatus$.whenCompleted(Status.scala:462)\r\n[info]   at org.scalatest.Status.withAfterEffect(Status.scala:373)\r\n[info]   at org.scalatest.Status.withAfterEffect$(Status.scala:371)\r\n[info]   at org.scalatest.SucceededStatus$.withAfterEffect(Status.scala:434)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:246)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r\n[info]   at scala.collection.immutable.List.foreach(List.scala:334)\r\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r\n[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.Suite.run(Suite.scala:1114)\r\n[info]   at org.scalatest.Suite.run$(Suite.scala:1096)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:1583)\r\n[info]   Cause: java.lang.IllegalStateException: There are 1 possibly leaked file streams.\r\n[info]   at org.apache.spark.DebugFilesystem$.assertNoOpenStreams(DebugFilesystem.scala:54)\r\n[info]   at org.apache.spark.sql.test.SharedSparkSessionBase.$anonfun$afterEach$1(SharedSparkSession.scala:168)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite.eventually(ParquetFileFormatSuite.scala:31)\r\n[info]   at org.apache.spark.sql.test.SharedSparkSessionBase.afterEach(SharedSparkSession.scala:167)\r\n[info]   at org.apache.spark.sql.test.SharedSparkSessionBase.afterEach$(SharedSparkSession.scala:161)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite.afterEach(ParquetFileFormatSuite.scala:31)\r\n[info]   at org.scalatest.BeforeAndAfterEach.$anonfun$runTest$1(BeforeAndAfterEach.scala:247)\r\n[info]   at org.scalatest.Status.$anonfun$withAfterEffect$1(Status.scala:377)\r\n[info]   at org.scalatest.Status.$anonfun$withAfterEffect$1$adapted(Status.scala:373)\r\n[info]   at org.scalatest.SucceededStatus$.whenCompleted(Status.scala:462)\r\n[info]   at org.scalatest.Status.withAfterEffect(Status.scala:373)\r\n[info]   at org.scalatest.Status.withAfterEffect$(Status.scala:371)\r\n[info]   at org.scalatest.SucceededStatus$.withAfterEffect(Status.scala:434)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:246)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r\n[info]   at scala.collection.immutable.List.foreach(List.scala:334)\r\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r\n[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.Suite.run(Suite.scala:1114)\r\n[info]   at org.scalatest.Suite.run$(Suite.scala:1096)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:1583)\r\n[info]   Cause: java.lang.Throwable:\r\n[info]   at org.apache.spark.DebugFilesystem$.addOpenStream(DebugFilesystem.scala:35)\r\n[info]   at org.apache.spark.DebugFilesystem.open(DebugFilesystem.scala:75)\r\n[info]   at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:997)\r\n[info]   at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:75)\r\n[info]   at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:925)\r\n[info]   at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:710)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:451)\r\n[info]   at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:416)\r\n[info]   at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)\r\n[info]   at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\r\n[info]   at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)\r\n[info]   at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)\r\n[info]   at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)\r\n[info]   at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)\r\n[info]   at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)\r\n[info]   at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)\r\n```

Merged to master, thank you @dongjoon-hyun @HyukjinKwon 

Thank you, @HyukjinKwon and @yaooqinn !

Merged to master for Apache Spark 4.0.0.

For the record, I double-checked that the collation test is skipped.\r\n- https://github.com/apache/spark/actions/runs/12623804693/job/35173225914\r\n\r\n<img width="356" alt="Screenshot 2025-01-05 at 16 20 40" src="https://github.com/user-attachments/assets/257cecea-22e2-4716-a150-5e809a45c6dd" />\r\n

Thank you, @HyukjinKwon , @zhengruifeng , @yaooqinn .

Merged to master, thank you @dongjoon-hyun @HyukjinKwon @zhengruifeng 

Thank you, @williamhyun .

LGTM

This PR is ready.\r\n\r\ncc @dongjoon-hyun 

Merged to master for Apache Spark 4.0.0. Thank you, @williamhyun and @the-sakthi .

Thank you for taking a look at this. Please take a rest first and your time, @LuciferYang !

It seems that PR builder failure looks reasonable. It means this PR is unable to reproduce the MacOS Daily CI failures. WDTY?\r\n\r\nhttps://github.com/apache/spark/actions/runs/12622568770\r\n\r\n<img width="307" alt="Screenshot 2025-01-05 at 21 45 57" src="https://github.com/user-attachments/assets/e8989ea1-98ab-4ffe-836f-95ddd02239fe" />\r\n

@dongjoon-hyun Or should we revert  https://github.com/apache/spark/pull/49347 first  and resubmit it together with this one?\r\n\r\n- https://github.com/apache/spark/pull/49366\r\n

@HyukjinKwon this one is ready now

Merged to master~

After merging, MacOS CI is triggered here.\r\n- https://github.com/apache/spark/actions/runs/12636667017

> After merging, MacOS CI is triggered here.\r\n> \r\n> * https://github.com/apache/spark/actions/runs/12636667017\r\n\r\nThe test case `Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches` seems quite unstable. I checked back and found that a similar test case, `Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches`, failed frequently about two weeks ago. These two test cases are executed consecutively, and the error messages when they fail appear to be similar.\r\n\r\n`Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches`:\r\n- https://github.com/apache/spark/actions/runs/12448087908/job/34752303825\r\n- https://github.com/apache/spark/actions/runs/12496178777/job/34867666952\r\n- https://github.com/apache/spark/actions/runs/12520214986/job/34925552658\r\n- https://github.com/apache/spark/actions/runs/12573970266/job/35047528654\r\n- https://github.com/apache/spark/actions/runs/12563518255/job/35025494839\r\n\r\n![image](https://github.com/user-attachments/assets/b60c7b87-6802-4029-baf1-dd44b3546e8d)\r\n\r\n\r\n`Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches`\r\n\r\n- https://github.com/apache/spark/actions/runs/12636667017/job/35222481592\r\n\r\n![image](https://github.com/user-attachments/assets/4d5e664f-6fca-4b34-8073-6d81c1a489de)\r\n\r\n\r\nNeed to investigate further.\r\n\r\n\r\n

To @LuciferYang , you can stop investigating on this. IIUC, the situation is the same before your PR.\r\n\r\nKafka test suites have been unstable in MacOS environment.\r\n\r\nThank you for your precious time.

> To @LuciferYang , you can stop investigating on this. IIUC, the situation is the same before your PR.\r\n> \r\n> Kafka test suites have been unstable in MacOS environment.\r\n> \r\n> Thank you for your precious time.\r\n\r\nOK ~

cc @LuciferYang 

@LuciferYang \r\nTests passed.

Merged into master. Thanks @tedyu 

Thanks @dongjoon-hyun. I changed this to draft as some test failures in the CI. I will add a JIRA issue once this is ready.

Looks like Spark in many places replies some tricks during serialization. Skipping serialization will cause errors.

Java 21 test passed, revert to Java 17, will update pr description later

The daily tests for Java 21 have been restored. Thanks @dongjoon-hyun @HyukjinKwon \r\n\r\n- https://github.com/apache/spark/actions/runs/12614010192\r\n\r\n\r\n@jingz-db since you have a deeper understanding of this code, feel free to make further improvements.\r\n

thanks, merging to master!

@peter-toth I find this "reference lineage" is a clearer design, because\r\n1. When we optimize out a CTE relation, and the nested `WithCTE` inside it is self-contained, we don\

Thanks @cloud-fan for pinging me, I will try to check this change in a few days.

This PR looks ok to me, but I also feel that the linage tracking is overly complicated due to the `collectCTERefs` closure. I was trying to come up with a simpler solution in https://github.com/apache/spark/pull/49379. But if you still prefer this PR I can approve this one as well.

closed in favor of https://github.com/apache/spark/pull/49379

Please take another look.\r\n\r\nCTERelationDef does not contain anchor any longer -- when needed it is fetched from its child via pattern matching. Code is greatly simplified and all previous convoluted questions on recursionAnchor are now moot.\r\n\r\nI added several exceptions for the unsupported cases of Union under the CTE Definition.\r\n\r\nSubstitution rules for UnionLoop/Ref are added for 4 cases of Union under CTE Definition.\r\n\r\nCTERelationDef change has broken some tests, will work on those now.

Please update the PR title to have a full sentence.

@mridulm @JoshRosen @dongjoon-hyun Could you help take a look? Thanks!

@dongjoon-hyun Thanks for your feedback.  Moving the test suites (`ExternalShuffleServiceSuite`) around is because I added a new rpc `ApplicationRemoveTest` within `deploy` package that handled by Worker (as all the other rpc messages handled by the worker locates at `deploy` as well). And those two test suites can not access `ApplicationRemoveTest` due to the visibility issue.  Additionally, given that `ExternalShuffleService` is located within `deploy` package, I finally decided to move the test suites (`ExternalShuffleServiceSuite`) under `deploy` as well.\r\n\r\nI could also move the rpc `ApplicationRemoveTest` out of `deploy` for the visibility. It would just be a bit weird that all the rpc messages that are handled by the worker locate in `deploy` package except `ApplicationRemoveTest`.

After a second thinking, I decided to remove `Utils.isTesting` where `LocalSparkCluster.get` is already defined. `local-cluster` mode is [documented to be used in unit test only](https://spark.apache.org/docs/3.5.4/submitting-applications.html#master-urls). And in Spark, we do not has an explict `Utils.isTesting` check where `local-cluster` mode is enabled. We all tacitly acknowledge that `local-cluster` is equivalent to `Utils.isTesting=true`. This is what it is for a while. So users should take the risk by using `local-cluster` mode out of the unit test. Given this, I think we can remove `Utils.isTesting` for simplify. cc @JoshRosen @LuciferYang 

Would appreciate for another review, thanks!

Merged to master. Thanks @HyukjinKwon for the review

cc @zhengruifeng 

thanks, merged to master

Thanks @HyukjinKwon @zhengruifeng 

cc @dongjoon-hyun FYI

Thank you @dongjoon-hyun 

Hi, @LuciferYang . It seems that this PR causes massive failures on Apple Silicon CIs which uses this method.\r\n\r\n- https://github.com/apache/spark/actions/workflows/build_maven_java21_macos15.yml\r\n  - https://github.com/apache/spark/actions/runs/12616477101\r\n  - https://github.com/apache/spark/actions/runs/12614014000\r\n\r\nPreviously, there was a few flaky test pipelines, but now most of test pipelines fails. Could you double check the CI?\r\n<img width="304" alt="Screenshot 2025-01-05 at 10 33 15" src="https://github.com/user-attachments/assets/fe42af44-9546-4200-ba04-5b92c5cc627b" />\r\n

LGTM thank you

> LGTM. The similar way can be applied to classic over Py4J?\r\n\r\nIt seems classic PySpark already set the configs in batch by `applyModifiableSettings`, \r\n\r\nhttps://github.com/apache/spark/blob/4c39d6fa648a754d0b6585839e2803bc1e2c8cc1/python/pyspark/sql/session.py#L614-L628\r\n\r\n\r\nhttps://github.com/apache/spark/blob/4c39d6fa648a754d0b6585839e2803bc1e2c8cc1/python/pyspark/sql/session.py#L534-L547\r\n

thank you all, merged to master

thanks, merging to master!

thanks, merged to master

thanks, merged to master

PTAL @hvanhovell / @HyukjinKwon / @xupefei 

Merged to master.

@xupefei PTAL!

Merged to master.

thanks, merging to master!

cc @zhengruifeng 

thanks, merged to master

Thanks for reviewing @zhengruifeng 

I am going to merge this to recoevr the CI.

Merged to master.

Late LGTM

BTW, do we have a list that which dataframe operations are allowed in subqueries?

The remaining test failures are not related to this PR.

Thanks! merging to master.

cc @beliefer 

@HyukjinKwon Thank you for ping me. I will take a look a little later.

Merged to master.

thanks, merged to master

@cloud-fan @yaooqinn @huaxingao please let me know if this approach makes sense, or do you have any other suggestions to allow users to set `spark_catalog` to the built-in `V2SessionCatalog` explicitly? thanks in advance.

kindly ping @cloud-fan, can we get this in?

Merged to master, thank you @cloud-fan @pan3793 

Merged to master.

Merged to master.

Thanks @HyukjinKwon 

@yaooqinn PTAL if have free time, Thanks

Due to this is a breaking change, mainly impact for indicate, so close it first.

Thank you @xuzifu666

The remaining test failures are not related to this PR.

Thanks! merging to master.

cc @beliefer 

thanks, merging to master!

Merged to master.

Thank you for the fix!\r\n\r\nThe change LGTM, for test complexity, can you add a custom class test case like here?\r\nhttps://github.com/apache/spark/blob/master/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/streaming/ClientStreamingQuerySuite.scala#L426-L448

I believe this makes a 3.5 scala client running scala foreachbatch not able to run against a 4.0 spark server. But for 3.5, streaming scala is still under development, so this should be fine. But it should be worth noting somewhere about this breaking change. cc @HyukjinKwon  

Added a new test case for using a custom class with foreachBatch (as simple as the test case in foreach) and probably good enough for now.\r\nI do have tested the custom class case with a more complicated test case locally by launching a spark connect client running against a spark connect server, but somehow the same code does not work in the unit testing environment. I can try to improve that part in a follow up.

Merged to master.

The docker test failure is unrelated, thanks, merging to master!

Merged to master.

thanks, merged to master

thanks, merging to master!

in `sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveLateralColumnAliasReference.scala`:\r\n`newAggExprs` holds the expressions to keep in the Aggregate. Should it be `java.util.LinkedHashSet` ?

@tedyu Good point, thanks for pointing it out. Did it in a followup here: https://github.com/apache/spark/pull/49334

LGTM.

+1, LGTM. Merging to master.\r\nThank you, @camilesing and @HyukjinKwon @the-sakthi for review.

Merged to master.

Merged to master.

Merged to master.

cc @itholic @zhengruifeng 

cc @cloud-fan Thank you.

CI failure is not relevant.

The Spark Connect test failure is unrelated, thanks, merging to master!

Merged to master.

Merged to master.

Merged to master.

Merged to master.

merged to master

cc - @ericm-db @jingz-db - PTAL, thx !

@HeartSaVioR - could you PTAL, thanks !

@anishshri-db Would you mind fixing conflicts? Thanks!

> Would you mind fixing conflicts? Thanks!\r\n\r\n@HeartSaVioR - done ! PTAL

Thanks! Merging to master/4.0.

\r\nIs it convenient to support the judgment of how many times a common expression has been used?

thanks for the review, merging to master!

Test first

Happy New Year, @LuciferYang .

Happy New Year ~ @dongjoon-hyun , thank you ~

Merged to master.

Thanks @HyukjinKwon 

Merged into master. Thanks @HyukjinKwon 

should we add a simple test?

Sounds good! Added a unit test.

Merged to master.

thanks, merged to master

let me close this for now

Hi @LuciferYang @zhengruifeng, could you please help to reivew this when you have time? Many thanks.

Although it seems like there should be an issue, executing the command `build/mvn -DskipTests clean install -pl connector/protobuf -e -am`  should not trigger the `user-defined-protoc` profile. \r\n\r\nIs it a mistake in the pr description or does it trigger other issues when specifying `SPARK_PROTOC_EXEC_PATH `\r\n\r\n

@LuciferYang Hi, could you please help to look into it when you get time?

Merged into master. Thanks @morvenhuang 

@LuciferYang Thanks a lot for getting back so quickly.

merged to master

@cloud-fan could you help review? Thanks a lot!

thanks, merging to master!

Some tests need to be updated.

> Some tests need to be updated.\r\n\r\nUpdated golden files & scalastyle.

Tests passed. cc @cloud-fan 

Merged to master.

@MaxGekk could you take a look at this follow-up?

Merged to master.

Merged to master.

thanks, merging to master!

@MaxGekk This change is not required after #49319 and #49334. Underlying change will be removed here #49460. Closing this PR

LGTM

thanks, merging to master!

LGTM

thanks, merging to master!

Merged to master for Apache Spark 4.0.0.

thanks @LuciferYang \r\n\r\nmerged to master

merged to master

thanks, merged to master

@ericm-db - can you also format the PR description and explain in more detail what functionality this PR adds. Thx\r\n\r\nAlso - this is a user facing change right ?

@ericm-db - lets also add the tests such as `snapshotStartBatchId with transformWithState` for this change (changelog + avro enabled with evolution)

Thanks! Merging to master/4.0.

@hvanhovell please review

@gengliangwang can you take a look \r\n

@gengliangwang please check my reply

@vrozov sorry for the late reply. \r\nBefore we move forward, I think the changes in this PR are already covered in https://github.com/apache/spark/pull/45990. If you set `spark.sql.dataframeCache.logLevel` as `WARN`, you will see similar logs for cache/uncache. \r\ncc @anchovYu 

Change in https://github.com/apache/spark/pull/45990 provides troubleshooting options for the `CacheManager` and is useful for debugging memory leaks in the `CacheManager`. Problem with it is that it is not enabled by default (default is TRACE) and produces large amount of logging. This PR will enable early warning notifications and users can further troubleshoot `CacheManager` issues by setting `spark.sql.dataframeCache.logLevel`.

@gengliangwang, @anchovYu please check my reply.

@vrozov the changes of this PR overlaps with the PR https://github.com/apache/spark/pull/45990. How about we simply change the log level from TRACE to INFO?

@gengliangwang My understanding is that log level was set to TRACE intentionally and it should not be enabled by default. Please see comment on #45990\r\n> Because every query applies cache, this log could be huge and should be only turned on during some debugging process, and should not enabled by default in production.\r\n\r\nNote that warnings on line 129 and 145 coexist with changes from #45990 and provide early problem notification.\r\n\r\nThis PR originates from a real issue where I spent large amount of time first isolating memory leak to the `CacheManager` and then debugging it to the `unpersist()` call on a wrong data set. Should the warning be present in the first place, it would help to identify the problem much easier.\r\n\r\n

> Note that warnings on line 129 and 145 coexist with changes from https://github.com/apache/spark/pull/45990 and provide early problem notification.\r\n\r\nCan you provide more details. I think they are similar. Please check the method calls of `CacheManager.logCacheOperation`. \r\n

@gengliangwang Please check lines 129 and lines 145. Those are pre-existing warnings (existed prior to #45990 and were not removed as part of #45990) and they use `logWarning()`, not `CacheManager.logCacheOperation()` (that depends on `spark.sql.dataframeCache.logLevel` settings and are off by default).  I think, this is the right approach as changes in #45990 provide means to further troubleshoot `CacheManager` (without code debugging), and warnings in this PR provides early problem notification.\r\n\r\n> I think they are similar.\r\n\r\nThey are related, but serve different purpose. Entries that use `logWarning()` are enabled by default (including production), while `logCacheOperation()` are disabled by default and should only be enabled to debug caching issues.

@gengliangwang Please check my reply.\r\n\r\nTo clarify why I think #45990 and #49276 are related but do not overlap: changes in #45990 log (trace by default) messages when the cache is modified (an item is added or removed from the cache), while changes in #49276 log warning messages when the cache is expected to be modified by a user, but is *not* modified (an item is not added or is not removed) along with details about Dataset that is used in the call to `persist()` or `unpersist()`.\r\n

@gengliangwang Please check my reply.

@gengliangwang Please review

@gengliangwang Please review

@gengliangwang Please see my [response](https://github.com/apache/spark/pull/49276#discussion_r1956993969) to your [comment](https://github.com/apache/spark/pull/49276#discussion_r1956913586)

@gengliangwang ^^^

@gengliangwang ?

@hvanhovell @dongjoon-hyun Please take a look.

@hvanhovell ? @dongjoon-hyun ?

Ended up creating a new error framework because [we have existing tests expecting IllegalStateException](https://github.com/apache/spark/blob/ef37f9a8e202812ea6b710bfb084ce176ff5e63d/connector/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala#L1005). Also there might be some test cases used by downstream projects expecting the same @brkyvz 

Merged to master.

Merged to master.

Thanks! Merging to master.

LGTM.\r\n\r\nI think there are no major issues, Wenchen. Could we merge it, please?\r\nOutput formatting could be fixed asynchronously later.

LGTM, can you give some logging examples for some simple queries?

thanks, merging to master!

cc @cloud-fan 

This is a good catch! cc @jiangxb1987 @Ngone51 

Thanks, merged to master!

Thank you all for the review

@cloud-fan please take a look when you can

Merged to master.

Thank you @HyukjinKwon !

pyarrow 10.0 fails the whole pyspark\r\n\r\nhttps://github.com/zhengruifeng/spark/actions/runs/12464102622/job/34787749014

pyarrow 11.0 fails PS\r\nhttps://github.com/zhengruifeng/spark/actions/runs/12466553986/job/34794456175

pyarrow 12 also fails PS\r\nhttps://github.com/zhengruifeng/spark/actions/runs/12475279886/job/34818572182

pyarrow 13 also fails PS\r\nhttps://github.com/zhengruifeng/spark/actions/runs/12475508159/job/34819092935

pyarrow 16 also fail PS\r\nhttps://github.com/zhengruifeng/spark/actions/runs/12476053568/job/34820409194

Will send a separate PR to upgrade the minimum requirement of pyarrow to 11.0.0

thanks, merged to master

thanks, merged to master

merged to master

This is all of it, right?

> This is all of it, right?\r\n\r\nyea, that is all :)

`Cache base image` is not a daily build, but let me also add it for debugging

thanks, merged to master

@cloud-fan @cashmand @gene-db could you help review? Thanks!

thanks, merging to master!

Encountered an error when pickling the lambda function. Closed this for now.

Can we have a micro benchmark?

Merged to master.

cc @cloud-fan 

Thanks for the review, merging to master !

cc @yaooqinn and @viirya 

Thank you, @viirya !

Late LGTM, thank you @dongjoon-hyun 

cc @cloud-fan

Merged to branch-3.5.

+1, LGTM. Merging to master.\r\nThank you, @vladimirg-db.

+1, LGTM. Merging to master.\r\nThank you, @vladimirg-db.

Merged to master.

cc @cloud-fan @wangyum 

thanks, merging to master!

can we also create a 3.5 branch backport?

Need to wait for the admin to configure `secrets.CODECOV_TOKEN`\r\nThe relevant PR is here: https://github.com/apache/spark/pull/49228

> Happy New Year, @panbingkun . Is there any update for CODECOV_TOKEN?\r\n\r\nHappy late New Year!\r\n(PS: I have been busy with some work recently, and I am very sorry for replying so late.)\r\n\r\nCurrently, it seems that no one has the permission to add this. It seems that only the original creator of the repo has this permission.

@dongjoon-hyun \r\n<img width="1100" alt="image" src="https://github.com/user-attachments/assets/84f7f309-cacb-411b-9efa-59f6dd3accbc" />\r\nI have sent an email to `private@spark.apache.org` to see if some administrators have the authority to help configure this.

https://issues.apache.org/jira/browse/INFRA-26413

Considering that it has already been added, I will merge this PR and observe it later.\r\n<img width="532" alt="image" src="https://github.com/user-attachments/assets/8d270ad3-2885-4bb6-a181-e0b89b7486c7" />\r\n

Thanks for the review @HyukjinKwon @dongjoon-hyun , merging to master! \r\nI will continue to observe it.

@LuciferYang Thanks for the review, merging to master!

cc @cloud-fan @gengliangwang this PR fixes `|> AGGREGATE` `GROUP BY` ordinal support :)

thanks, merging to master!

cc @cloud-fan @gengliangwang 

Shall we make `AliasResolution` to properly support `PipeExpression`?

> Shall we make AliasResolution to properly support PipeExpression?\r\n\r\n@cloud-fan I tried this, but the PipeExpression needs to check whether its child expression tree contains any aggregate functions, and this needs its child tree to be fully resolved first (which may not have happened yet by the time we run the `ResolveAliases` rule).\r\n

@dtenedor after a few iterations, `ResolveAliases` should see `PipeExpression` with its child resolved, right? The thing is that we should have consistent auto alias generation behavior between normal and pipe SQL syntaxes, e.g.\r\n```\r\n      case ne: NamedExpression => ne\r\n      case go @ GeneratorOuter(g: Generator) if g.resolved => MultiAlias(go, Nil)\r\n      case e if !e.resolved => u\r\n      case g: Generator => MultiAlias(g, Nil)\r\n      case c @ Cast(ne: NamedExpression, _, _, _) => Alias(c, ne.name)()\r\n      case e: ExtractValue if extractOnly(e) => Alias(e, toPrettySQL(e))()\r\n```

@cloud-fan I looked again and I was able to make this work in `ResolveAliases` by moving the check for the `PipeExpression` until after the existing check that the child of the `UnresolvedAlias` is itself resolved.

@dtenedor there are more test failures in the golden files.

thanks, merging to master!

ah good catch!

Thank you!

Closing this as it has been merged to 3.5 by https://github.com/apache/spark/commit/51fb84a54982719209c19136b1d72d2ef44726ee

Thanks! Merging to master.

Merged to master.

close it as it does not exist in 3.5.1 

@cashmand @cloud-fan could you help review? Thanks!

@cloud-fan could you help merge it? Thanks!

thanks, merging to master!

Thanks for the feedback @cloud-fan! Addressed 

hey @cloud-fan can we merge this to master and 4.0 please?

thanks, merging to master/4.0!

cc @MaxGekk @HyukjinKwon @cloud-fan Greetings, this small PR is ready for a review at your convenience :)

Thanks, fixed the relevant test failures. The last failure look flaky/unrelated.

I think the CI has an issue now\r\n```\r\nOh no! ðŸ’¥ ðŸ’” ðŸ’¥ The required version `23.12.1` does not match the running version `23.9.1`!\r\nTraceback (most recent call last):\r\nStart checking the generated codes in pyspark-connect.\r\n  File "/home/runner/work/spark/spark/./dev/check-protos.py", line 84, in <module>\r\n    check_protos(\r\n  File "/home/runner/work/spark/spark/./dev/check-protos.py", line 49, in check_protos\r\n    run_cmd(f"{SPARK_HOME}/dev/gen-protos.sh {module_name} {tmp}")\r\n  File "/home/runner/work/spark/spark/./dev/check-protos.py", line 43, in run_cmd\r\n    return subprocess.check_output(cmd.split(" ")).decode("utf-8")\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File "/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/subprocess.py", line 466, in check_output\r\nRUN: /home/runner/work/spark/spark/dev/gen-protos.sh connect /tmp/check_connect__protosnrkbs0co\r\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File "/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/subprocess.py", line 571, in run\r\n    raise CalledProcessError(retcode, process.args,\r\nsubprocess.CalledProcessError: Command \

> I think the CI has an issue now\r\n> \r\n> ```\r\n> Oh no! ðŸ’¥ ðŸ’” ðŸ’¥ The required version `23.12.1` does not match the running version `23.9.1`!\r\n> Traceback (most recent call last):\r\n> Start checking the generated codes in pyspark-connect.\r\n>   File "/home/runner/work/spark/spark/./dev/check-protos.py", line 84, in <module>\r\n>     check_protos(\r\n>   File "/home/runner/work/spark/spark/./dev/check-protos.py", line 49, in check_protos\r\n>     run_cmd(f"{SPARK_HOME}/dev/gen-protos.sh {module_name} {tmp}")\r\n>   File "/home/runner/work/spark/spark/./dev/check-protos.py", line 43, in run_cmd\r\n>     return subprocess.check_output(cmd.split(" ")).decode("utf-8")\r\n>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File "/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/subprocess.py", line 466, in check_output\r\n> RUN: /home/runner/work/spark/spark/dev/gen-protos.sh connect /tmp/check_connect__protosnrkbs0co\r\n>     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\r\n>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File "/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/subprocess.py", line 571, in run\r\n>     raise CalledProcessError(retcode, process.args,\r\n> subprocess.CalledProcessError: Command \

very likely, as many other PRs can pass the CI.

@cloud-fan @gene-db @cashmand Please help review, thanks!

@cloud-fan Thanks! I have made the changes you recommended.

thanks, merging to master!

thanks, merging to master!

Congratulations for your first commit.\r\n\r\nI added you, `James Baug`, to the Apache Spark contributor group and assigned SPARK-50616 to you.\r\n\r\nWelcome to the Apache Spark community, @jabbaugh !

The JIRA ticket number will be changed to a subtask as soon as I get my account on JIRA created.

Added RECURSIVE keyword to parser so that I can add at least one test as we agreed. I will change the PR description to also address this. Other comments will be resolved most likely tomorrow morning. Thanks again for reviewing.

thanks, merging to master!

please also add new function into `python/pyspark/sql/functions/__init__.py`

thanks, merged to master

@MaxGekk I addressed all comments. Failures seem unrelated, can you please take a look?

@MaxGekk CI has passed, we can merge it now

+1, LGTM. Merging to master.\r\nThank you, @mihailotim-db.

+1, LGTM. Merging to master.\r\nThank you, @mihailoale-db and @cloud-fan for review.

LGTM

The job `Build / Coverage (master, Scala 2.13, Hadoop 3, JDK 17)` (build_coverage.yml) has been suspended for 10 months now, as follows:\r\n- 1.https://app.codecov.io/github/apache?search=spark\r\n  <img width="1389" alt="image" src="https://github.com/user-attachments/assets/af5028ab-f81a-4b03-8c70-88b19936bd5b" />\r\n\r\n- 2.After clicking in above https://app.codecov.io/github/apache/spark\r\n  <img width="1393" alt="image" src="https://github.com/user-attachments/assets/71dffdc6-cbe7-4846-95c6-f55651342aa7" />\r\n  The last successful commit trigger was: [78f7c30](https://app.codecov.io/github/apache/spark/commit/78f7c30e140fd8cf4a80b783dd7e9ee4d1b4d7e2)\r\n```shell\r\n(base) âžœ  spark-community git:(master) âœ— GLA | grep 78f7c30\r\n78f7c30e140fd8cf4a80b783dd7e9ee4d1b4d7e2 - Nikola Mandic, nikola.mandic@databricks.com, 10 months ago : \r\n[SPARK-42328][SQL] Remove _LEGACY_ERROR_TEMP_1175 from error classes\r\n```\r\n\r\n- 3.This job should have failed every day for the past 10 months, as shown below:\r\n  https://github.com/apache/spark/actions/runs/12410482233/job/34646325911\r\n  <img width="1014" alt="image" src="https://github.com/user-attachments/assets/7005ca1b-e7ca-4846-8b5b-fe1be7aa7266" />\r\n\r\n- 4.I think we need `a repo type token` (`Repository upload token`)\r\n  https://docs.codecov.com/docs/codecov-tokens#repository-tokens\r\n  We can obtain it from the following page: https://app.codecov.io/gh/apache/spark/config/general\r\n  <img width="1242" alt="image" src="https://github.com/user-attachments/assets/8c3cffda-adb2-4eed-92b0-aee74f8dc243" />\r\n\r\n- 5.After obtaining the token on the above page, we can make the following `settings`\r\n  ref: \r\n     https://docs.codecov.com/docs/codecov-tokens#using-your-token\r\n     https://docs.codecov.com/docs/codecov-tokens#example-github-actions--official-codecov-action\r\n<img width="818" alt="image" src="https://github.com/user-attachments/assets/0477c084-9a73-4094-9c2b-80a5805b1a73" />\r\n<img width="802" alt="image" src="https://github.com/user-attachments/assets/89d80809-d806-44fb-80d6-09b852e792f9" />\r\n\r\n- 6.But unfortunately, I do not have the corresponding permissions (no `Settings` tab), as follows:\r\n<img width="1307" alt="image" src="https://github.com/user-attachments/assets/2ba141f7-5bf3-4895-8f7f-1f065f20bab0" />\r\n\r\n- So, who can help me configure `step 5`? @HyukjinKwon @zhengruifeng @dongjoon-hyun @yaooqinn 

Also, I think we can fix the issue with `codecov/codecov-action@v4` first and then consider upgrading.

PR submitted for fixing `codecov/codecov-action@v4` as: https://github.com/apache/spark/pull/49251

Merging to master.

Merged to master.

@cloud-fan Can you please help review this

+1, LGTM. Merging to master.\r\nThank you, @panbingkun and @HyukjinKwon for review.

Late LGTM

cc @cloud-fan @dongjoon-hyun, thank you in advance.

Thank you @HyukjinKwon @cloud-fan , merged to master

cc @cloud-fan 

cc @vicennial @juliuszsompolski FYI

> The fix itself should be fine but I wonder why the holder becomes null .. this should be a potential issue\r\n\r\n@HyukjinKwon I will try to reproduce the actual cause later

Merged to master, thank you @HyukjinKwon 

Thanks for the fix!\r\nSomething must have thrown in:\r\n```\r\n      this.holder = SparkConnectService.getOrCreateIsolatedSession(\r\n        req.getUserContext.getUserId,\r\n        req.getSessionId,\r\n        previousSessionId)\r\n    }\r\n```

Perhaps one of [these](https://github.com/vicennial/spark/blob/4f95a7f4dd2c2d30be549d2a90e52e44046e0726/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectSessionManager.scala#L95-L103), it would be interesting to know the suppressed error 

@viirya since this was your original bug fix (although 8 years ago)

Re-ran the failed workflow job to get it to pass

LGTM

LGTM, thank you for the fix!\r\n

The remaining test failures are not related to this PR.

Thanks! merging to master.

This is a simplified version made according to the suggestions of the review.\r\nThe first version is here: https://github.com/apache/spark/pull/48908\r\ncc @cloud-fan 

Merged to master.

+1, LGTM. Merging to master.\r\nThank you, @mihailom-db and @srielau for review.

also cc @utkarsh39

test failed\r\n```\r\n[info] TransientBestEffortLazyValSuite:\r\n[info] - TransientBestEffortLazyVal works *** FAILED *** (4 milliseconds)\r\n[info]   1 did not equal 2 (TransientBestEffortLazyValSuite.scala:52)\r\n```

> test failed\r\n> \r\n> ```\r\n> [info] TransientBestEffortLazyValSuite:\r\n> [info] - TransientBestEffortLazyVal works *** FAILED *** (4 milliseconds)\r\n> [info]   1 did not equal 2 (TransientBestEffortLazyValSuite.scala:52)\r\n> ```\r\n\r\nWhoops, I originally had a `CyclicBarrier` then changed it to a `CountDownLatch` (to make it a one-shot barrier) and forgot to add `await()` calls.

~Unfortunately, the new added tests failed in CI (they passed in my local), let me take a look~

thanks, merging to master!

Putting this PR up for discussion about this problem, as I may lack the context.  This PR has one way to support truncate and any future transform in this category (single attribute reference but multi-argument), but wondering if its overkill or is there any better way.  \r\n\r\nOr do we need to add an explicit TruncateTransform, and support it like BucketTransform?  \r\n\r\n@sunchao @aokolnychyi 

cc @srielau @gengliangwang here is a PR to improve the `RANDSTR` function as requested :)

cc @MaxGekk @HyukjinKwon @cloud-fan this is ready for another round at your convenience :)

The Spark Connect failure is unrelated, thanks, merging to master!

do we really need to change them? `::` and `+` are standard operators in scala

So the main reason is Seq type inconsistency, not `::`s per se.

The Spark Connect failure is unrelated, thanks, merging to master!

Merged to master.

After merging this, I verified like this. Thank you, @zhengruifeng .\r\n\r\n- https://github.com/apache/spark/actions/runs/12363796218\r\n\r\n<img width="551" alt="Screenshot 2024-12-16 at 16 21 37" src="https://github.com/user-attachments/assets/785495b1-b486-45b1-beb7-ed40d233a6bc" />\r\n

thanks @dongjoon-hyun for double check, I also triggered `Build / Python-only (branch-3.5)` https://github.com/apache/spark/actions/runs/12363962335\r\n\r\nit seems 3.5 daily builds were restored @LuciferYang 

https://github.com/apache/spark/actions/runs/12363796218 passed and I re-trigger the failed one in https://github.com/apache/spark/actions/runs/12363962335, once the tests pass, I will cut 3.5.4-rc3. Thank you, @zhengruifeng  and @dongjoon-hyun 

Great!

thanks @LuciferYang so much for help monitoring the CIs and resolving this failure!

merged to master

May I ask what was the root cause of the daily CI failure, @zhengruifeng ?\r\n

Oh, I meant when this bug happens, @zhengruifeng ? Does this bug introduced recently?

This bug was introduced in https://github.com/apache/spark/pull/49193 @dongjoon-hyun 

Just Test

Merged into master. Thanks @HyukjinKwon 

Latest `Build and deploy documentation` taskï¼š\r\n- https://github.com/apache/spark/actions/runs/12366293496/job/34512735986\r\n\r\n![image](https://github.com/user-attachments/assets/0d572012-cea8-468b-8c86-c4053a6b9596)\r\n

Let me hold on this PR until 3.5 is successfully cut

merged to master

https://github.com/changgyoopark-db/spark/runs/34523136586\r\nhttps://github.com/changgyoopark-db/spark/runs/34523226244\r\nhttps://github.com/changgyoopark-db/spark/runs/34523275936\r\n-> triggered three jobs.

All three jobs passed!

Merged to branch-3.5.

Thanks, @LuciferYang 

@cloud-fan Can you help first to see if the overall design is feasible? It involves a lot of UT, so I will modify it step by step.

Why do we care about common expressions in the original predicate? This is an optimization opportunity but what we should focus on is to avoid perf regression when pushing filter through project that duplicate expensive expressions.\r\n\r\nMaybe we should discuss this with a concrete example.

> Why do we care about common expressions in the original predicate? This is an optimization opportunity but what we should focus on is to avoid perf regression when pushing filter through project that duplicate expensive expressions.\r\n> \r\n> Maybe we should discuss this with a concrete example.\r\n\r\nAs the ut in pr, I hope that the common expression will only be calculated only once. `_common_expr_0` finally used in two Filters and `as e`.

Do you want to make it simpler, commonexprdef is not shared, and the original alias is ignored? This way the common expression will be evaluated multiple times, but less than before, especially in nested cases.

how about the rule `PushPredicateThroughJoin`?

> how about the rule `PushPredicateThroughJoin`?\r\n\r\nThere is no replaceAlias \u200b\u200blogic in PushPredicateThroughJoin.

@cloud-fan It seems that the change in filters order causes the execution plan to mismatch. Is there any good solution?

CI failure is not relevant.

This LGTM overall, my last suggestion is to narrow the scope of the `With` expression generated during filter pushdown. We should rewrite the `With` expression immediately at the end of the filter pushdown rule. However, this causes conflicts as we need to push filters through the `Project` generated by `With` in the next iteration, and we need to have a way to make it idempotent.\r\n\r\nMy idea is, when rewriting `With` inside `Filter`, and the generated `Project` is the same as the `Project` above `Filter`, we remove the above `Project`.

> My idea is, when rewriting `With` inside `Filter`, and the generated `Project` is the same as the `Project` above `Filter`, we remove the above `Project`.\r\n\r\nThere is a problem. The new iteration generates different refs. Do you have any suggestions?

After more thought, I think we should think of filter pushdown in a different way. Once you push a predicate through `Project` and expand the attribute reference into an expensive expression, even only once, there is a risk of perf regression, because that expensive expression will be evaluated twice: once in the `Filter` being pushed down, once in the `Project` stays up.\r\n\r\nA safe approach is that when pushing filters through `Project`, the filters should promise to produce the expensive expressions as attributes, so that we can rewrite the `Project` during the pushdown to use these attributes, to make sure the expensive expressions are only evaluated once. This is kind of we pushing down filters and part of the `Project` together. We can extend the `With` expression to use pre-defined Alias to support it.

> After more thought, I think we should think of filter pushdown in a different way. Once you push a predicate through `Project` and expand the attribute reference into an expensive expression, even only once, there is a risk of perf regression, because that expensive expression will be evaluated twice: once in the `Filter` being pushed down, once in the `Project` stays up.\r\n> \r\n> A safe approach is that when pushing filters through `Project`, the filters should promise to produce the expensive expressions as attributes, so that we can rewrite the `Project` during the pushdown to use these attributes, to make sure the expensive expressions are only evaluated once. This is kind of we pushing down filters and part of the `Project` together. We can extend the `With` expression to use pre-defined Alias to support it.\r\n\r\n\r\nThis seems to be back to sharing with across nodes. Sharing with across nodes is more complicated. The current solution cannot be evaluated only once, but compared with before will definitely bring benefits, especially when dealing with multi-layer nested exponential inline situations. At present, It seems also safe to rewrite with batch after each push down predicate batch.\r\n

cc @cloud-fan 

cc @yaooqinn FYI

Merged into branch-3.5. Thanks @yaooqinn 

cc @panbingkun 

Good catch, thank you for helping to fix it.

Merged to master.

late LGTM

Although the PR looks correct, could you re-trigger the failed Python test pipeline, @itholic ?

+1, LGTM. Merging to master.\r\nThank you, @sarutak and @HyukjinKwon for review.

Test first

The document build failed. Let me investigate first.

I\

for `the PRs for branch-3.5`, I think it is not affected, because `branch-3.5` still uses the old image and ignores the `PYTHON_TO_TEST`\r\n```\r\n    - name: Run tests\r\n      env: ${{ fromJSON(inputs.envs) }}\r\n      shell: \

Merged to master.

thanks all,\r\nI am going to create a 3.5 PR to double check it

checked with https://github.com/apache/spark/pull/49195, the 3.5 PRs should be fine

Thank you for double-checking, @zhengruifeng !

nit: How about adding `[PYTHON]` to the PR title?

Thank you for updating.

Merged to master. Thank you, @sarutak and @MaxGekk .

Thank you @dongjoon-hyun and @MaxGekk !

@hvanhovell , @cloud-fan - could you please take a look here?

Merged to master.

Is it ready, @panbingkun ?

> Is it ready, @panbingkun ?\r\n\r\nYeah, thanks!

@HyukjinKwon Since 2.24.3 includes the following fix:\r\n\r\n- https://github.com/apache/logging-log4j2/issues/3234\r\n- https://github.com/apache/logging-log4j2/pull/3235\r\n\r\n\r\nI believe we can try this change again: [[MINOR][TESTS] Use SparkContext.setLogLevel in QuietTest ](https://github.com/apache/spark/pull/48966)

Thanks! Merging to master.

Merged to master.

@cloud-fan, @stefankandic, @stevomitric please review this urgently.

If this is really that performance critical, should we make `supportsBinaryEquality` a lazy val?

> If this is really that performance critical, should we make supportsBinaryEquality a lazy val?\r\n\r\nCan we improve it altogether? Right now it fetches the collator object and gets one of its properties. We can simply do integer comparisons against it.

> > If this is really that performance critical, should we make supportsBinaryEquality a lazy val?\r\n> \r\n> Can we improve it altogether? Right now it fetches the collator object and gets one of its properties. We can simply do integer comparisons against it.\r\n\r\n@stevomitric How is that an improvement though? If it were a val or a lazy val it would get computed only once, and now we have to do a comparison on each call, which for the interpreted path could happen billions of times.

@stefankandic Are you ok with the changes?

> @stefankandic Are you ok with the changes?\r\n\r\nI am now

+1, LGTM. Merging to master.\r\nThank you, @jovanpavl-db and @stefankandic @stevomitric for review.

cc @LuciferYang @dongjoon-hyun 

Merged to master~

cc @panbingkun 

LGTM, +1.\r\n\r\nAdditionally, there is another `GetJsonObjectEvaluator`, and I want to move it to `JsonExpressionEvalUtils.scala`, but I would like to do so after the completion of this PR for https://github.com/apache/spark/pull/48908, so that `GetJsonObject` and `JsonTuple` can share the `SharedFactory` in the file `JsonExpressionEvalUtils.scala` (The class `SharedFactory` access restrictions is `private[this]`).\r\n\r\n@cloud-fan Can you help review this PR https://github.com/apache/spark/pull/48908 in your free time? Thank you very much !

@ueshin should we also cover `MergeIntoWriter`, and perhaps other classes that can build Datasets and can use arbitrary Columns?

@hvanhovell I\

The remaining test failures are not related to this PR.

Thanks! merging to master.

cc @yaooqinn 

those links can only be view by Apache members(?). Us others only get 404 on those. \r\n

The title is still visible to you, @bjornjorgensen , right?\r\n\r\nNote that these are not considered CVE issues. I just added it as a tip.

ok, yes. 

Thank you for reviewing and giving me your feedbacks.

@peter-toth Please, have a look at the PR.

@peter-toth thanks a lot for your comment. Let us then wait for @cloud-fan to answer the question about JIRA, so that I know if I should restructure it. After that, hopefully we can merge this.

thanks, merging to master!

JIRA subticket created, and the PR description is changed with the new JIRA ticket number @cloud-fan \r\nThanks again for approving :)

@cloud-fan since you were involved in https://github.com/apache/spark/pull/21018 where the problematic code was introduced.

An alternative fix could be to mark the entire method as synchronized if we do not think these are on a critical path.

Merged to master. Thank you, @zhengruifeng .

cc @cloud-fan 

thanks, merging to master!

Merged to master. Thank you, @zhengruifeng .

@siying for both questions the answer is yes it is using v2. This is the trick I have here:\r\nhttps://github.com/apache/spark/blob/053af1a0fbc34debb729a3e86837b4aeaedb11ca/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBSuite.scala#L3025-L3040\r\n\r\nThe map will be updated inside commit()\r\n\r\n

@brkyvz mind taking a look? TY!

Thanks! Merging to master.

Thanks! Merging to master.

@HeartSaVioR The tests now pass : )

Thanks! Merging to master.

@HeartSaVioR PTAL

LGTM overall pending CI

https://github.com/ericm-db/spark/runs/34898167412\r\n\r\nCI is green. Thanks! Merging to master.

Please let me know if this is ready, @LuciferYang .

rebase and test again

this one is ready to go ~ @dongjoon-hyun

Thanks @dongjoon-hyun 

Test first

Thanks @dongjoon-hyun 

thank you!\r\nmerged to master

thanks, merging to master!

Added the benchmarks.

Added the benchmark results for JDK21

+1, LGTM. Merging to master.\r\nThank you, @vladimirg-db.

thanks @dongjoon-hyun 

cc @gaborgsomogyi and @viirya too from the following\r\n- https://github.com/apache/spark/pull/38306

Thanks for catching it! After adapting the suggestions from @HeartSaVioR it will be good to go.

Hi @dongjoon-hyun. Good point, [SPARK-50568](https://issues.apache.org/jira/browse/SPARK-50568) here it is!

Thanks! Merging to master.

Thank you for updating. Could you make CI happy, @steven-aerts ?

According to the CI results, this PR seems to introduce a binary compatibility issue.\r\n```\r\n[info] spark-examples: mimaPreviousArtifacts not set, not analyzing binary compatibility\r\n[error] java.lang.RuntimeException: Failed binary compatibility check against org.apache.spark:spark-core_2.13:3.5.0! Found 6 potential problems (filtered 4083)\r\n```\r\n\r\nFYI, if the change is a valid one, you can add the broken parts explicitly here, @steven-aerts .\r\n- https://github.com/apache/spark/blob/master/project/MimaExcludes.scala

> According to the CI results, this PR seems to introduce a binary compatibility issue.\r\n> \r\n> ```\r\n> [info] spark-examples: mimaPreviousArtifacts not set, not analyzing binary compatibility\r\n> [error] java.lang.RuntimeException: Failed binary compatibility check against org.apache.spark:spark-core_2.13:3.5.0! Found 6 potential problems (filtered 4083)\r\n> ```\r\n> \r\n> FYI, if the change is a valid one, you can add the broken parts explicitly here, @steven-aerts .\r\n> \r\n>     * https://github.com/apache/spark/blob/master/project/MimaExcludes.scala\r\n\r\n@dongjoon-hyun by keeping/migrating some stuff in `JsonProtocol` object annotated as `@deprecated` I was able to make CI happy.    Thanks for the suggestion.\r\nSome other tests are still running, but last time they succeeded. ðŸ¤ž 

@steven-aerts thanks for this PR, do we have any ETA for merge :)

cc @dongjoon-hyun @LuciferYang 

Merged into branch-3.5 for Apache Spark 3.5.4 RC2, thanks @pan3793 

Merged to master.

thanks, merged to master

late LGTM.

- After more than a `week` of investigation and research, when we upgraded `Kubernetes client` from `6.x` to `7.0` and switched `default` HttpClient implementation from `OkHttp` to `Vert.x`, the issue of `Kubernetes Integration test` failure has been resolved.\r\n- The root cause is that some thread attributes `daemon` of `Vert.x` are set to `false`, so that after using `ProcessBuilder` to start the spark application on `K8S` (based on `spark-submit`), the `main thread` has already ended, but the above threads will not end, resulting in the overall `process` based on `spark-submit` not ending either.

cc @dongjoon-hyun @pan3793 @Yikun @LuciferYang 

FYI, `okio` 1.7.16 was CVE-related update at that time, @panbingkun .\r\n- https://github.com/apache/spark/pull/47758\r\n\r\ncc @roczei , @yaooqinn from #47758 

I think this PR can wait a little longer, because after https://github.com/fabric8io/kubernetes-client/pull/6726 is fixed, this PR can be simpler, see the following discussion: https://github.com/fabric8io/kubernetes-client/issues/6709#issuecomment-2545071491

@dongjoon-hyun \r\nThe version `7.0.1` has been released, and this PR has also been updated synchronously. \r\nIt now looks much simpler.

Thank you for syncing with 7.0.1, @panbingkun .

The `LICENSE` of the newly added 4 jars are as follows:\r\n\r\n|Jar|LICENSE|URL|\r\n|----|----|--------------------|\r\n|io.vertx:vertx-auth-common|Apache License Version 2.0|https://github.com/eclipse-vertx/vertx-auth/blob/master/LICENSE.txt|\r\n|io.vertx:vertx-core|Eclipse Public License 2.0 or Apache Software License 2.0|https://github.com/eclipse-vertx/vert.x/blob/master/LICENSE.md|\r\n|io.vertx:vertx-web-client|Apache License Version 2.0|https://github.com/vert-x3/vertx-web/blob/master/LICENSE.txt|\r\n|io.vertx:vertx-web-common|Apache License Version 2.0|https://github.com/vert-x3/vertx-web/blob/master/LICENSE.txt|

BTW, sorry for the late response, @pan3793 .

I collected this as a subtask for the umbrella JIRA, SPARK-49524 (Improve K8s support).

This will be the last ..

Merged to master.

@jingz-db - test failures seem relevant ?

@jingz-db There seems to be test failure? https://github.com/jingz-db/spark/actions/runs/12474212623/job/34815951143\r\n

Thanks! Merging to master.

@jingz-db \r\nhttps://github.com/apache/spark/pull/47805#issuecomment-2564246240\r\nLooks like Thread.stop() is removed in Java 21 - it was deprecated. Looks like they no longer support the same operation but recommend to use a variable as signal flag. Could you please create a follow-up PR? Even better if you could run the failing test with JDK 21 setup. Thanks in advance!\r\ncc. @HyukjinKwon 

Just a quick note .. Seems like the test `test_value_state_ttl_expiration` is still flaky when old dependencies are used (https://github.com/apache/spark/actions/runs/12883552117/job/35918143550).\r\n\r\n@jingz-db mind taking a look please? I filed a JIRA at https://issues.apache.org/jira/browse/SPARK-50908

To see the bug, you can do any of the following:\r\n1)  Comment out the implementation of equalsIgnoreRuntimeFilter and hashCodeIgnoreRuntimeFilters from InMemoryV2FilterBatchScan.\r\nor\r\n2) Take the BatchScanExec from master and update the code and run the test

Pls note:\r\nActual source code changes are relatively few only 3 - 4 files modified. rest is newly added test resources to reproduce the issue. may be possible to write a similar bug test with minimal test resources. Can be done after review.

Could you re-trigger once more, @cashmand ?

Hi @dongjoon-hyun, it looks like the tests are passing now.

Actually @dongjoon-hyun, I just realized that ArrayType has the same nullability issue. Let me update the PR to also set the nullability correctly on ArrayType.

Thank you!

@dongjoon-hyun, the PR should be good to merge now. Thanks for your patience!

thanks, merging to master!

Previous chain of comments:\r\nFrom @anchovYu \r\nHi @ahshahid , thanks for the proposal and the PR. However, the current Dataframe cache design has a lot of design flaws, I would worry that improving the cache hit rate in this case will make these problems worse:\r\n\r\nstale results\r\nThe current design of Dataframe cache is unable to proactively detect data staleness. Consider the case the table is changed by some other applications - Dataframe cache never knows that and it leads to outdated results. In this case, increasing the hit rate potentially increases the risk of hitting stale data.\r\nunstable performance\r\nSince Dataframe cache is shared across Spark sessions and applied automatically, one session could use the data cached by another session in the same driver, or no longer be able to use the cached data when it is invalidated by another session in the same driver, all without any notice. It results in unpredictable performance for even two consecutive executions on the same query in one session in the same driver. Similarly, increasing the hit rate may make it easier to encounter such unstable performance issues.\r\ncc @cloud-fan\r\n\r\n\r\nFrom @ahshahid \r\n@anchovYu\r\nI see, what you are saying.\r\nActually this PR arose as sub - pr for the issue [spark-45959](https://issues.apache.org/jira/browse/SPARK-45959).\r\nThe PR for the same is https://github.com/apache/spark/pull/49124\r\nThe increase of the cache hit is a by product of the above PR.\r\nFrom my experience and even the current customer use cases, this issue ( [spark-45959](https://github.com/apache/spark/pull/49124)) has caused increased in compile time from say under a minute to anything ranging from 2hrs - 8 hrs +.\r\nWhether the issues of stale cache etc increase or not would very much depend upon the nature of the dataframe cached .\r\nAlso most of the time the user would cache a dataframe and then keep building new data frames from the base. So in that sense, even currently , there is no way to prevent new data frames from not using cached data , if the sub-plan matches.\r\nAnd whatever fix goes in for stale cache resolution issue would automatically apply to this change too.\r\nSo pls consider this PR per se not in the light of increasing the cache hit, but as a requirement to resolve the issue of [spark-45959](https://issues.apache.org/jira/browse/SPARK-45959).

cc @HyukjinKwon and @LuciferYang 

Merged to master.

Thank you, @HyukjinKwon !

late LGTM\r\nThanks @dongjoon-hyun 

For the record, I really appreciated your various reports and participating including this from your perspective. It leads the Apache Spark community most robust and complete. We need more active participations like yours.

From my understanding, `toJSON` is a proper API and it was added in Spark 2.0.0.\r\nIt is documented [here](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html#toJSON--), at the official `DataSet` Spark documentation. On the internet, I see a lot of code examples using this `toJSON` API to construct series of JSON strings from DataSet/DataFrame, and I have not heard one place which says it is not a proper API or it is getting deprecated.\r\nThe workaround that @HyukjinKwon proposed works in my case, so I want to hear your thoughts about whether it is worth extending `DataSet` API with `toJSON(jsonOptions: Map[String, String]` method. These JSON options are also well documented [here](https://spark.apache.org/docs/3.5.1/sql-data-sources-json.html), and I think it would be better for users to have a method `toJSON(options)` for their purpose instead of using the workaround that was proposed above.\r\nPlease share your thoughts @cloud-fan @MaxGekk @HyukjinKwon because this one is critical for timestamp precision in `JSON` converted strings from the `DataSet`.

> Currently, the timestamp returned by toJSON method will have millisecond precision, but sometimes, users need better precision like micro.\r\n\r\nI would consider to change the default timestamp pattern, and output timestamp with microseconds precision by default in builtin text datasources. I believe it is worth to do in the release 4.0.

Thank you for comments. I will close this PR for now, as changing the default timestamp pattern seems as better option.

@stevomitric @mihailom-db please take a look

@MaxGekk can you also take a look?

+1, LGTM. Merging to master.\r\nThank you, @stefankandic and @mihailom-db for review.

Hi @harshmotw-db, @hvanhovell, @cloud-fan in continuation to https://github.com/apache/spark/pull/49080 discussion and [SPARK-50525](https://issues.apache.org/jira/browse/SPARK-50525). 

@cloud-fan this change will impact users indeed, but it was done in considerations to https://github.com/apache/spark/pull/49080 and https://github.com/apache/spark/pull/48909. If you think it would be better to extend repartition in the similar fashion as `InsertMapSortInGroupingExpressions`, we can discuss that as well. We will have a bit for inconsistency in this case.

consistency is good but breaking change is scary, even if the old behavior returns the wrong result.

@MaxGekk could you please check this PR again? what are your thoughts?

We can rename `InsertMapSortInGroupingExpressions` to `InsertMapSortExpression` and match the `Repartition` plan in this rule to insert `MapSort` expression

@ostronaut what does it take to get this PR moving?

> @ostronaut what does it take to get this PR moving?\r\n\r\nNo blockers for now! If this is fine, i will implement changes as suggested by @cloud-fan, where map will be changed to sorted map in same way as InsertMapSortInGroupingExpressions. \r\n\r\nIf you have any other comments, please let me know!

@cloud-fan, @hvanhovell, @MaxGekk ready for review after applying all suggestions!

There is a test failure in `AdaptiveQueryExecSuite`, may be related?

> There is a test failure in `AdaptiveQueryExecSuite`, may be related?\r\n\r\n[- SPARK-47148: AQE should avoid to submit shuffle job on cancellation](https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala#L914) test failed on initial run, but after re-run it succeeded. So i dont think its related to this PR. Most likely this test is unstable.\r\n\r\n**Note**: the issue was: `scala.package.Seq.apply[org.apache.spark.SparkException](error).++[Throwable](scala.Option.apply[Throwable](error.getCause())).++[Throwable](scala.Predef.wrapRefArray[Throwable](error.getSuppressed())).exists(((e: Throwable) => e.getMessage().!=(null).&&(e.getMessage().contains("coalesce test error")))) was false (AdaptiveQueryExecSuite.scala:940)`\r\n\r\n

Hi @cloud-fan, @MaxGekk. Can we merge this PR?

The Spark Connect failure is unrelated, thanks, merging to master!

According to the logs, 4 suites failed due to this.\r\n```\r\norg.apache.spark.sql.DataFrameSuite\r\norg.apache.spark.sql.DSV2CharVarcharTestSuite\r\norg.apache.spark.sql.FileSourceCharVarcharTestSuite\r\norg.apache.spark.sql.HiveCharVarcharTestSuite\r\n```

In addition to the newly added test case, the other three failures (including Hive module) seems to be originated by other PR, not this optimizer PR.\r\n- https://github.com/apache/spark/actions/workflows/build_non_ansi.yml\r\n<img width="691" alt="Screenshot 2025-01-12 at 12 15 33" src="https://github.com/user-attachments/assets/f9205aa0-bae5-4fba-9772-056b1e80a11b" />

Here is a follow-up.\r\n- https://github.com/apache/spark/pull/49457

cc @wangyum 

cc @cloud-fan

Merged to master.\r\n\r\nCould you make a backporting PR, @pan3793 ?

cc @LuciferYang as the release manager of Apache Spark 3.5.4, too.

> cc @LuciferYang as the release manager of Apache Spark 3.5.4, too.\r\n\r\nGot ~ thanks @dongjoon-hyun 

migration guide is updated at https://github.com/apache/spark/pull/49252 and https://github.com/apache/spark/pull/49256

thanks, merged to master

cc @zhengruifeng 

Thanks @dongjoon-hyun 

thanks, merged to master

thanks @dongjoon-hyun \r\n\r\nmerged to master

thanks, merging to master!

I made a follow-up.\r\n- #49401

@LuciferYang Do you know why the python codegen test failed [here](https://github.com/bogao007/spark/actions/runs/12268004743/job/34229117566)? I was using the `./dev/streaming-gen-protos.sh` script that you created in [this](https://github.com/apache/spark/pull/48815) PR. Do we need to update the `check-protos.py` to accommodate the changes in `StateMessage_pb2.py` and `StateMessage_pb2.pyi`?

Thanks! Merging to master.

Merged to master.

@MaxGekk Can you look at this PR since you implemented the `spark.sql.stableDerivedColumnAlias.enabled` config?

thanks, merging to master!

cc @cloud-fan @MaxGekk 

@MaxGekk \r\nI have renamed the title of the PR.\r\nTests passed.

gonna merge this as the PR is open already.

Merged to master.

Merged to master.

Could you review this PR, @viirya ?

Could you review this log PR for Spark 4, @LuciferYang ?

If the integrity of those file in question what about calculating the CRC by using (java.util.zip.CRC32) but only calculating and  logging out the value when TRACE level logging is requested.\r\n

If the PR only targets archives I am confused as for a zip file (like a jar) crc check is done by default when its content is unpacked. May I ask what was the use case where this might help?

Thank you for review and approval, @MaxGekk and @viirya .\r\n\r\nTo @attilapiros , yes, the goal is specifically to log the file size in addition to the file name. Nothing more. From the logs, a user can check if the file size is equal to the expected value. In addition, they can add the size to calculate the total traffic.

Do you have a further question, @attilapiros ?

Let me merge this~\r\n\r\nTo @attilapiros , your idea (auto verification?) sounds independent from this log-only PR. We can talk orthogonally later.

As we talk about zips where CRC check is already included we do not need to check the integrity once more it will be checked at unpack.\r\n

late LGTM ~ thanks @dongjoon-hyun 

cc @richardc-db and @viirya , too

also cc @pan3793 

FYI: SPARK-50235 causes Iceberg test failure (JVM crash), and this follow-up PR does not help, I may not be able to provide conclusions within a short time because I\

Thank you for the reporting, @pan3793 .

Thank you for re-triggerring the failed test pipelines.

> FYI: [SPARK-50235](https://issues.apache.org/jira/browse/SPARK-50235) causes Iceberg test failure (JVM crash), and this follow-up PR does not help, I may not be able to provide conclusions within a short time because I\

Iceberg has [an implementation similar to `ArrowColumnVector`](https://github.com/apache/iceberg/blob/main/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java)

Merged into branch-3.5 for Apache Spark 3.5.4 RC2. Thanks @dongjoon-hyun @viirya and @pan3793 

Thank you, @LuciferYang and all!

The method is newly added just in branch-3.5, not released yet.

@MJovan2002 could you please sync with master again, so we can merge this in.

LGTM, please update title to reflect the config name, also give a SQL name in the description, so that spark users can know which config you are adding.

In this PR we check the `preserveCharVarcharTypeInfo` flag in many places. Do we have a narrow wrist? IIUC we replace the char/varchar type with string type for all leaf scan nodes, are there any other entry points that can produce char/varchar type and need to check the flag?

@cloud-fan Only one more that I know of is Cast expression.

I think we only need to check the flag at these entry points. For other places, just do whatever is needed to handle char/varchar, as if they see char/varchar, it means the flag is turned on.

thanks, merging to master!

Not a test only issue, but a "rare" data race between the SparkConnect service and ExecuteThreadRunner.\r\n-> The client may get the wrong error code.

Overall, given the number of sporadic test failures, I think we (or I?) need to spend significant time dealing with all the data race problems in the SparkConnect code.

BTW, thank you, @changgyoopark-db , for taking a look at these `Spark Connect` issue. This has been a long-standing community headache, not only in CI level, but also the community-level trust on the maturity of `Spark Connect`. 

Created https://issues.apache.org/jira/browse/SPARK-50535 and https://issues.apache.org/jira/browse/SPARK-50534 for tracking known issues. I\

Yep, it looks good to go to me, @HyukjinKwon . Thank you, @changgyoopark-db and all.

Merged into branch-3.5 for Apache Spark 3.5.4 RC2, thanks @changgyoopark-db @dongjoon-hyun and @HyukjinKwon 

@dongjoon-hyun thanks for informing this. I precisely know what exactly went wrong with the test by looking at the stack trace - itâ€˜s now a test-only problem; unfortunately, Iâ€™m not available this week (no laptop access) so Iâ€™ll try to make a PR next Monday.

Unfortunately, it fails again today. Apparently, this backport has a chance to increase flakiness instead of reducing.\r\n\r\n- https://github.com/apache/spark/actions/runs/12296721038/job/34316344940\r\n![Screenshot 2024-12-12 at 10 17 55](https://github.com/user-attachments/assets/7fccd6cb-4d36-4151-b069-9cad67500eda)\r\n\r\n```\r\n[info] *** 1 TEST FAILED ***\r\n[error] Failed tests:\r\n[error] \torg.apache.spark.sql.connect.execution.ReattachableExecuteSuite\r\n```

cc @cloud-fan 

@allisonwang-db can you run `./build/mvn scalafmt:format -Dscalafmt.skip=false -Dscalafmt.validateOnly=false -Dscalafmt.changedOnly=false -pl sql/api -pl sql/connect/common -pl sql/connect/server -pl connector/connect/client/jvm`? Seems the format check failed

Thanks, merging to master

Nice refactoring, thank you!

thanks @xinrong-meng \r\n\r\nmerged to master

thanks, merged to master

@LuciferYang \r\n> will we simplify the content of dev/infra/Dockerfile?\r\n\r\nI think we can just let it alone and remove it after 3.5 EOL.

thanks, merged to master\r\n\r\nwill monitor the CIs

late LGTM.

cc - @ericm-db @HeartSaVioR - PTAL, thx !

> Shall we add the test for checking the default value? We can use the traditional approach with Spark 3.5 checkpoint.\r\n\r\nDone - added the test

Thanks! Merging to master.

cc @cloud-fan @dongjoon-hyun here is the PR with test cases fixed.

Thanks @dongjoon-hyun, fixed the unit test and manually re-ran it again to make sure.\r\n\r\n<img width="1225" alt="image" src="https://github.com/user-attachments/assets/8f022c78-e891-4aec-9884-7c572255e3d6">\r\n

cc @jiashenC as the original author.

@dongjoon-hyun sounds good, I added the empty commit as suggested. In the future I can include the `--author=...` option as well.

Merged to master for Apache Spark 4.0.0 on February 2025.\r\n\r\nThank you all!

For the record, this is merged with the following authorships.\r\n```\r\nLead-authored-by: Jiashen Cao <jiashenc@gatech.edu>\r\nCo-authored-by: Daniel Tenedorio <daniel.tenedorio@databricks.com>\r\n```

To @jiashenC , do you have an Apache JIRA ID? Please let me know your ID.

@dongjoon-hyun Sorry for slow response, I believe my JIRA ID is just `jiashenc`. \r\n\r\n@dtenedor Thank you for covering this for me! Yeah, I am definitely interested in contributing to Spark!

I added `jiashenc` to the Apache Spark contributor group and assigned SPARK-49565 to you, @jiashenC .\r\n\r\nWelcome to the Apache Spark community and congratulations for your first commit!

Merged to master.

thanks, merging to master!

Interestingly, it seems to fail in `master` branch somehow. Could you take a look at the failures, please, @cashmand and all?\r\n\r\n![Screenshot 2024-12-11 at 10 23 41](https://github.com/user-attachments/assets/f35d999f-2518-4ae6-a068-a54f62640ea7)\r\n\r\n- https://github.com/apache/spark/actions/runs/12280389975/job/34266758469\r\n- https://github.com/apache/spark/actions/runs/12280542565/job/34267268862\r\n\r\n```\r\n[info] *** 1 TEST FAILED ***\r\n[error] Failed: Total 12430, Failed 1, Errors 0, Passed 12429, Ignored 33, Canceled 1\r\n[error] Failed tests:\r\n[error] \torg.apache.spark.sql.VariantShreddingSuite\r\n[error] (sql / Test / test) sbt.TestsFailedException: Tests unsuccessful\r\n```

Sorry but let me revert this to recover the CI and unblock other PRs. Although it looks like a trivial Mal-formed error case testing, could you re-submit this PR to make it sure to pass all?

> Sorry but let me revert this to recover the CI and unblock other PRs. Although it looks like a trivial Mal-formed error case testing, could you re-submit this PR to make it sure to pass all?\r\n\r\nHi @dongjoon-hyun, I opened https://github.com/apache/spark/pull/49151, which is exactly the same, but updates the broken test. Can you please take a look, and merge if it looks okay?

Could you review this PR, @viirya ?

Looks good to me.\r\n\r\nOn Mon, Dec 9, 2024 at 2:04\u202fPM Dongjoon Hyun ***@***.***>\r\nwrote:\r\n\r\n> Could you review this PR, @viirya <https://github.com/viirya> ?\r\n>\r\n> â€”\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/spark/pull/49116#issuecomment-2529621569>, or\r\n> unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAAQZ576KCNM2HUMBT5RVLT2EYHWFAVCNFSM6AAAAABTJVOV4CVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKMRZGYZDCNJWHE>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n

Thank you!

Merged to master.

Thank you, @HyukjinKwon !

In 3.5, JDBCRDD class and InputToSql function are private, which makes this backport difficult.\r\nIn master, these are not private, so I\

thanks, merging to 3.5!

@MaxGekk please take a look when you have the time

+1, LGTM. Merging to master.\r\nThank you, @stefankandic.

Thanks! Merging to master.

Merging to master/4.0

@dongjoon-hyun  Could you review this renaming PR

@zhengruifeng  Could you review this PR

Could you please take a look? @dongjoon-hyun 

cc @yaooqinn Can we backport this one? It seems to enhance the robustness of the release process.

Merged into branch-3.5. Thanks @yaooqinn 

Got it. Thanks!

how is it going?

Needs some review from @hvanhovell ..

rebased ..

oops I meant to merge https://github.com/apache/spark/pull/49788. Sorry I will revert this

Questions based on questions on the mailing list thread\r\n- Is this supposed to work with cluster deploy mode?\r\n- If so, how would this work on YARN? It looks like it will only use an ephemeral port when running tests, I think YARN would also need to use an ephemeral port?\r\n- And if so, how is the connect server being secured? If this is running in a secured but shared environment, the connect server may be accessible by anyone else using the cluster.

> Is this supposed to work with cluster deploy mode?\r\n\r\nYes. For `--master ...`, It will run a local Spark Connect server where Spark Driver is running.\r\n\r\n> If so, how would this work on YARN? It looks like it will only use an ephemeral port when running tests, I think YARN would also need to use an ephemeral port?\r\n\r\nYes, it will launch a Spark Connect server locally.\r\n\r\n> And if so, how is the connect server being secured? If this is running in a secured but shared environment, the connect server may be accessible by anyone else using the cluster.\r\n\r\nThis is the same as Py4J server. It does not enable the auth feature by default. We could have the implementation in Spark Connect to but I would prefer to run this separately.\r\n

I am going to merge this if that sounds fine to you @Kimahriman 

 > > If so, how would this work on YARN? It looks like it will only use an ephemeral port when running tests, I think YARN would also need to use an ephemeral port?\r\n> \r\n> Yes, it will launch a Spark Connect server locally.\r\n> \r\n\r\nBut if it uses the default 15002 port, it will hit issues when multiple drivers are running on the same node, unless it uses an ephemeral port.\r\n\r\n> This is the same as Py4J server. It does not enable the auth feature by default. We could have the implementation in Spark Connect to but I would prefer to run this separately.\r\n\r\nWhere is auth not enabled for Py4J? It looks like it always generates a secret that the client uses to connect to it https://github.com/apache/spark/blob/e2ef5a482604059197c79f1f7c305df4ce57213a/core/src/main/scala/org/apache/spark/api/python/Py4JServer.scala#L32\r\n

I think we can address them all anyway because we already have Py4J that exactly works like Spark Connect server.

@Kimahriman mind I if I run it separately? We already have the logic of running it in any event.

> @Kimahriman mind I if I run it separately? We already have the logic of running it in any event.\r\n\r\nFine either way, brought this up on the mailing list to bring a little more awareness

Yup, thanks for chiming in. Let me followup.\r\n\r\nMerged to master and branch-4.0.

am working on it now

Merged to master.

cc @LuciferYang 

Thank you for letting me know @dongjoon-hyun 

Merged to branch-3.5 for Apache Spark 3.5.4.

To @LuciferYang , please let me know if you have any blocker or need my help during your Spark 3.5.4 RC1 preparation step. AFAIK, this is the last one we have been waiting for.

> To @LuciferYang , please let me know if you have any blocker or need my help during your Spark 3.5.4 RC1 preparation step. AFAIK, this is the last one we have been waiting for.\r\n\r\nYes, I think this should be the last one. Thank you very much for helping resolve many issues over the past few days. @dongjoon-hyun 

@dejankrak-db @stevomitric please take a look, thanks!

@cloud-fan can you also take a look?

thanks, merging to master!

@HeartSaVioR - could you PTAL at the small test change ? Thx\r\n\r\n@LuciferYang reported some runs being flaky on the original PR here - https://github.com/apache/spark/pull/48913#discussion_r1862459541

Thanks! Merging to master.

Also, cc @cloud-fan since this is about Apache Spark configuration namespace.

Could you review this, @LuciferYang ?

Merged into master. Thanks @dongjoon-hyun 

Thank you, @LuciferYang 

I converted this to `Draft` due to the `Conflicting files`. You can convert this back to a normal PR when the PR is ready.\r\n\r\n![Screenshot 2024-12-06 at 13 06 33](https://github.com/user-attachments/assets/8d47000b-ec1b-4223-b319-3e0e471e2b06)\r\n

- https://github.com/pmenon/spark/actions/runs/12207099390/job/34057905545\r\n\r\n![image](https://github.com/user-attachments/assets/9410b3a4-e7c1-4084-9efa-dc31961bfb9b)

thanks, merging to master!

cc - @cloud-fan @HeartSaVioR - PTAL, thx !

> to ensure that the serialization path is hit and working as expected\r\n\r\nYes please. This is the path we had an issue I assume.

> Yes please. This is the path we had an issue I assume.\r\n\r\nDone - added the suite

Thanks! Merging to master.

cc @yaooqinn and @cloud-fan 

Could you review this PR when you have some time, @huaxingao ?

Thank you, @huaxingao !

Merged to master for Apache Spark 4.0.0 on February 2025.

cc @LuciferYang 

Could you review this PR when you have some time, @huaxingao ?

Thank you, @huaxingao !

Merged into master/branch-3.5. Thanks @dongjoon-hyun @huaxingao 

Thank you, @LuciferYang .

cc @dtenedor , @cloud-fan , @gengliangwang , @MaxGekk 

Could you review this PR when you have some time, @huaxingao ?

Thank you, @huaxingao !

Merged to master for Apache Spark 4.0.0 on February 2025.

How did this work before? Can you give an example?

> How did this work before? Can you give an example?\r\n\r\nImagine you have next MySQL tables\r\n`my_schema.table1`\r\n`myyschema.table2`\r\n\r\nAnd you do next operation in Spark:\r\n```\r\nspark.conf.set("spark.sql.catalog.myCatalog", JDBCTableCatalog.getClass.getName)\r\nspark.sql("SHOW TABLES IN myCatalog.my_schema")\r\n```\r\n\r\nBefore (because provided name is not escaped):\r\n`my_schema.table1`\r\n`myyschema.table2`\r\n\r\nAfter (because we escaped characters):\r\n`my_schema.table1`

Mind adding the JIRA

cc @attilapiros, @dongjoon-hyun 

Checking!

Thanks Attila. I look into this.

I checked how it looked before my change and I see some issues there as well. There are some missing skips what I planned to fix and there are -8 tasks in many places. Anyway I investigate the issue further.\r\n\r\n<img width="2039" alt="image" src="https://github.com/user-attachments/assets/04926eaa-15eb-444e-be52-d92c3c5e2b5b" />\r\n

As your solution changed you should update the PR description accordingly.\r\n\r\nBy introducing `total_tasks` the already existing `num_tasks` lost it purpose: \r\nAt least from both the `BatchPage` and the `AllJobsPage` `num_tasks` is replaced by `total_tasks`. \r\nThis does nots seems to me right! Especially as `numSkippedTasks` is based on the `total_tasks`.\r\n\r\nMoreover as I see the unit test does not test the new `total_tasks`.\r\nWDYT is it possible to write a test which covers the problem we seen in the UI? That would be the best.\r\n\r\n\r\n

thanks for the review, merging to master!

Merged to master.

cc @WweiL @HeartSaVioR 

@HeartSaVioR \r\nCan you review this PR ?

I think it should be renamed as\r\n```\r\n[SPARK-50360][FOLLOWUP][MINOR] Make readVersion lazy value\r\n```\r\n

@HeartSaVioR \r\nPlease take another look.\r\nTest failure was not caused by the PR.

Thanks! Merging to master.

Forgot to call out +1 

need to rebase or re-trigger the GA, thank you  @zjuwangg 

> need to rebase or re-trigger the GA, thank you @zjuwangg\r\n\r\nDone.

Merged to branch-3.5 for Apache Spark 3.5.4.

Merged to master.\r\n\r\nCould you make a backporting PR to branch-3.5, @wangyum ?

branch-3.5: https://github.com/apache/spark/pull/49105/files

migration guide is updated at https://github.com/apache/spark/pull/49252 and https://github.com/apache/spark/pull/49256

cc @yaooqinn @dongjoon-hyun @huaxingao 

@CuiYanxiang  Thanks for your fix, but this issue already fixed by https://github.com/apache/spark/pull/49087

cc @HeartSaVioR @liviazhu-db 

Thank you, @LuciferYang and @HeartSaVioR .

```\r\n[info] - reattach after connection expired *** FAILED *** (222 milliseconds)\r\n[info]   "UNKNOWN" did not contain "INVALID_HANDLE.SESSION_NOT_FOUND" (ReattachableExecuteSuite.scala:74)\r\n[info]   org.scalatest.exceptions.TestFailedException:\r\n[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$5(ReattachableExecuteSuite.scala:74)\r\n[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$5$adapted(ReattachableExecuteSuite.scala:67)\r\n[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withRawBlockingStub(SparkConnectServerTest.scala:206)\r\n[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withRawBlockingStub$(SparkConnectServerTest.scala:201)\r\n[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.withRawBlockingStub(ReattachableExecuteSuite.scala:30)\r\n[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$4(ReattachableExecuteSuite.scala:67)\r\n[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$4$adapted(ReattachableExecuteSuite.scala:60)\r\n[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withClient(SparkConnectServerTest.scala:195)\r\n[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withClient$(SparkConnectServerTest.scala:187)\r\n[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.withClient(ReattachableExecuteSuite.scala:30)\r\n[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$3(ReattachableExecuteSuite.scala:60)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r\n[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r\n[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)\r\n[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)\r\n[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r\n[info]   at scala.collection.immutable.List.foreach(List.scala:431)\r\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r\n[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.Suite.run(Suite.scala:1114)\r\n[info]   at org.scalatest.Suite.run$(Suite.scala:1096)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n[info]   at java.lang.Thread.run(Thread.java:750)\r\njava.lang.IllegalStateException: \r\n        operationId: 594ad2ad-d569-49b2-8daa-c5721972f1b3 with status Analyzed\r\n        is not within statuses List(Finished, Failed, Canceled) for event Closed\r\n        \r\n\tat org.apache.spark.sql.connect.service.ExecuteEventsManager.assertStatus(ExecuteEventsManager.scala:261)\r\n\tat org.apache.spark.sql.connect.service.ExecuteEventsManager.postClosed(ExecuteEventsManager.scala:229)\r\n\tat org.apache.spark.sql.connect.service.ExecuteHolder.$anonfun$close$1(ExecuteHolder.scala:240)\r\n\tat org.apache.spark.sql.connect.service.ExecuteHolder.$anonfun$close$1$adapted(ExecuteHolder.scala:234)\r\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n```\r\n\r\n@changgyoopark-db This test often fails on the branch-3.5. Could you help take a look?\r\n\r\n- this pr: https://github.com/LuciferYang/spark/actions/runs/12193286059/job/34015311895\r\n- branch-3.5 daily test: https://github.com/apache/spark/actions/runs/12178461881/job/33988670611\r\n- branch-3.5 change pipeline:  https://github.com/apache/spark/actions/runs/11933977672/job/33262174405\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/04b1cadf-3d91-4864-8403-5f2aaff089de)\r\n

```\r\n[info] - reattach after connection expired *** FAILED *** (222 milliseconds)\r\n[info]   "UNKNOWN" did not contain "INVALID_HANDLE.SESSION_NOT_FOUND" (ReattachableExecuteSuite.scala:74)\r\n[info]   org.scalatest.exceptions.TestFailedException:\r\n[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n```\r\nThere is a test failure, but it seems to be unrelated to the current pr. I have retriggered the test, and even if this test case fails again, I will merge this PR first to fix the build. @dongjoon-hyun @HeartSaVioR 

> ```\r\n> [info] - reattach after connection expired *** FAILED *** (222 milliseconds)\r\n> [info]   "UNKNOWN" did not contain "INVALID_HANDLE.SESSION_NOT_FOUND" (ReattachableExecuteSuite.scala:74)\r\n> [info]   org.scalatest.exceptions.TestFailedException:\r\n> [info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n> [info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n> [info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n> [info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n> [info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$5(ReattachableExecuteSuite.scala:74)\r\n> [info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$5$adapted(ReattachableExecuteSuite.scala:67)\r\n> [info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withRawBlockingStub(SparkConnectServerTest.scala:206)\r\n> [info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withRawBlockingStub$(SparkConnectServerTest.scala:201)\r\n> [info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.withRawBlockingStub(ReattachableExecuteSuite.scala:30)\r\n> [info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$4(ReattachableExecuteSuite.scala:67)\r\n> [info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$4$adapted(ReattachableExecuteSuite.scala:60)\r\n> [info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withClient(SparkConnectServerTest.scala:195)\r\n> [info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withClient$(SparkConnectServerTest.scala:187)\r\n> [info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.withClient(ReattachableExecuteSuite.scala:30)\r\n> [info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$3(ReattachableExecuteSuite.scala:60)\r\n> [info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n> [info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r\n> [info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r\n> [info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r\n> [info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r\n> [info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)\r\n> [info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)\r\n> [info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n> [info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n> [info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n> [info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\r\n> [info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\r\n> [info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)\r\n> [info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)\r\n> [info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)\r\n> [info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)\r\n> [info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\r\n> [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)\r\n> [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)\r\n> [info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)\r\n> [info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\r\n> [info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r\n> [info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)\r\n> [info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\r\n> [info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r\n> [info]   at scala.collection.immutable.List.foreach(List.scala:431)\r\n> [info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r\n> [info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r\n> [info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r\n> [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\r\n> [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\r\n> [info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\r\n> [info]   at org.scalatest.Suite.run(Suite.scala:1114)\r\n> [info]   at org.scalatest.Suite.run$(Suite.scala:1096)\r\n> [info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\r\n> [info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\r\n> [info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r\n> [info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\r\n> [info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\r\n> [info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)\r\n> [info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n> [info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n> [info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n> [info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)\r\n> [info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n> [info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n> [info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n> [info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n> [info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n> [info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n> [info]   at java.lang.Thread.run(Thread.java:750)\r\n> java.lang.IllegalStateException: \r\n>         operationId: 594ad2ad-d569-49b2-8daa-c5721972f1b3 with status Analyzed\r\n>         is not within statuses List(Finished, Failed, Canceled) for event Closed\r\n>         \r\n> \tat org.apache.spark.sql.connect.service.ExecuteEventsManager.assertStatus(ExecuteEventsManager.scala:261)\r\n> \tat org.apache.spark.sql.connect.service.ExecuteEventsManager.postClosed(ExecuteEventsManager.scala:229)\r\n> \tat org.apache.spark.sql.connect.service.ExecuteHolder.$anonfun$close$1(ExecuteHolder.scala:240)\r\n> \tat org.apache.spark.sql.connect.service.ExecuteHolder.$anonfun$close$1$adapted(ExecuteHolder.scala:234)\r\n> \tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\r\n> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n> \tat java.lang.Thread.run(Thread.java:750)\r\n> ```\r\n> \r\n> @changgyoopark-db This test often fails on the branch-3.5. Could you help take a look?\r\n> \r\n> * this pr: https://github.com/LuciferYang/spark/actions/runs/12193286059/job/34015311895\r\n> * branch-3.5 daily test: https://github.com/apache/spark/actions/runs/12178461881/job/33988670611\r\n> * branch-3.5 change pipeline:  https://github.com/apache/spark/actions/runs/11933977672/job/33262174405\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/1475305/393149546-04b1cadf-3d91-4864-8403-5f2aaff089de.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzM0NzE4MTQsIm5iZiI6MTczMzQ3MTUxNCwicGF0aCI6Ii8xNDc1MzA1LzM5MzE0OTU0Ni0wNGIxY2FkZi0zZDkxLTQ4NjQtODQwMy01ZjJhYWZmMDg5ZGUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTIwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEyMDZUMDc1MTU0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NWNmNzRlYWQ0OGRhYmEwOWMwZTVhZTEzNzg3NzkxMmZmNTczMzRmYmJlOGYwMTEwMzU1MjMwZjM5OGY2MmQzNiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.pHGFDyblZQlrnyeQZY0RSnUnPaC5XSUHzje17pmiub8)\r\n\r\n@changgyoopark-db Does it still require backporting https://github.com/apache/spark/pull/43546 to  resolve the issue, or are there any other workarounds? \r\n\r\nalso cc @juliuszsompolski @hvanhovell @HyukjinKwon and @panbingkun 

FYI: https://issues.apache.org/jira/browse/SPARK-50510

Merged into branch-3.5. Thanks @dongjoon-hyun @HeartSaVioR @panbingkun 

@dongjoon-hyun  Could you review this renaming PR

@zhengruifeng  Could you review this PR

Adding @cloud-fan, @stefankandic, @stevomitric, @uros-db - please take a look when you find some time, thanks!

@stefankandic, @stevomitric, please take a look at the PR as discussed and let me know if you see any issues, thanks!

@cloud-fan, could you please take a look at the PR and help merge it if there are no outstanding issues/concerns? I have addressed the feedback from Stefan and Uros. Thanks!

To confirm: this PR is adding/extending the related DDL commands for specifying table/view level collation, and we will open a followup PR to leverage the table/view collation to determine default collations for input queries of DML commands later?

@cloud-fan, thanks for the review and for providing some great suggestions! I have addressed them all and changed/refactored the code accordingly, I believe the PR should be ready for merge, please let me know if you have any additional comments, thanks a lot!

Created https://issues.apache.org/jira/browse/SPARK-50522 and linked PR to it

thanks, merging to master!

The remaining test failures are not related to this PR.

Thanks! merging to master.

Could you review this documentation PR, @huaxingao ?

Thank you so much, @huaxingao !

Thank you, @HyukjinKwon .

Merged to master for Apache Spark 4.0.0 on February 2025.

Could you review this documentation PR, @gengliangwang ?

Thank you so much, @gengliangwang !

Let me merge this to bring it to Apache Spark 3.5.4 too.

Merged to master/3.5.\r\n\r\ncc @LuciferYang as the release manager of Apache Spark 3.5.4.

@gene-db This PR blocks Variants from being used in repartitioning.

@hvanhovell Can you please go over this PR again? Thanks!

Merging to master

cc @dongjoon-hyun @gengliangwang @cloud-fan here is the PR to enable the SQL pipe syntax configuration by default.

After checking the failures, we need to wrap all failed test cases with the following guard or similar logic, @dtenedor .\r\n```\r\nwithSQLConf(SQLConf.OPERATOR_PIPE_SYNTAX_ENABLED.key -> "true") {\r\n...\r\n}\r\n```

@dongjoon-hyun correct, that query is not working as of today because this subtask is not yet completed [1]. The pending PR [2] should fix it. As mentioned on the review thread for that PR, we probably just need to fix the last remaining test case. If you prefer, we can merge that PR before merging this one so that the documentation reflects the current state, or we can merge this one first. Either way is OK to me, up to you.\r\n\r\n[1] https://issues.apache.org/jira/browse/SPARK-49565\r\n\r\n[2] https://github.com/apache/spark/pull/48724

@dongjoon-hyun do you think we can merge it now? I think this feature is ready.

thanks, merging to master!

@dongjoon-hyun thanks for your reviews!!

Merged to master.

@MaxGekk Can you take a look at this one since you have the context of the previous PR?

+1, LGTM. Merging to master.\r\nThank you, @stefankandic and @stevomitric for review.

@dongjoon-hyun thanks, add the JIRA number

Merged to master/3.5.

@MaxGekk please review this.

@vladimirg-db for context.

thanks, merging to master!

Also cc @LuciferYang 

@andrej-db The newly added test seems to have failed:\r\n\r\n- https://github.com/andrej-db/spark/actions/runs/12177256095/job/33964732193\r\n\r\n![image](https://github.com/user-attachments/assets/fcadde46-b4ea-4e9d-8b0e-e8b93f01d895)

Thank you @andrej-db 

also cc @dongjoon-hyun 

Merged to branch-3.5 for Apache Spark 3.5.4.

Merged to master.

cc @dongjoon-hyun 

also cc @Yikun 

step `Free up disk space`\r\n\r\nbefore:\r\n```\r\nFilesystem      Size  Used Avail Use% Mounted on\r\noverlay          73G   63G   11G  87% /\r\ntmpfs            64M     0   64M   0% /dev\r\nshm              64M     0   64M   0% /dev/shm\r\n/dev/root        73G   63G   11G  87% /__w\r\ntmpfs           3.2G  1.2M  3.2G   1% /run/docker.sock\r\ntmpfs           7.9G     0  7.9G   0% /proc/acpi\r\ntmpfs           7.9G     0  7.9G   0% /proc/scsi\r\ntmpfs           7.9G     0  7.9G   0% /sys/firmware\r\nRemoving large packages\r\nFilesystem      Size  Used Avail Use% Mounted on\r\noverlay          73G   57G   17G  78% /\r\ntmpfs            64M     0   64M   0% /dev\r\nshm              64M     0   64M   0% /dev/shm\r\n/dev/root        73G   57G   17G  78% /__w\r\ntmpfs           3.2G  1.2M  3.2G   1% /run/docker.sock\r\ntmpfs           7.9G     0  7.9G   0% /proc/acpi\r\ntmpfs           7.9G     0  7.9G   0% /proc/scsi\r\ntmpfs           7.9G     0  7.9G   0% /sys/firmware\r\n```\r\n\r\n\r\n\r\nafter:\r\n```\r\nFilesystem      Size  Used Avail Use% Mounted on\r\noverlay          73G   55G   19G  76% /\r\ntmpfs            64M     0   64M   0% /dev\r\nshm              64M     0   64M   0% /dev/shm\r\n/dev/root        73G   55G   19G  76% /__w\r\ntmpfs           3.2G  1.2M  3.2G   1% /run/docker.sock\r\ntmpfs           7.9G     0  7.9G   0% /proc/acpi\r\ntmpfs           7.9G     0  7.9G   0% /proc/scsi\r\ntmpfs           7.9G     0  7.9G   0% /sys/firmware\r\nRemoving large packages\r\nFilesystem      Size  Used Avail Use% Mounted on\r\noverlay          73G   49G   25G  67% /\r\ntmpfs            64M     0   64M   0% /dev\r\nshm              64M     0   64M   0% /dev/shm\r\n/dev/root        73G   49G   25G  67% /__w\r\ntmpfs           3.2G  1.2M  3.2G   1% /run/docker.sock\r\ntmpfs           7.9G     0  7.9G   0% /proc/acpi\r\ntmpfs           7.9G     0  7.9G   0% /proc/scsi\r\ntmpfs           7.9G     0  7.9G   0% /sys/firmware\r\n```\r\n \r\nSo applying a separate image can save ~8G for python tests.

thanks, merged to master, I will monitor the CI status

+1, late LGTM.

I have already created a new PR https://github.com/apache/spark/pull/49159 for this feature (switch `default` HttpClient implementation from `OkHttp` to `Vert.x`).\r\nI will close this PR. cc @dongjoon-hyun @pan3793 

Thanks! Merging to master/3.5.

@liviazhu-db @HeartSaVioR branch-3.5 build failed\r\n\r\n- https://github.com/apache/spark/actions/runs/12191807324/job/34011354774\r\n\r\n![image](https://github.com/user-attachments/assets/59965486-ea41-4689-b460-af9ea06862e4)\r\n\r\n

Try fix: https://github.com/apache/spark/pull/49087

Good job on testing different `NonLeafStatements` with empty compounds!

thanks, merging to master!

cc @siying @brkyvz PTAL

Merging to master.

Can we file a JIRA, and add it into the PR title?

cc @yaooqinn @dongjoon-hyun @cloud-fan 

thanks, merging to master!

Thanks for the review!

+1, LGTM. Merging to master.\r\nThank you, @yaooqinn.

@ueshin I have removed the TODO comments and enabled those tests. The SQL E2E tests only seem to test the Deserializer. Do you know of any real use-cases for the Serializer? The tests in `ArrowEncoderSuite.scala` do test both.

@harshmotw-db let me merge this to master. It is up to you if you want to address the comments in a follow-up.

Merged to master.

@dongjoon-hyun I submitted a follow-up PR #49472 to fix it. Thanks. cc @xinrong-meng 

Thank you!

cc @HyukjinKwon this fixes circular import issue mentioned from https://github.com/apache/spark/pull/48964#discussion_r1868569335

I think probably we need a test to guard `bin/pyspark --remote local`

Thank you for fixing it. I also encountered this issue while verifying a bug.\r\n```shell\r\n(pyspark) âžœ  spark-community git:(master) âœ— ./bin/pyspark --remote "sc://127.0.0.1:8888"\r\nPython 3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:57:01) [Clang 17.0.6 ] on darwin\r\nType "help", "copyright", "credits" or "license" for more information.\r\n/Users/panbingkun/Developer/spark/spark-community/python/pyspark/shell.py:77: UserWarning: Failed to initialize Spark session.\r\n  warnings.warn("Failed to initialize Spark session.")\r\nTraceback (most recent call last):\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/shell.py", line 52, in <module>\r\n    spark = SparkSession.builder.getOrCreate()\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/session.py", line 495, in getOrCreate\r\n    from pyspark.sql.connect.session import SparkSession as RemoteSparkSession\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/connect/session.py", line 53, in <module>\r\n    from pyspark.sql.connect.dataframe import DataFrame\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/connect/dataframe.py", line 75, in <module>\r\n    from pyspark.sql.connect.group import GroupedData\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/connect/group.py", line 43, in <module>\r\n    from pyspark.sql.connect.functions import builtin as F\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/connect/functions/__init__.py", line 22, in <module>\r\n    from pyspark.sql.connect.functions.builtin import *  # noqa: F401,F403\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/connect/functions/builtin.py", line 62, in <module>\r\n    from pyspark.sql.connect.udf import _create_py_udf\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/connect/udf.py", line 38, in <module>\r\n    from pyspark.sql.connect.column import Column\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/connect/column.py", line 110, in <module>\r\n    @with_origin_to_class(["to_plan"])\r\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/errors/utils.py", line 313, in <lambda>\r\n    return lambda cls: with_origin_to_class(cls, ignores)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/errors/utils.py", line 318, in with_origin_to_class\r\n    and is_debugging_enabled()\r\n        ^^^^^^^^^^^^^^^^^^^^^^\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/errors/utils.py", line 56, in is_debugging_enabled\r\n    spark = SparkSession.getActiveSession()\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/utils.py", line 344, in wrapped\r\n    from pyspark.sql.connect.session import SparkSession\r\nImportError: cannot import name \

gonna close this for now

Merged to master and 3.5, thank you @huangxiaopingRD @dongjoon-hyun @zhengruifeng 

Merged to master.

late LGTM.\xa0

+1, LGTM. Merging to master.\r\nThank you, @yaooqinn.

Thank you, @MaxGekk

Merged to master.

Merged to master.

Thank you, @HyukjinKwon !

All tests passed.\r\n- https://github.com/dongjoon-hyun/spark/actions/runs/12145595104/job/33876924609

Merged to master.

Thank you, @HyukjinKwon !

Late +1&LGTM

Please remove draft and add [SQL] component to the title of the PR.

+1, LGTM. Merging to master.\r\nThank you, @jovanm-db and @stefankandic @mihailom-db for review.

yes, @dongjoon-hyun I plan to split them all

BTW, please double-check all CIs after merging this, @zhengruifeng . We frequently broke other CIs.

> BTW, please double-check all CIs after merging this, @zhengruifeng . We frequently broke other CIs.\r\n\r\nSure, I will keep an eye on other CIs.

Thanks @dongjoon-hyun , merged to master

late LGTM.\xa0

late LGTM, Thank @zhengruifeng 

CI passed. cc @HyukjinKwon could you take a look when you find some time?

Merged to master.

Merged into master. Thanks @yaooqinn @panbingkun 

Thank you @dongjoon-hyun ~

Merging to master.

cc @dongjoon-hyun @cloud-fan @gengliangwang this is ready for review at your convenience.

@dongjoon-hyun sounds good, I switched the config change back for this PR and can enable it later in a subsequent PR instead.

BTW, I added `SPARK-50344` to the PR title because this covers `AS` too, @dtenedor . 

Thanks, merging to master

Merged to master.

![image](https://github.com/user-attachments/assets/684a3887-b6dd-4e4b-8dae-d47809fe25af)\r\n\r\nAfter I released rc2, I found that all PRs related to SPARK-50430 in branch-3.5 were reverted. Will this affect the correctness?  @HyukjinKwon

Ya, thank you for reverting this from branch-3.5.\r\n- https://github.com/apache/spark/commit/0fbe292774a856ae49c436e9eb83441e9c38f7de

cc @HyukjinKwon 

Merged to master.

Thank you, @HyukjinKwon !

Merged to master~

@dongjoon-hyun Sure, retriggered [here](https://github.com/ueshin/apache-spark/actions/runs/12150631150/job/33936515254). \r\nBut my GHA can\

Ah, this specific test fails in forked repositories for some reasons.

Merged to master.

thanks, merging to master!

Failed test seems to be unrelated to my changes

@MaxGekk Could you please help with merge?

@MaxGekk thank you for your time. Tests are passed

+1, LGTM. Merging to master.\r\nThank you, @Alexvsalexvsalex and @stefankandic for review.

Thanks for putting in the work for this.\r\n\r\n@cloud-fan initially suggests a class diagram for the new structure. This sounds like a reasonable idea to begin with.

Here is the diagram for the main components (validation mechanics like `ResolutionValidator` is omitted):\r\n\r\n[Analyzer.pdf](https://github.com/user-attachments/files/18005023/Analyzer.pdf)\r\n

@MaxGekk, resolved the comments, thanks!

thanks, merging to master!

cc @HyukjinKwon 

Merged to master.

@ulysses-you @cloud-fan could you pls review this PR?

So we are "guessing" the intention of the user-specified sort and re-ordering the sort keys so that the data clustering meets our requirement. I think this is dangerous. It looks safer just to restore the old behavior: we add a new sort node in `V1Writes` and that sort shouldn\

ping @cloud-fan @ulysses-you 

ping @cloud-fan @ulysses-you

Merged to master.

thanks, merged to master

BTW, how many test PRs are you going to open, @xinrong-meng ?\r\n\r\nDo you think we can do that in a single batch instead of many small ones like this?

Thanks @dongjoon-hyun , certainly, I will handle the rest in a single batch

Hi, \r\nI am curious, what is the status of this PR? Was it abandoned ?

> Hi, I am curious, what is the status of this PR? Was it abandoned ?\r\n\r\nit seems that nobody is interested in reviewing this pr. à²¥_à²¥

hey @viirya mind taking a quick look at this one? Fixes using `ConstantColumnVector`s with the col->row memory leak fix you merged a bit ago

@dongjoon-hyun I found that the previous PR of this one, which is https://github.com/apache/spark/pull/48767, was merged into both master and branch-3.5, but this pr was only merged into master. Then I manually copied the new test cases from this pr and found that they fail in branch-3.5. \r\n\r\nSo should we backport this patch to branch-3.5? I have created a backporting: https://github.com/apache/spark/pull/49131

Oh, is this a regression then, @LuciferYang ?

> Oh, is this a regression then, @LuciferYang ?\r\n\r\nI think so...

@sadikovi @HyukjinKwon please take a look. Thanks!

Merged to master.

Merged to master for Apache Spark 4.0.0 on February 2025.

Thank you @dongjoon-hyun 

I think it is ready for the review now. cc @HyukjinKwon and also cc @xupefei FYI

Merged to master. Thanks @HyukjinKwon and @xupefei for the review!

cc @HyukjinKwon @zhengruifeng @LuciferYang @pan3793 

Merged to master.\r\nThank you,\xa0@HyukjinKwon @zhengruifeng @pan3793 @LuciferYang .

After the pr, the permissions for file `dev/spark-test-image-util/docs/build-docs` are correct.\r\n\r\n```shell\r\n(base) âžœ  spark-trunk git:(master) âœ— ls -al dev/spark-test-image-util/docs/build-docs\r\n-rwxr-xr-x@ 1 panbingkun  staff  2794 Nov 29 18:59 dev/spark-test-image-util/docs/build-docs\r\n```

Merged to master

Merged to master, thank you!

Test first.\r\n\r\nhttps://github.com/netty/netty/security/advisories/GHSA-xq3w-v528-46rv

cc @dongjoon-hyun and @panbingkun 

LGTM, If we can confirm that the netty version generated by sbt compilation is consistent, it may be better, as:\r\nhttps://github.com/apache/spark/pull/48771#issuecomment-2461131711

> LGTM, If we can confirm that the netty version generated by sbt compilation is consistent, it may be better, as: [#48771 (comment)](https://github.com/apache/spark/pull/48771#issuecomment-2461131711)\r\n\r\nchecked, all passed

Merged into master. Thanks @dongjoon-hyun @yaooqinn and @panbingkun 

Merged to master.

Adding @cloud-fan @davidm-db @dejankrak-db @dusantism-db @dtenedor 

Is there any way we can test this change? Just whether the scopes are entered/exited correctly, I assume frames will be tested with stored procedures.

@dusantism-db, scopes are tested in E2E tests and also in new `SqlScriptingExecutionSuite`. I will try to think of some tests to test `frames`. This can also be a follow-up because in the current state of things we have only 1 frame. When we introduce **error handling** mechanism, we will have the opportunity to test frames.

+1, LGTM. All GAs [(job link)](https://github.com/miland-db/spark/actions/runs/12282162552/job/34314043552) passed. Merging to master.\r\nThank you, @miland-db and @davidm-db @dusantism-db @dejankrak-db for review.

@zhengruifeng @HyukjinKwon @xinrong-meng this is the alternative/follow-on to https://github.com/apache/spark/pull/48038 that includes implementing the input side JVM batching in addition to the Python API updates

> what about adding a new test that likely fails with `Table->Table` signature but succeeds with new iterator signature?\r\n\r\nThe only thing that should fail with the Table -> Table is something that goes out of memory

Gentle ping for potential inclusion in 4.0

cc @yaooqinn @cloud-fan @MaxGekk 

Merged to master.\r\nThank you,\xa0@cloud-fan and all!

Late +1

> Late +1\r\n\r\nThank you @yaooqinn !

Hi @MaxGekk can you please review this PR again? Thanks!

LGTM

Merged to master and branch-4.0.

Merged to master.

Thank you @HyukjinKwon !

That makes I/O intensive python udfs much faster! LGTM, thank you!

We can do but I feel like want to add a conf first, and promote this to the udf level parameter if people actually find this useful.

Merged to master.

What if a user wants to increase concurrency specifically for an I/O-intensive UDF? Is it possible to configure the number of threads for each UDF individually, rather than relying on a global setting? Thank you!\r\n\r\nEdit: I just saw @zhengruifeng made similar comment.

We can add a parameter like `useArrow` in UDF but I would like to actually see if this option is commonly used before we go and add another parameter there.

let me convert it to draft first to avoiding merge by mistake

thanks, merged to master

cc @LuciferYang @dongjoon-hyun 

Merged into master. Thanks @panbingkun 

> +1, LGTM.\r\n\r\nThank you, @dongjoon-hyun â¤ï¸

CI passed. Thanks! Merging to master.

Kudos to @LuciferYang providing the better way to fix this!

@cloud-fan Failures seem unrelated, same test is failing in other PRs as well. Can you check please?

cc @dongjoon-hyun 

@HyukjinKwon \r\nTests passed. Please take a look.\r\n\r\nThanks

Merged to master.

@MaxGekk done

@srielau PTAL

thanks, merging to master!

thanks, merging to master!

cc @MaxGekk @cloud-fan 

thanks, merging to master!

cc @cloud-fan @zsxwing 

Merged to master, thank you @cloud-fan 

I did a bit more investigation on this. The original problem that https://github.com/apache/spark/pull/45039 tried to fix is Spark can\

> check if the field name contains special char or not in HiveExternalCatalog#tryGetHiveCompatibleSchema\r\n+1 

Thank you for reverting this before cutting branch, @cloud-fan .

Thanks! Merging to master.

- https://github.com/HyukjinKwon/spark/actions/runs/12045784573/job/33585064883\r\n\r\n```\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o107.setLogLevel.\r\n: java.util.ConcurrentModificationException\r\n\tat java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1784)\r\n\tat java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)\r\n\tat java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276)\r\n\tat java.base/java.util.WeakHashMap$ValueSpliterator.forEachRemaining(WeakHashMap.java:1217)\r\n\tat java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)\r\n\tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)\r\n\tat java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)\r\n\tat java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)\r\n\tat java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\r\n\tat java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)\r\n\tat org.apache.logging.log4j.core.LoggerContext.updateLoggers(LoggerContext.java:776)\r\n\tat org.apache.logging.log4j.core.LoggerContext.updateLoggers(LoggerContext.java:766)\r\n\tat org.apache.spark.util.Utils$.setLogLevel(Utils.scala:2322)\r\n\tat org.apache.spark.util.Utils$.setLogLevelIfNeeded(Utils.scala:2331)\r\n\tat org.apache.spark.SparkContext.setLogLevel(SparkContext.scala:400)\r\n\tat org.apache.spark.api.java.JavaSparkContext.setLogLevel(JavaSparkContext.scala:675)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n```\r\n\r\nseems the problem still exists. There may  another thread traversing `org.apache.logging.log4j.core.util.internal.InternalLoggerRegistry#loggerRefByNameByMessageFactory`, but its entry point is not `Utils.setLogLevel`.\r\n\r\n

yeah lemme take a look\r\n

will make a PR again

cc @MaxGekk @cloud-fan 

thanks, merging to master!

cc @viirya 

If you want this in Apache Spark 3.5.x, feel free to open a backport, @pan3793 .

@dongjoon-hyun thanks, I opened https://github.com/apache/spark/pull/49044 for 3.5

cc @yaooqinn @wangyum 

What is the label for this PR?\r\n\r\n

@LuciferYang I\

Maybe `[BUILD]` is ok too

Thanks @cloud-fan, also cc @MaxGekk.

thanks, merging to master!

Merged to master.

`cloneProperties` was using `forEach` right ? The stacetrace does not seem to match the code ?

There was a bit of code change here. It will be matched after https://github.com/apache/spark/pull/49036 and https://github.com/apache/spark/pull/48993 ..

Thank you for reverting this from `branch-3.5`.\r\n- https://github.com/apache/spark/commit/f7c48fe24fd8fb5f7f3eb9b3bec2659d99e4307c

This should be the last PR

Merged to master.

![Uploading image.pngâ€¦]()\r\n\r\nAll Passed

Thanks @dongjoon-hyun 

Merged to master.\r\nThank you,\xa0@LuciferYang !

Thanks @panbingkun 

merged to master\r\n

By using environment variables `SKIP_LOCAL_M2`, the logic of `skip maven local resolver` is limited to `dev/mima` only, and the logic executed `on GA` and `on local` is already consistent.\r\n\r\nBased on this discussion https://github.com/coursier/coursier/issues/2942#issuecomment-2145674637, `coursier` (sbt relies on it) will not solve this problem in the short term.\r\n\r\n@LuciferYang @dongjoon-hyun Can we upgrade `sbt` through this workaround?

The cost time of test `dev/mima`\r\n\r\n- Before\r\n  https://github.com/panbingkun/spark/actions/runs/12045769413/job/33590035837\r\n  <img width="994" alt="image" src="https://github.com/user-attachments/assets/e9fc8ecc-6f40-4e0b-ad7e-fe1aa89f2df8">\r\n\r\n  https://github.com/panbingkun/spark/actions/runs/12060840876/job/33632313601\r\n  <img width="991" alt="image" src="https://github.com/user-attachments/assets/e5465409-c1ba-4fba-80d9-2da6bfcefedf">\r\n\r\n  https://github.com/panbingkun/spark/actions/runs/12022114883/job/33513845045\r\n  <img width="1007" alt="image" src="https://github.com/user-attachments/assets/77cbe1e3-9bc1-485c-b0b3-b789cbcf72ad">\r\n\r\n- After\r\n  https://github.com/panbingkun/spark/actions/runs/12060175806/job/33630180762\r\n  <img width="998" alt="image" src="https://github.com/user-attachments/assets/a56269bc-6777-449b-accd-a2e037abbdde">\r\n\r\n  https://github.com/panbingkun/spark/actions/runs/12049152851/job/33598573658\r\n  <img width="990" alt="image" src="https://github.com/user-attachments/assets/58e82ba9-f6b8-47ee-8f5a-16a4fa3b19e2">\r\n\r\n  https://github.com/panbingkun/spark/actions/runs/12049124750/job/33597496402\r\n  <img width="998" alt="image" src="https://github.com/user-attachments/assets/7541b89f-72f3-44a7-984d-2f7bbe502477">\r\n\r\n

From the stage at which the issue arises, perhaps it will no longer be a problem when we start comparing Spark 4.1 to version 4.0.

cc @HyukjinKwon @cloud-fan this is an alternative (and better) approach to https://github.com/apache/spark/pull/48888 to avoid necessary Python data source lookup. And it eliminates the need for users to change the static conf.

Merged to master.

Some issue with the Jackson serialization:\r\n\r\n```\r\nCause: java.lang.RuntimeException: shaded.parquet.com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class org.apache.parquet.schema.LogicalTypeAnnotation$ListLogicalTypeAnnotation and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.apache.parquet.hadoop.metadata.ParquetMetadata["fileMetaData"]->org.apache.parquet.hadoop.metadata.FileMetaData["schema"]->org.apache.parquet.schema.MessageType["fields"]->java.util.ArrayList[0]->org.apache.parquet.schema.GroupType["logicalTypeAnnotation"])\r\n```\r\n\r\nDigging deeper

Merged to master for Apache Spark 4.0.0 on February 2025.

@dongjoon-hyun thanks for the prompt follow up ðŸ™Œ 

Merged to master.

cc @zhengruifeng  @LuciferYang @dongjoon-hyun @HyukjinKwon

> Please make it sure the image size difference is reasonable because `Dockerfile.apt-get` will have a side-effect which increases the intermediate image layer size.\r\n> \r\n> Previously, we have a trick pattern to remove the intermediate image layer size, @panbingkun . After this PR, `Dockerfile.apt-get` removes that guard. So, we need to check the size difference manually to be safe, @panbingkun .\r\n\r\nOkay, let me check it again.

Merged to master.

This may have some negative impacts, as I have noticed some new flaky tests in recent GAs that might be related to this:\r\n\r\n- https://github.com/apache/spark/actions/runs/12044265847/job/33581052605\r\n- https://github.com/apache/spark/actions/runs/12043870433/job/33579994907\r\n- https://github.com/LuciferYang/spark/actions/runs/12034481725/job/33551257684\r\n- https://github.com/LuciferYang/spark/actions/runs/12034633745/job/33554178490\r\n\r\n\r\n```\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o20.setLogLevel.\r\n4768: java.util.ConcurrentModificationException\r\n4769\tat java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1784)\r\n4770\tat java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)\r\n4771\tat java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276)\r\n4772\tat java.base/java.util.WeakHashMap$ValueSpliterator.forEachRemaining(WeakHashMap.java:1217)\r\n4773\tat java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)\r\n4774\tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)\r\n4775\tat java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)\r\n4776\tat java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)\r\n4777\tat java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\r\n4778\tat java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)\r\n4779\tat org.apache.logging.log4j.core.LoggerContext.updateLoggers(LoggerContext.java:776)\r\n4780\tat org.apache.logging.log4j.core.LoggerContext.updateLoggers(LoggerContext.java:766)\r\n4781\tat org.apache.spark.util.Utils$.setLogLevel(Utils.scala:2322)\r\n4782\tat org.apache.spark.util.Utils$.setLogLevelIfNeeded(Utils.scala:2331)\r\n4783\tat org.apache.spark.SparkContext.setLogLevel(SparkContext.scala:400)\r\n4784\tat org.apache.spark.api.java.JavaSparkContext.setLogLevel(JavaSparkContext.scala:675)\r\n4785\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n4786\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n4787\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n4788\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n4789\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n4790\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n4791\tat py4j.Gateway.invoke(Gateway.java:282)\r\n4792\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n4793\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n4794\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n4795\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n4796\tat java.base/java.lang.Thread.run(Thread.java:840) \r\n```\r\n\r\nCould this pr potentially trigger the aforementioned concurrent exceptions? Do you have time to confirm it? Thanks @HyukjinKwon \r\n\r\nI have created https://issues.apache.org/jira/browse/SPARK-50434

let me make a fix

let me revert this - I think it becomes more flaky as @LuciferYang said. I am sorry for back and forth. I am taking a look for a proper fix

Thank you, @yaooqinn .

And, WDYT as the main author of this feature, @MaxGekk ?

+1, LGTM. Merging to master.\r\nThank you, @dongjoon-hyun and @yaooqinn @HyukjinKwon @zhengruifeng for review.

Thank you all!

A comment for the PR description https://github.com/apache/spark/pull/48827 doesnâ€™t seem to disable â€œDataFrameQueryContextâ€ if I understand correctly :)

Merged to master. Thanks @HyukjinKwon @xinrong-meng for the review.

@tgravescs Would you help to review this commit?

@tgravescs I just updated the commit as the comments. Please help review when you have free time.

what all testing have you done with this?  It would be nice to have a integration test with it but I know this would be difficult.\r\n\r\n

> +1 looks fine to me.\r\n\r\nThanks! Would you help merge it? \r\nI think this also should be included in spark 3.5.4

Due to the Structured Log change, this patch is not applicable to `branch-3.5`.\r\n\r\nPlease make a backporting PR to branch-3.5, @zjuwangg .

BTW, I added you to Apache Spark Contributor group in JIRA and assigned SPARK-50421 to you, @zjuwangg .\r\n\r\nWelcome to the Apache Spark community and thank you again.

cc @srielau @gengliangwang @cloud-fan 

After investigating this more, I find that it is not actually possible to make this change without causing unacceptable ambiguity in the grammar. Closing this.

cc @dongjoon-hyun 

Just the general experience that limiting the number of items in a directory is good practice when traversing directories is needed. Depending on the particular storage, deleting the shuffle data might require traversal.

> Just the general experience that limiting the number of items in a directory is good practice when traversing directories is needed. Depending on the particular storage, deleting the shuffle data might require traversal.\r\n\r\nIs that all justifications you can provide us?

I did some measurements: with two shuffles and 10.000 partitions I end up having 22,458 folders with 44,912 files and 1.41 GB. With ten sub-directories I get 22 folders with 43,775 files and 1.41 GB. Removing the former takes more than twice as long as removing the latter.

Merged to master.

Thank you, @HyukjinKwon 

A couple of small comments.

> > WIP: to be done in this PR after we agree on the interface implementation.\r\n> \r\n> Do you mean the interface impl in the changes at this moment or are there other decisions?\r\n\r\nI was considering testing APIs that have a new implementation, such as `tables()`, but now that is no longer needed - all APIs now use the existing implementation in SparkSession.

Merged to master.

cc @panbingkun , too.

I converted this to `Draft` back in order to prevent accidental merging (without JIRA ID), @LuciferYang . Feel free to make it back to normal PR after adding JIRA ID.

Could you please help review it again if you have time @dongjoon-hyun Thanks ~\r\n

Thanks @dongjoon-hyun and @zhengruifeng ~

thanks, merged to master

Merged to master.

@MaxGekk @cloud-fan please review this. This is an important fix for us, as we have queries failing. One example of the failed query is `set;`

How do they fail?

Updated with text from failure.

Scoping down the PR to only fixing the `;` problem, as remainder function fix is pretty involved to fix.

thanks, merging to master!

Adding @cloud-fan @dejankrak-db @davidm-db @dusantism-db @MaxGekk

thanks, merging to master!

Thank you @dtenedor \r\n\r\nMerged to master, thank you all!

Thank you, @HyukjinKwon and @zhengruifeng . \r\nMerged to master.

Although I saw the following PR, can we use the latest version, @panbingkun ?\r\n- #48897

Could you review this when you have some time, @yaooqinn ?

Thank you, @HyukjinKwon !

late LGTM.

Thank you, @panbingkun .

Merged to master. Thank you, @panbingkun .

cc @cloud-fan and @viirya 

Thank you, @viirya 

Merged to master to recover non-ANSI CI.\r\n- https://github.com/apache/spark/actions/workflows/build_non_ansi.yml

For the record, `catalyst` test pipeline is recovered successfully.\r\n- https://github.com/apache/spark/actions/runs/12001580128/job/33452303617

Thank you @dongjoon-hyun 

thank you @dongjoon-hyun !

+1, LGTM. Merging to master.\r\nThank you, @camilesing.

Thanks @raboof for pointing that out. Duly noted and updated the PR description accordingly.

Seems like some test failures are related:\r\n\r\n```\r\n[info] - SPARK-42823: multipart identifier support for specify database by --database option *** FAILED *** (2 minutes)\r\n[info]   =======================\r\n[info]   CliSuite failure output\r\n[info]   =======================\r\n[info]   Spark SQL CLI command line: ../../bin/spark-sql --master local --driver-java-options -Dderby.system.durability=test --conf spark.ui.enabled=false --conf spark.sql.legacy.emptyCurrentDBInCli=true --hiveconf javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/home/runner/work/spark/spark/target/tmp/spark-66f5d2b1-4fe4-4ab2-8eeb-7f2772bbb2a4;create=true --hiveconf hive.exec.scratchdir=/home/runner/work/spark/spark/target/tmp/spark-4369aa53-68ab-4ebf-9750-8f761102df34 --hiveconf conf1=conftest --hiveconf conf2=1 --hiveconf hive.metastore.warehouse.dir=/home/runner/work/spark/spark/target/tmp/spark-436aa139-c352-4a6d-a44e-729fd81ff01a --conf spark.sql.catalog.testcat=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog --conf spark.sql.catalog.testcat.driver=org.apache.derby.iapi.jdbc.AutoloadedDriver --conf spark.sql.catalog.testcat.url=jdbc:derby:memory:testcat;create=true --conf spark.sql.catalogImplementation=in-memory --database testcat.SYS\r\n[info]   Exception: java.util.concurrent.TimeoutException: Future timed out after [2 minutes]\r\n[info]   Failed to capture next expected output "spark-sql> SELECT CURRENT_CATALOG();" within 2 minutes.\r\n[info]   \r\n[info]   WARNING: Using incubator modules: jdk.incubator.vector\r\n[info]   Setting default log level to "WARN".\r\n[info]   To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n[info]   Exception in thread "main" java.sql.SQLException: Database \

cc @mridulm and @srowen FYI

I think this approach is not ideal.\r\n\r\nThe JVM allows intrinsically the execution of code if one is able to specify command line options. For example, as described in  https://docs.oracle.com/en/java/javase/17/docs/specs/man/java.html, `-XX:OnError=string` or `-XX:OnOutOfMemoryError=string` will grant you command execution without having to add Shell metacharacters, or anything messing with the class path will allow easy code execution via static constructors. All of this to say that the options themselves are the issue, and no filtering of their content will likely prevent arbitrary code execution.\r\n\r\nMy overall recommendation is to disallow said options in your deployment.

Hm, I somewhat agree in a way that this option actually has to be disallowed if the security concern is there in JDK itself.

Let me involve some more ppl who might be interested in here - cc @tgravescs @mridulm @JoshRosen 

@tgravescs There\

> spark.executor.extraJavaOptions could potentially allow the execution of unintended shell commands\r\n\r\nI think the assumption violates the original intention of JVM options, it is clearly documented in [Oracle Java 8 docs](https://docs.oracle.com/javase/8/docs/technotes/guides/troubleshoot/clopts001.html)\r\n\r\n> The script or command is specified using the `-XX:OnError=string` command-line option, where `string` is a single command, or a list of commands separated by semicolons.\r\n\r\nwith an example\r\n\r\n> ```\r\n> java -XX:OnError="cat hs_err_pid%p.log | mail support@acme.com" MyApp\r\n> ```\r\nit won\

I agree with @tgravescs.\r\nAnd the example that @pan3793 provided is an interesting usecase of using this functionality.

cc @cloud-fan @gengliangwang here is the `|> SET` operator :)

thanks, merging to master!

@HyukjinKwon @allisonwang-db  shall we implement `df.withColumn` with this way?

+1, LGTM. Merging to master.\r\nThank you, @dongjoon-hyun.

Thank you, @MaxGekk .

All tests passed.

Could you review this PR, @panbingkun ?

Could you review this PR when you have some time, @viirya ?

Do we need to remove `SPARK_MASTER_IP` together? as follows:\r\nhttps://github.com/apache/spark/blob/d9757b5653b9b221dcd571354435e5be41eb8861/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala#L37-L40

You are right. Thank you for review, @panbingkun . I addressed the comments.

Thank you for reviewing and approval, @yaooqinn .

Merged to master.\r\n\r\nThank you again, @panbingkun and @yaooqinn .

Thank you, Liang-Chi.

Not sure what "workflow run detection" is, or how to correct it.\r\nThe output of `git remote -vv` is:\r\n```\r\n# git remote -vv\r\norigin  https://github.com/philwalk/spark.git (fetch)\r\norigin  https://github.com/philwalk/spark.git (push)\r\n```

I enabled Build and Test on my forked repo.

The touched code seems to be borrowed from Apache Hive, would you like to also report the issue to the Hive community?

Got it. I approved it a few minutes ago, @philwalk .\r\n\r\n![Screenshot 2024-11-25 at 09 39 33](https://github.com/user-attachments/assets/132c2739-7301-445e-abb4-0ca5a256aa27)\r\n

[jira bug report](https://issues.apache.org/jira/browse/SPARK-50416)

> The touched code seems to be borrowed from Apache Hive, would you like to also report the issue to the Hive community?\r\n~Not at this time ..~\r\nIt seems that hive needs work to be compatible with `cygwin` or `msys2` bash shell environments.   There are a number of problems and I\

Is this waiting on me to do something? 

@cloud-fan please take a look when you get the chance.\r\n\r\nYou raised some valid concerns in the last PR about structs and maps so I think I was able to figure out a nice way to have it work for all data types :)

@MaxGekk can we go ahead and merge the change?

+1, LGTM. Merging to master.\r\nThank you, @stefankandic and @uros-db @stevomitric @dejankrak-db @vladanvasi-db for review.

+1, LGTM. Merging to master.\r\nThank you, @zhengruifeng.

I believe the cause was https://github.com/apache/spark/pull/48120. This is a followup for it.

Let me merge this one in any event, and see if the CI is fixed

Merged to master.

Got it. Thank you, @xupefei and @HyukjinKwon .

@HyukjinKwon @yaooqinn @allisonwang-db perhaps you could reopen and review? :heart: 

Merged to master.

gonna close this for now

Merged to master.

cc @dongjoon-hyun and @yaooqinn 

Thanks @dongjoon-hyun 

Thanks @dongjoon-hyun  again

Merged to master~

late lgtm

cc - @HeartSaVioR - PTAL, thx !

https://github.com/anishshri-db/spark/actions/runs/11964792826/job/33357779164\r\nI guess we have PySpark tests being impacted by this?

@HeartSaVioR - tried to run this locally and this fails even with/without my change. Let me merge back from master once

@HeartSaVioR - verified on master and I see the same assert locally as well. So don\

> @HeartSaVioR - verified on master and I see the same assert locally as well. So don\

Thanks! Merging to master.

thanks, merged to master

Could you review this PR, @panbingkun ?

LGTM, +1

Thank you so much, @panbingkun .

All tests passed already in the first commit, [6c9bdd1](https://github.com/apache/spark/pull/48924/commits/6c9bdd12a80cfe00962d30faa2581e71b4cc7fd2).\r\n\r\nI just resolved the conflicts.\r\n\r\nSince this is a one-liner PR, could you try to merge this PR via the following script, @panbingkun ?\r\n- https://github.com/apache/spark/blob/master/dev/merge_spark_pr.py

> All tests passed already in the first commit, [6c9bdd1](https://github.com/apache/spark/pull/48924/commits/6c9bdd12a80cfe00962d30faa2581e71b4cc7fd2).\r\n> \r\n> I just resolved the conflicts.\r\n> \r\n> Since this is a one-liner PR, could you try to merge this PR via the following script, @panbingkun ?\r\n> \r\n> * https://github.com/apache/spark/blob/master/dev/merge_spark_pr.py\r\n\r\nOkay, let me try, Thank you very much for providing me with such detailed guidance, â¤ï¸!

Merged to master.\r\nThank you, @dongjoon-hyun and @yaooqinn.

Thank you again to @dongjoon-hyun for guiding me step by step towards progress â¤ï¸ !

Thank you, @viirya !

BTW, after merging #48921, I need to rebase this PR to resolve the conflicts.

Rebased to master to resolve conflicts~

Merged to master for Apache Spark 4.0.0.

Merged to master.

@grundprinzip - Does this work replace the WIP you had in #45340?

The newly added UT passed already in the CIs.\r\n- https://github.com/dongjoon-hyun/spark/actions/runs/11958476555/job/33338110615\r\n```\r\n[info] - SPARK-50381: Support spark.master.rest.maxThreads (15 milliseconds)\r\n```

Thank you, @viirya !

> Looks good but mind listing up affected API in the PR description?\r\n\r\nDone!

Merged to master.

+1, LGTM. Merging to master.\r\nThank you, @yaooqinn.

Thank you, @MaxGekk 

cc @yaooqinn @viirya 

Unfortunately, it seems that the test case assumes ANSI setting only and broke non-ANSI CI. I made a follow-up, @cloud-fan and @viirya .\r\n\r\n- #48943

+1, LGTM. Merging to master.\r\nThank you, @camilesing.

cc @cloud-fan 

+1, LGTM. Merging to master.\r\nThank you, @wangyum.

cc - @HeartSaVioR - PTAL, thx !

cc @yaooqinn @LuciferYang 

thanks for the review, merging to master!

Merged to master.

late LGTM, thanks @zhengruifeng 

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

@gene-db @cloud-fan Can you please look at this? Thanks!

@dtenedor This PR prohibits `select distinct`, and `GROUP BY` was [already disabled](https://github.com/apache/spark/blob/ad46db4ef671d8829dfffba2780ba0f6b4f4e43d/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ExprUtils.scala#L203). It was believe that this code path would also disable other operations which require grouping but apparently not.

thanks, merging to master!

BTW, FYI, I added you to the Apache Spark `Committers` group in ASF JIRA system in a few minutes ago.\r\n\r\n**BEFORE**\r\n![Screenshot 2024-11-20 at 08 36 52](https://github.com/user-attachments/assets/ea6bcdaa-61f7-4a6c-9d47-b76be8482a9f)\r\n\r\n**AFTER**\r\n![Screenshot 2024-11-20 at 08 37 02](https://github.com/user-attachments/assets/ad082ca7-9d84-4a26-af1e-4bb116692946)

> BTW, FYI, I added you to the Apache Spark `Committers` group in ASF JIRA system in a few minutes ago.\r\n> \r\n> **BEFORE** ![Screenshot 2024-11-20 at 08 36 52](https://private-user-images.githubusercontent.com/9700541/388184657-ea6bcdaa-61f7-4a6c-9d47-b76be8482a9f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIzMTE4OTIsIm5iZiI6MTczMjMxMTU5MiwicGF0aCI6Ii85NzAwNTQxLzM4ODE4NDY1Ny1lYTZiY2RhYS02MWY3LTRhNmMtOWQ0Ny1iNzZiZTg0ODJhOWYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMjEzOTUyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Yjk2OWFhNjIxYmU4MTg5OWUwMjhmZTRiYmNkYjA3MTc3NWVmZjcyNjZkOWFiZmU3ZDYwMDZkMWQwNDhjMTJhYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.bkLUz7BHIEfWIy3JId1htlotbhLDNuSUQXRH6wwKetE)\r\n> \r\n> **AFTER** ![Screenshot 2024-11-20 at 08 37 02](https://private-user-images.githubusercontent.com/9700541/388184693-ad082ca7-9d84-4a26-af1e-4bb116692946.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIzMTE4OTIsIm5iZiI6MTczMjMxMTU5MiwicGF0aCI6Ii85NzAwNTQxLzM4ODE4NDY5My1hZDA4MmNhNy05ZDg0LTRhMjYtYWYxZS00YmIxMTY2OTI5NDYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMjJUMjEzOTUyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YjY0N2IzN2E2NjI2YjBmNDQwYjY1ODAzM2FlNDg3YjY5OTcxN2Q2MzM4YTk2ZDY0N2NhOTBiNjNmZjM0M2Y0NiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.ROOANAej7t8DdAGpG3gF3fk7m5mJXgYGzcfLPKz7Aag)\r\n\r\nOh, great, thank you! â¤ï¸

cc @cloud-fan @MaxGekk 

@panbingkun can you rebase it?

> @panbingkun can you rebase it?\r\n\r\nSure, updated, thanks!

- For the following cases,\r\n```scala\r\n    withTempView("t") {\r\n      val json = """{"a":1, "b":2, "c":3}"""\r\n      val df = Seq((json, "a", "b", "c")).toDF("json", "c1", "c2", "c3")\r\n      df.createOrReplaceTempView("t")\r\n\r\n      // The field names some foldable, some non-foldable.\r\n      val df3 = sql("SELECT json_tuple(json, \

@panbingkun can you answer https://github.com/apache/spark/pull/48908#discussion_r1891103621 ?

thanks, merging to master!

Thanks for the review! â¤ï¸

cc @itholic too

Merged to master.

Related issue [SPARK-50364](https://issues.apache.org/jira/browse/SPARK-50364)

+1, LGTM. All GAs passed. Merging to master.\r\nThank you, @karim-ramadan.

@karim-ramadan Congratulations with your first contribution to Apache Spark!

@MaxGekk thanks for the fast response and review

Merged to master.

Merged to master.

thanks, merged to master

Thanks @zhengruifeng 

Thank you, @LuciferYang .

thanks, merged to master

Thank you, @zhengruifeng .

Merged to master and 3.5

LGTM thank you!

Merged to master.

Merged to master.

cc @cloud-fan  @MaxGekk @dongjoon-hyun 

Hi @HyukjinKwon @MaxGekk \r\n\r\nThis is a minor patch.\r\nWould you please help review and merge it?\r\n\r\nThanks!\r\n\r\n

+1, LGTM. Merging to master.\r\nThank you, @mihailoale-db and @dtenedor for review.

@cloud-fan @MaxGekk could you review this change

I believe the failed test is not related to the changes:\r\n```\r\n[info] OracleIntegrationSuite:\r\n[info] org.apache.spark.sql.jdbc.v2.OracleIntegrationSuite *** ABORTED *** (10 minutes, 19 seconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 590 times over 10.014576434266667 minutes. Last failure message: ORA-12541: Cannot connect. No listener at host 10.1.0.4 port 40219. (CONNECTION_ID=j6h8+/l4RAezQ6fJKgL7/Q==)\r\n```\r\n\r\n+1, LGTM. Merging to master.\r\nThank you, @mihailom-db and @cloud-fan @srielau for review.

Merged to master.

Merged to master.

cc @dongjoon-hyun @LuciferYang 

Merged into master. Thanks @pan3793 and @HyukjinKwon 

> cc @gengliangwang\r\n\r\nThanks @MaxGekk !

This PR no longer needed (replaced by https://github.com/apache/spark/pull/48971)

my concern is that the pandas conversion is out of control, what about introducing a similar check in `numpy -> pyarrow`?

Merged into master for Spark 4.0. Thanks @yaooqinn @HyukjinKwon and @panbingkun 

LGTM

Merged to master, thank you @panbingkun @bjornjorgensen @HyukjinKwon 

> UDS is recommended for same-host processes communication\r\n\r\nShould we better change all usage in Python worker communication instead of this alone?

Is this a Unix-only feature? For Windows users, will it crash or are there workarounds available? 

@mihailom-db

@jovanm-db update title with the component [SQL]

Please merge in new master, so we do not have conflicts

thanks, merging to master!

@cloud-fan @yaooqinn I think this should handle nested LazyTries (as exhibited by the chain of plans in QueryExecutions) nicely.

```\r\nStarting test(python3.11): pyspark.ml.tests.connect.test_parity_torch_data_loader (temp output: /__w/apache-spark/apache-spark/python/target/33d857eb-2e53-4dd3-8745-bcf0ed55f7ef/python3.11__pyspark.ml.tests.connect.test_parity_torch_data_loader__g3_cd4_m.log)\r\nError: The operation was canceled.\r\n```\r\nThis test has been hanging on every single PR of mine in the last half a year...

Hi @juliuszsompolski , can you put a real example with DataFrame in the PR description to show the before/after error message?

I ran a test locally, the stack traces still look nasty to me\r\n```\r\njava.lang.UnsupportedOperationException\r\n\tat org.apache.hadoop.fs.http.AbstractHttpFileSystem.listStatus(AbstractHttpFileSystem.java:95)\r\n\tat org.apache.hadoop.fs.http.HttpsFileSystem.listStatus(HttpsFileSystem.java:23)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\r\n\tat scala.collection.immutable.List.map(List.scala:247)\r\n\tat scala.collection.immutable.List.map(List.scala:79)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:134)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:554)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:82)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:77)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:86)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:135)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:108)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:135)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:634)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:134)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:149)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:144)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:330)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:326)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:144)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:106)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1377)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1438)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:111)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:267)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:138)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:129)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:477)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:468)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:483)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:493)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:69)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:462)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1(SparkSQLCLIDriver.scala:580)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1$adapted(SparkSQLCLIDriver.scala:574)\r\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\r\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processLine(SparkSQLCLIDriver.scala:574)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:288)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1032)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1137)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1146)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.hadoop.fs.http.AbstractHttpFileSystem.listStatus(AbstractHttpFileSystem.java:95)\r\n\t\tat org.apache.hadoop.fs.http.HttpsFileSystem.listStatus(HttpsFileSystem.java:23)\r\n\t\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\r\n\t\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\r\n\t\tat scala.collection.immutable.List.map(List.scala:247)\r\n\t\tat scala.collection.immutable.List.map(List.scala:79)\r\n\t\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\r\n\t\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\r\n\t\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:178)\r\n\t\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:134)\r\n\t\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\r\n\t\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\r\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:554)\r\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\r\n\t\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:82)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:77)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:75)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:86)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:135)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:108)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:107)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:135)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:634)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:134)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:149)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:144)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:330)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:326)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:144)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:106)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1377)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 33 more\r\njava.lang.UnsupportedOperationException\r\n\tat org.apache.hadoop.fs.http.AbstractHttpFileSystem.listStatus(AbstractHttpFileSystem.java:95)\r\n\tat org.apache.hadoop.fs.http.HttpsFileSystem.listStatus(HttpsFileSystem.java:23)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\r\n\tat scala.collection.immutable.List.map(List.scala:247)\r\n\tat scala.collection.immutable.List.map(List.scala:79)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:134)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:554)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:82)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:77)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:86)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:135)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:108)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:135)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:634)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:134)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:149)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:144)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:330)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:326)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:144)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:106)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1377)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1438)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:111)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:267)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:138)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:129)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:477)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:468)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:483)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:493)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:69)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:462)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1(SparkSQLCLIDriver.scala:580)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1$adapted(SparkSQLCLIDriver.scala:574)\r\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\r\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processLine(SparkSQLCLIDriver.scala:574)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:288)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1032)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1137)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1146)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.hadoop.fs.http.AbstractHttpFileSystem.listStatus(AbstractHttpFileSystem.java:95)\r\n\t\tat org.apache.hadoop.fs.http.HttpsFileSystem.listStatus(HttpsFileSystem.java:23)\r\n\t\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\r\n\t\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\r\n\t\tat scala.collection.immutable.List.map(List.scala:247)\r\n\t\tat scala.collection.immutable.List.map(List.scala:79)\r\n\t\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\r\n\t\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\r\n\t\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:178)\r\n\t\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:134)\r\n\t\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\r\n\t\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\r\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:554)\r\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\r\n\t\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:82)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:77)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:75)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:86)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:135)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:108)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:107)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:135)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:634)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:134)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:149)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:144)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:330)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:326)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:144)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:106)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1377)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 33 more\r\n\r\norg.apache.spark.sql.execution.QueryExecutionException: java.lang.UnsupportedOperationException\r\n\tat org.apache.hadoop.fs.http.AbstractHttpFileSystem.listStatus(AbstractHttpFileSystem.java:95)\r\n\tat org.apache.hadoop.fs.http.HttpsFileSystem.listStatus(HttpsFileSystem.java:23)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\r\n\tat scala.collection.immutable.List.map(List.scala:247)\r\n\tat scala.collection.immutable.List.map(List.scala:79)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:134)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:554)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:82)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:77)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:86)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:135)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:108)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:135)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:634)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:134)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:149)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:144)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:330)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:326)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:144)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:106)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1377)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1438)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:111)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:267)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:138)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:129)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:477)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:468)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:483)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:493)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:69)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:462)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1(SparkSQLCLIDriver.scala:580)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1$adapted(SparkSQLCLIDriver.scala:574)\r\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\r\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processLine(SparkSQLCLIDriver.scala:574)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:288)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1032)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1137)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1146)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.hadoop.fs.http.AbstractHttpFileSystem.listStatus(AbstractHttpFileSystem.java:95)\r\n\t\tat org.apache.hadoop.fs.http.HttpsFileSystem.listStatus(HttpsFileSystem.java:23)\r\n\t\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\r\n\t\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\r\n\t\tat scala.collection.immutable.List.map(List.scala:247)\r\n\t\tat scala.collection.immutable.List.map(List.scala:79)\r\n\t\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\r\n\t\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\r\n\t\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:178)\r\n\t\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:134)\r\n\t\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\r\n\t\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\r\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:554)\r\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\r\n\t\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:82)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:77)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:75)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:86)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:135)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:108)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:107)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:135)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:634)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:134)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:149)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:144)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:330)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:326)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:144)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:106)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1377)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 33 more\r\n\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:88)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:462)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1(SparkSQLCLIDriver.scala:580)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1$adapted(SparkSQLCLIDriver.scala:574)\r\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\r\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processLine(SparkSQLCLIDriver.scala:574)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:288)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1032)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1137)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1146)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n```

Hmm, the exception looks like expected to me (one exception of the callsite that retrieved the Try, and one suppressed that shows the stacktrace of when the Try was originally calculated - in this case the same, because the Try is retrieved directly after calculation, but there can be situations when the Try is retrieved later, and having the original stacktrace is needed for debugging).\r\nBut, for some reason, the whole exception is printed three times.

Using Spark shell, it looks like expected to me:\r\n```\r\nscala> sql("create table foo using foo")\r\norg.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: foo. Make sure the provider name is correct and the package is properly registered and compatible with your Spark version. SQLSTATE: 42K02\r\n  at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:722)\r\n  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:672)\r\n  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:732)\r\n  at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableProvider(DataSourceV2Utils.scala:163)\r\n  at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:670)\r\n  at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:176)\r\n  at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:54)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\r\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:386)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:111)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:110)\r\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n  at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:54)\r\n  at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:48)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n  at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n  at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n  at scala.collection.immutable.List.foldLeft(List.scala:79)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n  at scala.collection.immutable.List.foreach(List.scala:334)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:234)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:230)\r\n  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:184)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:230)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:199)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:222)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:393)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:221)\r\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:95)\r\n  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:258)\r\n  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:634)\r\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:258)\r\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:257)\r\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:95)\r\n  at scala.util.Try$.apply(Try.scala:217)\r\n  at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1377)\r\n  at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1438)\r\n  at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:101)\r\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:76)\r\n  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:135)\r\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:129)\r\n  at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:477)\r\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:468)\r\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:483)\r\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:493)\r\n  ... 30 elided\r\nCaused by: java.lang.ClassNotFoundException: foo.DefaultSource\r\n  at scala.tools.nsc.interpreter.IMain$TranslatingClassLoader.findClass(IMain.scala:359)\r\n  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\r\n  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$6(DataSource.scala:656)\r\n  at scala.util.Try$.apply(Try.scala:217)\r\n  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\r\n  at scala.util.Failure.orElse(Try.scala:230)\r\n  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\r\n  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:732)\r\n  at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableProvider(DataSourceV2Utils.scala:163)\r\n  at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:670)\r\n  at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:176)\r\n  at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:54)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\r\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:386)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:111)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:110)\r\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n  at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:54)\r\n  at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:48)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n  at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n  at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n  at scala.collection.immutable.List.foldLeft(List.scala:79)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n  at scala.collection.immutable.List.foreach(List.scala:334)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:234)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:230)\r\n  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:184)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:230)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:199)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:222)\r\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:393)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:221)\r\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:95)\r\n  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:258)\r\n  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:634)\r\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:258)\r\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:257)\r\n  at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:95)\r\n  at scala.util.Try$.apply(Try.scala:217)\r\n  at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1377)\r\n  at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n  at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n  ... 41 more\r\n  Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n    at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:722)\r\n    at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:672)\r\n    at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:732)\r\n    at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableProvider(DataSourceV2Utils.scala:163)\r\n    at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:670)\r\n    at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:176)\r\n    at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:54)\r\n    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\r\n    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\r\n    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:386)\r\n    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:111)\r\n    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:110)\r\n    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n    at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:54)\r\n    at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:48)\r\n    at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n    at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n    at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n    at scala.collection.immutable.List.foldLeft(List.scala:79)\r\n    at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n    at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n    at scala.collection.immutable.List.foreach(List.scala:334)\r\n    at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:234)\r\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:230)\r\n    at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:184)\r\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:230)\r\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:199)\r\n    at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n    at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n    at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:222)\r\n    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:393)\r\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:221)\r\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:95)\r\n    at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:258)\r\n    at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:634)\r\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:258)\r\n    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n    at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:257)\r\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:95)\r\n    at scala.util.Try$.apply(Try.scala:217)\r\n    at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1377)\r\n    at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n    at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n    ... 41 more\r\n```

> Hi @juliuszsompolski , can you put a real example with DataFrame in the PR description to show the before/after error message?\r\n\r\n@cloud-fan done.

@yaooqinn looking at the example in the PR description, I think the new error message looks pretty clean now. Can you elaborate how to reproduce the verbose error message you mentioned before?

I tested with `SparkSQLCLIDriver` and there the printed exception looks OK:\r\n```\r\nspark-sql (default)> create table foo using foo;\r\n[DATA_SOURCE_NOT_FOUND] Failed to find the data source: foo. Make sure the provider name is correct and the package is properly registered and compatible with your Spark version. SQLSTATE: 42K02\r\norg.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: foo. Make sure the provider name is correct and the package is properly registered and compatible with your Spark version. SQLSTATE: 42K02\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:722)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:672)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:732)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableProvider(DataSourceV2Utils.scala:163)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:670)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:176)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:54)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:386)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:111)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:110)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:54)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:48)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:234)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:230)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:184)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:230)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:199)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:393)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:221)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:95)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:258)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:634)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:258)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:257)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:95)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1377)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1438)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:101)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:76)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:135)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:129)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:477)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:468)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:483)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:493)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:69)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:462)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1(SparkSQLCLIDriver.scala:580)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1$adapted(SparkSQLCLIDriver.scala:574)\r\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\r\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processLine(SparkSQLCLIDriver.scala:574)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:288)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1032)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1137)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1146)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:722)\r\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:672)\r\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:732)\r\n\t\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableProvider(DataSourceV2Utils.scala:163)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:670)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:176)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:54)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:386)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:111)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:110)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:54)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:48)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:234)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:230)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:184)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:230)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:199)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:222)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:393)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:221)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:95)\r\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:258)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:634)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:258)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:257)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:95)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1377)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 33 more\r\nCaused by: java.lang.ClassNotFoundException: foo.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$6(DataSource.scala:656)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\r\n\tat scala.util.Failure.orElse(Try.scala:230)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:732)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableProvider(DataSourceV2Utils.scala:163)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:670)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:176)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:54)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:386)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:111)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:110)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:54)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:48)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:234)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:230)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:184)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:230)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:199)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:393)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:221)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:95)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:258)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:634)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:258)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:706)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:257)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:95)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1377)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t... 33 more\r\n```\r\n\r\n@yaooqinn in your case, were you getting it multiple times because one is the console print of the error, and the other is the log4j error that you pointed to and that one was also redirected to console?

thanks, merging to master!

PR description is too long, moving the examples to comment.\r\n\r\n#### Before\r\n\r\nThere are 4 suppressed exception.\r\n\r\n```\r\nscala> sql("select make_timestamp(2024, 1, 1, 0, 0, 0, 1)").collect()\r\norg.apache.spark.SparkDateTimeException: [INVALID_TIMEZONE] The timezone: 1 is invalid. The timezone must be either a region-based zone ID or a zone offset. Region IDs must have the form \

Committed in https://github.com/apache/spark/commit/42ce6044e15bf09f10ee6bf507e2cbf02776764d, but author and PR metadata got truncated because of too long commit msg. Closing PR.

@siying @brkyvz PTAL

+1 to @brkyvz about the interface and I also prefer former on the suggestion.\r\n\r\nAlso, I\

> just introduce V3?\r\n\r\nThis is sadly not possible imho. Because v1 and v2 not only differ in whether having a "v2" in the first line, but their format are different. We likely still need to have v3 and v4.\r\n\r\n> Then, you should only have one Reader class. It just reads the first line, from there it knows whether it is v1 or something else.\r\n\r\nSimilarly for this one, I think a way out is to have a helper class that does this, i.e. it reads the version from the actual changelog file. Then the reader version should be decided by it. This is a tech debt indeed, I filed a spark ticket for it: https://github.com/WweiL/oss-spark/blob/8e439fcc43513f3048b51a8422c0ec136a0b624d/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala#L211-L215 Do you think we can resolve it in a separate PR?\r\n

@brkyvz \r\nThe diffs between v1 and v2 are not just about version string existed or not. They have different format about representing the operation.\r\n\r\nThat said, if we just want to avoid divergence of having multiple active versions, we probably need to cut the version differently, e.g. timeline. v3 understands the functionalities/features A, B, C and which features are used for this file is to put on the header, and reader parses the remaining differently based on the used features. v4 does not need to have the quite different storage format but just to enforce Spark (reader) to understand the features which were introduced with v4 to read the file for v4.\r\n\r\nI guess it\

I ended up adding the helper class `StateStoreChangelogReaderFactory` to construct the state reader. PTAL @siying @HeartSaVioR @brkyvz 

I ended up overriding the `FileContextBasedCheckpointFileManager` instead of FileSystem because it is constructed by default:\r\nhttps://github.com/WweiL/oss-spark/blob/9cf6dc873ff34412df6256cdc7613eed40716570/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CheckpointFileManager.scala#L212\r\n\r\nContext: We actually have this "no open stream verification" in `SharedSparkSession`: https://github.com/WweiL/oss-spark/blob/6fc176f4f34d73d6f6975836951562243343ba9a/sql/core/src/test/scala/org/apache/spark/sql/test/SharedSparkSession.scala#L167-L169\r\n\r\nSo actually all tests that extend it would have the check, including this one. But the problem with `FileContextBasedCheckpointFileManager` is it uses the `FileContext` API rather than the `FileSystem` API so this check is unintentionally circumvented. \r\n\r\nActually, in the fallback case here:\r\nhttps://github.com/WweiL/oss-spark/blob/9cf6dc873ff34412df6256cdc7613eed40716570/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CheckpointFileManager.scala#L220\r\n\r\n`FileSystemBasedCheckpointFileManager` would trigger this assert.\r\n\r\nGiven that in reality we are using the `FileContextBasedCheckpointFileManager` I decide to create another fileManager that overrides it, instead of force using `FileSystemBasedCheckpointFileManager`

@HeartSaVioR can I again ask for your help in merging this once the final comments are addressed?

Thanks! Merging to master.

Adding @cloud-fan @davidm-db @dejankrak-db @dusantism-db 

thanks, merging to master!

Merged to master.

@MaxGekk please take a look at this one.

@vladanvasi-db Could you fix tests, please:\r\n```\r\n[info] - group-by-all-mosha.sql *** FAILED *** (838 milliseconds)\r\n[info]   group-by-all-mosha.sql\r\n[info]   Expected "...   "expression" : "\

+1, LGTM. Merging to master.\r\nThank you, @vladanvasi-db.

LGTM thank you!

thanks @xinrong-meng , merged to master

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

+1, LGTM. Merging to master.\r\nThank you, @panbingkun and @dtenedor for review.

@LuciferYang If java21 is affected, it should have happened in the previous PR.

Thank you @LuciferYang @panbingkun, merged to master

shall we add a test for it?

@zhengruifeng I am looking into creating a unit test case.\r\n\r\nMeanwhile Proposal: Using only active vertices for label propagation in each superstep.\r\nHere are the proposed changed methods:\r\n\r\n\r\n\r\n\r\n\r\n\r\n```\r\n// Updates the vertex value only if it has changed.\r\n\r\ndef vertexProgram(vid: VertexId, attr: Long, message: Map[VertexId, Long]): VertexId = {\r\n  val newAttr = if (message.isEmpty) attr else message.maxBy(_._2)._1\r\n  if (newAttr == attr) {\r\n    attr // Vertex value remains unchanged\r\n  } else {\r\n    newAttr // Vertex value has changed\r\n  }\r\n}\r\n```\r\n\r\n\r\n\r\n```\r\n// Sends messages only if the source and destination vertex values are different.\r\n\r\ndef sendMessage(e: EdgeTriplet[VertexId, ED]): Iterator[(VertexId, Map[VertexId, Long])] = {\r\n  if (e.srcAttr != e.dstAttr) {\r\n    Iterator((e.srcId, Map(e.dstAttr -> 1L)), (e.dstId, Map(e.srcAttr -> 1L)))\r\n  } else {\r\n    Iterator.empty\r\n  }\r\n}\r\n```\r\n\r\n\r\n\r\n```\r\n// Adding `activeDirection = EdgeDirection.Either` to ensure only active vertices are processed.\r\n\r\nval initialMessage = Map[VertexId, Long]()\r\nPregel(lpaGraph, initialMessage, maxIterations = maxSteps, activeDirection = EdgeDirection.Either)(\r\n  vprog = vertexProgram,\r\n  sendMsg = sendMessage,\r\n  mergeMsg = mergeMessage)\r\n```\r\n\r\n@zhengruifeng Can you review it?

@awadhesh14 The fix itself seems good: when there are multiple `key` have the same maximum `value`, the result is not deterministic.\r\n\r\nI think we should add an end-to-end test in [LabelPropagationSuite](https://github.com/apache/spark/blob/48e207f4a2192d474f2c0f141b984ef0c36a78c3/graphx/src/test/scala/org/apache/spark/graphx/lib/LabelPropagationSuite.scala),\r\n\r\nthe test could be the mini-reproducer of the issue described in https://issues.apache.org/jira/browse/SPARK-42856, and we assert the results.\r\n\r\n

cc @yaooqinn FYI

<img width="943" alt="image" src="https://github.com/user-attachments/assets/ca3ba482-ba75-48d6-9945-86614582b6c2">\r\n\r\n`sql/hive` others passed

Merged to master, thank you @LuciferYang @zhengruifeng 

Thanks @yaooqinn and @zhengruifeng 

- https://github.com/LuciferYang/spark/actions/runs/11854044932/job/33035384447\r\n\r\nThe test was largely successful, except for the modules related to mllib, which seems to need some fix to the test cases on macos. I think we can fix them this in the followup.\r\n\r\n```\r\n test_data_loader (pyspark.ml.tests.connect.test_parity_torch_data_loader.TorchDistributorBaselineUnitTestsOnConnect.test_data_loader) ... WARNING: Using incubator modules: jdk.incubator.vector\r\nSetting default log level to "WARN".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\nStarted distributed training with 1 executor processes\r\nW1115 13:42:15.675000 65006 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\r\nW1115 13:42:15.675000 65006 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\r\nTraceback (most recent call last):\r\n  File "/Users/runner/work/spark/spark/python/target/5c60ac90-7dd0-42ad-9c5f-4700f5cf9926/tmpg209ldzx/train.py", line 8, in <module>\r\n    output = fn(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^\r\n  File "/Users/runner/work/spark/spark/python/pyspark/ml/torch/tests/test_data_loader.py", line 71, in train_function\r\n    return list(data_loader)\r\n           ^^^^^^^^^^^^^^^^^\r\n  File "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 484, in __iter__\r\n    return self._get_iterator()\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 415, in _get_iterator\r\n    return _MultiProcessingDataLoaderIter(self)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1138, in __init__\r\n    w.start()\r\n  File "/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py", line 121, in start\r\n    self._popen = self._Popen(self)\r\n                  ^^^^^^^^^^^^^^^^^\r\n  File "/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py", line 224, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File "/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py", line 288, in _Popen\r\n    return Popen(process_obj)\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File "/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File "/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File "/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File "/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/reduction.py", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can\

cc @HyukjinKwon @dongjoon-hyun @zhengruifeng @panbingkun FYI

LGTM

![image](https://github.com/user-attachments/assets/a8c305c5-e16c-4bd1-9db4-93b0f7f2d275)\r\n\r\nThe failed case is unrelated to the current PR, and @yaooqinn  is working on fixing the issue. I will merge this PR first and observe the test results. If there are any other issues, I can address them in the followup.

Merged into master. Thanks @zhengruifeng and @panbingkun 

cc @cloud-fan @manuzhang since you recently worked on similar changes

@cloud-fan all comments addressed

thanks, merging to master!

I am fine with this change but just wanted to make sure - does it affect any end-user usecase?

cc @cloud-fan 

thanks, merging to master!

Thanks! Merging to master.

The verification process is as follows:\r\n\r\n- Run the following command:\r\n```shell\r\nsh dev/spark-test-image/docs/build-docs-on-local\r\n```\r\n\r\n- The process of run is as follows:\r\n```shell\r\n[info] Note: Some input files use or override a deprecated API.\r\n[info] Note: Recompile with -Xlint:deprecation for details.\r\n[warn] multiple main classes detected: run \

> dumb questions: can we move the scripts to another dictionary?\r\n\r\nAllow me to try it.

> dumb questions: can we move the scripts to another dictionary?\r\n\r\nI have moved the script from `dev/spark-test-image/docs` to `dev/spark-test-image-utils/docs`, and the local testing is okay.

cc @HyukjinKwon @LuciferYang 

will verify the script later

> will verify the script later\r\n\r\nThank you very much! â¤ï¸

The script can executed successfully, thank you very much, @panbingkun . \r\n\r\nHowever, should the final generated results only exist in the `docs/site/` directory? It seems that copies of the generated `.html` files exist in many other places, such as the `sql/site/` directory and the `docs/api` directory. Additionally, since these files cannot currently be cleaned using commands like `sbt clean` or `mvn clean`, this results in many extra `.html` files being left in the project work space after each build.

> The script can executed successfully, thank you very much, @panbingkun .\r\n> \r\n> However, should the final generated results only exist in the `docs/site/` directory? It seems that copies of the generated `.html` files exist in many other places, such as the `sql/site/` directory and the `docs/api` directory. Additionally, since these files cannot currently be cleaned using commands like `sbt clean` or `mvn clean`, this results in many extra `.html` files being left in the project work space after each build.\r\n\r\n@LuciferYang Thank you very much for helping to verify! â¤ï¸\r\n\r\nI think the above issue is due to a problem with the script `build_api_docs.rb` itself that has existed in the past, as follows:\r\nhttps://github.com/apache/spark/blob/e03319fd9219da7162c12a15998d5718edc4c49e/docs/_plugins/build_api_docs.rb#L189-L193\r\n\r\nCan we solve this issue with a new separate PR?

LGTM, but  it would be best to wait for @zhengruifeng to take another look.\r\n\r\n

Add a note (I communicated privately with @LuciferYang and confirmed this). \r\n\r\n- If you encounter similar issues:\r\n```\r\nERROR: failed to solve: ubuntu:jammy-20240911.1: failed to resolve source metadata for docker.io/library/ubuntu:jammy-20240911.1: failed to authorize: failed to fetch oauth token: Post "https://auth.docker.io/token": read tcp 192.168.1.23:49300->54.236.113.205:443: read: connection reset by peer\r\n```\r\n\r\n- please add the `registry-mirrors` to the file `~/.docker/daemon.json`:\r\n\r\n```shell\r\n(base) âžœ  .docker pwd\r\n/Users/panbingkun/.docker\r\n(base) âžœ  .docker cat daemon.json\r\n{\r\n  "builder": {\r\n    "gc": {\r\n      "defaultKeepStorage": "20GB",\r\n      "enabled": true\r\n    }\r\n  },\r\n  "experimental": false,\r\n  "registry-mirrors": [\r\n    "https://registry.docker-cn.com",\r\n    "http://hub-mirror.c.163.com",\r\n    "https://docker.mirrors.ustc.edu.cn",\r\n    "https://dockerhub.azk8s.cn",\r\n    "https://mirror.ccs.tencentyun.com",\r\n    "https://registry.cn-hangzhou.aliyuncs.com",\r\n    "https://docker.mirrors.ustc.edu.cn",\r\n    "https://docker.1panel.live",\r\n    "https://atomhub.openatom.cn/",\r\n    "https://hub.uuuadc.top",\r\n    "https://docker.anyhub.us.kg",\r\n    "https://dockerhub.jobcher.com",\r\n    "https://dockerhub.icu",\r\n    "https://docker.ckyl.me",\r\n    "https://docker.awsl9527.cn"\r\n  ]\r\n}\r\n```\r\n

the script works well on my local machine, thanks @panbingkun 

> dumb questions: can we move the scripts to another dictionary?\r\n\r\n@zhengruifeng I suppose the script is intended to be used by developers, if so, maybe just put it at `dev/build-docs`?

> the script works well on my local machine, thanks @panbingkun\r\n\r\nThank you very much for helping to verify again! â¤ï¸

Merged to master.\r\nThank you, @zhengruifeng, @LuciferYang, @pan3793 .

thanks, merged to master

+1, LGTM. Merging to master.\r\nThank you, @vladimirg-db.

+1, LGTM. Merging to master.\r\nThank you, @vladimirg-db.

LGTM thank you!

which will work all for Spark Classic and Spark Connect.

> I think another way of doing this is to maintain a sized dictionary in Python side, and cache the value retrieved within the Python side.\r\n> \r\n> e.g.,\r\n> \r\n> * spark.get("a")\r\n>   \r\n>   * look up cacehd["a"] = v\r\n>     \r\n>     * if not, spark.get("a")\r\n> * spark.set("a", aa")\r\n>   \r\n>   * empty cache cached["a"]\r\n>   * spark.set("a")\r\n> \r\n> and create a dictioanry with TTL and max size\r\n\r\nA problem is that `spark.conf` is not the only entry point for config operations.\r\nUsers can also set/unset config by `spark.sql`

thanks, merged to master

cc @cloud-fan 

Merged to master and 3.5. Thank you @CuiYanxiang 

The implementation for this PR ended up getting too big. I can split this into separate PRs.

This PR now just covers the `EXTEND` operator.

cc @gengliangwang @cloud-fan this one is ready for review at your convenience.

thanks, merging to master!

@neilramaswamy - thx for writing up a detailed PR description !

Before reading through the code, I read the PR description and the direction looks great to me. I think the behavior should have been like the proposed one, and somehow we missed this. Thanks for the fix!

Thanks! Merging to master.

cc @cloud-fan @gengliangwang here is documentation support for the new SQL pipe syntax, which is nearly completed.

@dtenedor The doc looks good to me overall.\r\nDo we consider showing more examples like https://github.com/google/zetasql/blob/master/docs/pipe-syntax.md? 

@gengliangwang thanks for your review! I updated the docs with more examples and information per recommendation, please take another look.

Thanks, merging to master

@gene-db @cashmand @cloud-fan could you help review? Thanks!

@cloud-fan could you help review? Thanks!

thanks, merging to master!

@gene-db @chenhao-db This PR seems to wrongly point to an umbrella JIRA ticket, and the newly added conf `spark.sql.variant.allowReadingShredded` uses `true` as the default value.\r\nIf I understand correctly, https://github.com/apache/parquet-format/pull/461 is going to make incompatible changes on the shredding spec, if Spark delivers the current shredding implementation as-is in Spark 4.0, additional migration/compatible efforts will be required in the future.

I opened https://github.com/apache/spark/pull/49874 to disable the `spark.sql.variant.allowReadingShredded` by default

cc @HyukjinKwon 

Thank you, @yaooqinn ! \r\n\r\nMerged to master for Apache Spark 4.0.0 on February 2025.

Merging to master. Thank you, @srielau and @cloud-fan for review.

This was tested during RC vote.\r\n- https://github.com/dongjoon-hyun/spark/pull/22

Thanks @dongjoon-hyun 

Thank you, @viirya !

Merged to master for Apache Spark 4.0.0.

This was tested during RC vote.\r\n- https://github.com/dongjoon-hyun/spark/pull/21

Thanks @dongjoon-hyun 

Thank you, @viirya !

Merged to branch-3.5.

LGTM thank you!

> you can throw an exception if PYSPARK_PIN_THREAD is enabled.\r\n\r\nOh, if we can just throw an exception for `PYSPARK_PIN_THREAD` then the suggestions sounds reasonable to me. Let me address the comment. Thanks!

I think we might need resolve the bug from Scala side first.\r\nI left some comment from https://github.com/apache/spark/pull/47815/files#r1847514875.\r\nAlso cc @xupefei

Merged to master.\r\nThanks @HyukjinKwon @xinrong-meng for the review!

cc @dongjoon-hyun as the initiator of SPARK-44444, also cc @cloud-fan, thank you

Merged to master, thank you @dongjoon-hyun 

Merged to master, thank you!

LGTM thank you!

Merged to master.

Merged to master.

CI has passed: https://github.com/jingz-db/spark/runs/33636466911\r\n\r\nThanks! Merging to master.

Could you review this PR, @viirya ?

License Check passed.\r\n- https://github.com/dongjoon-hyun/spark/actions/runs/11825507951/job/32949550024\r\n\r\n![Screenshot 2024-11-13 at 13 00 21](https://github.com/user-attachments/assets/ac2535fd-6cca-473f-8214-435027f42875)\r\n

Thank you, @viirya !

Merged to master.

Thank you, @HyukjinKwon .

I backported this to branch-3.5 too~

Could you review this PR, @huaxingao ?

Thank you, @huaxingao . Merged to master/3.5.

+1, LGTM. Merging to master.\r\nThank you, @mihailom-db and @cloud-fan @dbatomic for review.

cc @dongjoon-hyun and @panbingkun 

Thanks @dongjoon-hyun ~

+1, late LGTM

Thanks @panbingkun 

@MaxGekk please take a look for this revert as we discussed offline.

The failed GA [Run / Run Docker integration tests](https://github.com/vladanvasi-db/spark/actions/runs/11816438758/job/32919748604#logs) is not related.\r\n\r\n+1, LGTM. Merging to master.\r\nThank you, @vladanvasi-db.

Merged to master.

LGTM thank you!

Merged to master.

cc @cloud-fan Can you take a look? I saw you originally implemented the Staged Table interface

FYI @manuzhang this PR intoroduces analogous interfaces to the ones you added in Write for staged tables

Thanks, merging to master

shall we add a test for it?

The remaining test failures are not related to this PR.

Thanks! merging to master.

merged to master

Merged to master.

fyi: I create follow-up ticket and working on it: SPARK-50310

also cc @panbingkun 

Merged to master.

Will setting the following config handle skew cases?\r\n```\r\n--conf spark.shuffle.accurateBlockSkewedFactor=10;\r\n--conf spark.shuffle.accurateBlockThreshold=209715200;\r\n--conf spark.shuffle.maxAccurateSkewedBlockNumber=60\r\n```

thanks, I am also considering similar optimization, we can do it later, since I plan to spin more envs off :)

> thanks, I am also considering similar optimization, we can do it later, since I plan to spin more envs off :)\r\n\r\nOkay, this PR is an abstraction for separating multiple images in the future (extracting `common logic`). \r\nHold it first and let me complete it later. \r\nThanks!

ðŸ‘ðŸ»  Thank you so much for taking over this, @yaooqinn .

All GitHub actions are passing now. Please take a look. @LuciferYang @dongjoon-hyun @HyukjinKwon @cloud-fan 

Thank you @dongjoon-hyun @LuciferYang 

Hi @yaooqinn , Thanks for bringing the HMS4 compatibility. I tried it and found one of the behaviour difference, reported in https://issues.apache.org/jira/browse/SPARK-50461. Could you please guide me, how I should be approaching this, to fix it?\r\ncc - @dongjoon-hyun \r\n\r\nThanks

cc @szehon-ho @cloud-fan @dongjoon-hyun

yea 3 should have lower priority.

After touching the code, I realized the `options` propagation is inconsistent in `DataFrame`, `DataFrameV2`, and `SQL`, and `SessionConfigSupport` also does not take effect on `SQL` cases (I think this is a bigger issue that should be fixed independently), I updated the code with assertions and add some test cases for DataFrame API. \r\n\r\n@szehon-ho @cloud-fan please take a look when you have time, thanks in advance.\r\n\r\n(the CI failures look irrelevant)

thanks, merging to master!

cc @liujiayi771 @cloud-fan @dongjoon-hyun 

kindly ping @cloud-fan 

thanks, merging to master!

cc @ueshin @HyukjinKwon @xinrong-meng @allisonwang-db 

@ueshin After thinking more about it, I think the new test case should pass instead of fail. My opinion is:\r\n1. In SQL, users can just write un-qualified column names to reference the outer plan. We should allow the same in DataFrame API.\r\n2. Eventually, Spark Connect is the main API and DataFrame is always lazy. Then `.outer()` is only needed to avoid ambiguity when there are column name conflicts, to explicitly reference the outer plan only.

@cloud-fan \r\n\r\n> In SQL, users can just write un-qualified column names to reference the outer plan. We should allow the same in DataFrame API.\r\n\r\nIn that case, `sf.col("a")` should also be allowed instead of `sf.col("a").outer()` in the above example?\r\n\r\n```py\r\nl.select(\r\n    "a",\r\n    (\r\n        r\r\n        .where(sf.col("b") == sf.col("a"))\r\n        .select(sf.sum("d"))\r\n        .scalar()\r\n    ),\r\n).show()\r\n```\r\n\r\notherwise, users may not know why it\

@ueshin I think `.where(sf.col("b") == sf.col("a"))` should be allowed in Spark Connect. For now we need at lease one `.outer()` to trigger lazy analysis for Classic Spark, but it\

I think there is another issue with the `outer` API.  The `where` or `filter` API can take a SQL expression, but we don\

thanks for the review, merging to master!

For anyone interested. I have opened this up so people can take a look. I still need to make a bunch of changes.

For anyone interested. This is mostly ready for review. I still looking at some of the developer API, most of it uses classic now, however in most cases the interfaces should suffice. Moving them back might make the lives of spark library developers at bit easier.

cc @HyukjinKwon @cloud-fan @dongjoon-hyun this is the last big change. PTAL.

> Please note that this does change a number of developer APIs:\r\n> Streaming Sink\r\n> Streaming Source\r\n> FileFormat\r\n> Command\r\n\r\nHow is FileFormat and Command related to the DataFrame API?

@cloud-fan they all use SparkSession in their in their interface. SparkSession also changes.\r\n\r\nFor the record I am fine going either way. The SparkSession interface works in most of the cases, and where it does not work you can import the ClassicConversions object. This would make it easier for some to migrate. However, I do think there is a benefit in being more clear here, because the only implementation of SparkSession that you will see is the classic one.

@hvanhovell `PlanGenerationTestSuite` fails

Merging to master/4.0

Finally. Thank you for delivering this, @hvanhovell and @cloud-fan .

Thank you for sharing the issue and analysis, @LuciferYang .

Could you take a look at the failure, @hvanhovell and @cloud-fan , please?

For the record, this is reverted from `branch-4.0`. Please make a PR for branch-4.0 with the fix.

> @dongjoon-hyun @LuciferYang this has to stop. Maven is not being tested at all. If we don\

For the record, `branch-4.0` Maven CI is recovered finally after the reverting while `master` branch Maven CIs are still broken.\r\n- https://github.com/apache/spark/actions/workflows/build_branch40_maven.yml\r\n\r\n![Screenshot 2025-01-27 at 13 16 54](https://github.com/user-attachments/assets/5eacafb9-b2c1-4d2c-88c5-883303ed7542)\r\n

@dongjoon-hyun SBT using the building is not failing now was it? In that case I am basically out of signal. I am not sure that such a quick revert is warranted, because it is - unfortunately - a bit of a big change.

Fix is here: https://github.com/apache/spark/pull/49701

On second thought, maybe we should also add a new image for lint? so that we can upgrade docs and lint separately

That works to me too

> On second thought, maybe we should also add a new image for lint? so that we can upgrade docs and lint separately\r\n\r\nfIne to me

Let me first understand the background of the previous PR https://github.com/apache/spark/pull/48690\r\n\r\n> On second thought, maybe we should also add a new image for lint? so that we can upgrade docs and lint separately\r\n\r\n\r\n+1

thanks for the review, I am going to close this PR in favor of https://github.com/apache/spark/pull/48826

Merged to master for Apache Spark 4.0 on February 2025.\r\n\r\nThank you, @zhengruifeng .

cc @HyukjinKwon @zhengruifeng @dongjoon-hyun @HeartSaVioR @bogao007 

Thank you, @LuciferYang , @zhengruifeng , @bogao007 .\r\n\r\nMerged to master for Apache Spark 4.0.0 on February 2024.

Thanks @dongjoon-hyun @zhengruifeng and @bogao007 

LGTM thank you!

Mind answering the rest of the questions in the PR description?

The failed linter failure is unrelated, thanks, merging to master!

Mind filien a JIRA please?

> Mind filien a JIRA please?\r\n\r\nWith pleasure, i have request a Jira account with email.

```shell\r\n./build/sbt -Phadoop-3 -Pkubernetes -Pkinesis-asl -Phive-thriftserver -Pdocker-integration-tests -Pyarn -Phadoop-cloud -Pspark-ganglia-lgpl -Phive -Pjvm-profiler clean package\r\n```\r\n\r\n```shell\r\n[info] Note: Some input files use or override a deprecated API.\r\n[info] Note: Recompile with -Xlint:deprecation for details.\r\n[warn] multiple main classes detected: run \

cc @LuciferYang @dongjoon-hyun 

also cc @HyukjinKwon 

LGTM, thank you!

Merged to master.

@HeartSaVioR Could you help review this change? Thanks!

CI failed with only docker integration test.\r\nhttps://github.com/bogao007/spark/runs/32741105303

Thanks! Merging to master.

cc @JoshRosen 

@JoshRosen Thanks! I changed to `private[this]` and updated the PR description to mention it.

Merged to master.

cc - @HeartSaVioR @liviazhu-db - PTAL, thx !

Thanks! Merging to master.

@HeartSaVioR Could you help review this change? Thanks!

> Thanks for making the change! BTW before the change, did the test fail with partial output because the write to input path is not completed?\r\n\r\nyep, you can check these 2 runs:\r\nhttps://github.com/bogao007/spark/actions/runs/11748270946/job/32732055783\r\nhttps://github.com/bogao007/spark/actions/runs/11746801388/job/32727414566

Thanks! Merging to master.

> @stevomitric Could you regenerate results of the benchmark CollationBenchmark, please. We should expect better numbers after your changes, right?\r\n\r\nThey are running, I will post them here once complete. We should expect same or better results.

Gentle ping, @stevomitric . Is the regenerated result ready?\r\n\r\nAlso, cc @panbingkun , FYI.

How about to update bechmark results after the revert https://github.com/apache/spark/pull/48804/commits/95b259ecc5c20649993e5d8173aa1b076c0c74ea

> How about to update bechmark results after the revert https://github.com/apache/spark/commit/95b259ecc5c20649993e5d8173aa1b076c0c74ea\r\n\r\nDecided to make this PR update the benchmarks, and not touch the `CollationFactory` class. Modifying the function in the way you proposed [here](https://github.com/apache/spark/pull/48804#discussion_r1834614134), resulted in a very small perf benefit.

Although I updated the PR title based on the AS-IS status, the PR description is still outdated. Could you make the PR description up-to-date with the AS-IS status, @stefankandic ?

> Could you make the PR description up-to-date with the AS-IS status?\r\n\r\nUpdated the description and added non-ascii collation benchmarks, @dongjoon-hyun .

Thank you all!

thanks, merging to master!

Merged to master.

Can we have a simple test?

+1, LGTM. Merging to master.\r\nThank you, @dejankrak-db.

> @milastdbx Could you review this PR, please.\r\n\r\nThanks @MaxGekk !

cc @rebo16v @srowen @HyukjinKwon @WeichenXu123 

Merged to master.

Adding: @cloud-fan @srielau @davidm-db @dejankrak-db @dusantism-db  

The linter failure is unrelated, thanks, merging to master!

@davidm-db @miland-db Rebased, you can review again

We can address comments later as they are kind of improvement. Let me merge it first. Thanks!

@cloud-fan Great, thanks. I will create follow up tasks for the improvements you suggested.

thanks, merging to master!

Merged to master.

@michTalebzadeh the idea is that we give Spark Connect users the same functionality as existing classic users.

cc @WeichenXu123 

Hi @WeichenXu123, @grundprinzip, @mengxr, Could you help review this PR. the CI got passed. Thx very much.

Hi @wbo4958 ,\r\n\r\nI just applied the following patch on your PR:\r\n\r\n```\r\ndiff --git a/mllib/src/main/resources/META-INF/services/org.apache.spark.ml.Estimator b/mllib/src/main/resources/META-INF/services/org.apache.spark.ml.Estimator\r\nindex f3fd21ad4c3..fe690cb8eeb 100644\r\n--- a/mllib/src/main/resources/META-INF/services/org.apache.spark.ml.Estimator\r\n+++ b/mllib/src/main/resources/META-INF/services/org.apache.spark.ml.Estimator\r\n@@ -17,4 +17,13 @@\r\n \r\n # Spark Connect ML uses ServiceLoader to find out the supported Spark Ml estimators.\r\n # So register the supported estimator here if you\

Sorry, clicked the wrong button!

I did play some more with different estimators and LinearRegression and RandomForrestRegressor worked nicely as well. I ran into some issues with the training summary and getting some of the attributes back but that was caught properly.\r\n\r\nThe generic base class of `Evaluator` is not yet supported though.

Hi @grundprinzip, I just updated this PR by supporting sparse vector/matrix and supporting the evaluate a dataset in a model. So for now, All attributes in LogisticRegressionModel have been supported. Please help review it again. Thx

Hello.  I suppose many design of this pr, especially the proto and server part, are similar to that in https://github.com/apache/spark/pull/40479.  So maybe at least this pr#40479  should at least be mentioned?

Hi @martin-g, @hvanhovell, Would you help review it again. Thx very much.

I guess you pinged me by mistake. \nI am not familiar with this code. 

had offline discussion with @wbo4958 , separate messages for vector/matrix are compact so more suitable for large params like initial model weights.

thanks, merged to master

Link to tests: https://github.com/Fokko/spark/actions/runs/11720553983/job/32646109185

Thank you, @Fokko . Any updates?

Thank you for updating the PR, @Fokko !

@dongjoon-hyun Any time, just bug fixes, so we should be good to go: https://github.com/apache/parquet-java/releases/tag/apache-parquet-1.14.4

Please make the PR description up-to-date and convert it from `Draft` to normal PR after the CI passes.

Merged to master for Apache Spark 4.0.0 on February 2025.\r\n\r\nThank you, @Fokko and @yaooqinn .

cc @cloud-fan 

@wangyum can you elaborate how AQE loses the data ordering?

@wangyum which step Spark drop the Sort operator? The logical re-optimizing step?

It is removed by `RemoveRedundantSorts` in `queryStagePreparationRules`:\r\nhttps://github.com/apache/spark/blob/01464b9ec1ca7baca96b1bea0fbb238c25d0eafb/sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala#L760

The ordering of child of `Window` can satisfies:\r\n```\r\nSort [year#282 ASC NULLS FIRST, course#281 ASC NULLS FIRST], true, 0\r\n+- Window [sum(earnings#283) windowspecdefinition(year#282 ASC NULLS FIRST, course#281 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS balance#280], [year#282 ASC NULLS FIRST, course#281 ASC NULLS FIRST]\r\n   +- Sort [year#282 ASC NULLS FIRST, course#281 ASC NULLS FIRST], false, 0       <---\r\n      +- ShuffleQueryStage 0\r\n```

So the key problem is `TakeOrderedAndProject` got lost after AQE optimization, which is a bug because it contains a project list that can reorder columns. @wangyum can you elaborate more about how `TakeOrderedAndProject` got lost?

Logical plan:\r\n```\r\nGlobalLimit 100\r\n+- LocalLimit 100\r\n   +- Sort [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST], true\r\n      +- Window [sum(earnings#264) windowspecdefinition(year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS balance#261], [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST]\r\n         +- Project [year#263, course#262, earnings#264]\r\n            +- Relation spark_catalog.default.t\r\n```\r\n\r\nInput physical plan in `AdaptiveSparkPlanExec`:\r\n```\r\nTakeOrderedAndProject(limit=100, orderBy=[year#263 ASC NULLS FIRST,course#262 ASC NULLS FIRST], output=[year#263,course#262,earnings#264,balance#261])\r\n+- Window [sum(earnings#264) windowspecdefinition(year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS balance#261], [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST]\r\n   +- Project [year#263, course#262, earnings#264]\r\n      +- FileScan parquet spark_catalog.default.t\r\n```\r\nAfter `RemoveRedundantProjects` and `EnsureRequirements` in `queryStagePreparationRules`:\r\n```\r\nTakeOrderedAndProject(limit=100, orderBy=[year#263 ASC NULLS FIRST,course#262 ASC NULLS FIRST], output=[year#263,course#262,earnings#264,balance#261])\r\n+- Window [sum(earnings#264) windowspecdefinition(year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS balance#261], [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST]\r\n   +- FileScan parquet spark_catalog.default.t\r\n\r\nTakeOrderedAndProject(limit=100, orderBy=[year#263 ASC NULLS FIRST,course#262 ASC NULLS FIRST], output=[year#263,course#262,earnings#264,balance#261])\r\n+- Window [sum(earnings#264) windowspecdefinition(year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS balance#261], [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST]\r\n   +- Sort [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST], false, 0\r\n      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=108]\r\n         +- FileScan parquet spark_catalog.default.t\r\n```\r\n\r\nSo Initial physical plan in `AdaptiveSparkPlanExec`:\r\n```\r\nTakeOrderedAndProject(limit=100, orderBy=[year#263 ASC NULLS FIRST,course#262 ASC NULLS FIRST], output=[year#263,course#262,earnings#264,balance#261])\r\n+- Window [sum(earnings#264) windowspecdefinition(year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS balance#261], [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST]\r\n   +- Sort [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST], false, 0\r\n      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=108]\r\n         +- FileScan parquet spark_catalog.default.t\r\n```\r\n\r\nAfter first query stage finished,\r\nLogical plan:\r\n```\r\nGlobalLimit 100\r\n+- LocalLimit 100\r\n   +- Sort [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST], true\r\n      +- Window [sum(earnings#264) windowspecdefinition(year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS balance#261], [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST]\r\n         +- LogicalQueryStage Project [year#263, course#262, earnings#264], ShuffleQueryStage 0\r\n\r\nSort [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST], true\r\n+- Window [sum(earnings#264) windowspecdefinition(year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS balance#261], [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST]\r\n   +- LogicalQueryStage Project [year#263, course#262, earnings#264], ShuffleQueryStage 0\r\n```\r\nCurrent physical plan:\r\n```\r\nTakeOrderedAndProject(limit=100, orderBy=[year#263 ASC NULLS FIRST,course#262 ASC NULLS FIRST], output=[year#263,course#262,earnings#264,balance#261])\r\n+- Window [sum(earnings#264) windowspecdefinition(year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS balance#261], [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST]\r\n   +- Sort [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST], false, 0\r\n      +- ShuffleQueryStage 0\r\n         +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=119]\r\n            +- *(1) ColumnarToRow\r\n               +- FileScan parquet spark_catalog.default.t\r\n```\r\n\r\nAfter [reOptimize](https://github.com/apache/spark/blob/c96a570b1b00c51f296ae2cbcfd45caa930fe995/sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala#L382),`EliminateLimits` will remove the limit, the logical plan:\r\n```\r\nSort [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST], true\r\n+- Window [sum(earnings#264) windowspecdefinition(year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS balance#261], [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST]\r\n   +- LogicalQueryStage Project [year#263, course#262, earnings#264], ShuffleQueryStage 0\r\n```\r\n\r\nThe physical plan:\r\n```\r\nWindow [sum(earnings#264) windowspecdefinition(year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS balance#261], [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST]\r\n+- Sort [year#263 ASC NULLS FIRST, course#262 ASC NULLS FIRST], false, 0\r\n   +- ShuffleQueryStage 0\r\n      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=119]\r\n         +- *(1) ColumnarToRow\r\n            +- FileScan parquet spark_catalog.default.t\r\n```\r\n\r\nThen the output column order changed,\r\nBefore:\r\n```\r\n0 = {AttributeReference@21872} year#263\r\n1 = {AttributeReference@21873} course#262\r\n2 = {AttributeReference@21874} earnings#264\r\n3 = {AttributeReference@21875} balance#261\r\n```\r\nAfter:\r\n```\r\n0 = {AttributeReference@21865} course#262\r\n1 = {AttributeReference@21866} year#263\r\n2 = {AttributeReference@21867} earnings#264\r\n3 = {AttributeReference@21868} balance#261\r\n```\r\n\r\n

the linter failure is unrelated, thanks, merging to master/3.5!

@wangyum does this bug also exist in 3.4? If yes please help to make a backport PR, thanks!

Yes. 3.4 also has this issue: https://github.com/apache/spark/pull/48907/files

To @wangyum and @cloud-fan , \r\n- I updated the JIRA information by adding `3.4.4` according to the above discussion.\r\n- However, I closed the PR on branch-3.4 because the branch reached the end-of-support.\r\n\r\nAnyway, thank you for reporting and fixing this!

Could you review this PR, @viirya ?

Merged to master~

thanks, merging to master!

Gentle ping, @cloud-fan .

thanks for the review, merging to master!

Merged to master.\r\n\r\nThank you @zhengruifeng @HyukjinKwon 

Merged to master.\r\n\r\nThank you @zhengruifeng @dongjoon-hyun 

Thanks! Merging to master.

cc @viirya 

Thank you, @HyukjinKwon and @viirya .

Merged to master.

Thank you!

Could you review this, @xinrong-meng and @zhengruifeng ?

Could you review this PR too, @HyukjinKwon ?

Could you review this PR, @viirya ?

Thank you so much, @yaooqinn !

Thank you, @viirya !

Merged to master. Thank you all.

Late LGTM, thanks!\r\nIt is reasonable to add this getter since we already have a setter

Thank you, @zhengruifeng 

+1, LGTM. Merging to master.\r\nThank you, @itholic.

Thanks @MaxGekk for the review

> So If I understand correctly, the shredding write chain may be like this:\r\nGet the expected shredded schema (DataType) through some ways (sampling or justs user defined?) -> parquet writer accepts the variant + shredded schema -> cast to shredded InternalRow -> write to parquet file (the actual col type is the group type corresponding to the shredded dataType).\r\n\r\nYes, exactly. For initial implementation and testing, I plan to set the schema explicitly. I think sampling makes sense as a better user experience in the long term, but needs some thought about the best way to implement it.\r\n\r\n> From this perspective, consider the integration with the lake format, lake format generally has its own reader or writer. I feel like that the lake format may still receive the raw variant data, and the same shredding logic must be implemented in the format\

The docker test failure is unrelated, thanks, merging to master!

+1, LGTM. Merging to master.\r\nThank you, @itholic.

Oh. Good to know. Thank you for closing. 

I believe you can add that rational as a comment here for the other future audiences. :)\r\n> I did not use lazy val here for testing .

cc @Ngone51 , @cloud-fan 

cc @viirya , too.

Addressed the comment. Thank you, @viirya .

Merged to master\r\n\r\nThank you @dongjoon-hyun @viirya @cloud-fan 

Thank you all, @viirya , @cloud-fan , @yaooqinn !

+1, LGTM. Merging to master.\r\nThank you, @itholic.

@MaxGekk @stefankandic can you please take a look at this PR?

+1, LGTM. Merging to master.\r\nThank you, @vladanvasi-db and @stefankandic @uros-db for review.

Had a conversation with @mihailom-db.\r\nWe should use `INTERVAL_ARITHMETIC_OVERFLOW`, add 2 subclasses to it: with hint and without hint and use "without_hint" variation for `Make[DT|YM]Interval`.

Update title please. No spaces between tags and the name of the PR should be the same as the ticket on jira.

> Update title please. No spaces between tags and the name of the PR should be the same as the ticket on jira.\r\n\r\nDone.

CI fails, but `testOnly org.apache.spark.sql.SparkSessionE2ESuite` locally works fine (I\

@srielau Does this look good to you?

You can remove [WIP] and Draft as well.

+1, LGTM. Merging to master.\r\nThank you, @gotocoding-DB and @mihailom-db @srielau for review.

@gotocoding-DB Congratulations with your first contribution to Apache Spark!

cc @cloud-fan, thank you in advance

Merged to master. Thank you for the review @dongjoon-hyun @cloud-fan 

we should change to exclude \r\n\r\n```\r\n<exclusion> \r\n       <groupId>io.netty</groupId> \r\n       <artifactId>*</artifactId> \r\n     </exclusion> \r\n```\r\n\r\nfrom zookeeper instead of\r\n\r\n```\r\n<exclusion> \r\n       <groupId>org.jboss.netty</groupId> \r\n       <artifactId>netty</artifactId> \r\n     </exclusion> \r\n```\r\n\r\n\r\n`org.jboss.netty` is the groupId of netty 3.x\r\n\r\n\r\nHowever, I believe it would be better to upgrade netty to 4.1.113, because zk 3.9.3 was compiled with a dependency on netty 4.1.113. Downgrading to use 4.1.110 may introduce uncertainties and potential issues.

I have verified that the compilation issue with `sbt` has been resolved after upgrading the netty version to `1.1.114`. UT `./python/run-tests --python-executables=python3 --testnames "pyspark.sql.tests.connect.test_connect_collection"` has also been successful.

I am fine as long as the tests work :-).

@panbingkun can you also simply check `bin/pyspark --remote "local[*]"`?\r\n\r\nwith simple queries like\r\n```\r\nIn [1]: spark.range(10).collect()\r\n24/11/06 10:50:00 WARN CheckAllocator: More than one DefaultAllocationManager on classpath. Choosing first found\r\nOut[1]:\r\n[Row(id=0),\r\n Row(id=1),\r\n Row(id=2),\r\n Row(id=3),\r\n Row(id=4),\r\n Row(id=5),\r\n Row(id=6),\r\n Row(id=7),\r\n Row(id=8),\r\n Row(id=9)]\r\n```\r\n\r\nThe whole pyspark connect was broken before, including all pyspark-connect tests and the REPL.

The verification process based on `SBT` and `Maven` compilation is as follows:\r\n\r\n- Based on `SBT` compile\r\n```shell\r\n./build/sbt -Phadoop-3 -Pkubernetes -Pkinesis-asl -Phive-thriftserver -Pdocker-integration-tests -Pyarn -Phadoop-cloud -Pspark-ganglia-lgpl -Phive -Pjvm-profiler clean package\r\n\r\n[info] Note: Some input files use or override a deprecated API.\r\n[info] Note: Recompile with -Xlint:deprecation for details.\r\n[warn] multiple main classes detected: run \

> @panbingkun can you also simply check `bin/pyspark --remote "local[*]"`?\r\n> \r\n> with simple queries like\r\n> \r\n> ```\r\n> In [1]: spark.range(10).collect()\r\n> 24/11/06 10:50:00 WARN CheckAllocator: More than one DefaultAllocationManager on classpath. Choosing first found\r\n> Out[1]:\r\n> [Row(id=0),\r\n>  Row(id=1),\r\n>  Row(id=2),\r\n>  Row(id=3),\r\n>  Row(id=4),\r\n>  Row(id=5),\r\n>  Row(id=6),\r\n>  Row(id=7),\r\n>  Row(id=8),\r\n>  Row(id=9)]\r\n> ```\r\n> \r\n> The whole pyspark connect was broken before, including all pyspark-connect tests and the REPL.\r\n\r\nDone, the verification process has been posted.

Thanks all @LuciferYang @zhengruifeng @HyukjinKwon @dongjoon-hyun !

cc @allisonwang-db @ueshin 

Seems mostly fine to me

@gene-db @HyukjinKwon @ueshin I have added support for Variant in Python UDAFs in my [latest commit](https://github.com/apache/spark/pull/48770/commits/8b556bd5ce8c26efa45821debb1113723bbfac02).

Noting that the tests that were failing earlier passed on the latest commit. Seems like a broken test on old versions

Merged to master.

+1, LGTM. Merging to master.\r\nThank you, @itholic.

+1, LGTM. Merging to master.\r\nThank you, @itholic.

Merged to master and 3.5\r\n\r\nThank you @viirya @dongjoon-hyun 

Thanks @dongjoon-hyun @yaooqinn 

Starting with this commit (800faf0abfa), I get an error with the following commands:\r\n```\r\nval testDf = spark.range(200000).selectExpr("id as a", "concat(\

It was fixed by #49021.

Just for the record, Iceberg community seems to report a bug against [this patch](https://github.com/apache/spark/pull/49131#issuecomment-2532146972).\r\n- https://github.com/apache/iceberg/pull/11731

Thanks @dongjoon-hyun. Please see https://github.com/apache/spark/pull/49131#issuecomment-2532341673 for some discussion.

Thank you for clarifying that swiftly!

cc @xinrong-meng 

LGTM, thank you!

Merged to master.\r\nThanks @HyukjinKwon @MaxGekk @xinrong-meng for the review!

cc @dongjoon-hyun @huaxingao 

Thank you @dongjoon-hyun 

Thank you @huaxingao 

Failed tests should be unrelated:\r\n\r\n```\r\n[error] Failed tests:\r\n[error] \torg.apache.spark.sql.connect.service.SparkConnectSessionHolderSuite\r\n```

Merged to master.

Oh, I think I skipped that part mistakenly when I merged it ..

Thank you for confirming~

This should be the last one

Merged to master.

Merged to master.

Thank you, @HyukjinKwon and @zhengruifeng .

thanks, merging to master!

thanks all, merged to master

@MaxGekk @stefankandic @stevomitric can you please take a look at this PR?

@MaxGekk can you please take a look at this PR, CI is passing and the PR is already approved by collation team?

+1, LGTM. Merging to master.\r\nThank you, @vladanvasi-db and @stefankandic for review.

also cc @uros-db @stefankandic @c27kwan

Merging to master. Thank you, @uros-db @stefankandic @cloud-fan for review.

cc @cloud-fan @dongjoon-hyun @LuciferYang thanks

Merged to master, thank you @LuciferYang 

~Currently, I have written a new script with reference to `connect-gen-protos.sh` and `connect-check-protos.py` without merging them into one. If it would be better to integrate them, perhaps by unifying them into a single script like `gen-protos.sh` and using different parameters to accomplish different tasks, please let me know.~

Test first

Thanks @dongjoon-hyun 

Could you review this PR, @viirya ?

Could you review this PR, @LuciferYang ?

We can use `--name NAME`?

> We can use `--name NAME`?\r\n\r\nNo, it doesn\

Thank you for review and comment @yaooqinn . :) 

> applied at the last time during the code path\r\n\r\nIt seems we need to revise the order of applying the application name alternatives, similar to the precedence strategy used for options, from hardcoded, CLI arguments, or configuration files.

We may revisit those area but we cannot change the existing order because it would be a breaking behavior change in the production, @yaooqinn . We can only introduce a new configuration like this.\r\n\r\n

Thank you so much, @LuciferYang .

Looking at this more, the proposed semantics would not be consistent with how aggregation works with pipe operators, where the GROUP BY expressions arrive out of the operator followed by the aggregate functions. I will close this.\r\n\r\nAs an aside, we might want to consider returning errors if any of the aggregate functions contain column references that are not aggregated, since the pipe operator returns the grouping expressions and it is always possible to use a projection (`|> SELECT`) afterwards based on the grouping/aggregate functions.

Can we file a JIRA please?

> Can we file a JIRA please?\r\n\r\nDid it, just forgot to update the tiltle. Done

+1, LGTM. Merging to master.\r\nThank you, @mihailotim-db and @HyukjinKwon for review.

Merged to master for Apache Spark 4.0.0 on February 2025.\r\nThank you, @HyukjinKwon and @zhengruifeng .

Thank you, @HyukjinKwon !

Merged to master.

I think the failed test is not related to the changes:\r\n```\r\n[info] OracleIntegrationSuite:\r\n[info] org.apache.spark.sql.jdbc.v2.OracleIntegrationSuite *** ABORTED *** (10 minutes, 19 seconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 589 times over 10.001613945816667 minutes. Last failure \r\n```\r\n\r\n+1, LGTM. Merging to master.\r\nThank you, @stefankandic and @dongjoon-hyun @HyukjinKwon for review.

@MaxGekk @stevomitric please take a look at this PR

@vladanvasi-db Could you retrigger only the failed GA [Run / Linters, licenses, and dependencies](https://github.com/vladanvasi-db/spark/actions/runs/11667441583/job/32487401576#logs)

@MaxGekk the CI has passed now, it is wrongly shown in the ui, all the build tests have passed after retriggering thisGA

+1, LGTM. Merging to master.\r\nThank you, @vladanvasi-db.

Used https://github.com/apache/spark/pull/42398 as a base. Thanks to @Hisoka-X for that pr!  

@beliefer @holdenk @khalidmammadov @HyukjinKwon Could you have a look at the PR since you reviewed this one https://github.com/apache/spark/pull/42398.

thanks, merging to master!

@LuciferYang Hi, this should fix the reattach execute suite failure in branch-3.5.

Merged to branch-3.5.

late LGTM

cc @dongjoon-hyun @HyukjinKwon  FYI\r\nThis is an issue that only exists in branch-3.5.

Feel free to merge when the PR passed the CI, @LuciferYang .

[5d60a74](https://github.com/apache/spark/pull/48744/commits/5d60a749180cf2d647e6b8bd08b86e2b9bd906e3)  is refer to https://github.com/apache/spark/pull/43501 and https://github.com/apache/spark/pull/43818/files 

> Feel free to merge when the PR passed the CI, @LuciferYang .\r\n\r\nOK

Merged into branch-3.5. Thanks @dongjoon-hyun ~

Merged to master. Thank you, @HyukjinKwon .

Could you review this PR, @LuciferYang ?

Merged into master for Spark 4.0. Thanks @dongjoon-hyun again ~

Thank you so much, @LuciferYang 

Let me backport this bug fix to `branch-3.5` (LTS) branch.

All tests passed.

Merged into master for Spark 4.0. Thanks @dongjoon-hyun 

Thank you, @LuciferYang 

For the record, this passed in Daily CI.\r\n- https://github.com/apache/spark/actions/runs/11653124333/job/32445167848\r\n\r\n<img width="650" alt="Screenshot 2024-11-04 at 14 01 40" src="https://github.com/user-attachments/assets/ac2b2fd8-2b7a-486e-876c-ea85ff8dc87e">\r\n

Could you review this when you have some time, @viirya ?

Merged to master for Apache Spark 4.0.0 on February 2025.

Added more test, @MaxGekk can we merge this?

@uros-db Are you ok w/ the changes?

Kind ping on this @MaxGekk, @uros-db.

in a follow-up, we should actually make `supportsTrimCollation = true` the default\r\n\r\nplease mention this intent in the PR description

@MaxGekk  can we check in this?

+1, LGTM. Merging to master.\r\nThank you, @jovanpavl-db and @uros-db @vladanvasi-db for review.

+1, LGTM. Merging to master.\r\nThank you, @vladimirg-db.

@stevomitric @stefankandic @cloud-fan please take a look

@cloud-fan can you please take a look at this PR?

Closing this because another approach was taken.

@dongjoon-hyun Could you review this PR?

Although we have still two failures, there are irrelevant to this test only PR . Let me merge this. Thank you, @xupefei .\r\n```\r\n[info] *** 2 TESTS FAILED ***\r\n[error] Failed tests:\r\n[error] \torg.apache.spark.sql.connect.service.SparkConnectSessionHolderSuite\r\n```

Merged to master. SPARK-49696 is assigned to you, @xupefei .\r\n\r\n<img width="263" alt="Screenshot 2024-11-02 at 15 55 20" src="https://github.com/user-attachments/assets/03023892-e3e1-4538-b1b3-1ec1b521dfc5">\r\n

> Merged to master. [SPARK-49696](https://issues.apache.org/jira/browse/SPARK-49696) is assigned to you, @xupefei .\r\n> \r\n> <img alt="Screenshot 2024-11-02 at 15 55 20" width="263" src="https://private-user-images.githubusercontent.com/9700541/382512484-03023892-e3e1-4538-b1b3-1ec1b521dfc5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA5MzI0ODYsIm5iZiI6MTczMDkzMjE4NiwicGF0aCI6Ii85NzAwNTQxLzM4MjUxMjQ4NC0wMzAyMzg5Mi1lM2UxLTQ1MzgtYjFiMy0xZWMxYjUyMWRmYzUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDZUMjIyOTQ2WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDY3NTZmMTFlOGMzYWM4MTBkMGUwNDM5NzMxNDNjNzE3Y2IxMjhmMGI5NTM5MTQzYmM1YTg1NGQyNWJmNzZkMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.crthuZFHDUQmd5mRy5zba3FEbCTwetekl3GP-jH1xe4">\r\n\r\nThank you. I\

Merged to master.

Merged to master.

Thank you @dongjoon-hyun and @HyukjinKwon !

cc @HyukjinKwon 

Could you review this, @xinrong-meng ?

Yeah, we have to change this.

Thank you, @HyukjinKwon !

Merged to master/3.5.

Merged to master.

Merged to master.

Could you check the pyspark unit test failures and Python linter failures, @ueshin ?

Merged to master.

Thank you, @viirya !

Thank you, @yaooqinn !

Merged to master/3.5.

cc - @jingz-db @HeartSaVioR - PTAL, thx !

https://github.com/anishshri-db/spark/actions/runs/11675450690/job/32510196280\r\n\r\n* Run / Build modules: pyspark-mllib, pyspark-ml, pyspark-ml-connect\r\n* Run / Run Docker integration tests\r\n\r\nThese failures are unrelated to this PR.

Thanks! Merging to master.

cc - @jerrypeng @HeartSaVioR - PTAL, thx !

Thanks! Merging to master.

Got it. Thank you for updating the PR description.

Merged to branch-3.5.

@HyukjinKwon @dongjoon-hyun It seems that the necessary testing pipeline was not triggered when this pr was merged:\r\n\r\n- https://github.com/apache/spark/actions/runs/11623531226/job/32370719419\r\n\r\n<img width="1028" alt="image" src="https://github.com/user-attachments/assets/b4323485-a518-4358-bc98-db4760bb4862">\r\n\r\nIt seems that this pr caused the failure of the branch-3.5 daily test:\r\n\r\n- https://github.com/apache/spark/actions/runs/11628383197\r\n- https://github.com/apache/spark/actions/runs/11642182106\r\n- https://github.com/apache/spark/actions/runs/11650793794\r\n\r\n<img width="1420" alt="image" src="https://github.com/user-attachments/assets/ff7da676-80da-46a2-9d70-82e6425120d5">\r\n\r\n\r\nThis issue can also be reproduced in local testing:\r\n\r\n```\r\ngit reset --hard 4205b795f893ef51441250cd48c094655ec42193 // before this one: [SPARK-50155][3.5] Move scala and java files to their default folders\r\nbuild/sbt clean "connect/testOnly org.apache.spark.sql.connect.execution.ReattachableExecuteSuite"\r\n\r\n[info] ReattachableExecuteSuite:\r\n[info] - reattach after initial RPC ends (2 seconds, 209 milliseconds)\r\n[info] - raw interrupted RPC results in INVALID_CURSOR.DISCONNECTED error (301 milliseconds)\r\n[info] - raw new RPC interrupts previous RPC with INVALID_CURSOR.DISCONNECTED error (40 milliseconds)\r\n[info] - client INVALID_CURSOR.DISCONNECTED error is retried when rpc sender gets interrupted (283 milliseconds)\r\n[info] - client INVALID_CURSOR.DISCONNECTED error is retried when other RPC preempts this one (435 milliseconds)\r\n[info] - abandoned query gets INVALID_HANDLE.OPERATION_ABANDONED error (37 milliseconds)\r\n[info] - client releases responses directly after consuming them (289 milliseconds)\r\n[info] - server releases responses automatically when client moves ahead (398 milliseconds)\r\n[info] - big query (1 second, 17 milliseconds)\r\n[info] - big query and slow client (7 seconds, 232 milliseconds)\r\n[info] - big query with frequent reattach (966 milliseconds)\r\n[info] - big query with frequent reattach and slow client (7 seconds, 703 milliseconds)\r\n[info] - long sleeping query (10 seconds, 145 milliseconds)\r\n[info] - Async cleanup callback gets called after the execution is closed (47 milliseconds)\r\n[info] Run completed in 34 seconds, 114 milliseconds.\r\n[info] Total number of tests run: 14\r\n[info] Suites: completed 1, aborted 0\r\n[info] Tests: succeeded 14, failed 0, canceled 0, ignored 0, pending 0\r\n[info] All tests passed.\r\n\r\ngit reset --hard 0229c0ea451c953e78cd273d9de2bc05962470a5 // this one: [SPARK-50176][CONNECT][3.5] Disallow reattaching after the session is closed\r\nbuild/sbt clean "connect/testOnly org.apache.spark.sql.connect.execution.ReattachableExecuteSuite"\r\n\r\n[info] - long sleeping query *** FAILED *** (67 milliseconds)\r\n[info]   org.apache.spark.sql.AnalysisException: [UNRESOLVED_ROUTINE] Cannot resolve function `sleep` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`].; line 1 pos 7\r\n[info]   at org.apache.spark.sql.connect.client.GrpcExceptionConverter$.$anonfun$errorFactory$2(GrpcExceptionConverter.scala:79)\r\n[info]   at org.apache.spark.sql.connect.client.GrpcExceptionConverter$.$anonfun$errorInfoToThrowable$2(GrpcExceptionConverter.scala:106)\r\n[info]   at scala.Option.map(Option.scala:230)\r\n[info]   at org.apache.spark.sql.connect.client.GrpcExceptionConverter$.errorInfoToThrowable(GrpcExceptionConverter.scala:104)\r\n[info]   at org.apache.spark.sql.connect.client.GrpcExceptionConverter$.toThrowable(GrpcExceptionConverter.scala:122)\r\n[info]   at org.apache.spark.sql.connect.client.GrpcExceptionConverter$.convert(GrpcExceptionConverter.scala:41)\r\n[info]   at org.apache.spark.sql.connect.client.GrpcExceptionConverter$$anon$1.hasNext(GrpcExceptionConverter.scala:52)\r\n[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.$anonfun$runQuery$2(SparkConnectServerTest.scala:235)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r\n[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r\n[info]   at org.scalatest.concurrent.TimeLimits$.failAfter(TimeLimits.scala:274)\r\n[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.$anonfun$runQuery$1(SparkConnectServerTest.scala:230)\r\n[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.$anonfun$runQuery$1$adapted(SparkConnectServerTest.scala:229)\r\n[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withClient(SparkConnectServerTest.scala:199)\r\n[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withClient$(SparkConnectServerTest.scala:191)\r\n[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.withClient(ReattachableExecuteSuite.scala:30)\r\n[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.runQuery(SparkConnectServerTest.scala:229)\r\n[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.runQuery$(SparkConnectServerTest.scala:228)\r\n[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.runQuery(ReattachableExecuteSuite.scala:30)\r\n[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.runQuery(SparkConnectServerTest.scala:259)\r\n[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.runQuery$(SparkConnectServerTest.scala:257)\r\n[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.runQuery(ReattachableExecuteSuite.scala:30)\r\n[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$44(ReattachableExecuteSuite.scala:373)\r\n[info]   at org.apache.spark.SparkFunSuite.withSparkEnvConfs(SparkFunSuite.scala:320)\r\n[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$43(ReattachableExecuteSuite.scala:372)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r\n[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r\n[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)\r\n[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)\r\n[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r\n[info]   at scala.collection.immutable.List.foreach(List.scala:431)\r\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r\n[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.Suite.run(Suite.scala:1114)\r\n[info]   at org.scalatest.Suite.run$(Suite.scala:1096)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n[info]   at java.lang.Thread.run(Thread.java:750)\r\n[info] - Async cleanup callback gets called after the execution is closed (44 milliseconds)\r\n[info] Run completed in 25 seconds, 431 milliseconds.\r\n[info] Total number of tests run: 15\r\n[info] Suites: completed 1, aborted 0\r\n[info] Tests: succeeded 14, failed 1, canceled 0, ignored 0, pending 0\r\n[info] *** 1 TEST FAILED ***\r\n[error] Failed tests:\r\n[error] \torg.apache.spark.sql.connect.execution.ReattachableExecuteSuite\r\n```\r\n\r\nCan you fix this issue? @changgyoopark-db 

We need to back-port the test change in https://github.com/apache/spark/commit/59e291d36c4a9d956b993968a324359b3d75fe5f

https://github.com/apache/spark/pull/48745 -> this should fix the issue - TEST ONLY.

Mind filing a JIRA please?

> Mind filing a JIRA please?\r\n\r\nThis is the JIRA ticket https://issues.apache.org/jira/browse/SPARK-49565

@HyukjinKwon this PR LGTM. If you agree would you mind to help merge it at your convenience? ðŸ™ 

cc @cloud-fan @gengliangwang this is ready to go if you guys agree :)

@jiashenC can you follow https://github.com/apache/spark/pull/48724/checks?check_run_id=32492121529 to set up Github Action so that it can run Spark tests?

There is a test failure in `PlanParserSuite`, @jiashenC can you take a look?

@jiashenC can you fix it? Otherwise I can take over the PR?

> @jiashenC can you fix it? Otherwise I can take over the PR?\r\n\r\nHey @dtenedor, sorry I was recently very occupied by some other things. I can fix this by the end of next week. If that is too late, please go ahead taking over this PR. 

@jiashenC sure, it just looks like the `PlanParserSuite` needs to be updated:\r\n\r\n```\r\n  test("hive-style single-FROM statement") {\r\n    assertEqual("from a select b, c", table("a").select($"b", $"c"))\r\n    assertEqual(\r\n      "from db.a select b, c where d < 1", table("db", "a").where($"d" < 1).select($"b", $"c"))\r\n    assertEqual("from a select distinct b, c", Distinct(table("a").select($"b", $"c")))\r\n\r\n    // Weird "FROM table" queries, should be invalid anyway\r\n    val sql1 = "from a"\r\n    checkError(\r\n      exception = parseException(sql1),\r\n      condition = "PARSE_SYNTAX_ERROR",\r\n      parameters = Map("error" -> "end of input", "hint" -> ""))\r\n\r\n    val sql2 = "from (from a union all from b) c select *"\r\n    checkError(\r\n      exception = parseException(sql2),\r\n      condition = "PARSE_SYNTAX_ERROR",\r\n      parameters = Map("error" -> "\

+1 for the revised test code..\r\n\r\nCould you make a PR while keeping the original code and adding your test code change as an additional commit, @dtenedor ? Then, I can keep the authorships (@jiashenC as the lead author, @dtenedor as a co-author) while merging your new PR.

@dongjoon-hyun sure, I can help with this. Thanks for your reviews + help so far.

Thank you for helping this, @dtenedor .

Here is a copy of this PR with the test case fixed: https://github.com/apache/spark/pull/49120.

This is resolved via https://github.com/apache/spark/pull/49120 .

cc @yaooqinn 

Thank you for handling all the upstream works and approving this, @yaooqinn .

Merged to master for Apache Spark 4.0.0 on February 2025.

Merged to master for Apache Spark 4.0.0 on February 2025.\r\n\r\nThank you, @zhengruifeng .

Late LGTM, thank you!

Let me update the benchmark result of `CollationBenchmark` and `CollationNonASCIIBenchmark`\r\n\r\n- `CollationBenchmark`\r\nJDK 17: https://github.com/panbingkun/spark/actions/runs/11611219685\r\nJDK 21: https://github.com/panbingkun/spark/actions/runs/11611222464\r\n\r\n- `CollationNonASCIIBenchmark`\r\nJDK 17: https://github.com/panbingkun/spark/actions/runs/11611241052\r\nJDK 21: https://github.com/panbingkun/spark/actions/runs/11611243469\r\n

@uros-db @dongjoon-hyun @stefankandic @MaxGekk \r\nThe detailed explanation has been updated, this PR is ready for review. \r\nThank you very much for the review, if you has free time. â¤ï¸

- branch-50189\r\n  - `org.apache.spark.sql.execution.benchmark.CollationBenchmark`\r\n     JDK 17: https://github.com/panbingkun/spark/actions/runs/11656707625\r\n     JDK 21: https://github.com/panbingkun/spark/actions/runs/11656709727\r\n\r\n  - `org.apache.spark.sql.execution.benchmark.CollationNonASCIIBenchmark`\r\n    JDK 17: https://github.com/panbingkun/spark/actions/runs/11656712196\r\n    JDK 21: https://github.com/panbingkun/spark/actions/runs/11656714145\r\n\r\n- master\r\n  - `org.apache.spark.sql.execution.benchmark.CollationBenchmark`\r\n    JDK 17: https://github.com/panbingkun/spark/actions/runs/11660016606\r\n    JDK 21: https://github.com/panbingkun/spark/actions/runs/11660017905\r\n\r\n  - `org.apache.spark.sql.execution.benchmark.CollationNonASCIIBenchmark`\r\n    JDK 17: https://github.com/panbingkun/spark/actions/runs/11660023396\r\n    JDK 21: https://github.com/panbingkun/spark/actions/runs/11660024905

<img width="1366" alt="image" src="https://github.com/user-attachments/assets/2be85af2-e5f4-4e74-a5bf-2d912a344a90">\r\n

To @MaxGekk and all. \r\n- SPARK-50216 (`Investigate UTF8_BINARY regression`) is filed as a blocker issue for Apache Spark 4.0.0.

Merged to master for Apache Spark 4.0.0 on February 2025.\r\n\r\nThank you, @panbingkun , @MaxGekk , @uros-db .

> To @MaxGekk and all.\r\n> \r\n> * [SPARK-50216](https://issues.apache.org/jira/browse/SPARK-50216) (`Investigate UTF8_BINARY regression`) is filed as a blocker issue for Apache Spark 4.0.0.\r\n\r\nThanks!

cc @zhengruifeng @HyukjinKwon @LuciferYang 

cc @hvanhovell @grundprinzip 

> Generally, a useful change. Can you please update the Python client as well.\r\n\r\nSure, allow me to do it, thank you very much for the review! â¤ï¸

> Generally, a useful change. Can you please update the Python client as well.\r\n\r\nDone.

A high-level question: what about introducing a general method like `getProperty(key: String): String`, then we can reuse it to expose more similar properties?

> A high-level question: what about introducing a general method like `getProperty(key: String): String`, then we can reuse it to expose more similar properties?\r\n\r\nIs this adding this method for `PB` protocol?

LGTM

> +1 for the idea. Could you rebase this once more, @panbingkun ?\r\n\r\nSorry, I have been struggling with a new job recently,\r\nand it has been updated!\r\nThanks.

Merged to master.

I will monitor and followup the build.

Could you review this PR, @LuciferYang ?

Thank you, @LuciferYang .

Thank you, @yaooqinn !

Since this is a trivial renaming in test resources and one example file, let me merge this. Thank you again.\r\nMerged to master.

+1, LGTM

Thank you, @panbingkun .

Thanks to @HyukjinKwon , today I will add the PR description.\r\n\r\n

Merged to master.

thanks @HyukjinKwon 

LGTM thank you!

Merged to master.

Please note that. This depends on the following to provide the simplest example.\r\n- #48711

Thank you, @zhengruifeng and @yaooqinn .\r\nMerged to master for Apache Spark 4.0.0 on February 2025.

cc @HyukjinKwon 

Thank you so much, @zhengruifeng and @yaooqinn .\r\nMerged to master for Apache Spark 4.0.0 on February 2025.

All `core` tests passed. \r\n\r\nCould you review this PR, @viirya ?

Thank you, @viirya !

Thank you, @yaooqinn !

Merged to master for Apache Spark 4.0.0 on February 2025.

cc - @ericm-db @HeartSaVioR - PTAL, thx !

Looks like the build failure is valid.

https://github.com/anishshri-db/spark/actions/runs/11692726936/job/32566257480\r\nAll green.

Thanks! Merging to master.

Could you review this, please, @xinrong-meng ?

LGTM thank you!

Thank you so much, @xinrong-meng and @zhengruifeng .\r\nMerged to master for Apache Spark 4.0.0 on February 2025.

Could you review this PR, @huaxingao ?

Thank you, @huaxingao !

Merged to master for Apache Spark 4.0.0 on February 2025.

@MaxGekk tests passed, one unrelated suite failed\r\n![image](https://github.com/user-attachments/assets/ccbaf949-1689-4071-9970-596f066e88c0)\r\n

+1, LGTM. Merging to master.\r\nThank you, @vladimirg-db.

Merged to master.

Nice work!\r\n\r\nJust wanted to add that PySparkHistogramPlotBase.get_bins can also benefit from this change to remove numpy dependency later.

LGTM, thank you!

According to the commit list, it looks safe. Please let me know when the PR is ready, @LuciferYang .

> According to the commit list, it looks safe. Please let me know when the PR is ready, @LuciferYang .\r\n\r\nready ~

Merged to master~

LGTM, thank you!

We might need to retrigger CI though.

thanks @xinrong-meng , merged to master

- RegExpReplaceBenchmark\r\n```scala\r\nobject RegExpReplaceBenchmark extends SqlBasedBenchmark {\r\n  private val N = 1000000 // 10_000_00\r\n  private val M = 100\r\n\r\n  private val df = spark.range(N).to(new StructType().add("id", "int")).\r\n    withColumn("subject", concat(col("id"), lit("-"), col("id") * 2)).\r\n    withColumn("regexp", lit("(\\\\d+)")).\r\n    withColumn("rep", lit("num"))\r\n\r\n  private def doBenchmark(): Unit = {\r\n    df.selectExpr("regexp_replace(subject, regexp, rep)").noop()\r\n  }\r\n\r\n  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\r\n    runBenchmark("regexp_replace") {\r\n      val benchmark = new Benchmark("regexp_replace", N, output = output)\r\n      benchmark.addCase("optimize", M) { _ =>\r\n        doBenchmark()\r\n      }\r\n      benchmark.run()\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n- Before (Run `3` times)\r\n```shell\r\nRunning benchmark: regexp_replace\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 40159 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.1\r\nApple M2\r\nregexp_replace:                           Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            389            402          12          2.6         389.4       1.0X\r\n\r\n\r\nRunning benchmark: regexp_replace\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 41644 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.1\r\nApple M2\r\nregexp_replace:                           Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            406            416          14          2.5         405.5       1.0X\r\n\r\n\r\nRunning benchmark: regexp_replace\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 39469 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.1\r\nApple M2\r\nregexp_replace:                           Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            385            395          11          2.6         385.0       1.0X\r\n\r\n```\r\n\r\n- After (Run `3` times)\r\n```\r\nRunning benchmark: regexp_replace\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 35651 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.1\r\nApple M2\r\nregexp_replace:                           Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            341            357          23          2.9         340.9       1.0X\r\n\r\n\r\nRunning benchmark: regexp_replace\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 37869 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.1\r\nApple M2\r\nregexp_replace:                           Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            368            379          12          2.7         367.8       1.0X\r\n\r\n\r\nRunning benchmark: regexp_replace\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 34783 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.1\r\nApple M2\r\nregexp_replace:                           Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            328            348          16          3.0         328.2       1.0X\r\n```

@panbingkun I think the benchmark is useful in catching regressions in the expression. Could you add it in?

> @panbingkun I think the benchmark is useful in catching regressions in the expression. Could you add it in?\r\n\r\nAllow me to do it.

The running result of `org.apache.spark.sql.execution.benchmark.StringFunctionsBenchmark`\r\nJDK 17: https://github.com/panbingkun/spark/actions/runs/11604887252\r\nJDK 21: https://github.com/panbingkun/spark/actions/runs/11604890283

> @panbingkun I think the benchmark is useful in catching regressions in the expression. Could you add it in?\r\n\r\nUpdated.

Merged to master.

Thanks all @MaxGekk @dongjoon-hyun @yaooqinn @HyukjinKwon @zhengruifeng @zeal-thinker ! â¤ï¸

+1, LGTM. Merging to master.\r\nThank you, @xinrong-meng and @zhengruifeng for review.

Thank you @MaxGekk ! : )

cc @dongjoon-hyun @MaxGekk 

+1, LGTM. Merging to 3.5.\r\nThank you, @panbingkun.

cc @cloud-fan @MaxGekk 

> > How was this patch tested?\r\n> > Pass GA.\r\n> \r\n> @panbingkun Just in case, there are checks in tests for all cases? otherwise "Pass GA" means nothing.\r\n\r\nOkay, let me add something.

> > How was this patch tested?\r\n> > Pass GA.\r\n> \r\n> @panbingkun Just in case, there are checks in tests for all cases? otherwise "Pass GA" means nothing.\r\n\r\nDone.

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

LGTM

cc @dongjoon-hyun @LuciferYang 

Merged to master. Thank you, @panbingkun 

@siying, I discussed this with @anishshri-db but it will be difficult to propagate this uniqueID across `load` and `commit`/`commitAsync` calls (so that the `load` acquire pass the id to the `commit` release). This will likely need a higher-level API change which we can consider for a larger long-term fix.

@liviazhu-db - i am not sure we need the latest commit. should we roll back to the prev version ?

Looks like CI only failed with docker integration which is unrelated.\r\nhttps://github.com/liviazhu-db/spark/runs/32409391731

Thanks! Merging to master.

Could you review this, @LuciferYang ?

Merged into mater for Spark 4.0. Thanks @dongjoon-hyun 

Thank you so much, @LuciferYang !

Hi! @HeartSaVioR, I skimmed the history to find someone who might be able to review this change and came across you. Would you have time to have a look?

Thank you, @zhengruifeng and @MaxGekk .\r\nMerged to master for Apache Spark 4.0.0 on February 2025.

@dongjoon-hyun Please take a look again. Thanks.

How much does it get improved?

Does the benchmark result say `session.conf` is slower than `SQLConf.get`?

> Does the benchmark result say `session.conf` is slower than `SQLConf.get`?\r\n\r\n`session.conf` is quicker than `SQLConf.get`.

@cloud-fan `override def conf` finished, do you have any other comments?

@dongjoon-hyun @HyukjinKwon The GA failure seems unrelated. Please take a look again.

@LuciferYang @dongjoon-hyun @cloud-fan @HyukjinKwon Thank you all.

+1, LGTM. Merging to master.\r\nThank you, @itholic.

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

Thank you, @panbingkun and @MaxGekk . \r\n\r\nCan we have this fix in branch-3.5, too?\r\n\r\nBTW, to @panbingkun , this seems not `[INFRA]` category.

> Thank you, @panbingkun and @MaxGekk .\r\n> \r\n> Can we have this fix in branch-3.5, too?\r\n> \r\n> BTW, to @panbingkun , this seems not `[INFRA]` category.\r\n\r\nBranch-3.5: https://github.com/apache/spark/pull/48700

also cc @yaooqinn 

Ack. Thank you, @zhengruifeng . Have a safe travel~

I have some time in next 2 days, will keep an eye on branch-3.5 jobs.\r\n\r\nThanks all for the reviews, merging to master

It seems `branch-3.5` is fine this time, I will investigate other dependencies later\r\n\r\nhttps://github.com/apache/spark/actions/runs/11702277950/job/32609733134\r\nhttps://github.com/apache/spark/actions/runs/11702265393/job/32590131747

[Run / Build modules: sql - other tests](https://github.com/itholic/spark/actions/runs/11603964645/job/32311941897#logs) passed on the previous commit (the last one is just rebasing).\r\n\r\n+1, LGTM. Merging to master.\r\nThank you, @itholic.

cc @cloud-fan @MaxGekk 

+1, LGTM. Merging to master.\r\nThank you, @panbingkun and @HyukjinKwon for review.

Thanks all @MaxGekk @HyukjinKwon @dongjoon-hyun â¤ï¸ !

@jingz-db - can u check if the lint failure is related ?

https://github.com/jingz-db/spark/actions/runs/11787064120/job/32835291681\r\nNow the CI is green. @jingz-db Thanks for fixing this!

Thanks! Merging to master.

Thanks! Merging to master.

Could you review this PR, @attilapiros ?

Could you review this PR, @viirya ?

Thank you, @attilapiros !

Merged to branch-3.5.

Thank you, @viirya , too.

After a minor import statement merging, I verified this manually.\r\n```scala\r\n- import java.nio.file.Files\r\n- import java.nio.file.Paths\r\n+ import java.nio.file.{Files, Paths}\r\n```\r\n\r\n```\r\n[info] BlockManagerDecommissionIntegrationSuite:\r\n[info] - SPARK-32850: BlockManager decommission should respect the configuration (enabled=false) (8 seconds, 477 milliseconds)\r\n[info] - SPARK-32850: BlockManager decommission should respect the configuration (enabled=true) (7 seconds, 182 milliseconds)\r\n[info] - verify that an already running task which is going to cache data succeeds on a decommissioned executor after task start (11 seconds, 871 milliseconds)\r\n[info] - verify that an already running task which is going to cache data succeeds on a decommissioned executor after one task ends but before job ends (2 seconds, 884 milliseconds)\r\n[info] - verify that shuffle blocks are migrated (3 seconds, 504 milliseconds)\r\n[info] - verify that both migrations can work at the same time (3 seconds, 376 milliseconds)\r\n[info] - SPARK-36782 not deadlock if MapOutput uses broadcast (3 seconds, 362 milliseconds)\r\n[info] - SPARK-46957: Migrated shuffle files should be able to cleanup from executor (32 seconds, 227 milliseconds)\r\n[info] Run completed in 1 minute, 13 seconds.\r\n[info] Total number of tests run: 8\r\n[info] Suites: completed 1, aborted 0\r\n[info] Tests: succeeded 8, failed 0, canceled 0, ignored 0, pending 0\r\n[info] All tests passed.\r\n[success] Total time: 82 s (01:22), completed Oct 28, 2024, 9:01:32\u202fPM\r\n```\r\n\r\nMerged to master for Apache Spark 4.0.0 on February 2025.

Could you review this PR, @attilapiros ?

Could you review this PR, @viirya ?

Thank you, @viirya !\r\n\r\nMerged to master for Apache Spark 4.0.0.

cc  @MaxGekk @cloud-fan 

+1, LGTM. Merging to master.\r\nThank you, @panbingkun and @cloud-fan for review.

BTW, is this a subtask of SPARK-44728 , @zhengruifeng ?

@dongjoon-hyun thanks for the review, let me add recent PRs to [SPARK-44728](https://issues.apache.org/jira/browse/SPARK-44728)

Thank you, @zhengruifeng !

Type hints failed weirdly as https://github.com/xinrong-meng/spark/actions/runs/11812734311/job/32908478871, ignoring `#type: ignore` comments.\r\nRebased master

Merged to master.

cc. @zsxwing @brkyvz @viirya Appreciated for reviewing. Thanks in advance.\r\ncc. @cloud-fan as well, since this PR changes the co-used logical/physical nodes between batch and streaming sources.

also cc @HyukjinKwon @dongjoon-hyun @HeartSaVioR @bogao007 FYI

Thanks @pan3793 ~

BTW, to @LuciferYang , we need to wait for [SPARK-48755](https://issues.apache.org/jira/browse/SPARK-48755) people as you pinged already.

> According to the PR description, this is a regression due to [SPARK-48755](https://issues.apache.org/jira/browse/SPARK-48755) ?\r\n> \r\n> * [[SPARK-48755][SS][PYTHON] transformWithState pyspark base implementation and ValueState support\xa0#47133](https://github.com/apache/spark/pull/47133)\r\n> \r\n> Thank you for discovering this regression and fixing before Apache Spark 4.0.0 release, @LuciferYang .\r\n\r\nYes

> BTW, to @LuciferYang , we need to wait for [SPARK-48755](https://issues.apache.org/jira/browse/SPARK-48755) people as you pinged already.\r\n\r\nGot it ~

Gentle ping once more, @bogao007 and @HeartSaVioR , because the change is reported at https://github.com/apache/spark/pull/47133 .

Merged into master for Spark 4.0. Thanks @pan3793 @dongjoon-hyun @HeartSaVioR and @bogao007 ~

Could you review this PR, @LuciferYang ? There is a dependably alert on this.\r\n- https://github.com/apache/spark/security/dependabot/105

The CI is running here.\r\n- https://github.com/dongjoon-hyun/spark/actions/runs/11548078997

@dongjoon-hyun a failed test case in k8s it. Shall we retrigger it?\r\n\r\n```\r\n[info] - A Spark job with two executors with OnDemand PVC volumes *** FAILED *** (24 seconds, 931 milliseconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 2 times over 11.106691292999999 seconds. Last failure message: An error has occurred.. (VolumeSuite.scala:32)\r\n[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite.eventually(KubernetesSuite.scala:45)\r\n[info]   at org.apache.spark.deploy.k8s.integrationtest.VolumeSuite.checkDisk(VolumeSuite.scala:32)\r\n[info]   at org.apache.spark.deploy.k8s.integrationtest.VolumeSuite.$anonfun$$init$$24(VolumeSuite.scala:169)\r\n[info]   at org.apache.spark.deploy.k8s.integrationtest.VolumeSuite.$anonfun$$init$$24$adapted(VolumeSuite.scala:167)\r\n[info]   at org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite.$anonfun$runSparkApplicationAndVerifyCompletion$12(KubernetesSuite.scala:488)\r\n[info]   at org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite.$anonfun$runSparkApplicationAndVerifyCompletion$12$adapted(KubernetesSuite.scala:488)\r\n[info]   at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\r\n[info]   at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\r\n[info]   at scala.collection.AbstractIterable.foreach(Iterable.scala:935)\r\n[info]   at org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite.runSparkApplicationAndVerifyCompletion(KubernetesSuite.scala:488)\r\n[info]   at org.apache.spark.deploy.k8s.integrationtest.VolumeSuite.$anonfun$$init$$22(VolumeSuite.scala:160)\r\n```

Thank you, @yaooqinn and @LuciferYang .\r\n\r\nTo @LuciferYang , sure. I re-triggered it.

All tests passed. Merged to master for Apache Spark 4.0.0 on February 2025.

cc @LuciferYang 

I verified that this works, @LuciferYang . Could you review this please?

Also, cc @viirya . Could you help me by reviewing this?

Could you review this PR, @zhengruifeng ?

Thank you so much, @zhengruifeng !

late LGTM

Thank you for reporting and reviewing, @LuciferYang .

late LGTM

Thank you, @panbingkun and @viirya .

For the record, Maven CI becomes healthy back after this PR.\r\n- https://github.com/apache/spark/actions/workflows/build_maven.yml\r\n  - https://github.com/apache/spark/actions/runs/11554580358\r\n- https://github.com/apache/spark/actions/workflows/build_maven_java21.yml\r\n  - https://github.com/apache/spark/actions/runs/11555608079

thanks @dongjoon-hyun , merged to master

Could you ping the author of `zstd-jni` to get the confirmation?

Never mind. I filed an issue and pinged you.\r\n- https://github.com/luben/zstd-jni/issues/329

Hi @luben, sorry to bother you\r\nCan you help confirm if the `1.5.6-7` version of  `zstd-jni` is officially released?\r\n- Because I can observe the following information:\r\nhttps://mvnrepository.com/artifact/com.github.luben/zstd-jni/1.5.6-7\r\n<img width="488" alt="image" src="https://github.com/user-attachments/assets/a38e68a2-d8b2-4377-9523-d85e2b86449d">\r\n\r\n- However, it does not have a tag description:\r\nhttps://github.com/luben/zstd-jni/tags\r\n\r\nThank you very much! â¤ï¸

> Never mind. I filed an issue and pinged you.\r\n> \r\n> * [v1.5.6-7 is released officially?\xa0luben/zstd-jni#329](https://github.com/luben/zstd-jni/issues/329)\r\n\r\nThanks ! â¤ï¸

Thank you for the confirmation, @luben .\r\n\r\nThen, merged to master for Apache Spark 4.0.0 on February 2025.

Thank you, @zhengruifeng .

Since the removed code is irrelevant to the PR builder of `master` branch, let me merge this as a part of post-Apache-Spark-3.4.4-release manager work. Thank you again\r\n\r\nMerged to master.

Merged to master.

@LuciferYang @wangyum @sunchao could you please review this PR?

> It looks good to me. Is the PR ready, @panbinkun ?\n\nYep,thanks. â¤ï¸

It looks good to me, @panbingkun . Is the PR ready?

> It looks good to me, @panbingkun . Is the PR ready?\n\nYep,thanks. â¤ï¸

Surprisingly this caused the test failures in Spark Connect specifically for Mac .... \r\n\r\n\r\n```\r\n/python/run-tests --python-executables=python3 --testnames "pyspark.sql.tests.connect.test_connect_collection"\r\n```\r\n\r\nWith this PR:\r\n\r\n```\r\ngoogle.protobuf.runtime_version.VersionError: Detected incompatible Protobuf Gencode/Runtime versions when loading spark/connect/base.proto: gencode 5.28.3 runtime 5.28.2. Runtime version cannot be older than the linked gencode version. See Protobuf version guarantees at https://protobuf.dev/support/cross-version-runtime-guarantee.\r\n```\r\n\r\nWithout this PR:\r\n\r\n```\r\nRunning PySpark tests. Output is in /.../spark/python/unit-tests.log\r\nWill test against the following Python executables: [\

thanks @HyukjinKwon for addressing this issue. Python developers had been blocked for 2 weeks. \r\n\r\nIIRC, this is not the first time that some Python tests only fails with MacOS.\r\nThis kind of issue is hard to fix, I actually attempted to resolve it by reverting some suspicious PRs and comparing with versions of OSS GA, but failed to find the root cause.\r\n\r\nIs it possible to add a lightweight MacOS GA job to guard basic PySpark functionality? @dongjoon-hyun @Yikun @LuciferYang \r\n\r\n

> Surprisingly this caused the test failures in Spark Connect specifically for Mac ....\r\n> \r\n> ```\r\n> /python/run-tests --python-executables=python3 --testnames "pyspark.sql.tests.connect.test_connect_collection"\r\n> ```\r\n> \r\n> With this PR:\r\n> \r\n> ```\r\n> google.protobuf.runtime_version.VersionError: Detected incompatible Protobuf Gencode/Runtime versions when loading spark/connect/base.proto: gencode 5.28.3 runtime 5.28.2. Runtime version cannot be older than the linked gencode version. See Protobuf version guarantees at https://protobuf.dev/support/cross-version-runtime-guarantee.\r\n> ```\r\n> \r\n> Without this PR:\r\n> \r\n> ```\r\n> Running PySpark tests. Output is in /.../spark/python/unit-tests.log\r\n> Will test against the following Python executables: [\

Thank you for the head-up and recovery.

Are all current PySpark tests run in a container environment? Even if the os is specified as mac-os, are the existing images still based on Ubuntu? Or is this Python-Only on macOS joob supposed to run in a physical machine environment?

I think I have identified this issue and will submit a new PR this afternoon to solve it.

> Are all current PySpark tests run in a container environment? Even if the os is specified as mac-os, are the existing images still based on Ubuntu? Or is this Python-Only on macOS joob supposed to run in a physical machine environment?\r\n\r\nyes, the are always container job.\r\nwe should add a MacOS non-container job for this purpose, or make a MacOS image instead of Ubuntu image

- When we use `sbt` to compile the `spark project`, in directory `assembly/target/scala-2.13/jars/`, we will find `netty related dependencies`, some are version `4.1.110.Final`, but other are version `4.1.113.Final`, as follow:\r\n```shell\r\n(pyspark) âžœ  jars git:(master) âœ— ls -1 netty-*4*\r\nnetty-all-4.1.110.Final.jar\r\nnetty-buffer-4.1.113.Final.jar\r\nnetty-codec-4.1.113.Final.jar\r\nnetty-codec-http-4.1.110.Final.jar\r\nnetty-codec-http2-4.1.110.Final.jar\r\nnetty-codec-socks-4.1.110.Final.jar\r\nnetty-common-4.1.113.Final.jar\r\nnetty-handler-4.1.113.Final.jar\r\nnetty-handler-proxy-4.1.110.Final.jar\r\nnetty-resolver-4.1.113.Final.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-linux-aarch_64.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-linux-x86_64.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-osx-aarch_64.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-osx-x86_64.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-windows-x86_64.jar\r\nnetty-transport-4.1.113.Final.jar\r\nnetty-transport-classes-epoll-4.1.113.Final.jar\r\nnetty-transport-classes-kqueue-4.1.110.Final.jar\r\nnetty-transport-native-epoll-4.1.113.Final-linux-aarch_64.jar\r\nnetty-transport-native-epoll-4.1.113.Final-linux-riscv64.jar\r\nnetty-transport-native-epoll-4.1.113.Final-linux-x86_64.jar\r\nnetty-transport-native-kqueue-4.1.110.Final-osx-aarch_64.jar\r\nnetty-transport-native-kqueue-4.1.110.Final-osx-x86_64.jar\r\nnetty-transport-native-unix-common-4.1.113.Final.jar\r\n```\r\n\r\n\r\nand `./python/run-tests --python-executables=python3 --testnames "pyspark.sql.tests.connect.test_connect_collection"` will failed.\r\n```shell\r\n======================================================================\r\nERROR [0.002s]: test_to_pandas (pyspark.sql.tests.connect.test_connect_collection.SparkConnectCollectionTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/tests/connect/test_connect_collection.py", line 108, in test_to_pandas\r\n    self.connect.sql(query).toPandas(),\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/connect/session.py", line 753, in sql\r\n    data, properties, ei = self.client.execute_command(cmd.command(self._client))\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/connect/client/core.py", line 1109, in execute_command\r\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/connect/client/core.py", line 1517, in _execute_and_fetch\r\n    for response in self._execute_and_fetch_as_iterator(\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/connect/client/core.py", line 1494, in _execute_and_fetch_as_iterator\r\n    self._handle_error(error)\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/connect/client/core.py", line 1764, in _handle_error\r\n    self._handle_rpc_error(error)\r\n  File "/Users/panbingkun/Developer/spark/spark-community/python/pyspark/sql/connect/client/core.py", line 1849, in _handle_rpc_error\r\n    raise SparkConnectGrpcException(str(rpc_error)) from None\r\npyspark.errors.exceptions.connect.SparkConnectGrpcException: <_MultiThreadedRendezvous of RPC that terminated with:\r\n\tstatus = StatusCode.INTERNAL\r\n\tdetails = "Encountered end-of-stream mid-frame"\r\n\tdebug_error_string = "UNKNOWN:Error received from peer  {created_time:"2024-11-06T16:13:26.71221+08:00", grpc_status:13, grpc_message:"Encountered end-of-stream mid-frame"}"\r\n>\r\n\r\n----------------------------------------------------------------------\r\nRan 9 tests in 12.791s\r\n\r\nFAILED (errors=5)\r\n\r\nGenerating XML reports...\r\nGenerated XML report: target/test-reports/TEST-pyspark.sql.tests.connect.test_connect_collection.SparkConnectCollectionTests-20241106161316.xml\r\nGenerated XML report: target/test-reports/TEST-pyspark.sql.tests.connect.test_connect_basic.SparkConnectSQLTestCase-20241106161316.xml\r\n\r\nHad test failures in pyspark.sql.tests.connect.test_connect_collection with python3; see logs.\r\n```\r\n\r\n- When we use `maven` to compile the `spark project`, in directory `assembly/target/scala-2.13/jars`, we will find `netty related dependencies`,  all are version `4.1.110.Final`, as follow:\r\n```shell\r\n(pyspark) âžœ  jars git:(master) âœ— ls -1 netty-*4*\r\nnetty-all-4.1.110.Final.jar\r\nnetty-buffer-4.1.110.Final.jar\r\nnetty-codec-4.1.110.Final.jar\r\nnetty-codec-http-4.1.110.Final.jar\r\nnetty-codec-http2-4.1.110.Final.jar\r\nnetty-codec-socks-4.1.110.Final.jar\r\nnetty-common-4.1.110.Final.jar\r\nnetty-handler-4.1.110.Final.jar\r\nnetty-handler-proxy-4.1.110.Final.jar\r\nnetty-resolver-4.1.110.Final.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-linux-aarch_64.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-linux-x86_64.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-osx-aarch_64.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-osx-x86_64.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-windows-x86_64.jar\r\nnetty-transport-4.1.110.Final.jar\r\nnetty-transport-classes-epoll-4.1.110.Final.jar\r\nnetty-transport-classes-kqueue-4.1.110.Final.jar\r\nnetty-transport-native-epoll-4.1.110.Final-linux-aarch_64.jar\r\nnetty-transport-native-epoll-4.1.110.Final-linux-riscv64.jar\r\nnetty-transport-native-epoll-4.1.110.Final-linux-x86_64.jar\r\nnetty-transport-native-kqueue-4.1.110.Final-osx-aarch_64.jar\r\nnetty-transport-native-kqueue-4.1.110.Final-osx-x86_64.jar\r\nnetty-transport-native-unix-common-4.1.110.Final.jar\r\n```\r\n\r\nAnd `./python/run-tests --python-executables=python3 --testnames "pyspark.sql.tests.connect.test_connect_collection"` will succeed.\r\n```shell\r\n(pyspark) âžœ  spark-community git:(master) âœ— ./python/run-tests --python-executables=python3 --testnames "pyspark.sql.tests.connect.test_connect_collection"\r\nRunning PySpark tests. Output is in /Users/panbingkun/Developer/spark/spark-community/python/unit-tests.log\r\nWill test against the following Python executables: [\

```\r\n<exclusion> \r\n       <groupId>org.jboss.netty</groupId> \r\n       <artifactId>netty</artifactId> \r\n     </exclusion> \r\n```\r\n\r\nHere, what is excluded is the dependency on netty 3.x. The groupId for netty 3.x and 4.x is different.

Below is the dependency exclusion for `netty...`\r\n> ```\r\n> <exclusion> \r\n>        <groupId>org.jboss.netty</groupId> \r\n>        <artifactId>netty</artifactId> \r\n>      </exclusion> \r\n> ```\r\n> \r\n> Here, what is excluded is the dependency on netty 3.x. The groupId for netty 3.x and 4.x is different.\r\n\r\n<img width="802" alt="image" src="https://github.com/user-attachments/assets/103ce95c-9fb6-464a-bd2b-00e2ff82cfe1">\r\n

A new pr for it: https://github.com/apache/spark/pull/48771

we should change to exclude \r\n\r\n```\r\n<exclusion> \r\n       <groupId>io.netty</groupId> \r\n       <artifactId>*</artifactId> \r\n     </exclusion> \r\n```\r\n\r\nfrom zookeeper instead of\r\n\r\n```\r\n<exclusion> \r\n       <groupId>org.jboss.netty</groupId> \r\n       <artifactId>netty</artifactId> \r\n     </exclusion> \r\n```\r\n\r\n\r\n`org.jboss.netty` is the groupId of netty 3.x

> we should change to exclude\r\n> \r\n> ```\r\n> <exclusion> \r\n>        <groupId>io.netty</groupId> \r\n>        <artifactId>*</artifactId> \r\n>      </exclusion> \r\n> ```\r\n> \r\n> from zookeeper instead of\r\n> \r\n> ```\r\n> <exclusion> \r\n>        <groupId>org.jboss.netty</groupId> \r\n>        <artifactId>netty</artifactId> \r\n>      </exclusion> \r\n> ```\r\n> \r\n> `org.jboss.netty` is the groupId of netty 3.x\r\n\r\nUnfortunately, this issue still exists.\r\n\r\n<img width="560" alt="image" src="https://github.com/user-attachments/assets/8a3ec825-8b7e-4a8f-8ec7-e7d509155f24">\r\n\r\n```shell\r\n(pyspark) âžœ  spark-community git:(master) âœ— cd assembly/target/scala-2.13/jars\r\n(pyspark) âžœ  jars git:(master) âœ— ls -1 netty-*\r\nnetty-all-4.1.110.Final.jar\r\nnetty-buffer-4.1.113.Final.jar\r\nnetty-codec-4.1.113.Final.jar\r\nnetty-codec-http-4.1.110.Final.jar\r\nnetty-codec-http2-4.1.110.Final.jar\r\nnetty-codec-socks-4.1.110.Final.jar\r\nnetty-common-4.1.113.Final.jar\r\nnetty-handler-4.1.113.Final.jar\r\nnetty-handler-proxy-4.1.110.Final.jar\r\nnetty-resolver-4.1.113.Final.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-linux-aarch_64.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-linux-x86_64.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-osx-aarch_64.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-osx-x86_64.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final-windows-x86_64.jar\r\nnetty-tcnative-boringssl-static-2.0.66.Final.jar\r\nnetty-tcnative-classes-2.0.66.Final.jar\r\nnetty-transport-4.1.113.Final.jar\r\nnetty-transport-classes-epoll-4.1.113.Final.jar\r\nnetty-transport-classes-kqueue-4.1.110.Final.jar\r\nnetty-transport-native-epoll-4.1.113.Final-linux-aarch_64.jar\r\nnetty-transport-native-epoll-4.1.113.Final-linux-riscv64.jar\r\nnetty-transport-native-epoll-4.1.113.Final-linux-x86_64.jar\r\nnetty-transport-native-kqueue-4.1.110.Final-osx-aarch_64.jar\r\nnetty-transport-native-kqueue-4.1.110.Final-osx-x86_64.jar\r\nnetty-transport-native-unix-common-4.1.113.Final.jar\r\n```

> > we should change to exclude\r\n> > ```\r\n> > <exclusion> \r\n> >        <groupId>io.netty</groupId> \r\n> >        <artifactId>*</artifactId> \r\n> >      </exclusion> \r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > from zookeeper instead of\r\n> > ```\r\n> > <exclusion> \r\n> >        <groupId>org.jboss.netty</groupId> \r\n> >        <artifactId>netty</artifactId> \r\n> >      </exclusion> \r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > `org.jboss.netty` is the groupId of netty 3.x\r\n> \r\n> Unfortunately, this issue still exists.\r\n> \r\n> <img alt="image" width="560" src="https://private-user-images.githubusercontent.com/15246973/383474575-8a3ec825-8b7e-4a8f-8ec7-e7d509155f24.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA4ODQwNTQsIm5iZiI6MTczMDg4Mzc1NCwicGF0aCI6Ii8xNTI0Njk3My8zODM0NzQ1NzUtOGEzZWM4MjUtOGI3ZS00YThmLThlYzctZTdkNTA5MTU1ZjI0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMDYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTA2VDA5MDIzNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU2MzA3NTc5NjdiNTFlM2ExODI3YzFhNmMxMmUzZTk0NTdjNWVkYzczMjQ5NTI0N2IxMjJjZjBmNDA5NGE3NWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.LMvsgdv6phrE2BB_C0JgRIY2nxi96mKK2xMbFKTJul4">\r\n> ```shell\r\n> (pyspark) âžœ  spark-community git:(master) âœ— cd assembly/target/scala-2.13/jars\r\n> (pyspark) âžœ  jars git:(master) âœ— ls -1 netty-*\r\n> netty-all-4.1.110.Final.jar\r\n> netty-buffer-4.1.113.Final.jar\r\n> netty-codec-4.1.113.Final.jar\r\n> netty-codec-http-4.1.110.Final.jar\r\n> netty-codec-http2-4.1.110.Final.jar\r\n> netty-codec-socks-4.1.110.Final.jar\r\n> netty-common-4.1.113.Final.jar\r\n> netty-handler-4.1.113.Final.jar\r\n> netty-handler-proxy-4.1.110.Final.jar\r\n> netty-resolver-4.1.113.Final.jar\r\n> netty-tcnative-boringssl-static-2.0.66.Final-linux-aarch_64.jar\r\n> netty-tcnative-boringssl-static-2.0.66.Final-linux-x86_64.jar\r\n> netty-tcnative-boringssl-static-2.0.66.Final-osx-aarch_64.jar\r\n> netty-tcnative-boringssl-static-2.0.66.Final-osx-x86_64.jar\r\n> netty-tcnative-boringssl-static-2.0.66.Final-windows-x86_64.jar\r\n> netty-tcnative-boringssl-static-2.0.66.Final.jar\r\n> netty-tcnative-classes-2.0.66.Final.jar\r\n> netty-transport-4.1.113.Final.jar\r\n> netty-transport-classes-epoll-4.1.113.Final.jar\r\n> netty-transport-classes-kqueue-4.1.110.Final.jar\r\n> netty-transport-native-epoll-4.1.113.Final-linux-aarch_64.jar\r\n> netty-transport-native-epoll-4.1.113.Final-linux-riscv64.jar\r\n> netty-transport-native-epoll-4.1.113.Final-linux-x86_64.jar\r\n> netty-transport-native-kqueue-4.1.110.Final-osx-aarch_64.jar\r\n> netty-transport-native-kqueue-4.1.110.Final-osx-x86_64.jar\r\n> netty-transport-native-unix-common-4.1.113.Final.jar\r\n> ```\r\n\r\nOk ~ I\

The final solution is to upgrade `netty` to version `1.1.114` first, and then upgrade `zookeeper`.\r\nI have verified locally that the `netty...` generated by compiling with `sbt` is version `1.1.114`

> Also, to make testing easier, shall we expose the cache expiration time as a conf (internal?) ? This way, we can manipulate this value to be tiny (few seconds) during testing to recreate the failure scenario(s)\r\n\r\nYeah .. I have been testing it by manually changing the expiry time... but let me avoid adding it here for now but in a separate PR.

Merged to master.

I am debugging the flakiness, and seems like this causes the tests flaky for some reasons. I will revert this first.

Thank you for investigating and reverting, Hyukjin.

cc @allisonwang-db @cloud-fan @dtenedor 

[Run / Build modules: pyspark-mllib, pyspark-ml, pyspark-ml-connect](https://github.com/ueshin/apache-spark/actions/runs/11714014185/job/32627949792#logs) seems to have been canceled for some reason. Rerun the jobs.

The failing tests are not related to this PR.

Merged to master.

@cloud-fan can you please take a look at this one?

thanks, merging to master!

The lint error for this PR is an open MyPy bug:\r\n\r\nhttps://github.com/python/mypy/issues/11165

cc @itholic 

cc @JoshRosen @viirya @dongjoon-hyun @yaooqinn 

Also, cc @wangyum , @peter-toth and @bersprockets , too.

> but the second execution path is not always picked because it has a trigger condition: the LIMIT N must be greater than TOP_K_SORT_FALLBACK_THRESHOLD\r\n\r\ngreater -> smaller?

thanks for the review, merging to master!

Thank you, @cloud-fan and all.

> @urosstan-db Are you going to raise the error from somewhere?\r\n\r\nNo for now, not in Spark to be more precise.

Could you review this PR, @LuciferYang ?

Merged into master. Thanks @dongjoon-hyun 

Thank you so much, @LuciferYang .

For the record, Python 3.9 Daily CI is healthy.\r\n- https://github.com/apache/spark/actions/workflows/build_python_3.9.yml\r\n  - https://github.com/apache/spark/actions/runs/11525385608

Thank you @dongjoon-hyun  @MaxGekk  just filed https://issues.apache.org/jira/browse/SPARK-50123

+1, LGTM. Merging to master/3.5.\r\nThank you, @yaooqinn and @dongjoon-hyun for review.

Thank you all. ðŸ˜ƒ 

If CI passes, hopefully it can be merged today

+1, LGTM. Merging to master.\r\nThank you, @markonik-db and @mihailom-db @srielau for review.

+1, LGTM. Merging to master.\r\nThank you, @itholic.

ah, the ticket SPARK-50016 is wrong. @itholic could provide a correct one.

Oh, it should be SPARK-50116. Let me update the JIRA

cc @dongjoon-hyun FYI

Could you resolve the conflicts, @LuciferYang ?

> Could you resolve the conflicts, @LuciferYang ?\r\n\r\ndone

Thank you for the question from @HeartSaVioR . This was also an issue I intended to discuss in [SPARK-49821](https://issues.apache.org/jira/browse/SPARK-49821), and I apologize for forgetting to leave comments yesterday. \r\n\r\nRegarding the `spark-connect` module, there is a script called `dev/connect-gen-protos.sh`. This script is used by developers to manually regenerate the corresponding `_pb2.py` and `_pb2.pyi` files and submit them when changes are made to the `.proto` files in the `connect` module. Personally, I think we should provide a similar way to automate this process in the future, but as my understanding of Python is very superficial, I think we need @HyukjinKwon  to confirm if there is a more reasonable solution.

@HeartSaVioR @dongjoon-hyun Can we merge this one first? I have created a new Jira ticket: SPARK-50139, to remind us that we should provide a tool to assist in regenerating `StateMessage_pb2.py` and `StateMessage_pb2.pyi`. We can complete this TODO after determining the most reasonable approach for PySpark.

If there is no further comments until Tomorrow (72 hours after https://github.com/apache/spark/pull/48654#issuecomment-2439177691), I believe you can merge this.

> If there is no further comments until Tomorrow (72 hours after [#48654 (comment)](https://github.com/apache/spark/pull/48654#issuecomment-2439177691)), I believe you can merge this.\r\n\r\nGot it ~

> Would we want to also add StateMessage.java to gitignore so that we don\

Ah OK, so the generated file is placed in target, not source directory. Sounds good.

Thank you for the sync-up with them, @HeartSaVioR .

@LuciferYang Thanks for working on this change! QQ: for the dev loop after this is merged, if we add changes to `StateMessage.proto`, would command `build/sbt clean package` be enough to automatically generate the java file and get picked up by the compiler?

To @bogao007 , yes, exactly.\r\n\r\nFYI, Apache Spark has many `proto` files (including this) and use them (except this) in that way.\r\n```\r\n$ find . -type f -name "*.proto" | grep -v test\r\n./core/src/main/protobuf/org/apache/spark/status/protobuf/store_types.proto\r\n./sql/core/src/main/java/org/apache/spark/sql/execution/streaming/StateMessage.proto\r\n./sql/connect/common/src/main/protobuf/spark/connect/relations.proto\r\n./sql/connect/common/src/main/protobuf/spark/connect/catalog.proto\r\n./sql/connect/common/src/main/protobuf/spark/connect/base.proto\r\n./sql/connect/common/src/main/protobuf/spark/connect/example_plugins.proto\r\n./sql/connect/common/src/main/protobuf/spark/connect/types.proto\r\n./sql/connect/common/src/main/protobuf/spark/connect/expressions.proto\r\n./sql/connect/common/src/main/protobuf/spark/connect/commands.proto\r\n./sql/connect/common/src/main/protobuf/spark/connect/common.proto\r\n```

> Thanks for working on this change! QQ: for the dev loop after this is merged, if we add changes to `StateMessage.proto`, would command `build/sbt clean package` be enough to automatically generate the java file and get picked up by the compiler?\r\n\r\n@bogao007  Yes, it would be enough.\r\n\r\nThanks for your help! @dongjoon-hyun 

Merged into master for Spark 4.0. Thanks @dongjoon-hyun @HeartSaVioR and @bogao007 

Thank you all again!

Although we can set `ignoreLeadingWhiteSpace` to `true` to display it correctly, the type `float` & `double` can still be displayed correctly `without` setting it, \r\n```sql\r\nspark-sql (default)> select from_csv(\

cc @MaxGekk @HyukjinKwon @cloud-fan 

@panbingkun There are CSV options:\r\n- ignoreLeadingWhiteSpace\r\n- ignoreTrailingWhiteSpace\r\n\r\nThey are off in read by default, but when you set them on, do they solve your issue?

> @panbingkun There are CSV options:\r\n> \r\n> * ignoreLeadingWhiteSpace\r\n> * ignoreTrailingWhiteSpace\r\n> \r\n> They are off in read by default, but when you set them on, do they solve your issue?\r\n\r\n- Taking the following as an example:\r\n```sql\r\n# parse error\r\nspark-sql (default)> select from_csv(\

Is this behavior consistent with reading CSV files?

> Is this behavior consistent with reading CSV files?\r\n\r\n\r\n- test data file `test.csv`\r\n  ```csv\r\n  1, 1\r\n  ```\r\n\r\n- Before this PR\r\n  A.schema = "a INT, b DOUBLE"\r\n  ```scala\r\n  test("read csv test.csv") {\r\n    val df = spark.read.schema("a INT, b DOUBLE").csv(testFile("test-data/test.csv"))\r\n    df.show(true)\r\n  }\r\n  ```\r\n  result\r\n  ```\r\n  +---+---+\r\n  |  a|  b|\r\n  +---+---+\r\n  |  1|1.0|\r\n  +---+---+\r\n  ```\r\n\r\n  B.schema = "a INT, b INT"\r\n  ```scala\r\n  test("read csv test.csv") {\r\n    val df = spark.read.schema("a INT, b INT").csv(testFile("test-data/test.csv"))\r\n    df.show(true)\r\n  }\r\n  ```\r\n  result\r\n  ```\r\n  +---+----+\r\n  |  a|   b|\r\n  +---+----+\r\n  |  1|NULL|\r\n  +---+----+\r\n  ```\r\n\r\n- After this PR\r\nA.schema = "a INT, b DOUBLE"\r\n  ```scala\r\n  test("read csv test.csv") {\r\n    val df = spark.read.schema("a INT, b DOUBLE").csv(testFile("test-data/test.csv"))\r\n    df.show(true)\r\n  }\r\n  ```\r\n\r\n  result:\r\n  ```\r\n  +---+---+\r\n  |  a|  b|\r\n  +---+---+\r\n  |  1|1.0|\r\n  +---+---+\r\n  ```\r\n\r\n  B.schema = "a INT, b INT"\r\n  ```scala\r\n  test("read csv test.csv") {\r\n    val df = spark.read.schema("a INT, b INT").csv(testFile("test-data/test.csv"))\r\n    df.show(true)\r\n  }\r\n  ```\r\n\r\n  result:\r\n  ```\r\n  +---+---+\r\n  |  a|  b|\r\n  +---+---+\r\n  |  1|  1|\r\n  +---+---+\r\n  ```\r\n\r\n- Conclusion: regardless of whether this PR is applied or not, the logic of `reading CSV files` is consistent with that of `from_csv`.

So this PR is trying to update both `from_csv` and reading CSV files?

> So this PR is trying to update both `from_csv` and reading CSV files?\r\n\r\nYes, the underlying logic for `reading CSV files` and `from_csv` uses `UnivocityParser`\r\n- v1\r\nhttps://github.com/apache/spark/blob/a08859760a41bde9642307483c5d282c82d31bff/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala#L58\r\n\r\n- v2\r\nhttps://github.com/apache/spark/blob/a08859760a41bde9642307483c5d282c82d31bff/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVPartitionReaderFactory.scala#L56\r\n\r\n- CsvToStructs(`from_csv`)\r\nhttps://github.com/apache/spark/blob/a08859760a41bde9642307483c5d282c82d31bff/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala#L129\r\n\r\n

> Let\

+1, LGTM. Merging to master.\r\nThank you, @itholic.

Seems the ticket SPARK-50015 is incorrect. Please, give me correct JIRA ticket.

Merged to master. Thanks @HyukjinKwon @xupefei for the review!

@gengliangwang This [line](https://github.com/apache/spark/blob/28c3dbd213a98fe50ea2d97f257e46f4aff62fd6/sql/core/src/main/scala/org/apache/spark/sql/avro/AvroOptions.scala#L106) is why we need to move `AvroFileFormat`

``

Merged to master.

cc @cloud-fan @gengliangwang :)

Thank you @dtenedor for the consideration suggestions! I have updated the code accordingly and added more test cases.\r\n

kindly ping @dtenedor 

Thanks again for the review! @dtenedor \r\n\r\nPlease feel free to let me know any other comments!

thanks, merging to master!

LGTM, thank you!

Merged to master for Apache Spark 4.0.0 on February 2025.\r\nThank you, @zhengruifeng and @xinrong-meng .

thank you @dongjoon-hyun and @xinrong-meng so much!

Thank you, @zhengruifeng .

LGTM, thank you!

Thank you, @xinrong-meng .

Merged to master for Apache Spark 4.0.0 on February 2025.

For the record, `PyPy 3.10` Daily CI passed today.\r\n- https://github.com/apache/spark/actions/workflows/build_python_pypy3.10.yml\r\n  - https://github.com/apache/spark/actions/runs/11520734751

All tests passed.

cc @hvanhovell too

I think that makes sense, is this a recent version or do we need to upgrade even more?

Thank you, @HyukjinKwon , @LuciferYang , @grundprinzip .\r\n\r\nMerged to master for Apache Spark 4.0.0 on February 2025.

@dongjoon-hyun It seems that after this pr, the `spark-protobuf` module will fail to compile using Maven:\r\n\r\n**Before:**\r\n\r\n```\r\ngit reset --hard b0dc6546a0b720f86ddbaa87e4d055557160af54\r\nbuild/mvn -DskipTests clean install -pl connector/protobuf -e -am\r\n\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Reactor Summary for Spark Project Parent POM 4.0.0-SNAPSHOT:\r\n[INFO] \r\n[INFO] Spark Project Parent POM ........................... SUCCESS [  2.854 s]\r\n[INFO] Spark Project Tags ................................. SUCCESS [  5.257 s]\r\n[INFO] Spark Project Sketch ............................... SUCCESS [  5.023 s]\r\n[INFO] Spark Project Common Utils ......................... SUCCESS [ 16.851 s]\r\n[INFO] Spark Project Local DB ............................. SUCCESS [  8.222 s]\r\n[INFO] Spark Project Networking ........................... SUCCESS [ 11.172 s]\r\n[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  9.049 s]\r\n[INFO] Spark Project Variant .............................. SUCCESS [  3.260 s]\r\n[INFO] Spark Project Unsafe ............................... SUCCESS [ 10.125 s]\r\n[INFO] Spark Project Connect Shims ........................ SUCCESS [  2.640 s]\r\n[INFO] Spark Project Launcher ............................. SUCCESS [  6.882 s]\r\n[INFO] Spark Project Core ................................. SUCCESS [01:43 min]\r\n[INFO] Spark Project SQL API .............................. SUCCESS [ 29.428 s]\r\n[INFO] Spark Project Catalyst ............................. SUCCESS [02:02 min]\r\n[INFO] Spark Project SQL .................................. SUCCESS [02:33 min]\r\n[INFO] Spark Protobuf ..................................... SUCCESS [ 24.383 s]\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD SUCCESS\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time:  08:34 min\r\n[INFO] Finished at: 2024-10-28T11:12:10+08:00\r\n[INFO] ------------------------------------------------------------------------\r\n```\r\n\r\n**After:**\r\n\r\n```\r\ngit reset --hard b0dc6546a0b720f86ddbaa87e4d055557160af54\r\nbuild/mvn -DskipTests clean install -pl connector/protobuf -e -am\r\n\r\n[INFO] --- protoc-jar:3.11.4:run (default) @ spark-protobuf_2.13 ---\r\n[INFO] Resolving artifact: com.google.protobuf:protoc:4.28.3, platform: osx-aarch_64\r\nprotoc-jar: executing: [/var/folders/j2/cfn7w6795538n_416_27rkqm0000gn/T/protoc17161577217517635203.exe, --version]\r\nlibprotoc 28.3\r\n[INFO] Protoc command: /var/folders/j2/cfn7w6795538n_416_27rkqm0000gn/T/protoc17161577217517635203.exe\r\n[INFO] Additional include types: /var/folders/j2/cfn7w6795538n_416_27rkqm0000gn/T/protocjar1650670058665477047/include\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Reactor Summary for Spark Project Parent POM 4.0.0-SNAPSHOT:\r\n[INFO] \r\n[INFO] Spark Project Parent POM ........................... SUCCESS [  5.220 s]\r\n[INFO] Spark Project Tags ................................. SUCCESS [  5.724 s]\r\n[INFO] Spark Project Sketch ............................... SUCCESS [  6.586 s]\r\n[INFO] Spark Project Common Utils ......................... SUCCESS [ 17.162 s]\r\n[INFO] Spark Project Local DB ............................. SUCCESS [  7.086 s]\r\n[INFO] Spark Project Networking ........................... SUCCESS [ 13.517 s]\r\n[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  8.659 s]\r\n[INFO] Spark Project Variant .............................. SUCCESS [  3.383 s]\r\n[INFO] Spark Project Unsafe ............................... SUCCESS [  9.294 s]\r\n[INFO] Spark Project Connect Shims ........................ SUCCESS [  2.865 s]\r\n[INFO] Spark Project Launcher ............................. SUCCESS [  5.692 s]\r\n[INFO] Spark Project Core ................................. SUCCESS [01:39 min]\r\n[INFO] Spark Project SQL API .............................. SUCCESS [ 26.080 s]\r\n[INFO] Spark Project Catalyst ............................. SUCCESS [01:56 min]\r\n[INFO] Spark Project SQL .................................. SUCCESS [02:27 min]\r\n[INFO] Spark Protobuf ..................................... FAILURE [  7.295 s]\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD FAILURE\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time:  08:02 min\r\n[INFO] Finished at: 2024-10-28T11:20:52+08:00\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal com.github.os72:protoc-jar-maven-plugin:3.11.4:run (default) on project spark-protobuf_2.13: Execution default of goal com.github.os72:protoc-jar-maven-plugin:3.11.4:run failed: Cannot read the array length because "<local6>" is null -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal com.github.os72:protoc-jar-maven-plugin:3.11.4:run (default) on project spark-protobuf_2.13: Execution default of goal com.github.os72:protoc-jar-maven-plugin:3.11.4:run failed: Cannot read the array length because "<local6>" is null\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:333)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:255)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:201)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:361)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:314)\r\nCaused by: org.apache.maven.plugin.PluginExecutionException: Execution default of goal com.github.os72:protoc-jar-maven-plugin:3.11.4:run failed: Cannot read the array length because "<local6>" is null\r\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:133)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:328)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:255)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:201)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:361)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:314)\r\nCaused by: java.lang.NullPointerException: Cannot read the array length because "<local6>" is null\r\n    at com.github.os72.protocjar.Protoc.extractStdTypes (Protoc.java:372)\r\n    at com.github.os72.protocjar.maven.ProtocJarMojo.performProtoCompilation (ProtocJarMojo.java:397)\r\n    at com.github.os72.protocjar.maven.ProtocJarMojo.execute (ProtocJarMojo.java:374)\r\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:328)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:316)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:212)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:174)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:75)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:162)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:159)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:906)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:255)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:201)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:361)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:314)\r\n[ERROR] \r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR] \r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException\r\n[ERROR] \r\n[ERROR] After correcting the problems, you can resume the build with the command\r\n[ERROR]   mvn <args> -rf :spark-protobuf_2.13\r\n```\r\n\r\nThe Maven daily tests for the past two days have all failed to compile, and I haven\

Thank you for reporting.\r\n\r\nLet me check, @LuciferYang .

According to the log message string `local6`, it seems `Maven` plugin issue, `protoc-jar-maven-plugin`, because SBT works fine.\r\n- https://github.com/os72/protoc-jar-maven-plugin/issues/104

Let me try the workaround, `optimizeCodegen=true`, in the above link.

Here is the PR, @LuciferYang .\r\n- https://github.com/apache/spark/pull/48673

cc @HyukjinKwon \r\nThis is intentionally focusing on Python side only. For Java-side, we have a separate PR for further discussion.

All tests passed.

Could you review this PR too, @zhengruifeng and @xinrong-meng ?

Could you review this PR, @viirya ?

Thank you, @HyukjinKwon . Merged to master for Apache Spark 4.0.0 on February 2025.

Thank you, @viirya .

Could you review this follow-up, @viirya ?

All tests passed.

Thanks @dongjoon-hyun 

Thank you, @viirya !

This is only a follow-up to make it sure the enforcement logic. The dependency was updated and tested already in the previous PR. Let me merge this~

@stefankandic @uros-db Could you look at the PR, please.

@panbingkun Could you please help review this?

Could you review this PR, @viirya ?

Looks good to me. Thanks.\r\n\r\nOn Thu, Oct 24, 2024 at 10:19\u202fAM Dongjoon Hyun ***@***.***>\r\nwrote:\r\n\r\n> Could you review this PR, @viirya <https://github.com/viirya> ?\r\n>\r\n> â€”\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/spark/pull/48639#issuecomment-2435895262>, or\r\n> unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAAQZ5723KAWRRAMQJ3YNU3Z5ET3PAVCNFSM6AAAAABQRLUNPCVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDIMZVHA4TKMRWGI>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n

Thank you, @viirya !

All CIs passed. Let me merge this because this is a doc-only change in the repository.

Thank you, @huaxingao !

+1, LGTM. Merging to master.\r\nThank you, @jovanm-db and @LuciferYang for review.

cc @HyukjinKwon @dongjoon-hyun , Do you think we can directly clean up these dev settings?\r\n\r\n

Merged to master for Apache Spark 4.0.0 on February 2025.

Thanks @dongjoon-hyun ~

Merged to master.

Thank you @HyukjinKwon !

+1, LGTM. Merging to master.\r\nThank you, @itholic.

+1, LGTM. Merging to master.\r\nThank you, @itholic.

cc @HyukjinKwon @zhengruifeng 

can you fill the PR description with a JIRA filed?

Hi ,\r\n\r\nI want to contribute to the project and can help out. Please let me know what to do!\r\n\r\nThanks!\r\n

Hi @Pshak-20000 thank you for reaching out!\r\n\r\nA good starting point would be to look at the open issues on the [Apache Spark JIRA](https://issues.apache.org/jira/projects/SPARK/issues).That will give you a good idea of the areas you can contribute to.\r\n\r\nAlso, participating in code reviews is a great way to get familiar with the codebase.\r\nYou can find more detailed guidance on https://spark.apache.org/contributing.html good luck with your journey into Spark contributions!

Merged to master, thank you!

cc @cloud-fan 

cc @agubichev @andylam-db 

thanks, merging to master/4.0!

@MaxGekk please have a look when you can; also @uros-db to confirm the UTF8_LCASE part

+1, LGTM. All GAs [passed](https://github.com/stefankandic/spark/actions/runs/11496826191/job/32005804311). Merging to master.\r\nThank you, @stefankandic and @uros-db for review.

@yaooqinn as an author of https://github.com/apache/spark/pull/46006, I would like to get your review here :).  

Does this approach still work when the resultset given by limit 1 is empty?

> Does this approach still work when the resultset given by limit 1 is empty?\r\n\r\nThis is a good point. Shall we keep the information schema query as a fallback when the table is empty?

> Does this approach still work when the resultset given by limit 1 is empty?\r\n\r\nThis is actually fine as we won\

@yaooqinn makes sense, thanks! I have updated the PR with fallback to the querying of metadata table.

One more question about array_ndims, does it return a constant for a specific column, in other words, is it possible that Postgres does not update system meta because the dimension value from CTAS is not deterministic?

What is your concern here?

@yaooqinn are you questioning the reasoning behind metadata not being correct for CTAS created tables or I misunderstood your question?

Hi @PetarVasiljevic-DB,\r\n\r\nThe experiment I conducted shows that executing array_ndim on the same column can result in different values across different rows. 

Maybe we\

If postgres array has row with different array dimensionality (array_dims returning different values for different rows) - then from Spark perspective there is really **no correct answer to what `array_dims` we should use** given that spark cannot process this rows anyway. Given that there is **no correct answer, all answers are wrong answers**, so returning `rand()` (which `limit 1` basically does) is same as reading it from metadata, as for Spark these are all incorrect.\r\n\r\nHowever, there is one scenario that could be better - but I don\

I see and it makes sense. Thanks @milastdbx. @yaooqinn may I suggest that we at least fallback to the value of 1 if we read 0 from metadata? It is expected to read the ArrayType so dimensionality of array should be at least 1. So something like this:\r\n\r\n```\r\nmetadata.putLong("arrayDimension", Math.max(1, rs.getLong(1)))\r\n```\r\n\r\nIt doesn\

It makes sense to me w/ 1 as the default dimension value or maybe we can fail directly when encountering 0. Also cc @cloud-fan 

Can you rebase master and retry the GA?

Tests are passing now.

Merged to master, thank you @PetarVasiljevic-DB 

+1, LGTM. Merging to master.\r\nThank you, @markonik-db and @mihailom-db @zhengruifeng @HyukjinKwon for review.

The lint failure is unrelated, thanks, merging to master!

To @xupefei , could you add multiple empty commits on this PR in order to verify your claim?\r\n\r\n> CI will tell.

Failing on unrelated tests.

Sorry but let me revert this because this failed twice a day already in `master` commit builder, @xupefei .\r\n\r\nI guess there might be more failures on PR Builders after this PR.

This is reverted via the following.\r\n- https://github.com/apache/spark/commit/125c1da1810a650f68794c7e5f93dca4827dfe7b

Sounds good!\r\nWe should refactor these tests to use only two threads. That would definitely resolve this issue.

@dongjoon-hyun Could you disable `Cancellation APIs in SparkSession are isolated` and link it to `SPARK-48139`?

To @xupefei , I believe we should handle them separately. You can apply most of your patch if you exclude these lines.\r\n\r\n```\r\n-  // TODO(SPARK-48139): Re-enable `SparkSessionE2ESuite.interrupt tag`\r\n-  ignore("interrupt tag") {\r\n+  test("interrupt tag") {\r\n```\r\n\r\nIn other words, you should file and use a different JIRA ID for the rest of your contribution because SPARK-48139 is dedicated to this `ignore("interrupt tag")` exactly.

If you need the rest of patch, please open a new PR (by excluding `ignore` part) with new ID. Then, we can merge it back without brining back this flakiness.

PR created at https://github.com/apache/spark/pull/48736.

thanks, merging to master!

late LGTM.

+1, LGTM. Merging to master.\r\nThank you, @itholic.

+1, LGTM. Merging to master.\r\nThank you, @mihailoale-db and @vladimirg-db for review.

+1, LGTM. Merging to master.\r\nThank you, @zhengruifeng.

+1, LGTM. Merging to master.\r\nThank you, @jovanm-db.

Merged to master.

The failed tests is related to your changes, it seems. Please, fix it:\r\n```\r\n[info] - TRUNCATE TABLE using V1 catalog V1 command: truncate a partition of non partitioned table *** FAILED *** (97 milliseconds)\r\n[info]   "[_LEGACY_ERROR_TEMP_1267]" did not equal "[PARTITIONS_NOT_FOUND]" (SparkFunSuite.scala:346)\r\n[info]   Analysis:\r\n[info]   "[_LEGACY_ERROR_TEMP_1267]" -> "[PARTITIONS_NOT_FOUND]"\r\n```

@itholic Please, fix test failures.

Merged to master. Thanks @MaxGekk for the review!

merged to master

Thanks @MaxGekk for the review. Just applied the comments

+1, LGTM. Merging to master.\r\nThank you, @itholic.

cc @pan3793 and @dongjoon-hyun FYI

also cc @panbingkun 

Are there any negative impacts of disabling these features? Is it possible to retain these features through upgrading dependencies or code changes?

Gentle ping, @wayneguow . Could you answer the above questions?

> Gentle ping, @wayneguow . Could you answer the above questions?\r\n\r\n@dongjoon-hyun Sorry, I forgot about this, I will confirm the details and give the final solution and reason in the next few days.

Thank you so much!

After doing further research, I found out why there are these two warning logs in Spark 4.0 version, but not in Spark 3.x and earlier versions:\r\n\r\n1. When Spark 4.0 upgraded `jetty` 10 to 11, it also upgraded `jesery` 2.41 to 3.0.12, related PR: #45154\r\n\r\n2. Relevant changes in `jesery` 3.x version(https://github.com/eclipse-ee4j/jersey/commit/c522d34d1c119ce84973978623b9d1fedd8da493):\r\n\r\n- MessagingBinders.java:\r\n`DATASOURCE("jakarta.activation.DataSource")` has been added to `EnabledProvidersBinder`.\r\n- WadlFeature.java:\r\nAdded some check logic and warning logs.\r\n\r\nIf the following two dependencies are in the class path, there will be no corresponding warning logs, but we excluded it in this PR: #25481\r\n`jakarta.activation:jakarta.activation-api`\r\n`jakarta.xml.bind:jakarta.xml.bind-api`

> Are there any negative impacts of disabling these features? Is it possible to retain these features through upgrading dependencies or code changes?\r\n\r\nFor the first questionï¼š\r\n\r\n- For setting `holder.setInitParameter(CommonProperties.PROVIDER_DEFAULT_DISABLE, "DATASOURCE")`, from code https://github.com/eclipse-ee4j/jersey/blob/d9658aa3064236abf1280f16e1705454e5d0b599/core-common/src/main/java/org/glassfish/jersey/message/internal/MessagingBinders.java#L210-L256, since the `DATASOURCE` will be directly removed from `enabledProviders` after the parameter is set, the `providerBinder.bind(binder, provider);` operation will not be performed, so the effect is the same as when printing the warning log.\r\n\r\n- For setting `holder.setInitParameter(ServerProperties.WADL_FEATURE_DISABLE, "true")`, from code https://github.com/eclipse-ee4j/jersey/blob/2.x/core-server/src/main/java/org/glassfish/jersey/server/wadl/WadlFeature.java#L51-L57 , the effect is the same, it returns `false`, so there is no negative impact.\r\n\r\nFor the second question:\r\nUpgrading to the current jersey version does not work,  related code and logic still exists in the latest version. The logic of printing relevant warning logs is to find relevant classes in the class path. What we can do is to put the corresponding jars in the class path, but there seems to be a greater risk in implementing this, that is the conflict between `javax` and `jakarta`.

Thank you for sharing that, @wayneguow .

Given that, this PR is the best and safe way to handle these, right, @wayneguow ?

> After doing further research, I found out why there are these two warning logs in Spark 4.0 version, but not in Spark 3.x and earlier versions:\r\n> \r\n> 1. When Spark 4.0 upgraded `jetty` 10 to 11, it also upgraded `jesery` 2.41 to 3.0.12, related PR: [[SPARK-47118][BUILD][CORE][SQL][UI] Migrate from Jetty 10 to Jetty 11\xa0#45154](https://github.com/apache/spark/pull/45154)\r\n> 2. Relevant changes in `jesery` 3.x version([eclipse-ee4j/jersey@c522d34](https://github.com/eclipse-ee4j/jersey/commit/c522d34d1c119ce84973978623b9d1fedd8da493)):\r\n> \r\n> * MessagingBinders.java:\r\n>   `DATASOURCE("jakarta.activation.DataSource")` has been added to `EnabledProvidersBinder`.\r\n> * WadlFeature.java:\r\n>   Added some check logic and warning logs.\r\n> \r\n> If the following two dependencies are in the class path, there will be no corresponding warning logs, but we excluded it in this PR: #25481 `jakarta.activation:jakarta.activation-api` `jakarta.xml.bind:jakarta.xml.bind-api`\r\n\r\nWe upgraded from 2.41 to 3.0.x. Actually, there was a similar registration in 2.41 as well, with just a slight difference in the namespace between `javax` and `jakarta`ï¼š\r\n\r\n\r\nhttps://github.com/eclipse-ee4j/jersey/blob/3eac07f0b10ada9ba4828785ad2cc0ff5cf19f9f/core-common/src/main/java/org/glassfish/jersey/message/internal/MessagingBinders.java#L172\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/904d037a-1f6d-4607-941b-fa910711c005)\r\n\r\n\r\n\r\nWhy didn\

> If the following two dependencies are in the class path, there will be no corresponding warning logs, but we excluded it in this PR: https://github.com/apache/spark/pull/25481\r\n> - `jakarta.activation:jakarta.activation-api`\r\n> - `jakarta.xml.bind:jakarta.xml.bind-api`\r\n\r\nA little bit of further investigation:\r\n\r\nSpark already pulls `jakarta.xml.bind-api` into the runtime classpath, in `mllib` module via `jpmml-model` 1.4\r\nhttps://github.com/apache/spark/blob/98f276730d1dcfb2732a78439270b2578a777a15/dev/deps/spark-deps-hadoop-3-hive-2.3#L122\r\n\r\nwhile this version actually contains `javax.xml.bind.` classes.\r\n\r\n`jpmml` migrated to `jakarta.xml.bind.` since 1.5, see https://github.com/jpmml/jpmml-model/issues/41.\r\n\r\nI think Spark eventually needs to follow the Jakarta EE specification to migrate to `jakarta` namespace, I did a quick search on Spark\

> > If the following two dependencies are in the class path, there will be no corresponding warning logs, but we excluded it in this PR: #25481\r\n> > \r\n> > * `jakarta.activation:jakarta.activation-api`\r\n> > * `jakarta.xml.bind:jakarta.xml.bind-api`\r\n> \r\n> A little bit of further investigation:\r\n> \r\n> Spark already pulls `jakarta.xml.bind-api` into the runtime classpath, in `mllib` module via `jpmml-model` 1.4\r\n> \r\n> https://github.com/apache/spark/blob/98f276730d1dcfb2732a78439270b2578a777a15/dev/deps/spark-deps-hadoop-3-hive-2.3#L122\r\n> \r\n> while this version actually contains `javax.xml.bind.` classes.\r\n> \r\n> `jpmml` migrated to `jakarta.xml.bind.` since 1.5, see [jpmml/jpmml-model#41](https://github.com/jpmml/jpmml-model/issues/41).\r\n> \r\n> I think Spark eventually needs to follow the Jakarta EE specification to migrate to `jakarta` namespace, I did a quick search on Spark\

Also cc @zhengruifeng for the ML part.

@LuciferYang and @wayneguow \r\n\r\nIt seems we need to change the code for `pmml-model` 1.4.8 -> 1.7.1 upgrade, so I am a bit worry about the compatibility, I suggest:\r\n1, splitting this PR, can we make a separate PR for `pmml-model` upgrade first?\r\n2, adding additional tests to make sure `pmml-model 1.7.1` can successfully load previous models. We can save 1~2 pmml models into `mllib/src/test/resources/ml-models` with Spark 3.5, and then load them in the tests. you can refer to https://github.com/apache/spark/commit/6b7527e381591bcd51be205853aea3e349893139 \r\n\r\nalso cc @WeichenXu123 

> @LuciferYang and @wayneguow\r\n> \r\n> It seems we need to change the code for `pmml-model` 1.4.8 -> 1.7.1 upgrade, so I am a bit worry about the compatibility, I suggest: 1, splitting this PR, can we make a separate PR for `pmml-model` upgrade first? 2, adding additional tests to make sure `pmml-model 1.7.1` can successfully load previous models. We can save 1~2 pmml models into `mllib/src/test/resources/ml-models` with Spark 3.5, and then load them in the tests. you can refer to [6b7527e](https://github.com/apache/spark/commit/6b7527e381591bcd51be205853aea3e349893139)\r\n> \r\n> also cc @WeichenXu123\r\n\r\nOkay, I agree with your suggestion, let me first ensure that `pmml-model` can be upgraded without loss of compatibility.

cc @MaxGekk @cloud-fan 

> @panbingkun Could you resolve conflicts, please.\r\n\r\nUpdated, thanks!

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

cc @dongjoon-hyun @LuciferYang thanks

Merged to master.

Thank you @HyukjinKwon @dongjoon-hyun @LuciferYang 

@uros-db , please take a look at the PR once you have time, thnx!

also, please enable GHA in your fork

> also, please enable GHA in your fork\r\n\r\nShould be enabled now, thnx

+1, LGTM. Merging to master.\r\nThank you, @dejankrak-db and @uros-db for review.

@dejankrak-db Congratulations with your first contribution to Apache Spark!

+1, LGTM. Merging to master.\r\nThank you, @itholic.

Merged to master.

cc. @cloud-fan Would you mind taking a look? Please cc. to others if you are busy with other stuff. Thanks!

cc @allisonwang-db @dtenedor 

cc @gene-db 

Hi ,\r\n\r\nI want to contribute to the project and can help out. Please let me know what to do!\r\n\r\nThanks!

The tests failures are not related. Thanks merging to master.

+1, LGTM. Merging to master.\r\nThank you, @itholic.

@stefankandic @c27kwan @cstavr @olaky @zhipengmao-db Please, review this PR.

Merging to master. Thank you, @stefankandic @uros-db @cloud-fan for review.

+1, LGTM. Merging to master.\r\nThank you, @itholic.

Thanks @MaxGekk for the review

yeah

Reverted renaming changes

thanks, merging to master!

cc @MaxGekk @cloud-fan 

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

> +1, LGTM. Merging to master. Thank you, @panbingkun.\r\n\r\nThanks @MaxGekk â¤ï¸

cc @MaxGekk @cloud-fan

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

Thank you, @panbingkun and @MaxGekk .

Thank all, @MaxGekk and @dongjoon-hyun â¤ï¸

Merged to master.

Merged to master.

Merged to master.

Late LGTM, thank you!

cc. @hvanhovell Please review and approve. Thanks in advance.

Thanks! Merging to master.

thanks, merged to master

+1, LGTM. Merging to master.\r\nThank you, @panbingkun and @dongjoon-hyun for review.

cc @zhengruifeng This should fix all in https://github.com/apache/spark/actions/runs/11447673972

Merged to master.

fixing it now ... 

I think you should sync your branch to the lastest master 

+1, LGTM. Merging to master.\r\nThank you, @stevomitric.

@stefankandic @stevomitric can you plese review this PR?

also cc: @mihailom-db 

thanks, merging to master!

thanks, merging to master!

+1, LGTM. Merging to master.\r\nThank you, @itholic.

+1, LGTM. Merging to master.\r\nThank you, @jovanm-db and @mihailom-db @stefankandic for review.

Merged to master.

@MaxGekk @HyukjinKwon Do we want to not support try_make_interval in pyspark? This PR seems to block it https://github.com/apache/spark/pull/46975. I believe we should update that make_interval is not supported as well, as CalendarIntervalType is not supported.

Should I just then remove it from python API? As we cannot really call it there.

e.g., you can call `df.select(make_interval(...)).show()`

Aha, ok, so I should only change the test to use some other type of collection of data that is not `collect()` and then verify the results.

Yeah, i think so.

Ready for merge now @MaxGekk @HyukjinKwon 

+1, LGTM. Merging to master.\r\nThank you, @mihailom-db and @HyukjinKwon for review.

cc @MaxGekk @cloud-fan 

thanks, merging to master!

> thanks, merging to master!\r\n\r\nThanks! â¤ï¸

Thank you, @panbingkun and @cloud-fan .

The logic for constructing the histogram is mostly copied from Pandas on Spark to maintain parity. We can refactor it for reuse in a follow-up.

@zhengruifeng @HyukjinKwon may I get a review please?

merged to master

Thank you @zhengruifeng !

+1, LGTM. Merging to master.\r\nThank you, @itholic.

Merged to master. Thanks @zhengruifeng for review!

Merged to master.

Late LGTM, thanks!

cc @karuppayya @viirya @cloud-fan 

gentle ping @viirya @cloud-fan  for review.

@manuzhang can you rebase with the latest master and re-trigger the CI?

@cloud-fan Done!

thanks, merging to master!

+1, LGTM. Merging to master.\r\nThank you, @itholic.

Merged to master. Thanks @zhengruifeng for review

The issue is a sort of correctness/data loss (as watermark advances faster than it should be), but it\

cc. @zsxwing @viirya Would you mind taking a look? Thanks!

> The issue is a sort of correctness/data loss (as watermark advances faster than it should be), but it\

We actually have another issue with this which needs some API design for the fix; we only persist the final watermark value into commit log. This could lead to tricky scenario, like following:\r\n\r\nSuppose a streaming query having streaming source A, B, C. The query is running as scheduled manner (with Trigger.AvailableNow), hence the query is expected to be stopped once the all available data is processed.\r\n\r\n* 1st query run: there are new available data for A and B. Spark processes the data and decides the watermark for next batch to be 0, because there is no data observed in C.\r\n* 2nd query run: it starts from the committed batch of 1st trigger, watermark = 0. there are new available data for B and C. Spark processes the data and decides the watermark for next batch to be 0, because there is no data observed in A.\r\n\r\nThis is incorrect, because Spark has processed the data from all streaming sources A, B, and C. This is happening because we maintain the "in-memory map" to track the value of each watermark node and this is not persisted.\r\n\r\nTo solve this, we need a consistent order (or alias) of these watermark nodes "across query runs", which is tricky. We could probably rely on the traversal order like we do for stateful op ID, but it might be ideal if we could think of allowing users to set alias to the node/operator.\r\n

@viirya Just to make 100% sure, do you plan to review the change or is it good to go as it is?

Thanks! Merging to master.

Thank you again, @HeartSaVioR and @viirya .

Merged into master for Spark 4.0. Thanks @HyukjinKwon 

+1, LGTM. Merging to master.\r\nThank you, @zhengruifeng and @HyukjinKwon for review.

Merged to master.

Late LGTM, thank you!

cc @xinrong-meng 

Late LGTM, thank you!

Merged to master.

cc @zhengruifeng 

Merged to master.

Merged to master.

Merged to master.

+1, LGTM. Merging to master.\r\nThank you, @itholic.

cc @dtenedor and @HyukjinKwon 

Merged to master.

Waiting for all GitHub actions passed.

@KazMiddelhoek Could you resolve conflicts, please.

+1, LGTM. Merging to master.\r\nThank you, @KazMiddelhoek.

thanks, merging to master!

cc @yaooqinn 

Merged to master.

cc - @ericm-db @HeartSaVioR - PTAL, thx !

Thanks! Merging to master.

@dongjoon-hyun @HyukjinKwon @LuciferYang @zeal-thinker thanks for the review. Merging to master

jenkins merge

thanks, merging to master!

Could you review this backporting PR, @viirya ?

Thank you, @viirya . Merged to branch-3.4.

Could you review this backporting PR, @viirya ?

Thank you, @viirya . Merged to branch-3.5.

Thank you, @viirya .

Merged to branch-3.4.

Thank you, @viirya .

Merged to branch-3.5.

@stevomitric Please, fix the test failure. It seems it is related:\r\n```\r\n[info] - collation on non-explicit default collation *** FAILED *** (2 milliseconds)\r\n[info]   Incorrect evaluation (codegen off): SYSTEM.BUILTIN.UTF8_BINARY, actual: SYSTEM.BUILTIN.UTF8_BINARY, expected: UTF8_BINARY (ExpressionEvalHelper.scala:261)\r\n[info]   org.scalatest.exceptions.TestFailedException:\r\n```

thanks, merging to master!

+1, LGTM. Merging to master.\r\nThank you, @jovanpavl-db and @uros-db for your review.

@jasonli-db Hi, nice to meet you. Could you briefly review this change? Thanks!

@HyukjinKwon @dongjoon-hyun Hi, can you possibly review this change? (nobody\

Merged to master.

can you fill the PR description please?

+1, LGTM. Merging to master.\r\nThank you, @zhipengmao-db and @HyukjinKwon for review.

cc @cloud-fan 

thanks, merging to master!

Merged to master.

+1, LGTM. Merging to master.\r\nThank you, @zhipengmao-db.

+1, LGTM. Merging to master.\r\nThank you, @itholic.

Merged to master.

Thanks for the fix!!

Merged to master.

Could you review this PR when you have some time, @HyukjinKwon ?

Thank you, @HyukjinKwon !

Merged to master.

Thank you, @HyukjinKwon and @yaooqinn . I made backporting PRs.\r\n- https://github.com/apache/spark/pull/48549\r\n- https://github.com/apache/spark/pull/48550

Could you review this PR when you have some time, @viirya ?

Thank you, @viirya !

+1, LGTM. Merging to master (the changes conflict w/ branch-3.5, @dongjoon-hyun please, open a separate PR w/ backport if it is needed).\r\nThank you, @dongjoon-hyun and @viirya for review.

Thank you, @MaxGekk and @viirya . I made a backport.\r\n- https://github.com/apache/spark/pull/48547\r\n- https://github.com/apache/spark/pull/48548

thank you @Yikun @dongjoon-hyun and @HyukjinKwon \r\n\r\nmerged to master

Thank all, it seems this the cache take effect in both master https://github.com/apache/spark/actions/runs/11399304381/job/31717882423 and my repo https://github.com/zhengruifeng/spark/actions/runs/11399270535/job/31717789932

To @zhengruifeng , unfortunately, it seems that this broke branch-3.5 and branch-3.4. Could you take a look at that?\r\n\r\n```\r\nError: buildx failed with: ERROR: unable to prepare context: path "./dev/infra/base/" not found\r\n```\r\n\r\n- branch-3.5: https://github.com/apache/spark/actions/runs/11402525656/job/31727732218\r\n- branch-3.4: https://github.com/apache/spark/actions/runs/11400770840/job/31744444617

Sorry, but let me revert this first for now in order to recover the release branch CIs as a release manager for Apache Spark 3.4.4 EOL release.

This is reverted via https://github.com/apache/spark/commit/aaecab3d3c8b116e4aa32b2b26ad6a1b32f2a80a

Let me revert SPARK0-50011 too because it never works seamlessly until now in all CI combinations.

oops, we discussed old branches before, but thought no change needed.\r\nmy bad, let me take a look

cc @MaxGekk 

+1, LGTM. Merging to 3.4.\r\nThank you, @panbingkun.

cc @MaxGekk 

+1, LGTM. Merging to 3.5.\r\nThank you, @panbingkun.

cc @MaxGekk 

Thanks all!

> It would be nice if you provide some reasons for reverting the bug fix.\r\n\r\nOkay, let me add something for it.

> > Why are the changes needed?\r\n> > Only revert.\r\n> \r\n> It would be nice if you provide some reasons for reverting the bug fix.\r\n\r\nDone.

+1, LGTM. Merging to master.\r\nThank you, @panbingkun and @HyukjinKwon @zhengruifeng @dongjoon-hyun for review.

cc @cloud-fan @gengliangwang here is aggregation support. This should be the last non-trivial pipe operator left.

friendly ping @cloud-fan @gengliangwang :) 

the spark connect test failure is unrelated and flaky, thanks, merging to master!

Could you review this PR, @viirya ?

Merged to master.

Thank you, @HyukjinKwon !

Thank you, @viirya !

+1, LGTM. Merging to master.\r\nThank you, @uros-db and @HyukjinKwon for review.

+1, LGTM. Merging to master.\r\nThank you, @uros-db and @HyukjinKwon for review.

This PR was simply rebased to `master` branch to bring the following update.\r\n- #48638\r\n- #48643 \r\n- #48644\r\n- #48646

All tests passed. Thank you, @yaooqinn .\r\nMerged to master for Apache Spark 4.0.0 on February 2025.

@cloud-fan 

Mind filing a JIRA please?

> Mind filing a JIRA please?\r\n\r\nDone in SPARK-50033

The pyspark failure is unrelated, thanks, merging to master!

Could you review this infra PR for Python 3.13 support, @huaxingao ?\r\n\r\nThe image build succeeded already and I manually validate. And, the CI result is irrelevant to this PR because Python 3.13 is only used in Daily CI seperately.\r\n- https://github.com/apache/spark/actions/workflows/build_python_3.13.yml

Thank you, @huaxingao !

For the record, `grpc` installation error is gone successfully.\r\n- https://github.com/apache/spark/actions/runs/11392144577/job/31698382766

@uros-db @stefankandic @stevomitric @vladanvasi-db Could you take a look at the changes, please.

> @uros-db @stefankandic @stevomitric @vladanvasi-db Could you take a look at the changes, please.\r\n\r\nSynced offline with @uros-db, we decided to go with a bit different implementation of trim string function. Will push changes soon and it will again be ready for the review. Sorry for not setting pr as WIP.

also, look out for any scalastyle issues

@MaxGekk can you please take a look?

> @jovanpavl-db Could you resolve conflicts, please. In general, LGTM.\r\n\r\nDone.

+1, LGTM. Merging to master.\r\nThank you, @jovanpavl-db and @uros-db @vladanvasi-db for review.

thanks @dongjoon-hyun for reviews!

Have offline discussion with ruifeng:\r\n\r\n1. we need to also refresh the image cache job, otherwise cache speed up will not work: \r\nhttps://github.com/apache/spark/blob/master/.github/workflows/build_infra_images_cache.yml\r\n  - Fix paths and add doc path\r\n  - add a step for doc image cache\r\n\r\nthe infra image cache is a separate job will be triggered when the dockerfile PR is merged (we first make sure ci passed and the dockerfile do the right fix/upgrade/change, then do the [post fresh](https://github.com/apache/spark/blob/75b86667ee7607d3523d7ce75c1022752142a443/.github/workflows/build_infra_images_cache.yml#L24))\r\n\r\n2. For branch cache/job images, we could keep original behavior that means:\r\n\r\n**For branch jobs (doc and test):**\r\ncache: ghcr.io/apache/spark/apache-spark-github-action-image-cache:branch-3.5\r\njob: apache-spark-ci-image:${{ inputs.branch }}-${{ github.run_id }}\r\n\r\n**For master jobs:**\r\ntest:\r\ncache: ghcr.io/apache/spark/apache-spark-github-action-image-cache:master\r\njob: apache-spark-ci-image:${{ inputs.branch }}-${{ github.run_id }}\r\n\r\ndoc:\r\ncache: ghcr.io/apache/spark/apache-spark-github-action-image-**docs**-cache:master\r\njob: apache-spark-ci-image-**docs**:${{ inputs.branch }}-${{ github.run_id }}

thanks @Yikun , sending a fix in https://github.com/apache/spark/pull/48533

Unfortunately, it turns out that the follow-up also broke the release branches\r\n- https://github.com/apache/spark/pull/48533#issuecomment-2422878623\r\n\r\nAs a release manager of Apache Spark 3.4.4 EOL release, I reverted SPARK-50011 (this and the follow-up commit) for now. Sorry about that.\r\n- https://github.com/apache/spark/commit/aaecab3d3c8b116e4aa32b2b26ad6a1b32f2a80a\r\n- https://github.com/apache/spark/commit/6f710cd9c561c911066108b7d659d5de44099757

Can we re-start cleanly to make it sure to pass all CI combinations?

@itholic should we use PySparkLogger instead?

> should we use PySparkLogger instead?\r\n\r\n@HyukjinKwon The logging file is using PySparkLogger under the hood.

Yes it is already using the `PySparkLogger` so it should be fine:\r\n\r\nhttps://github.com/apache/spark/blob/b0414b19a0ea5ffa10067154c00009259419f045/python/pyspark/sql/connect/client/logging.py#L31

Merged to master.

thanks, merged to master

cc. @cloud-fan @hvanhovell Could you please take a look? Thanks!

Yeah we could even include the "additional" metric entry if it was artificially made with default value, if someone claims that they want to distinguish the case explicitly. I\

@HeartSaVioR how can a `CollectMetricsExec` node get lost? That is somewhat concerning...

UPDATE: @hvanhovell and I had an offline talk. I wasn\

Revert PR: https://github.com/apache/spark/pull/48590

Merged to master.

Fixed the failed tests. Thanks!

+1, LGTM. Merging to master.\r\nThank you, @itholic.

@xinrong-meng please rebase to enable a recent test image refactor

Rebased, thank you!

Merged to master, thank you!

@zhengruifeng would you please review?

Thank you @dongjoon-hyun!

+1, LGTM. Merging to master.\r\nThank you, @panbingkun and @cloud-fan for review.

+1, LGTM. Merging to master.\r\nThank you, @itholic.

the linter failure is unrelated, thanks, merging to 3.4!

thanks, merging to 3.5!

Merged to master, thank you @panbingkun @HyukjinKwon 

cc @dtenedor @chenhao-db @srielau 

Thanks. merging to master.

Can we include the benchmark result files too? See also "Testing with GitHub Actions workflow" at https://spark.apache.org/developer-tools.html

@uros-db @mihailom-db @viktorluc-db Could you review this PR, please.

@mrk-andreev Could you intergrate your benchmark into `CollationBenchmark`, please, as @uros-db pointed out https://github.com/apache/spark/pull/48501#pullrequestreview-2385543767. Otherwise we might forget to re-run your benchmark while benchmarking collation related code.

> @mrk-andreev Could you intergrate your benchmark into CollationBenchmark, please, as @uros-db pointed out https://github.com/apache/spark/pull/48501#pullrequestreview-2385543767. Otherwise we might forget to re-run your benchmark while benchmarking collation related code.\r\n\r\n@MaxGekk , done. 

cc: @MaxGekk\r\n\r\n# Related work\r\n\r\nThis is not related to my code changes but rather to the benchmarks we are modifying. It might be worth starting a separate thread in the dev mailing list or creating an additional ticket in Jira, which I would be happy to handle.\r\n\r\n## Blackhole\r\n\r\nI would like to point out that the current implementation of org.apache.spark.benchmark.Benchmark::addCase does not use any form of Blackhole ([Blackhole in JMH](https://github.com/openjdk/jmh/blob/master/jmh-core/src/main/java/org/openjdk/jmh/infra/Blackhole.java#L155)), which could lead to dead-code elimination. However, I have not observed this issue in the existing tests. This is likely due to the complexity and side effects of the code being benchmarked, which prevents such elimination.\r\n\r\nWould it be a good idea to consider adding this as a feature in the future?\r\n\r\n### Context\r\n\r\n`org.apache.spark.benchmark.Benchmark::addCase`\r\n\r\n```\r\n  def addCase(name: String, numIters: Int = 0)(f: Int => Unit): Unit = {\r\n    addTimerCase(name, numIters) { timer =>\r\n      timer.startTiming()\r\n      f(timer.iteration)\r\n      timer.stopTiming()\r\n    }\r\n  }\r\n```\r\n\r\n## Async-profiler\r\n\r\nI suggest adding [Async Profiler](https://github.com/async-profiler/async-profiler), a low-overhead sampling profiler, to all benchmark runs. This will help us identify the causes of performance degradation.\r\n\r\nWould it also be worth considering adding this as a feature in the future?

Hi @MaxGekk, @stevomitric, \r\n\r\nDoes this PR need any additional changes? Are there any blockers we should address? Let me know how I can help to move it forward!

+1, LGTM. Merging to master.\r\nThank you, @mrk-andreev and @stevomitric @uros-db for review.

@dongjoon-hyun I do agree that the title is not reflecting the right change, but opening a new PR might be an overkill and also cause confusion in development later. The main point of this PR is what ticket suggests `Remove the ANSI config suggestion in INVALID_URL`. I would suggest renaming the title, but keeping the changes in one PR, as these changes suggest the reason for removing and also the solution/substitute suggestion we provide for spark users. Separating them would make developer work much harder, and the second PR you suggested would be mostly tests changes.

> @dongjoon-hyun I do agree that the title is not reflecting the right change, but opening a new PR might be an overkill and also cause confusion in development later. The main point of this PR is what ticket suggests `Remove the ANSI config suggestion in INVALID_URL`. I would suggest renaming the title, but keeping the changes in one PR, as these changes suggest the reason for removing and also the solution/substitute suggestion we provide for spark users. Separating them would make developer work much harder, and the second PR you suggested would be mostly tests changes.\r\n\r\nNo, I disagree with your opinion, "opening a new PR might be an overkill and also cause confusion in development later", @jovanm-db . Technically, `adding try_parse_url` and `recommending try_parse_url` is not the same at all. They should have two JIRA IDs. :) 

+1, LGTM. Merging to master.\r\nThank you, @jovanm-db and @dongjoon-hyun @HyukjinKwon @srielau @mihailom-db for review.

@jovanm-db Congratulations with your first contribution to Apache Spark!

Thank you, @jovanm-db and @MaxGekk and all.\r\n\r\nCongratulations to @jovanm-db .

@MaxGekk @HyukjinKwon I added tests and fixed one test in DataFrameSuite, as it was not completely using aggFn parameter

+1, LGTM. Merging to master.\r\nThank you, @mihailom-db and @srielau @HyukjinKwon for review.

cc @cloud-fan , @ulysses-you , @yaooqinn , @LuciferYang from #47533 

Thank you, @LuciferYang and @cloud-fan . Could you review once more? I addressed all comments.

Could you review this too, @viirya ? This is a little very flaky in these days in CIs. Multiple times a day.

Thank you, @LuciferYang , @cloud-fan , @viirya . Let me merge this to stabilize our CIs.

late lgtm!

can you fill the PR description please?

@MaxGekk the problem here is that legacy_error_temp_2042 would have returned the same message as ARITHMETIC_OVERFLOW, but someone just added a new method in QueryExecutionErrors for codeGen path, even though in eval path we threw the same error, but just under different class. I can add a golden file test, to throw this error, but I am not sure if golden files call codeGen or eval path?

@mihailom-db `checkErrorInExpression` checks both codegen and non-codegen

I see, I found tests in `IntervalExpressionSuite.scala` but they did not check for anything in the error, they just checked that error is there. Will update them so they do the right check.

@MaxGekk updated the test, it seems the whole class is just checking if error is thrown and not actually what errors are thrown, so codeGen and eval might be different errors for even more expressions

+1, LGTM. Merging to master.\r\nThank you, @mihailom-db.

+1, LGTM. Merging to master.\r\nThank you, @uros-db and @stefankandic @mihailom-db for review.

cc @cloud-fan @dongjoon-hyun, thanks

Thank you, @yaooqinn , @cloud-fan , @HyukjinKwon .\r\nMerged to master for Apache Spark 4.0.0 on February 2025.

Thank you all, @cloud-fan @HyukjinKwon @dongjoon-hyun 

- Benchmark code as follows:\r\n```scala\r\nobject RandstrBenchmark extends SqlBasedBenchmark {\r\n  private val N = 1000000 // 1_000_00\r\n  private val M = 100\r\n\r\n  private val df = spark.range(N).to(new StructType().add("id", "int"))\r\n\r\n  private def doBenchmark(): Unit = {\r\n    df.selectExpr("randStr(10, 20)").noop()\r\n  }\r\n\r\n  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\r\n    runBenchmark("randStr") {\r\n      val benchmark = new Benchmark("randStr", N, output = output)\r\n      benchmark.addCase("optimize", M) { _ =>\r\n        doBenchmark()\r\n      }\r\n      benchmark.run()\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n- Before (run 3 times)\r\n```\r\nRunning benchmark: randStr\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 12265 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0.1\r\nApple M2\r\nrandStr:                                  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            107            123          21          9.4         106.9       1.0X\r\n\r\n\r\nRunning benchmark: randStr\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 11298 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0.1\r\nApple M2\r\nrandStr:                                  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            107            113           7          9.3         107.0       1.0X\r\n\r\n\r\nRunning benchmark: randStr\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 11181 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0.1\r\nApple M2\r\nrandStr:                                  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            107            112           5          9.4         106.7       1.0X\r\n```\r\n\r\n- After (run 3 times)\r\n```\r\nRunning benchmark: randStr\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 10362 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0.1\r\nApple M2\r\nrandStr:                                  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                             98            104           6         10.2          98.0       1.0X\r\n\r\n\r\nRunning benchmark: randStr\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 10380 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0.1\r\nApple M2\r\nrandStr:                                  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                             98            104           7         10.2          97.9       1.0X\r\n\r\n\r\nRunning benchmark: randStr\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 10230 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0.1\r\nApple M2\r\nrandStr:                                  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                             97            102           4         10.3          97.5       1.0X\r\n```

cc @dtenedor @MaxGekk @cloud-fan 

Can we rewrite `RandStr` to a new expression that extends `Nondeterministic`? We can use `InheritAnalysisRules` so that the `Nondeterministic` expression is still available in the expression tree.

> Can we rewrite `RandStr` to a new expression that extends `Nondeterministic`? We can use `InheritAnalysisRules` so that the `Nondeterministic` expression is still available in the expression tree.\r\n\r\nLet me give it a try.

+1, LGTM. Merging to master.\r\nThank you, @panbingkun and @cloud-fan for review.

@zhengruifeng @HyukjinKwon may I get a review please?

merged to master

Thank you @zhengruifeng !

cc @dongjoon-hyun @LuciferYang 

@dongjoon-hyun I tested the maven build, run `dev/test-dependencies.sh` to ensure that runtime deps are affected, and just run UT of a few modules(sts, yarn) using maven locally.

We can add the following code to `build_and_test.yml` to have GA execute a Maven test once.\r\n\r\n```\r\nmaven-test:\r\n    name: "Run Maven Test"\r\n    permissions:\r\n      packages: write\r\n    uses: ./.github/workflows/maven_test.yml\r\n```

cc @dtenedor @MaxGekk 

It seems the failed tests is related:\r\n```\r\n[info] - randstr function *** FAILED *** (181 milliseconds)\r\n[info]   Map("inputName" -> "`length`", "inputType" -> "INT or SMALLINT", "inputExpr" -> ""a"", "sqlExpr" -> ""randstr(a, 10)"") did not equal Map("inputName" -> "length", "inputType" -> "INT or SMALLINT", "inputExpr" -> ""a"", "sqlExpr" -> ""randstr(a, 10)"") (SparkFunSuite.scala:362)\r\n```

> It seems the failed tests is related:\r\n> \r\n> ```\r\n> [info] - randstr function *** FAILED *** (181 milliseconds)\r\n> [info]   Map("inputName" -> "`length`", "inputType" -> "INT or SMALLINT", "inputExpr" -> ""a"", "sqlExpr" -> ""randstr(a, 10)"") did not equal Map("inputName" -> "length", "inputType" -> "INT or SMALLINT", "inputExpr" -> ""a"", "sqlExpr" -> ""randstr(a, 10)"") (SparkFunSuite.scala:362)\r\n> ```\r\n\r\nThanks!\r\nYes, I have fixed it, waiting for CI.

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

cc @dongjoon-hyun and @panbingkun 

Just wanted to add that dev/create-release/spark-rm/Dockerfile also has gfortran installed.

LGTM

Merged to master.

@LuciferYang @dongjoon-hyun since you created these methods

+1, Agree with @dongjoon-hyun 

LGTM, the tests in Build (pull_request_target) all passed. Thank you!

Thank you so much, @xinrong-meng .\r\nMerged to master/3.5/3.4.

cc @maryannxue 

thanks, merging to master!

LGTM, thank you!

Thank you so much, @xinrong-meng .\r\nMerged to master/3.5/3.4.

cc @yaooqinn @ulysses-you 

thanks, merging to master/3.5!

Merged to master.

cc @HyukjinKwon 

Merged to master.

Thank you, @tinglongliao-db and @HyukjinKwon .

thanks, merging to master!

@hvanhovell fyi

Reopened.

NB: An impl note for future reference.\r\n\r\nScala allows this.type to support the correct type derivation or compilation error on the function input and output types against derived types. For Java however, the _migration/upgrade_ case of sql.Dataset => sql.Dataset works fine but using more specific derived input types does not as this.type has no meaning.  classic.transform(sql.Dataset => classic.Dataset) : classic.Dataset with a lambda _does_ work which allows specific output types if needed later down the line so no functional impact unless derived functions are really needed.

Merging to master/4.0\r\n

@srowen 

I would say no to this change. It adds complexity, and is quite specific to your deployment use case, where you want to allow users a lot of control to provision their own Spark clusters but are worried about resource usage. This is what resource managers are for, and, this kind of sanitizaiton is what you can do in your own app if needed.

@hvanhovell fyi

Would not the `TransformingEncoder` be enough of a customization point to implement the custom encoders provided by at least `frameless` (I am not familiar enough with the others)?\r\n\r\nI guess it not ideal as it would require a rewrite of that part of the library. But at least from my experiments creating custom encoders by first creating `AgnosticEncoders` is much easier than creating the `ExpressionEncoders` directly. And based on the comments by @hvanhovell it seems that is a better approach for downstream libraries.

 > extra black box indirection that will not allow optimisation (e.g. constant folding etc.)\r\n \r\n I am not sure I am following here. Is the black box you talking about the "code hiding" inside the `Codec` type? I am not following why that would not be possible for Spark to do constant folding by just executing that code?

> > extra black box indirection that will not allow optimisation (e.g. constant folding etc.)\r\n> \r\n> I am not sure I am following here. Is the black box you talking about the "code hiding" inside the `Codec` type? I am not following why that would not be possible for Spark to do constant folding by just executing that code?\r\n\r\nSpark can only optimise Expressions, not general jvm byte code.  Similarly, as Codec\

replaced by https://github.com/apache/spark/pull/50023 

+1, LGTM. Merging to master.\r\nThank you, @vladimirg-db.

thank you @dongjoon-hyun !

The difference between this and https://github.com/apache/spark/pull/48452 is that the `Invoke object` in this version is implemented using `Scala`.\r\nScala(`Invoke object`): https://github.com/apache/spark/pull/48473\r\nJava(`Invoke object`): https://github.com/apache/spark/pull/48452

If we use `Java` to write `Invoke object`, we must write it as:\r\nhttps://github.com/apache/spark/pull/48452/files#diff-5dcf9919b53bb052a961f4143ed75bc8567d917b016e83ce0175fe918c7800caR124-R126\r\n<img width="528" alt="image" src="https://github.com/user-attachments/assets/13ac2192-2a8f-476f-bc36-7ef858fe2b22">\r\nOtherwise, compilation will fail.

If we use `Java` to implement it, there is a difference between `Java version` and `the original logic` when an exception occurs, as shown below:\r\n```sql\r\nselect schema_of_json(\

cc @MaxGekk @cloud-fan 

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

+1, LGTM. Merging to master.\r\nThank you, @jovanpavl-db and @uros-db for review.

Thanks @MaxGekk for the review! Applied the comments

@itholic Could you fix the failed tests:\r\n```\r\n[info] - verify corrupt column *** FAILED *** (3 milliseconds)\r\n[info]   (non-codegen mode) Expected error message is `The field for corrupt records must be string type and nullable`, but `[INVALID_CORRUPT_RECORD_TYPE] The column `_unparsed` for corrupt records must have the nullable STRING type, but got "BOOLEAN". SQLSTATE: 42804` found (ExpressionEvalHelper.scala:226)\r\n```

+1, LGTM. Merging to master.\r\nThank you, @itholic.

cc @wangyum @cloud-fan 

cc @vitaliili-db

> thanks for making this change - however, please add collation-related tests as well, see:\r\n> \r\n> ```\r\n> test("StringSplit expression with collated strings")\r\n> ```\r\n> \r\n> in `CollationRegexpExpressionsSuite.scala`\r\n\r\nThank you for your guidance. The relevant tests have been added.

Please investigate more databases, then we make the decision which is the more suitable behavior.

> Please investigate more databases, then we make the decision which is the more suitable behavior.\r\n\r\nFor the split(str, regex, limit) function, the limit parameter controls the number of times regex is applied. I checked the related functions of mainstream databases. Except for presto and trino, there are basically no similar functions.\r\n| Database/SQLEngine | Behavior of this function |\r\n|--------|--------|\r\n| PostgreSQL | There is no similar function |\r\n| MySQL | There is no similar function |\r\n| Oracle | There is no similar function |\r\n| MariaDB | There is no similar function |\r\n| Microsoft SQL Server  | There is no similar function |\r\n| Hive  | There is no similar function |\r\n| PrestoDB  | The behavior is consistent with this PR, and the functionality is implemented earlier than Spark |\r\n| Trino  | The behavior is consistent with this PR, and the functionality is implemented earlier than Spark |\r\n

+1, LGTM. Merging to master.\r\nThank you, @itholic.

cc @yaooqinn @LuciferYang 

cc @MaxGekk @cloud-fan 

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

late LGTM

cc @MaxGekk @cloud-fan 

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

 I found that this one has made the SubExprEliminationBenchmark unexecutable.\r\n\r\n```\r\ngit reset --hard f3b2535d8d92c2210501f15c5845dd589414ffe3   // before this one \r\nbuild/sbt clean "sql/Test/runMain org.apache.spark.sql.execution.SubExprEliminationBenchmark"\r\n```\r\n\r\n`SubExprEliminationBenchmark` can be executed successfully.\r\n\r\n```\r\ngit reset --hard 2a1301133138ba0d5e2d969fc6428153903ffff1 // this one\r\nbuild/sbt clean "sql/Test/runMain org.apache.spark.sql.execution.SubExprEliminationBenchmark"\r\n```\r\n\r\nthen\r\n\r\n```\r\n[info] Running benchmark: from_json as subExpr in Filter\r\n[info]   Running case: subExprElimination false, codegen: true\r\n[info] 00:40:56.209 ERROR org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Failed to compile the generated Java code.\r\n[info] org.codehaus.commons.compiler.InternalCompilerException: Compiling "GeneratedClass" in File \

https://github.com/apache/spark/blob/be0ae1388cd87d8264bdcb41332d18961c1b592d/sql/core/src/test/scala/org/apache/spark/sql/execution/SubExprEliminationBenchmark.scala#L80-L86\r\n\r\nIf `numCols` is reduced to 330, the `SubExprEliminationBenchmark` can be executed successfully, but this is already pushing the limit. Exceeding 330 will cause the generated Java code for this case to exceed the 64kb limit. I am currently using GA  to run `SubExprEliminationBenchmark` with `val numCols = 330` to determine if the comparison data in the `SubExprEliminationBenchmark` meets expectations:\r\n\r\n- before this one: https://github.com/LuciferYang/spark/actions/runs/12656665329/job/35276791286\r\n- after this one: https://github.com/LuciferYang/spark/actions/runs/12661794444/job/35285627294\r\n

Thank you for noticing this issue, let me investigate it.

\r\n- before this one : commit f3b2535d8d92c2210501f15c5845dd589414ffe3  with `numCols=330`\r\n\r\n```\r\nOpenJDK 64-Bit Server VM 17.0.13+11-LTS on Linux 6.8.0-1017-azure\r\nAMD EPYC 7763 64-Core Processor\r\nfrom_json as subExpr in Filter:           Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nsubExprElimination false, codegen: true            2766           3046         284          0.0    27655915.4       1.0X\r\nsubExprElimination false, codegen: false           2884           2940          51          0.0    28837016.0       1.0X\r\nsubExprElimination true, codegen: true              886            901          25          0.0     8860269.8       3.1X\r\nsubExprElimination true, codegen: false             854            857           3          0.0     8539656.4       3.2X\r\n```\r\n\r\n- after this one: commit 2a1301133138ba0d5e2d969fc6428153903ffff1 with `numCols=330`\r\n\r\n```\r\nOpenJDK 64-Bit Server VM 17.0.13+11-LTS on Linux 6.8.0-1017-azure\r\nAMD EPYC 7763 64-Core Processor\r\nfrom_json as subExpr in Filter:           Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nsubExprElimination false, codegen: true            3314           3570         277          0.0    33143936.7       1.0X\r\nsubExprElimination false, codegen: false           2823           3027         193          0.0    28234617.4       1.2X\r\nsubExprElimination true, codegen: true             3501           3579          68          0.0    35009514.9       0.9X\r\nsubExprElimination true, codegen: false             841            865          28          0.0     8409037.4       3.9X\r\n```\r\n\r\n- master: commit 194aa18821c04f068864cc4cf9e3124c54ae7c44 with `numCols=330`\r\n\r\n```\r\nOpenJDK 64-Bit Server VM 17.0.13+11-LTS on Linux 6.8.0-1017-azure\r\nAMD EPYC 7763 64-Core Processor\r\nfrom_json as subExpr in Filter:           Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nsubExprElimination false, codegen: true            3445           3643         270          0.0    34451139.9       1.0X\r\nsubExprElimination false, codegen: false           3017           3122          91          0.0    30169801.5       1.1X\r\nsubExprElimination true, codegen: true             3205           3290          95          0.0    32052385.0       1.1X\r\nsubExprElimination true, codegen: false             768            797          30          0.0     7683521.6       4.5X\r\n```\r\n\r\nIt seems that there is a performance regression in the `withFilter` scenario with `subExprElimination true, codegen: true`, showing `3501, 3579, 68, 0.0, 35009514.9, 0.9X`.

Thank you for reporting, @LuciferYang .

The number of data rows in this benchmark is a bit small (100 rows). \r\nhttps://github.com/apache/spark/blob/8bd7789872b42c91fe9b3bbd73cc44fca865cf5c/sql/core/src/test/scala/org/apache/spark/sql/execution/SubExprEliminationBenchmark.scala#L125\r\nLet me first verify the performance data locally with more data rows.

cc @dongjoon-hyun @LuciferYang 

Merged into master for Spark 4.0. Thanks @panbingkun 

+1, LGTM. Merging to master.\r\nThank you, @harshmotw-db and @cloud-fan for review.

@pan3793 Can you have a look on this, since this is a follow up on SPARK-47118 

LGTM pending tests. @HeartSaVioR can you please help merge this if tests pass? Thanks in advance!

@WweiL  master compile failed after this one merged:\r\n\r\n\r\n- https://github.com/apache/spark/actions/runs/12801160621/job/35690190495#step:10:2619\r\n\r\n![image](https://github.com/user-attachments/assets/080ddccf-9793-42fb-a104-c20d479bd86e)\r\n\r\n\r\n```\r\n[error] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBStateStoreCheckpointFormatV2Suite.scala:626:41: Option[Map[Long,Array[Array[String]]]] does not take parameters\r\n[error]       val res2 = metadata.stateUniqueIds(0).map { uniqueIds =>\r\n[error]                                         ^\r\n[error] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBStateStoreCheckpointFormatV2Suite.scala:634:85: value sorted is not a member of Nothing\r\n[error]         if (!versionToUniqueIdFromStateStore(version).sorted.sameElements(uniqueIds.sorted)) {\r\n[error] \r\n```\r\n\r\n

+1, LGTM. Merging to master.\r\nThank you, @stefankandic and @uros-db @jovanpavl-db @HyukjinKwon for review.

@cloud-fan could you review? Thanks!

The newly added test seems to be failed

Fixed.

@cloud-fan could you help merge it? Thanks!

+1, LGTM. Merging to master/3.5/3.4.\r\nThank you, @chenhao-db and @cloud-fan for review.

thanks, merging to master!

+1, LGTM. Merging to master.\r\nThank you, @uros-db and @HyukjinKwon for review.

+1, LGTM. Merging to master.\r\nThank you, @uros-db.

@Hisoka-X @MaxGekk 

+1, LGTM. Merging to master.\r\nThank you, @cloud-fan and @yaooqinn @Hisoka-X for review.

+1, LGTM. Merging to master.\r\nThank you, @itholic.

+1, LGTM. Merging to master.\r\nThank you, @mihailom-db and @srielau for review.

Merged to master. Thanks @MaxGekk for the review!

LGTM, thank you!

thanks, merged to master

Merged to master, thank you!

cc @dongjoon-hyun @yaooqinn 

Hi @dongjoon-hyun @yaooqinn could you please take a look?\r\n\r\n

Let me close this PR since SPARK-37687 is resolved via\r\n- #49159 

Merged to master, thank you!

<img width="677" alt="image" src="https://github.com/user-attachments/assets/02ba2a1b-debe-4fa9-ad3a-ed6c712ac2b1">\r\n

> @milastdbx Could you review this PR, please.\r\n\r\nThanks @MaxGekk @milastdbx 

Merged to master.

@cloud-fan @dongjoon-hyun @HyukjinKwon In the PR, I am trying to close the door of raising Spark exceptions without error conditions/classes. Unfortunately, at the moment this is one of a couple ways when users might get `SparkThrowable` in which `condition` (`error class`) is `null`.

Thank you, @dongjoon-hyun for review. Merging to master.

cc @panbingkun @MaxGekk 

Test suite seems to pass - there was one error w.r.t. downloading which is unrelated.\r\n```\r\n[error] lmcoursier.internal.shaded.coursier.error.FetchError$DownloadingArtifacts: Error fetching artifacts:\r\n[error] https://maven-central.storage-download.googleapis.com/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar: download error: Caught java.io.IOException (Server returned HTTP response code: 503 for URL: https://maven-central.storage-download.googleapis.com/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar) while downloading https://maven-central.storage-download.googleapis.com/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar\r\n[error]\r\n[error]         at lmcoursier.internal.shaded.coursier.Artifacts$.$anonfun$fetchArtifacts$9(Artifacts.scala:365)\r\n```

Do we have corresponding cases? You can use UT to cover it.

@panbingkun @MaxGekk \r\nPlease take another look.

The following was not related to this PR\r\n```\r\n[info] - SPARK-43923: commands send events ((streaming_query_command {\r\n[info]   query_id {\r\n[info]     id: "89ea6117-1f45-4c03-ae27-f47c6aded093"\r\n[info]     run_id: "89ea6117-1f45-4c03-ae27-f47c6aded093"\r\n[info]   }\r\n[info]   stop: true\r\n[info] }\r\n[info] ,None)) *** FAILED *** (10 seconds, 25 milliseconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 661 times over 10.00008105 seconds. Last failure message: VerifyEvents.this.listener.executeHolder.isDefined was false No events have been posted in Timeout(Span(10, Seconds)). (SparkConnectServiceSuite.scala:878)\r\n[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:348)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:347)\r\n[info]   at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)\r\n[info]   at org.apache.spark.sql.connect.planner.SparkConnectServiceSuite$VerifyEvents.executeHolder(SparkConnectServiceSuite.scala:878)\r\n[info]   at org.apache.spark.sql.connect.planner.SparkConnectServiceSuite$VerifyEvents.onCompleted(SparkConnectServiceSuite.scala:898)\r\n[info]   at org.apache.spark.sql.connect.planner.SparkConnectServiceSuite.$anonfun$new$27(SparkConnectServiceSuite.scala:536)\r\n[info]   at \r\n```

@LuciferYang \r\nI have created https://issues.apache.org/jira/browse/SPARK-50727 for this PR.\r\n\r\n@MaxGekk \r\nPlease review.

@srielau @panbingkun @nchammas @cloud-fan Could you review the PR, please.

Except for a few comments, LTGM.

Merged to master.

@yaooqinn, since you reviewed my last PR (which accidentally caused this regression), can you take a look? Thanks!

All done @yaooqinn, thanks for the review.

Merged to master. Thank you very much @neilramaswamy 

thanks @MaxGekk \r\nmerged to master

Please take a look when you have the time @mihailom-db @cloud-fan 

@cloud-fan can you take a look again as this PR is blocking some future changes that are needed?

LGTM. One question: Are we also going to rename `errorClass` in classifyException from JDBCUtils?\r\n

> Are we also going to rename errorClass in classifyException from JDBCUtils?\r\n\r\nYep, let me rename it in the PR since it is related.

Merging to master/4.0. Thank you, @ivanjevtic-db @cloud-fan for review.

Merging the trivial fix to master.

@srielau @panbingkun @nchammas @cloud-fan Could you review the PR, please.

LGTM.\r\nnit: Do we need to update the following?\r\n<img width="1397" alt="image" src="https://github.com/user-attachments/assets/462c3014-12dc-4331-a9d0-2bc7e3daa108">\r\n

@panbingkun Thank you for review.\r\n1. `UIUtils`: The name `errorClass` is regexp group name. Not related to the changes.\r\n2. `SQLJsonProtocolSuite`: it is just an example in a test. It could handle old input.

cc @cloud-fan 

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

@panbingkun Do you plan to add codeGen support for `StructsToJson` and `JsonToStructs` expression as well?

> JsonToStructs\r\n\r\nYes!

cool. If you are not planning for `StructsToJson`, then I will take your code as ref and try to implement it.

sorry for naive question, any reason why `to_json` is not suitable for using `Invoke` & `RuntimeReplaceable`? I was checking proto equivalent function `toProtobuf` expression which extends `RuntimeReplaceable`

Could you review this when you have some time? cc @dongjoon-hyun @cloud-fan 

+CC @dongjoon-hyun Thoughts on providing users a way to override it ?\r\nI am leaning towards this being a nice enhancement.

### Manual verification is as follows:\r\n- To verify if there will be errors during compilation, in the class `CheckAnalysis`, change the call to `SparkThrowable#getCondition` to call `SparkThrowable#getErrorClass`\r\n  <img width="1321" alt="image" src="https://github.com/user-attachments/assets/6aa95fc9-5f56-4785-8dcb-09eef5998609">\r\n\r\n- Run the following compilation commands\r\n```\r\n./build/sbt -Phadoop-3 -Pkinesis-asl -Pdocker-integration-tests -Phive -Phadoop-cloud -Pspark-ganglia-lgpl -Phive-thriftserver -Pyarn -Pkubernetes -Pvolcano Test/package streaming-kinesis-asl-assembly/assembly connect/assembly\r\n```\r\n\r\n- Compilation will ultimately fail\r\n  <img width="1393" alt="image" src="https://github.com/user-attachments/assets/013e2200-cc8c-4347-a51f-55d0cdf13a02">\r\n

cc @MaxGekk @LuciferYang 

Whether we can only ban calls to `SparkThrowable#getErrorClass` in Scala code. If so, we should clarify it in the PR title and description.

Merged to master.

> As you are here, may I ask you to fix other places:\r\n> \r\n> ```\r\n> find . -name "*.scala" -print0|xargs -0 grep \

+1, LGTM. Merging to master.\r\nThank you, @exmy.

@HyukjinKwon sorry for the problematic PR. I re-upload it and hopefully now it is OK.

We also have a bunch of similar logic in this test in somewhere else, do you mind to add a helper function and integrate them? 

can you rebase/push commits? the test has not been triggered

https://github.com/siying/spark/actions/runs/11300222066/job/31432731337

Merged to master.

+1, LGTM. Merging to master.\r\nThank you, @stefankandic and @zhipengmao-db for review.

Merged to master.

@HyukjinKwon Please take a look If you have time, thanks ~\r\n\r\nI have checked the Maven test and SparkR on Windows test on GA, and briefly tested spark-shell and connect-shell after ` dev/make-distribution.sh --tgz -Phive`. They are all functioning successful with this pr.

Merged to master.

Thanks @HyukjinKwon 

GA passed. Merged into master for Spark 4.0. Thanks @panbingkun @MaxGekk @HyukjinKwon @yaooqinn 

> GA passed. Merged into master for Spark 4.0. Thanks @panbingkun @MaxGekk @HyukjinKwon @yaooqinn\r\n\r\nThanks all!

I am going to merge this to fix up the build.\r\n\r\nMerged to master.

cc - @HeartSaVioR @WweiL - PTAL, thx !

Merged to master.

Will merge this to fix up the build. Otherwise, I will revert this and https://github.com/apache/spark/commit/2af653688c20dde87eebaa6bd4dc21123fab74cc if it still fails.

Merged to master.

cc @MaxGekk 

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

Irrelevant tests failed, retriggering:\r\n```\r\nERROR [3.661s]: test_listener_events (pyspark.sql.tests.streaming.test_streaming_listener.StreamingListenerTests.test_listener_events)\r\n\r\nERROR [0.145s]: test_streaming_progress (pyspark.sql.tests.streaming.test_streaming_listener.StreamingListenerTests.test_streaming_progress)\r\n```

cc @zhengruifeng @HyukjinKwon would you please review thanks!

We may later port those expected_fig_data dictionaries to a separate JSON file for easier auditing if the number of tests increases

Merged to master, thank you!

Merged to master.

Build was broken so I reverted it.

cc @cloud-fan @gengliangwang this is the PR to support LIMIT/OFFSET + sorting. There are a few more changes in the `AstBuilder` for this one but still contained only in the parser.

Can you re-trigger Github Action jobs?

@cloud-fan can you please merge?

thanks, merging to master!

The following is mentioned on the downloadings page (https://spark.apache.org/docs/3.5.3/#downloading)\r\n> Java 8 prior to version 8u371 support is deprecated as of Spark 3.5.0\r\n\r\nFor some reason, I misread this and was under the impression that support for Java 8 was deprecated in its entirety as of 3.5.0.\r\n\r\nI added a new commit where I updated the documentation and included Java 8 back in, with an exception for versions prior to 8u371.

Current changes are fine to me, but please do not disrupt the template for the [r description, every item must be filled out, and the title of the pr should also adhere to the standard format(can refer to [https://spark.apache.org/contributing.html](https://spark.apache.org/contributing.html)). And are there any other cases where the description of Java 8 versions is not accurate?\r\n\r\n

Merged to master.

ping @wangyum @cloud-fan 

Loop more people who may have time to review @viirya @HyukjinKwon @ulysses-you @dongjoon-hyun @LuciferYang 

cc @LuciferYang @dongjoon-hyun @HyukjinKwon

GA has passed, I will restore some testing logic.

<img width="1069" alt="image" src="https://github.com/user-attachments/assets/c0f6ac74-abd3-49a0-aab0-f50c92574a80">\r\n\r\n\r\nall test passed

Merged into master. Thanks @panbingkun @HyukjinKwon 

Merged to master.

<img width="276" alt="image" src="https://github.com/user-attachments/assets/6dac9117-32a9-443c-9d02-1c6d0cb2a8df">\r\n\r\nall maven test passed

cc @hvanhovell @HyukjinKwon @dongjoon-hyun  FYI

Merged into master. Thanks @hvanhovell and @HyukjinKwon 

After merging this PR, Maven daily test has been restored:\r\n\r\n- Java 17: https://github.com/apache/spark/actions/runs/11274643792\r\n\r\n<img width="758" alt="image" src="https://github.com/user-attachments/assets/53526dc9-faca-4001-8bba-0790b9fd1116">\r\n\r\n\r\n- Java 21: https://github.com/apache/spark/actions/runs/11275643152\r\n\r\n<img width="769" alt="image" src="https://github.com/user-attachments/assets/78de5e9d-c774-4301-8fbe-61ab809a330d">\r\n\r\n\r\n---\r\nI just thought of a question that needs confirmation. In which directory should the `spark-connect-shims.jar` be located in the distribution? Currently, after executing `dev/make-distribution.sh --tgz`, it exists in the `jars` directory but not in the `jars/connect-repl` directory. Is this expected? @hvanhovell @HyukjinKwon 

> FWIW, I think it\

actually yeah seems like all REPL are broken after Maven build. Taking a look.

> actually yeah seems like all REPL are broken after Maven build. Taking a look.\r\n\r\nwe should move `spark-connect-shims.jar` from `jars` to `jars/connect-repl `

@LuciferYang mind creating a PR when you find some time? ðŸ™ 

> @LuciferYang mind creating a PR when you find some time? ðŸ™\r\n\r\nOK ~

The GA jobs actually all passed. Thanks, merging to master!

PR to move Avro files: https://github.com/apache/spark/pull/48650

oh forgot - we need to add the stream run id to the Avro encoder cache key, otherwise we may risk some unintended re-use of avro encoders. we should limit the size of that cache and add expiry to it

Thank you!\r\n\r\nOn Mon, Nov 25, 2024, 8:33\u202fPM Jungtaek Lim ***@***.***> wrote:\r\n\r\n> Closed #48401 <https://github.com/apache/spark/pull/48401> via 331d0bf\r\n> <https://github.com/apache/spark/commit/331d0bf30092be62191476e4a679b403e1a369b9>\r\n> .\r\n>\r\n> â€”\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/spark/pull/48401#event-15430081816>, or\r\n> unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ABIAE62V4SZFXMEYAIZ72DT2CP2Z3AVCNFSM6AAAAABPU7JMVCVHI2DSMVQWIX3LMV45UABCJFZXG5LFIV3GK3TUJZXXI2LGNFRWC5DJN5XDWMJVGQZTAMBYGE4DCNQ>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n

@LuciferYang am looking into this, think I have to add a dependency in the sql/core pom.xml\r\ncc @HeartSaVioR @dongjoon-hyun 

https://github.com/apache/spark/actions/runs/11255598249/job/31295526084\r\n\r\n```\r\nscaladoc error: fatal error: object scala in compiler mirror not found.\r\nError:  Failed to execute goal net.alchim31.maven:scala-maven-plugin:4.9.1:doc-jar (attach-scaladocs) on project spark-connect-shims_2.13: MavenReportException: Error while creating archive: wrap: Process exited with an error: 1 (Exit value: 1) -> [Help 1]\r\nError:  \r\nError:  To see the full stack trace of the errors, re-run Maven with the -e switch.\r\nError:  Re-run Maven using the -X switch to enable full debug logging.\r\nError:  \r\nError:  For more information about the errors and possible solutions, please read the following articles:\r\nError:  [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\r\nError:  \r\nError:  After correcting the problems, you can resume the build with the command\r\nError:    mvn <args> -rf :spark-connect-shims_2.13\r\nError: Process completed with exit code 1.\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/ddb57ff3-1c60-49e7-a715-694605c6c754)\r\n

Thanks @hvanhovell ~

Merged into master for fix maven compile issue. Thanks @hvanhovell ~

- https://github.com/apache/spark/actions/runs/11255598249/job/31311358712\r\n```\r\nError: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/ApproximatePercentileQuerySuite.scala:121: value makeRDD is not a member of org.apache.spark.SparkContext\r\nError: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:75: value id is not a member of org.apache.spark.rdd.RDD[org.apache.spark.sql.columnar.CachedBatch]\r\nError: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:82: value env is not a member of org.apache.spark.SparkContext\r\nError: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:88: value env is not a member of org.apache.spark.SparkContext\r\nError: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:185: value parallelize is not a member of org.apache.spark.SparkContext\r\nError: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:481: value cleaner is not a member of org.apache.spark.SparkContext\r\nError: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:500: value parallelize is not a member of org.apache.spark.SparkContext\r\nError: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:940: value addSparkListener is not a member of org.apache.spark.SparkContext\r\nError: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:943: value listenerBus is not a member of org.apache.spark.SparkContext\r\nError: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:947: value removeSparkListener is not a member of org.apache.spark.SparkContext\r\nError: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:1667: value listenerBus is not a member of org.apache.spark.SparkContext\r\nError: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:1668: value addSparkListener is not a member of org.apache.spark.SparkContext\r\nError: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:1673: value partitions is not a member of org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\r\n```\r\n\r\nAlthough I can successfully build after this pr locally, the retriggered Maven daily test still failed (with a different error from before). Further investigation is needed.  It feels like another weird classpath issue. Do you have any suggestion on the aforementioned error? @hvanhovell 

> * https://github.com/apache/spark/actions/runs/11255598249/job/31311358712\r\n> \r\n> ```\r\n> Error: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/ApproximatePercentileQuerySuite.scala:121: value makeRDD is not a member of org.apache.spark.SparkContext\r\n> Error: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:75: value id is not a member of org.apache.spark.rdd.RDD[org.apache.spark.sql.columnar.CachedBatch]\r\n> Error: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:82: value env is not a member of org.apache.spark.SparkContext\r\n> Error: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:88: value env is not a member of org.apache.spark.SparkContext\r\n> Error: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:185: value parallelize is not a member of org.apache.spark.SparkContext\r\n> Error: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:481: value cleaner is not a member of org.apache.spark.SparkContext\r\n> Error: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:500: value parallelize is not a member of org.apache.spark.SparkContext\r\n> Error: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:940: value addSparkListener is not a member of org.apache.spark.SparkContext\r\n> Error: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:943: value listenerBus is not a member of org.apache.spark.SparkContext\r\n> Error: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:947: value removeSparkListener is not a member of org.apache.spark.SparkContext\r\n> Error: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:1667: value listenerBus is not a member of org.apache.spark.SparkContext\r\n> Error: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:1668: value addSparkListener is not a member of org.apache.spark.SparkContext\r\n> Error: ] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:1673: value partitions is not a member of org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\r\n> ```\r\n> \r\n> Although I can successfully build after this pr locally, the retriggered Maven daily test still failed (with a different error from before). Further investigation is needed. It feels like another weird classpath issue. Do you have any suggestion on the aforementioned error? @hvanhovell\r\n\r\nI am trying further fix: https://github.com/apache/spark/pull/48403/files

merged to master.

@WeichenXu123 Thank you so much for making the fix.\r\nHow can i know if its available in OSS? and what version can I test it on?

+1, LGTM. Merging to master.\r\nThank you, @RaleSapic.

@RaleSapic Congratulations with your first contribution to Apache Spark!

+1, LGTM. Merging to 3.4.\r\nThank you, @panbingkun.

cc @cloud-fan @dongjoon-hyun thanks

The Python job failure is irrelevant.\r\n\r\nMerged to master, thank you @cloud-fan 

cc @MaxGekk \r\nI am still verifying the PR for branch-3.4 locally.\r\nThanks!

+1, LGTM. Merging to 3.5.\r\nThank you, @panbingkun.

Merged to master.

- branch 3.4 + scala 2.12.17\r\n```scala\r\n(base) âžœ  spark-trunk git:(branch-3.4) âœ— serialver -classpath core/target/spark-core_2.12-3.4.4-SNAPSHOT.jar:build/scala-2.12.17/lib/scala-library.jar org.apache.spark.util.Lazy\r\norg.apache.spark.util.Lazy:    private static final long serialVersionUID = 7964587975756091988L;\r\n```

@panbingkun thanks a lot for confirming the SerialVersionUID!

cc @JoshRosen would you mind taking another look? thanks

convert to draft for now to avoid merge by mistake

Decompiling with `cfr-decompiler` generates:\r\n\r\n```\r\n/*\r\n * Decompiled with CFR 0.152.\r\n *\r\n * Could not load the following classes:\r\n *  scala.Function0\r\n *  scala.reflect.ScalaSignature\r\n */\r\npackage org.apache.spark.util;\r\n\r\nimport java.io.Serializable;\r\nimport scala.Function0;\r\nimport scala.reflect.ScalaSignature;\r\n\r\n@ScalaSignature(bytes="\\u0006\\u0005}2Q!\\u0002\\u0004\\u0001\\u00119A\\u0001b\\t\\u0001\\u0003\\u0002\\u0013\\u0006I\\u0001\\n\\u0005\\u0006e\\u0001!\\ta\\r\\u0005\\to\\u0001A)\\u0019)C\\u0005q!)Q\\b\\u0001C\\u0001}\\tiAK]1og&,g\\u000e\\u001e\

based on the [Decompiled code](https://github.com/apache/spark/pull/48391#issuecomment-2418737719), I think the `TransientLazy` works as expected, would you mind taking another look?\r\n\r\n@JoshRosen @cloud-fan 

thanks, merging to master!

cc @cloud-fan @HyukjinKwon @dongjoon-hyun this is a breaking change. I am fine with putting this up for discussion on the dev list, but I also feel that this well within the mandate for Spark 4.

+1, LGTM. Merging to master.\r\nThank you, @huangxiaopingRD.

Merged to master.

+1, LGTM. Merging to master.\r\nThank you, @dusantism-db.

+1, LGTM. Merging to master.\r\nThank you, @jovanpavl-db and @uros-db @mihailom-db for review.

@jovanpavl-db Is https://issues.apache.org/jira/browse/SPARK-49661 correct JIRA for the PR? If not, re-open it.

+1, LGTM. Merging to master.\r\nThank you, @panbingkun and @HyukjinKwon for review.

@panbingkun I think we should backport the fix. Could you open separate PRs, please, because your changes cause conflicts in `branch-3.5` (and maybe in `branch-3.4`).

> @panbingkun I think we should backport the fix. Could you open separate PRs, please, because your changes cause conflicts in `branch-3.5` (and maybe in `branch-3.4`).\r\n\r\nSure, allow me to complete it.

- I overlooked `another scenario` in the `PR description`, when an error occurs, the `function name prompted` may not match the `actual function name`, eg:\r\n<img width="1395" alt="image" src="https://github.com/user-attachments/assets/9cd7ace2-2406-4951-a658-32c9f4be6b4a">\r\n\r\n```sql\r\nspark-sql (default)> select random("1");\r\n[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "rand(1)" due to data type mismatch: The first parameter requires the ("INT" or "BIGINT") type, however "1" has the type "STRING". SQLSTATE: 42K09; line 1 pos 7;\r\n\

revert pr: https://github.com/apache/spark/pull/48530

After the master branch merges the revert PR, I will submit PRs to `branch-3.5` and `branch-3.4` respectively to revert these changes.

branch-3.5 revert: https://github.com/apache/spark/pull/48531\r\nbranch-3.4 revert: https://github.com/apache/spark/pull/48532

The Docker integration test failure seems not related to this PR.\r\n\r\ncc @MaxGekk @srielau fyi

+1, LGTM. Merging to master.\r\nThank you, @itholic.

Merged to master.

cc. @cloud-fan Please take a look, thanks!

I see no further feedback from others. Thanks! Merging to master.

CI passed. cc @MaxGekk @srielau fyi

+1, LGTM. Merging to master.\r\nThank you, @itholic.

wait 6.1.1 release https://github.com/apache/datasketches-java/releases

Thanks @dongjoon-hyun 

@cloud-fan Can you look at this PR? Thanks!

@beliefer @MaxGekk I don\

@harshmotw-db Could you re-trigger only the failed GitHub action, please.

+1, LGTM. Merging to master.\r\nThank you, @harshmotw-db and @beliefer for review.

> https://github.com/apache/parquet-java/issues/3021: Upgrade Avro dependency to 1.11.4\r\n\r\nSpark is currently using Avro 1.12.0

> +1, LGTM. Thank you, @panbingkun and @LuciferYang .\r\n> \r\n> I removed the Avro related content from PR description because it could be misleading as pointed by Yang Jie.\r\n> \r\n> Merged to master for Apache Spark 4.0.0 for February 2025.\r\n\r\nThanks, I am very pleased to hear the `official release date` of our `Spark 4.0.0`.

cc @hvanhovell 

Let me merge this @hvanhovell please let me know if there are some more things to change, I will make another PR.

Merged to master.

the pyspark failure is unrelated, thanks, merging to master!

Merging to master

Hi ,\r\n\r\nI want to contribute to the project and can help out. Please let me know what to do!\r\n\r\nThanks!

@HeartSaVioR Addressed your comment, could you help take another look? Thanks!

Create a new ticket https://issues.apache.org/jira/browse/SPARK-50270 to track the metrics change

Thanks! Merging to master.

Merged to master.

cc @viirya 

Thank you, @viirya ! Merged to master.

Merged to master.

Merged to master.

What should be EXECUTE_QUERY errors? 

Moved tests to V2 Suites.

@MaxGekk I am ready for a review.

jenkins trigger all

> @ivanjevtic-db Could you fix the failed tests like:\r\n> \r\n> ```\r\n> [info] - Error conditions are correctly formatted *** FAILED *** (112 milliseconds)\r\n> [info]   "...  ]\r\n> [info]         },\r\n> [info]         "[EXECUTE_QUERY" : {\r\n> [info]           "message" : [\r\n> [info]             "Execution of the query: <query>."\r\n> [info]           ]\r\n> [info]         },\r\n> ```\r\n\r\nFixed

+1, LGTM. Merging to master.\r\nThank you, @itholic.

Thanks @MaxGekk for the review!

+1, LGTM. Merging to master.\r\nThank you, @itholic.

Merged to master.

After this PR, we can pretty speed up to add compatibility tests for each API\r\n\r\ncc @HyukjinKwon @hvanhovell FYI

Thanks for the review @hvanhovell \r\nI addressed comment and the CI passed now, so could you take a look again when you find some time?\r\nThanks!

Merged to master. Thanks all for the review.

thanks, merged to master

Can we add some details on why we are reverting to the description please ? For example: bug still exists in scenario foo, or new bug introduced for xyz, etc ?\nThanks

@mridulm thanks, let me update the description:\r\nthe reason is that the fix potentially causes other issues, so still need discussion and investigation.

Merged to master.

What if we introduce the following wrapper in place of the Scala lazy value ?\r\n```\r\nclass LazyRetry[T](compute: => T) {\r\n  private var initialized = false\r\n  private var value: Option[T] = None\r\n\r\n  def get: T = synchronized {\r\n    if (!initialized) {\r\n      try {\r\n        value = Some(compute)\r\n        initialized = true\r\n      } catch {\r\n        case e: Exception =>\r\n          // So that it can retry on the next call\r\n          initialized = false\r\n          throw e\r\n      }\r\n    }\r\n    value.getOrElse(throw new RuntimeException("Computation not successful"))\r\n  }\r\n}\r\n```\r\nThis wrapper doesn\

python linter failed

Merged to branch-3.5.

Thank you for helping to fix it!\r\nThanks all!

cc @huaxingao @dongjoon-hyun 

Merged to master. Thanks @viirya 

Thanks @huaxingao 

cc @cloud-fan @gengliangwang here is the support for UNION ALL and other set operations.

thanks, merging to master!

Hi @attilapiros and @dongjoon-hyun, given that @holdenk did not object, can we consider shipping the docs change to make the docs match the code? And we can revert both patches if there are new issues identified before 4.0.0 GA

This PR depends on https://github.com/apache/spark/pull/48355

This PR depends on https://github.com/apache/spark/pull/47932 and https://github.com/apache/spark/pull/47895

Thank you for addressing the comments! Merging to master

cc @MaxGekk @dongjoon-hyun @HyukjinKwon @attilapiros If you have time, please help review this clean revert , which has caused the compilation failure of branch-3.5.

Thanks @dongjoon-hyun ~

Merged to branch-3.5.

BTW, you can revert directly next time to recover CIs. If you are going to revert immediately the patch you merged, @LuciferYang . :)

> BTW, you can revert directly next time to recover CIs. If you are going to revert immediately the patch you merged, @LuciferYang . :)\r\n\r\nGot it, thanks  @dongjoon-hyun 

Thank you, @LuciferYang .

Merged to master. Thank you, @LuciferYang and @MaxGekk .

Thanks @dongjoon-hyun and @MaxGekk 

The tests are now passing. Could you make a review @MaxGekk? Thanks.

+1, LGTM. Merging to master.\r\nThank you, @ivanjevtic-db.

@rangadi Hello, nice to meet you here! This is a minor code optimization as part of an attempt to get rid of the use of global locks in the spark connect service code. Could you review the code please? The code semantics should be the same as before. Thanks.

Merged to master.

+1, LGTM. Merging to master.\r\nThank you, @jovanpavl-db and @uros-db @mihailom-db for review.

+1, LGTM. Merging to master.\r\nThank you, @dusantism-db.

also cc @WeichenXu123 for visibility

I think we should pass raw estimates to the model and calculate encodings in transform()\r\nSo we can apply different smoothing factors without having to re-fit\r\nMakes sense?  Will work on this ...\r\nhttps://github.com/apache/spark/blob/5a67c503ce1fea57de5429ff915783d14ba0f7cf/mllib/src/main/scala/org/apache/spark/ml/feature/TargetEncoder.scala#L253

> I think we should pass raw estimates to the model and calculate encodings in transform() So we can apply different smoothing factors without having to re-fit Makes sense? Will work on this ...\r\n> \r\n> https://github.com/apache/spark/blob/5a67c503ce1fea57de5429ff915783d14ba0f7cf/mllib/src/main/scala/org/apache/spark/ml/feature/TargetEncoder.scala#L253\r\n\r\ndone!\r\n\r\n

@srowen @zhengruifeng 

@zhengruifeng 

@HyukjinKwon @zhengruifeng  

Merged to master.

cc @srielau @cloud-fan @MaxGekk 

I think the failed test is not related to your changes:\r\n```\r\norg.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite\r\n```\r\nI re-ran it locally.\r\n\r\n+1, LGTM. Merging to master.\r\nThank you, @itholic.

Merged to master.

Thank you, @HyukjinKwon .

cc @LuciferYang 

cc @dongjoon-hyun FYI 

> spark.kubernetes.allocation.pods.allocator\r\n\r\n@pan3793 \r\nSorry I do not to see how that limitation comes to the picture and if it had an effect before why would this PR resolve it. Could you elaborate it a bit more?\r\n\r\n

https://github.com/apache/spark/blob/9cf6dc873ff34412df6256cdc7613eed40716570/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala#L161-L170\r\n\r\n@attilapiros previously, the executor failure tracking code is located at `ExecutorPodsAllocator`, which only takes effect when `spark.kubernetes.allocation.pods.allocator=direct`.

@pan3793 you are right! Do you want to open PR to update those descriptions?

https://github.com/apache/spark/pull/48358 is opened to update the docs

Unfortunately, that seems to be an undesirable side-effect , @attilapiros and @pan3793 .

cc @holdenk for the case `spark.kubernetes.allocation.pods.allocator=statefulset`.\r\n\r\nIIUC, Holden designed `statefulset` not to be affected by the failure. What is your opinion, @holdenk . I overlooked your use case. If this is improper side-effect, feel free to revert this.

> If this is improper side-effect, feel free to revert this.\r\n\r\nIf failure tracking is only need when `spark.kubernetes.allocation.pods.allocator=direct` I can easily add the necessary extra check to the `ExecutorPodsLifecycleManager` too.

Merged to master for Apache Spark 4.0.0 on February 2025.\r\n\r\nThank you, @HyukjinKwon and @itholic .

Merged to master.

New `NumPy 2` seems to have a different output style in Python 3.13. Of course, the values are correct.\r\n- https://github.com/apache/spark/actions/runs/11186188886/job/31100777649\r\n```\r\n**********************************************************************\r\nFile "/__w/spark/spark/python/pyspark/core/rdd.py", line 2463, in __main__.RDD.sampleStdev\r\nFailed example:\r\n    sc.parallelize([1, 2, 3]).sampleStdev()\r\nExpected:\r\n    1.0\r\nGot:\r\n    np.float64(1.0)\r\n**********************************************************************\r\nFile "/__w/spark/spark/python/pyspark/core/rdd.py", line 2436, in __main__.RDD.stdev\r\nFailed example:\r\n    sc.parallelize([1, 2, 3]).stdev()\r\nExpected:\r\n    0.816...\r\nGot:\r\n    np.float64(0.816496580927726)\r\n**********************************************************************\r\n```

I filed [SPARK-49882](https://issues.apache.org/jira/browse/SPARK-49882).\r\n\r\nAlthough [SPARK-48710](https://issues.apache.org/jira/browse/SPARK-48710) fixed to use NumPy 2.0 compatible types. Python 3.13 requires NumPy 2.1 ([SPARK-49869](https://issues.apache.org/jira/browse/SPARK-49869)) and seems to reveal another instances of differences.

will take a look

cc @wangyum 

@MaxGekk Could you provide correct `sqlState` for the introduced error class.

> Waiting for CI.\r\n\r\nThey all passed besides the Kubernetes one.

I believe [Run / Run Spark on Kubernetes Integration test](https://github.com/mihailoale-db/spark/actions/runs/11179691790/job/31079896402#logs) is not related to the changes.\r\n\r\n+1, LGTM. Merging to master.\r\nThank you, @mihailoale-db.

> Should we remove this comment?\r\n\r\nYes, please, do that.

@MaxGekk All the CIs passed except the Kubernetes one.

I believe the test failure [Run / Run Spark on Kubernetes Integration test](https://github.com/mihailoale-db/spark/actions/runs/11181027475/job/31094662961#logs) is not related to the changes.\r\n\r\n+1, LGTM. Merging to master.\r\nThank you, @mihailoale-db.

+CC @shardulm94 

Gentle reminder @mridulm @pan3793 @HyukjinKwon @shardulm94 

Gentle ping @mridulm @pan3793 @HyukjinKwon @shardulm94 . Please review the change

Merged to master.

The Oracle integration test is not related to the changes, and passed on the previous run:\r\n```\r\n[info] OracleIntegrationSuite:\r\n[info] org.apache.spark.sql.jdbc.OracleIntegrationSuite *** ABORTED *** (10 minutes, 18 seconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 599 times over 10.014775141716667 minutes. Last failure message: ORA-12541: Cannot connect. No listener at host 10.1.0.31 port 44657. (CONNECTION_ID=jYNU58dhQbyjNykzpYuyew==)\r\n```\r\n\r\n+1, LGTM. Merging to master.\r\nThank you, @mihailom-db.

Merged to branch-3.4. Thank you, @stefankandic and @MaxGekk .

Merged to branch-3.5.\r\n\r\nThank you, @stefankandic and @MaxGekk .

@MaxGekk this PR is ready for merge, let me know when you re-review it so I can rerun the CIs, so we do not break something, as these runs are from yesterday.

+1, LGTM. Merging to master.\r\nThank you, @mihailom-db.

cc: @cloud-fan to take a look.

thanks, merging to master!

Image build passed.\r\n```\r\n$ docker run -it --rm ghcr.io/dongjoon-hyun/apache-spark-ci-image:master-11152626644 python3.13 --version\r\nPython 3.13.0rc3\r\n```

Could you review this PR, @viirya ?

Thank you, @viirya . Merged to master.

Hi, @HyukjinKwon . According to the description of SPARK-46645, the problem happened when the test case started to run, right? Please let me know if I missed something here.

Thank you, @HyukjinKwon !

Could you review this PR, @huaxingao ?

Thank you so much, @huaxingao . Merged to master for Apache Spark 4.0.0.

Merged to master.

+1, LGTM. Merging to master.\r\nThank you, @uros-db.

cc @cloud-fan FYI..

@HyukjinKwon @hvanhovell @cloud-fan @dongjoon-hyun \r\nWould it be OK to add `storageLevel` arg to Dataset `localCheckpoint` API? It would simplify the use case in Delta MergeIntoMaterializeSource that I describe, and there is a precedent with `persist` having a `storageLevel` arg.

```\r\nStarting test(python3.11): pyspark.ml.tests.connect.test_parity_torch_data_loader (temp output: /__w/apache-spark/apache-spark/python/target/59e82264-3602-4d7c-a002-35841701fa06/python3.11__pyspark.ml.tests.connect.test_parity_torch_data_loader__feru625e.log)\r\nError: The operation was canceled.\r\n```\r\nThis has been hanging and timing out and never passed on my every Spark PR in the last month...

@juliuszsompolski can you re-trigger the failed GA job?

The test failure of `AdaptiveQueryExecSuite` is not related to changes. I re-ran it locally, BTW.\r\n\r\n+1, LGTM. Merging to master.\r\nThank you, @dusantism-db.

ping @cloud-fan 

ping @milastdbx Do you have time to take a look?

I will add JIRA item as well in future, if we decide to go with this PR.

@MaxGekk What do you think about this change?\r\n@gengliangwang 

cc @HyukjinKwon 

IIRC, this is a long-standing behavior to avoid unexpected nullability. Could you further explain why do we have to support non-nullable user-provided schema?\r\n\r\ncc @cloud-fan who probably has the most context on this one

> IIRC, this is a long-standing behavior to avoid unexpected nullability. Could you further explain why do we have to support non-nullable user-provided schema?\r\n> \r\n> cc @cloud-fan who probably has the most context on this one\r\n\r\nI have to talk with users, but for now, they explicitly wanted to have nullability of provided schema, and I suppose they want exception during parsing if some field is null.

We tried to fix this few times, e.g., https://github.com/apache/spark/pull/17293 and https://github.com/apache/spark/pull/14124. The breakage was pretty severe

> We tried to fix this few times, e.g., #17293 and #14124. The breakage was pretty severe\r\n\r\nThanks a lot @HyukjinKwon, what can we do for user here, they want to get an error when column is null or missing

Is this only a problem for file source v2? If yes I think we should just fix file source v2 to respect `LEGACY_RESPECT_NULLABILITY_IN_TEXT_DATASET_CONVERSION`

@vicennial This PR changed after your stamp. Could you re-review the change again?\r\n@hvanhovell Please take a look :D

Good to have this to unblock streaming use case!

LGTM - will merge after CI

Seems unrelated. Triggering a re-run.\r\n```\r\n  File "/__w/spark/spark/python/pyspark/install.py", line 166, in install_spark\r\n    raise OSError("Unable to download %s." % pretty_pkg_name)\r\nOSError: Unable to download spark-3.0.1 for Hadoop hadoop3.2.\r\n```

Merging to master. Thanks!\r\n

Could you review this when you have some time, @viirya ?

Thank you, @viirya . Merged to master.

For the record, `PythonForeachWriterSuite` passed with this PR on MacOS environment successfully.\r\n- https://github.com/apache/spark/actions/runs/11168954174/job/31048570789

cc - @neilramaswamy @HeartSaVioR - PTAL, thx !

> Probably also want a suite for TTL \r\n\r\nYea, added tests to the TTL suites also

cc @HeartSaVioR 

linter error seems to be unrelated, but for 100% sure, shall we rebase with the latest master and push so that CI would be triggered again? You may want to also pull master branch of your repo to be up to date.

@HeartSaVioR - done. CI looks green now

Thanks! Merging to master.

Thank you, @viirya !

@cloud-fan please take a look at this when you get the chance

+1, LGTM. Merging to master.\r\nThank you, @stefankandic.

shall we backport the fix to 3.5 as well?

Just a test for the new datasketches-memory release, no need to pay attention.\r\n\r\nhttps://github.com/apache/datasketches-memory/issues/213

@uros-db Are you ok w/ the changes?

+1, LGTM. Merging to master.\r\nThank you, @jovanpavl-db and @uros-db @mihailom-db for review.

I have no issues with IntelliJ IDEA 2024.2.3, btw, have you checked this?\r\n\r\n![image](https://github.com/user-attachments/assets/06854115-bf56-47f8-959f-23cc322dcc1d)\r\n

With an older version of IntelliJ it looks like this:\r\n![grafik](https://github.com/user-attachments/assets/28c9b9d5-f430-42b1-9af6-1a336070d843)\r\n\r\nThe `--release` option is enabled but the target bytecode versions differ from you screenshot.\r\n\r\nWith latest IntelliJ they look like in your screenshot.\r\n\r\nDo the suggested properties in pom.xml do any harm? Other build environments might also benefit from this. The error is quite opaque.\r\n\r\nOr shall we rather document the minimum IntelliJ version somewhere?

I am using IntelliJ IDEA 2021.2 (Community Edition), which does not work without this fix.

cc. @cloud-fan Would you mind take a look at this? Thanks!

cc. @viirya 

Thanks! Merging to master/3.5/3.4.

thanks, merging to 3.5!

cc @viirya and @huaxingao from #48251

Thank you @dongjoon-hyun 

Thank you, @viirya . \r\n\r\nAlso, cc @hvanhovell and @HyukjinKwon too because this is PySpark Connect 3.5 compatibility test change.

Merged to branch-3.5 to recover the CIs.

For the record, CI is recovered after this PR.\r\n- https://github.com/apache/spark/actions/workflows/build_python_connect35.yml\r\n    - https://github.com/apache/spark/actions/runs/11133416087/job/30939457155\r\n\r\n```\r\nSkipped tests in pyspark.sql.tests.connect.test_parity_types with python3:\r\n...\r\ntest_cast_to_udt_with_udt (pyspark.sql.tests.connect.test_parity_types.TypesParityTests.test_cast_to_udt_with_udt) ... skip (0.001s)\r\n...\r\n```

> If we need to open this, shall we move the package location from `internal` to a more proper package location, @holdenk ?\r\n\r\nSeems reasonable to me.\r\n

Could you review this when you have some time, @huaxingao ?

Also, cc @viirya and @HyukjinKwon 

Thank you, @viirya .

Merged to master for Apache Spark 4.0.0.

lgtm

test failures are unrelated, failed to download an artifact

cc @sunchao 

Thanks for the ping @dongjoon-hyun ! will take a look in the next few days

Merged to master, thanks @szehon-ho !

Thank you for review @sunchao !

> > Currently, there are no tests for the NULLIF function. We should add tests to prevent regressions.\r\n> \r\n> There are some tests in:\r\n> \r\n> https://github.com/apache/spark/blob/c0a1ea2a4c4218fc15b8f990ed2f5ea99755d322/sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala#L327-L334\r\n> \r\n> Implementation of `nullif` depends on the SQL config `spark.sql.alwaysInlineCommonExpr`. How about to test the expr w/ enabled/disabled the flag?\r\n\r\nAdded

+1, LGTM. Merging to master.\r\nThank you, @ivanjevtic-db.

cc @peter-toth @viirya 

Merged to branch-3.5. Thank you, @cloud-fan , @viirya , @peter-toth .

Can we add a test?

> Can we add a test?\r\n\r\nSorry for the late reply. I am on holiday these days. I will try to simplify SQL as UT later.

Test added, can you help take a look again? Thank you. @HyukjinKwon 

cc @cloud-fan Can you help take a look, thanks.

@zml1206 which Spark version starts to have this bug?

> @zml1206 which Spark version starts to have this bug?\r\n\r\nI encountered this bug in 3.4.3, do I need to verify other versions?

`AQEPropagateEmptyRelation` was added in version 3.2. I tested this bug starting from 3.2. @cloud-fan 

thanks, merging to master!

@zml1206 can you help to open backport PRs to 3.5 and 3.4? thanks!

> @zml1206 can you help to open backport PRs to 3.5 and 3.4? thanks!\r\n\r\nDone, #48505 #48506

Hi @dongjoon-hyun could you please take a look, thank you !

Could you make CI happy, @prathit06 ?

+1, LGTM. Merging to master.\r\nThank you, @exmy.

@dongjoon-hyun, this issue has been around since the time that we added multiple stateful operators to Structured Streaming. The Left-semi join logic that you linked previously is correct, and Jungtaek preserves that behavior in this PR.

https://issues.apache.org/jira/browse/SPARK-40925\r\nIt was 3.4.0, sorry I forgot that there was a long time between we fixed a bug to support multiple stateful operators partially and we fixed the watermark mechanism to support stream-stream join followed by stateful operator. This is about former. 

Thank you, @HeartSaVioR .

Fix makes sense, will thoroughly review tests shortly.

@xuanyuanking @neilramaswamy Would you mind continue reviewing? Thanks in advance!

@neilramaswamy \r\nYeah I have to call it out as "state watermark" rather than "watermark for eviction". I originally thought `state watermark <= watermark for eviction`, but you\

Build failure came from Spark connect which is not relevant:\r\n\r\n```\r\norg.apache.spark.sql.connect.execution.ReattachableExecuteSuite\r\n```

cc. @viirya @brkyvz Hopefully requesting review. Thanks in advance.

Just pointing this out for completeness: another way to fix this issue would be to make sure that the state watermark is never larger than the watermark for late events. You can do this by computing the real state watermark to be the minimum of the computed state watermark and the watermark for late events.\r\n\r\nNot saying that you should do this, but it would fix the issue too. 

The overall direction of watermark is to advance as fast as we see safe and also not break the simplicity of current watermark model (so a sort of trade-off).\r\n\r\nI might not put the design discussion into JIRA ticket, but I got an input internally when I designed supporting multiple stateful operators - why not just introduce state watermark for all stateful operators, and define output watermark based on state watermark e.g. based on completed windows for window aggregation. This technically delays the advance of watermark by one batch "per operator", due to the mechanism of how we calculate and propagate watermark (at the planning rather than within microbatch). So we rejected it and decided to tolerate some tricky situation like this.\r\n\r\nThat said, the way we do is by design/intention. If you see the feedback from @andrzejzera who reported the correctness issue, he even said it\

cc. @viirya @brkyvz Friendly reminder.\r\n

Thanks all for the review! Merging to master/3.5/3.4.

(porting to 3.5/3.4 in progress... need to make some change as state data source reader is only available in 4.0, just small change in testing)

Please review @pan3793 @cloud-fan @HyukjinKwon 

Hi @pan3793 @HyukjinKwon . I have raised the request for master in https://github.com/apache/spark/pull/48337 . Please review.\r\nIn master the get all partitions request is migrated to function getAllPartitionsOf, which has the implementation of HIVE-27505. If hive is upgraded to 3 from 2.x, this change will not be required anymore. Therefore, I believe this fix is more relevant to the lower versions of Spark as well. \r\nThank you

This is blocked by \r\n- https://github.com/apache/hadoop/pull/7079

For the record, all tests passed except K8s Integration Tests trying to download 3.4.1 directly. So, I casted +1 for Apache Hadoop 3.4.1 RC3. I will convert this to a normal PR after the official release.\r\n```\r\n[info] Exception in thread "main" java.lang.RuntimeException: [unresolved dependency: org.apache.hadoop#hadoop-aws;3.4.1: not found]\r\n```

All tests passed. Merged to master for Apache Spark 4.0.0 on February 2025.

> Thank you. Is this ready, @panbingkun ?\r\n\r\nYeah, thanks! â¤ï¸

Merged to master.

cc - @HeartSaVioR @siying - PTAL, thx !

Thanks! Merging to master.

cc @dongjoon-hyun 

https://github.com/bogao007/spark/actions/runs/11433439704/job/31805443260\r\nIt only failed with org.apache.spark.sql.SparkSessionE2ESuite.

Thanks! Merging to master.

Hi, @yaooqinn . It would be great if we can have this test coverage in the release branches. Could you review this as the author?

Please note that `MsSqlServer` suites are currently broken in `branch-3.5/3.4`. This PR is very helpful during the investigation because we can investigate on Mac environment.\r\n- https://github.com/apache/spark/actions/runs/11069480476/job/30784081196 (branch-3.5)\r\n- https://github.com/apache/spark/actions/runs/11067744916/job/30784071170 (branch-3.4)\r\n```\r\n[info] *** 3 SUITES ABORTED ***\r\n[error] Error during tests:\r\n[error] \torg.apache.spark.sql.jdbc.v2.MsSqlServerNamespaceSuite\r\n[error] \torg.apache.spark.sql.jdbc.v2.MsSqlServerIntegrationSuite\r\n[error] \torg.apache.spark.sql.jdbc.MsSqlServerIntegrationSuite\r\n```

Late LGTM

For the record, the daily CIs of branch-3.5/branch-3.4 are recovered with this and [SPARK-47949](https://issues.apache.org/jira/browse/SPARK-47949) . Thank you, @yaooqinn .

@mrk-andreev Could you fix the failed tests:\r\n```\r\n[info] - Error conditions are correctly formatted *** FAILED *** (103 milliseconds)\r\n[info]   "...023"\r\n[info]     },\r\n[info]     "SCALAR_[FUNCTION_NOT_COMPATIBLE" : {\r\n[info]       "message" : [\r\n[info]         "ScalarFunction <scalarFunc> not overrides method \

Waiting for CI. @mrk-andreev Could you re-trigger the failed GitHub action:\r\n[Run / Build modules: api, catalyst, hive-thriftserver](https://github.com/mrk-andreev/spark/actions/runs/11258166651/job/31304210334#logs)

@MaxGekk fixed. \r\n\r\n[![Build](https://github.com/mrk-andreev/spark/actions/workflows/build_main.yml/badge.svg)](https://github.com/mrk-andreev/spark/actions/workflows/build_main.yml)

+1, LGTM. Merging to master.\r\nThank you, @mrk-andreev.

This seams to be the only `raise IOError` we have.\r\n\r\n![image](https://github.com/user-attachments/assets/d1b57a80-e18e-49a9-b157-0362fce78ef1)\r\n

Merged to master.

thanks, merging to master/3.5!

gas-connector -> gcs-connector 

LGTM 

Thank you, @bjornjorgensen and @huaxingao .\r\nMerged to master.

cc @viirya @allisonwang-db @gengliangwang 

Looks like it causes CTE test failures:\r\n\r\n\r\n```\r\n[error] Failed tests:\r\n[error] \torg.apache.spark.sql.CTEInlineSuiteAEOn\r\n[error] \torg.apache.spark.sql.CTEInlineSuiteAEOff\r\n```

@peter-toth View is not the only issue here, people can do `val df = sql("WITH ...")` and use this `df` multiple times to build a new DataFrame

thanks for the review, merging to master!

Anyways, this PR works and we can keep it as it is. But https://github.com/peter-toth/spark/commit/809b38c325f11d97aa634def6391a212c9aa5c98 looks like a much simpler solution for the problem.

@peter-toth since the inner CTE can reference outer CTE relations, I think we need to propagate the `outerCTEId` parameter when calling `buildCTEMap` for the CTE main query.

It looks like we need to take care of the case when the entire `WithCTE` can be eliminated.

@peter-toth I double-checked this by manually building such a logical plan, and everything is fine. Let me explain what happens:\r\n- when we call `buildCTEMap` for the CTE definition of `r2`, it walks through the entire `WithCTE` node including all CTE definitions and the main query, so the references to `r1` will be recorded in the out-going-ref-count of `r2`.\r\n- since `r2` is not referenced, we decrease the ref count of `r1` and finally inline `r1`.\r\n\r\n

@peter-toth the reference to `r1` is not recorded as the out-going-ref-count of `t1`, but the outer `r2`, according to the current code, which I think is reasonable.

Or maybe we could clean/inline WithCTE nodes locally and propagate up the remaining outer references in a bottom-up manner.

It seems the clearest solution is to do ref count for each `WithCTE` individually. I think generating new IDs is one way to do it, and another idea is to introduce ref counting context for each `WithCTE`.

Yeah, if you are Ok with generating new Ids then I can open a PR from https://github.com/peter-toth/spark/commit/7716dbfd7bca729f481a6bbf811fbcd30925de7c.

Yes please, we can discuss more on your PR later.

Opened a PR here: https://github.com/apache/spark/pull/48429

cc @HyukjinKwon 

Merged to master.

Merged to master.

Merged into master for Spark 4.0. Thanks @HyukjinKwon 

Got it. Thank you. That sounds enough for now because Java 23 is not LTS.\r\nMerged to master.

Thanks @dongjoon-hyun 

We should say more about how the dead lock happens, e.g. the order how these two threads obtain locks.

> We should say more about how the dead lock happens, e.g. the order how these two threads obtain locks.\r\n\r\ndone, updated in the pr desc

thanks, merging to master!

It needs more discussion on how to resolve this issue, let me revert it for now

will be reverted in https://github.com/apache/spark/pull/48362

Followup to https://github.com/apache/spark/pull/48269 \r\n@yaooqinn @dongjoon-hyun 

@yaooqinn should I call it "[FOLLOWUP][SPARK-49801][PYTHON][PS][BUILD] Update pandas to 2.2.3" ?

The PR title LGTM

cc @HyukjinKwon @zhengruifeng 

the python doctest highly depends on the `show` format, will this break them?

> the python doctest highly depends on the `show` format, will this break them?\r\n\r\n\r\n@zhengruifeng \r\n<del> It looks like it is, </del>\r\n\r\nFrom the GA results, it seems that this will not break them, only some UT failed, which is unrelated to `doctest`\r\n<img width="968" alt="image" src="https://github.com/user-attachments/assets/a2f0af67-2247-47a7-a6c9-bca50c806f4b">\r\n<img width="595" alt="image" src="https://github.com/user-attachments/assets/2a295207-9f32-4287-9369-069c9050fa99">\r\n\r\nIf we think it should be fixed, I can update it together.\r\nBecause it looks too `weird`, the separate between command is too inconsistent.

Merged into master for Spark 4.0. Thanks @zhengruifeng and @dongjoon-hyun 

thanks @LuciferYang and @dongjoon-hyun !

Thank you for making a PR, @fe2s .

Merged to master/3.5/3.4.

cc - @HeartSaVioR - PTAL, thx !

Thanks! Merging to master.

What do you mean by `ghfix`, @zhengruifeng ?

@dongjoon-hyun sorry, this PR was made by mistake

cc @yaooqinn and @LuciferYang 

Thank you, @HyukjinKwon .\r\nMerged to master/3.5/3.4.

late LGTM

Could you review this, @huaxingao ? After backporting, I found that the difference of `import` in branch-3.5.\r\n\r\nIn CI, the compilation already passed and `Unit Test` is running.\r\n```\r\n========================================================================\r\nRunning Spark unit tests\r\n========================================================================\r\n```

Thank you so much always, @huaxingao !

Merged to branch-3.5 to recover CIs.

cc @gengliangwang @cloud-fan here is the SQL pipe JOIN operator. This is the last "simple" one that is just adding a single "with*" method call that already exists in the AstBuilder.

the CI failure is not related

The spark connect failure is unrelated, thanks, merging to master!

Merged to master.

Because I messed up...

Shall we make this `Draft` to prevent accidental merging for now, please?

BTW, please create JIRA ticker and link it in the PR title.

> BTW, please create JIRA ticker and link it in the PR title.\r\n\r\nAlso the [SQL] tag as well.

> BTW, please create JIRA ticker and link it in the PR title.\r\n\r\nDone.

thanks, merging to master!

@HyukjinKwon Could you take a look at this documentation improvement on `RuntimeConfig#get`? Thanks.

Hi @HyukjinKwon , I reformated the code, but the `Documentation Generation` still fails. Do you have any ideas how I can fix it?

+1, LGTM. Merging to master.\r\nThank you, @xi-db and @HyukjinKwon @vicennial for review.

cc @HyukjinKwon 

cc @dongjoon-hyun 

Merged to master. Thank you, @panbingkun and @HyukjinKwon .

cc @LuciferYang @dongjoon-hyun 

Merged into master. Thanks @panbingkun 

cc @MaxGekk @cloud-fan 

Yea I think this is common sense. Do other systems have `array_contains` function and how do they handle null?

> array_contains\r\n\r\n- doris\r\nhttps://doris.apache.org/docs/2.0/sql-manual/sql-functions/array-functions/array-contains/\r\n<img width="785" alt="image" src="https://github.com/user-attachments/assets/1100bebc-ed03-4f3e-aa80-f57c60139dd1">\r\n<img width="404" alt="image" src="https://github.com/user-attachments/assets/7ceeac17-f0c9-47de-bfe8-135aa091b198">\r\n\r\n\r\n- mysql\r\n```sql\r\nmysql> SELECT FIND_IN_SET(\

> Yea I think this is common sense. Do other systems have `array_contains` function and how do they handle null?\r\n\r\nCurrent behavior seems to match how we handle IN lists (and those are in the SQL standard):\r\nselect null in (1, 2, null);\r\n-- returns NULL, not true.\r\n\r\nFundamentally, if array_contains() uses equality to establish whether an element belongs to an array, then NULL = NULL returns NULL, not true.\r\n\r\nSee also https://docs.databricks.com/en/sql/language-manual/functions/array_contains.html#examples

> > Yea I think this is common sense. Do other systems have `array_contains` function and how do they handle null?\r\n> \r\n> Current behavior seems to match how we handle IN lists (and those are in the SQL standard): select null in (1, 2, null); -- returns NULL, not true.\r\n> \r\n> Fundamentally, if array_contains() uses equality to establish whether an element belongs to an array, then NULL = NULL returns NULL, not true.\r\n> \r\n> See also https://docs.databricks.com/en/sql/language-manual/functions/array_contains.html#examples\r\n\r\nThis is really `a rule` that goes against common sense, and I believe most data processors will be troubled by this rule.

Give another counter example:\r\n```shell\r\nspark-sql (default)> select array_distinct(array(1, 2, 3, null, 3,null));\r\n[1,2,3,null]\r\nTime taken: 0.055 seconds, Fetched 1 row(s)\r\n\r\nspark-sql (default)> select array_union(array(1, 2, 3, null), array(1, 3, 5, null));\r\n[1,2,3,null,5]\r\nTime taken: 0.067 seconds, Fetched 1 row(s)\r\n```

cc @srielau @cloud-fan 

Thanks for the fix!

Added test. Thanks!

Merged to master.\r\nThanks @srielau and @MaxGekk for the review!

cc @itholic 

Merged to master.

Hi @dongjoon-hyun , the entire `DelegatingCatalogExtension` class is marked as Evolving, which already means semi-public APIs. We can probably add `@DeveloperApi` to all DS v2 APIs?\r\n\r\nAnd yes, we want to backport this to 3.5.

Merged to master/3.5! Thank you, @cloud-fan and all!

Hi @dongjoon-hyun @cloud-fan Could you please let me know if there is a plan or scheduled release date for Spark-354?\r\n\r\nthanks, -yuan

@zhengruifeng @HyukjinKwon may I get your review please?

Merged to master, thank you!\r\n

@cloud-fan 

Thanks! Merging to master.

cc @dongjoon-hyun @cloud-fan @sunchao @huaxingao 

Thank you for the answers. Feel free to merge, @viirya .

Merged to master. Thanks @viirya @dongjoon-hyun 

Thank you @dongjoon-hyun @huaxingao 

Hi, @viirya . It seems that Apache Spark 3.5 PyConnect Client still expects a failure.\r\n- https://github.com/apache/spark/actions/workflows/build_python_connect35.yml\r\n\r\nI made a follow-up. Although the failure is a kind of Apache Spark 4.0.0 test CI, we need to disable this test case from branch-3.5 branch.\r\n- https://github.com/apache/spark/pull/48307

Actually  I wanted to make a fix like this a long time ago, and gave up after reading ANSI spec because UDT cannot be casted to any type according to it IIRC.

Thank you for the alternative, @cloud-fan .

Could you review this PR, @huaxingao ?

Thank you, @huaxingao !

Thank you again, Huaxin and Mridul.\r\nMerged to master.

Also, cc @panbingkun since he is working on this area.

Thank you, @LuciferYang and @pan3793 .\r\nMerged to master for Apache Spark 4.0.0.

Thanks @dongjoon-hyun and @pan3793 

> Also, cc @panbingkun since he is working on this area.\r\n\r\n+1, LGTM. \r\nThanks @dongjoon-hyun 

Thanks  @panbingkun ~

@dongjoon-hyun good point, but I think we cannot reuse existing sql side `.sql` files for now, because this test is mainly for python side feature, e.g.\r\n```\r\n        df0 = self.spark.range(10)\r\n        df1 = self.spark.sql(\r\n            "SELECT * FROM {df} WHERE id > ?",\r\n            args=[1],\r\n            df=df0,\r\n        )\r\n```\r\nwhich takes a PySpark dataframe `df0` as a named argument.\r\n\r\nBut probably it is doable to introduce a similar framework for PySpark in the future.

LGTM, thank you!

Thanks, @zhengruifeng . It makes sense.

thanks all, merged to master

Merged to master for Apache Spark 4.0.0.

merged to master

@cloud-fan - a tiny followup cleanup.

cc @srielau @cloud-fan 

Thanks @srielau for the review! Just adjusted the comments :)

@itholic please update description of the PR to follow the changes added.

Gentle ping @wangyum @zhengruifeng 

Gentle ping @cloud-fan @wangyum 

Hi @cloud-fan , can this be merged now?

thanks, merging to master/3.5!

cc @itholic 

Merged to master. Thanks for following up!

cc @LuciferYang  @cloud-fan @dongjoon-hyun

also cc @tgravescs and @pan3793 

Is it possible to add a new test case with MiniYARNCluster? @zuston 

> Is it possible to add a new test case with MiniYARNCluster? @zuston\r\n\r\nFrom my sight, this is hard to simulate this case to reproduce in test case. But I have verified in our internal cluster, the detail verification could be found in this blog: https://zuston.vercel.app/publish/resource-leak-of-spark-yarn-allocator#Verification

The code is OK for me, but it would be even better if we could continuously ensure that this behavior meets expectations by adding an additional test case. \r\n\r\nAlso, could you provide more detailed information in the `How was this patch tested?` section? The description `In our internal hadoop cluster` is too opaque box, we need a reproducible verification method to allow reviewers to confirm that the issue truly exists and has been fixed.

> Does it mean, with this change, the waste resource could be leveraged so that all spark jobs can use more resources and execute faster? Or just shrink the gap of metric between the Spark event log and YARN?\r\n\r\nThe unrelease resource will be still occupied at least 10 min in the Yarn ResouceManager, but these resources are not used by spark. So these resources are wasted.\r\n\r\nAnd the gap of yarn collected vcore * seconds metrics and spark collected vcore * seconds from all finished spark jobs is the wasted resource.\r\n\r\nI will attach some online cluster report if possible.

Looks reasonable to me, but it would be better if @tgravescs could take a look.

Could you help review this? @tgravescs 

Thanks, +1.

This PR looks good. YARN-11702 proposes a general method. I am not sure whether it is related to this PR.\r\n\r\n### JIRA\r\nYARN-11702: Fix Yarn over allocating containers\r\nhttps://issues.apache.org/jira/browse/YARN-11702\r\nFix Version/s: 3.5.0

> This PR looks good. YARN-11702 proposes a general method. I am not sure whether it is related to this PR.\r\n> \r\n> ### JIRA\r\n> YARN-11702: Fix Yarn over allocating containers https://issues.apache.org/jira/browse/YARN-11702 Fix Version/s: 3.5.0\r\n\r\nThanks for your reply. I have seen YARN-11702, from my side, it solves the concurrency allocation problem about the AM and RM connection, and it is only valid that scoped in the resouce request (not including the scheduling request)

> Looks reasonable to me, but it would be better if @tgravescs could take a look.\r\n\r\nCould you help take another look? @mridulm 

## size(map_from_arrays(array(...), array(...)))\r\n### Benchmark code:\r\n```scala\r\nobject SizeBenchmark extends SqlBasedBenchmark {\r\n  private val N = 10_000_00\r\n  private val M = 100\r\n\r\n  private val path = "/Users/panbingkun/Developer/spark/spark-community/SizeBenchmark"\r\n  private val df = spark.range(N).to(new StructType().add("id", "int")).\r\n    withColumn("id1", col("id") + 1).\r\n    withColumn("id2", col("id") + 2).\r\n    withColumn("id3", col("id") + 3).\r\n    withColumn("id4", col("id") + 4).\r\n    withColumn("id5", col("id") + 5)\r\n  df.write.parquet(path)\r\n  private val table = spark.read.parquet(path)\r\n\r\n  private def doBenchmark(): Unit = {\r\n    table.selectExpr("size(map_from_arrays(array(id, id1, id2), array(id3, id4, id5)))").noop()\r\n  }\r\n\r\n  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\r\n    runBenchmark("size") {\r\n      val benchmark = new Benchmark("size", N, output = output)\r\n      benchmark.addCase("optimize", M) { _ =>\r\n        doBenchmark()\r\n      }\r\n      benchmark.run()\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\n### Benchmark Result:\r\n#### Before\r\n```shell\r\nRunning benchmark: size\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 15653 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0\r\nApple M2\r\nsize:                                     Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            142            157           8          7.0         142.0       1.0X\r\n\r\n\r\nRunning benchmark: size\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 17672 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0\r\nApple M2\r\nsize:                                     Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            160            177          25          6.3         159.9       1.0X\r\n\r\n\r\nRunning benchmark: size\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 15140 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0\r\nApple M2\r\nsize:                                     Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            141            151          13          7.1         140.6       1.0X\r\n\r\n```\r\n\r\n#### After\r\n```shell\r\nAfter:\r\n\r\nRunning benchmark: size\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 3923 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0\r\nApple M2\r\nsize:                                     Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                             24             39          13         42.4          23.6       1.0X\r\n\r\n\r\nRunning benchmark: size\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 3778 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0\r\nApple M2\r\nsize:                                     Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                             31             38           7         32.1          31.2       1.0X\r\n\r\n\r\nRunning benchmark: size\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 3040 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0\r\nApple M2\r\nsize:                                     Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                             23             30           7         42.8          23.4       1.0X\r\n```

## size(map_from_entries(array(...)))\r\n### Benchmark code:\r\n```scala\r\nobject SizeBenchmark extends SqlBasedBenchmark {\r\n  private val N = 10_000_00\r\n  private val M = 100\r\n\r\n  private val path = "/Users/panbingkun/Developer/spark/spark-community/SizeBenchmark"\r\n  private val df = spark.range(N).to(new StructType().add("id", "int")).\r\n    withColumn("id1", col("id") + 1).\r\n    withColumn("id2", col("id") + 2).\r\n    withColumn("id3", col("id") + 3).\r\n    withColumn("id4", col("id") + 4).\r\n    withColumn("id5", col("id") + 5)\r\n  df.write.parquet(path)\r\n  private val table = spark.read.parquet(path)\r\n\r\n  private def doBenchmark(): Unit = {\r\n    table.selectExpr("size(map_from_entries(array(struct(id, id3))))").noop()\r\n  }\r\n\r\n  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\r\n    runBenchmark("size") {\r\n      val benchmark = new Benchmark("size", N, output = output)\r\n      benchmark.addCase("optimize", M) { _ =>\r\n        doBenchmark()\r\n      }\r\n      benchmark.run()\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\n### Benchmark Result:\r\n#### Before\r\n```shell\r\nRunning benchmark: size\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 12723 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0\r\nApple M2\r\nsize:                                     Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            105            127          19          9.5         104.9       1.0X\r\n\r\n\r\nRunning benchmark: size\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 13554 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0\r\nApple M2\r\nsize:                                     Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            122            136           9          8.2         121.8       1.0X\r\n\r\n\r\nRunning benchmark: size\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 12055 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0\r\nApple M2\r\nsize:                                     Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                            105            121          12          9.5         105.3       1.0X      \r\n```\r\n\r\n#### After\r\n```shell\r\nRunning benchmark: size\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 3246 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0\r\nApple M2\r\nsize:                                     Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                             22             32           8         46.1          21.7       1.0X\r\n\r\n\r\nRunning benchmark: size\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 3312 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0\r\nApple M2\r\nsize:                                     Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                             23             33          18         42.7          23.4       1.0X\r\n\r\n\r\nRunning benchmark: size\r\n  Running case: optimize\r\n  Stopped after 100 iterations, 3236 ms\r\n\r\nOpenJDK 64-Bit Server VM 17.0.10+7-LTS on Mac OS X 15.0\r\nApple M2\r\nsize:                                     Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\noptimize                                             20             32          15         48.9          20.4       1.0X\r\n```

@zhengruifeng may I get a review please? Thank you!

Hello @xinrong-meng! I was wondering if I could contribute to spark! \r\nI am currently a Data Engineer with a British Bank and I have developed sound knowledge over pyspark through the last several months. \r\nCould you guide me further over prospective issues I could work on! \r\n\r\nFor now I am reading up on merged pull requests to get a picture on the issues taken up so far\r\nThanks! 

Merged to master.

Thank you @zhengruifeng and @HyukjinKwon !

Hi @abhinavgudipati thanks for reaching out! Itâ€™s great to hear that youâ€™re interested in contributing to Spark.\r\nA good starting point would be to look at the open issues on the [Apache Spark JIRA](https://issues.apache.org/jira/projects/SPARK/issues). Since youâ€™re already reading up on merged pull requests, youâ€™re on the right track! That will give you a good idea of the areas you can contribute to. You might want to check out the PySpark components since you have experience there.\r\nAlso, participating in code reviews is a great way to get familiar with the codebase.\r\nYou can find more detailed guidance on https://spark.apache.org/contributing.html good luck with your journey into Spark contributions!

Merged to master.

any update on this one @panbingkun 

Thanks! Merging to master.

cc @HyukjinKwon 

Merged to master.

Merging to master.

The test failure is unrelated, thanks, merging to master!

Might need to run `dev/reformat-python` :)

Merged to master.

cc @cloud-fan would you mind taking a look when you find some time? thanks

friendly ping @cloud-fan 

thanks, merging to master!

Thank you so much, @panbingkun , @cloud-fan , @zhengruifeng , @LuciferYang !

Thanks all @cloud-fan @zhengruifeng @LuciferYang @dongjoon-hyun ! â¤ï¸

cc @MaxGekk @cloud-fan 

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

Merged to master.

thanks, merging to master!

Merged to master.

Thanks @HyukjinKwon !

@dongjoon-hyun @mridulm @HyukjinKwon Can you please help to review this PR.

> @dongjoon-hyun @mridulm @HyukjinKwon Can you please help to review this PR.\r\n\r\nPlease let me know if there is anything further to be addressed for this PR.

Merged to master, thank you!

LGTM thank you!

Merged to master.

BTW, please fill the PR description, @hvanhovell . 

Gentle ping, @hvanhovell .

Merging.

Thank you, @hvanhovell .

This pull request is intended for branch-3.5 and cherry-pick to branch-3.4. #48214 is the corresponding master branch pull request.

Merged to branch-3.5 and 3.4.

cc @cashmand For changes in the README file.

@cloud-fan @HyukjinKwon can you please look at this PR? Thanks!

Merged to master.

BTW, according to the original code, this seems to be very old bug, right? Then, we need to backport this to `branch-3.4` too.

Thank you, @cnauroth and @yaooqinn .\r\nMerged to master.

@dongjoon-hyun and @yaooqinn , thanks very much for the review and commit!

Merging.

Merging.

@cloud-fan @JoshRosen \r\nGiven the change, do you think it would make sense to add a copy() API to Dataset, sth like\r\n```\r\n/**\r\n * Create a copy of this Dataset with a fresh execution.\r\n *\r\n * While a Dataset object caches its query plan, this will create a new Dataset that will\r\n * start from scratch from the parsed logical plan.\r\n */\r\ndef copy(): Dataset[T] = {\r\n  new Dataset[T](this.sparkSession, this.queryExecution.logical, this.encoder)\r\n}\r\n```\r\nso that users can re-run all stages that are otherwise lazy cached?\r\nIt was relevant also before this change, because I don\

Same issue with Pytorch dataloader:\r\n```\r\nStarting test(python3.11): pyspark.ml.tests.connect.test_parity_torch_data_loader (temp output: /__w/apache-spark/apache-spark/python/target/48a24a54-af73-4bf1-8b17-c2962e25b85c/python3.11__pyspark.ml.tests.connect.test_parity_torch_data_loader__t8i37nvz.log)\r\nError: The operation was canceled.\r\n```\r\nðŸ¤· 

thanks, merging to master!

ping @cloud-fan Please help me review this PR.

thanks, merging to master!

@cloud-fan Thank you!

Manually tested the connect-jvm-client and connect-server modules using Maven, and all passed.\r\n\r\nMerged into master. 

@juliuszsompolski Hi again! I know... this change is a bit complicated, so I wouldn\

New test failure caused by the updated `interrupt` method semantics: to be fixed late today.\r\n\r\n```\r\n[info] - interrupt all - background queries, foreground interrupt *** FAILED *** (127 milliseconds)\r\n[info]   ListBuffer("18e26c82-502f-48bb-8028-328963d8e6d3") had length 1 instead of expected length 2 Interrupted operations: ListBuffer(18e26c82-502f-48bb-8028-328963d8e6d3). (SparkSessionE2ESuite.scala:79)\r\n[info]   org.scalatest.exceptions.TestFailedException:\r\n```

Merged to master.

Merged to master.

FYI there is a similar effort in https://github.com/databricks/runtime/pull/82979\r\nBut it got reverted https://github.com/databricks/runtime/pull/98628/\r\nDouble confirm: have we checked with Kent Marten on this one?\r\n

@mihailom-db how about splitting it to multiple PRs?

cc - @ericm-db @HeartSaVioR - PTAL, thx !

Thanks! Merging to master.

Test first

Thanks @dongjoon-hyun 

cc @cloud-fan \r\n\r\n

Merged into master. Thanks, @cloud-fan, for raising the issue and providing a thorough review.

After set `-XX:PerMethodRecompilationCutoff=10000`:\r\n![profile_executor_after](https://github.com/user-attachments/assets/148208d5-4144-46b6-b75b-6a9e23c169e5)\r\n

This change looks like a significant performance improvement from just increasing PerMethodRecompilationCutoff\r\n\r\nIn the update to the docs, it says â€œset the JVM flags, for exampleâ€¦â€\r\n\r\nMare there further options you know of that could need tuning, in which case would it be worth expanding on those flags? Not necessarily within the config description, but potentially within the performance tuning pages, so that this update/tuning is a bit easier to approach for users with less knowledge of JVM options?

Merged to master.

cc @HyukjinKwon @grundprinzip @TakawaAkirayo 

kindly ping @hvanhovell 

Merged to master.

maybe cc @viirya 

It appears that StandaloneSchedulerBackend is also supported.

cc @gengliangwang @LuciferYang 

@dongjoon-hyun the previous issue happens condition is not accurate. I have updated the PR description with more generic words.

Ping, @pan3793 .

@dongjoon-hyun thanks for checking, you are right, I am fixing the issue. Converting to DRAFT now.

@dongjoon-hyun I fixed the issue and verified the following cases:\r\n- configuring `spark.log.structuredLogging.enabled` in `spark-defaults.conf` with both `true` and `false`\r\n- running `sbin/start-history-server.sh` and `sbin/start-master.sh` and watching logs

I have updated the number after this change.

@srielau @panbingkun @nchammas @cloud-fan Could you review the PR, please.

Just noting for reference that my previous attempt to do some of this work is captured in https://github.com/apache/spark/pull/46543. You may find the list of tasks/TODOs there handy.

Merging to master. Thank you, @srielau @panbingkun @cloud-fan for review.

Merging to master.

Merged to master.

+1, LGTM. Merging to master.\r\nThank you, @HyukjinKwon.

For the record, this is blocked by:\r\n- https://github.com/lightbend/genjavadoc/issues/364\r\n- https://github.com/com-lihaoyi/Ammonite/pull/1561

Thank you for tracking this, @panbingkun .

compile warning deprecation\r\n```shell\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/TestUtils.scala:424:11: method write in class Files is deprecated\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.TestUtils.createTempScriptWithExpectedOutput, origin=com.google.common.io.Files.write\r\n[error]     Files.write(script, file, StandardCharsets.UTF_8)\r\n[error]           ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala:170:30: method getAllStatistics in class FileSystem is deprecated\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback.f.$anonfun, origin=org.apache.hadoop.fs.FileSystem.getAllStatistics\r\n[error]     val f = () => FileSystem.getAllStatistics.asScala.map(_.getThreadStatistics.getBytesRead).sum\r\n[error]                              ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala:201:34: method getAllStatistics in class FileSystem is deprecated\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.deploy.SparkHadoopUtil.getFSBytesWrittenOnThreadCallback.threadStats, origin=org.apache.hadoop.fs.FileSystem.getAllStatistics\r\n[error]     val threadStats = FileSystem.getAllStatistics.asScala.map(_.getThreadStatistics)\r\n[error]                                  ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/deploy/history/BasicEventFilterBuilder.scala:160:13: class SparkListenerExecutorBlacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.deploy.history.BasicEventFilter._acceptFn, origin=org.apache.spark.scheduler.SparkListenerExecutorBlacklisted, version=3.1.0\r\n[error]     case e: SparkListenerExecutorBlacklisted => liveExecutors.contains(e.executorId)\r\n[error]             ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/deploy/history/BasicEventFilterBuilder.scala:161:13: class SparkListenerExecutorUnblacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorUnexcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.deploy.history.BasicEventFilter._acceptFn, origin=org.apache.spark.scheduler.SparkListenerExecutorUnblacklisted, version=3.1.0\r\n[error]     case e: SparkListenerExecutorUnblacklisted => liveExecutors.contains(e.executorId)\r\n[error]             ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/deploy/history/HistoryAppStatusStore.scala:75:40: value isBlacklisted in class ExecutorSummary is deprecated (since 3.1.0): use isExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.deploy.history.HistoryAppStatusStore.replaceExecutorLogs, origin=org.apache.spark.status.api.v1.ExecutorSummary.isBlacklisted, version=3.1.0\r\n[error]       source.totalShuffleWrite, source.isBlacklisted, source.maxMemory, source.addTime,\r\n[error]                                        ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/deploy/history/HistoryAppStatusStore.scala:77:14: value blacklistedInStages in class ExecutorSummary is deprecated (since 3.1.0): use excludedInStages instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.deploy.history.HistoryAppStatusStore.replaceExecutorLogs, origin=org.apache.spark.status.api.v1.ExecutorSummary.blacklistedInStages, version=3.1.0\r\n[error]       source.blacklistedInStages, source.peakMemoryMetrics, source.attributes, source.resources,\r\n[error]              ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala:219:13: method append in class Files is deprecated\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.deploy.worker.DriverRunner.runDriver.initialize, origin=com.google.common.io.Files.append\r\n[error]       Files.append(header, stderr, StandardCharsets.UTF_8)\r\n[error]             ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala:194:13: method write in class Files is deprecated\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.deploy.worker.ExecutorRunner.fetchAndRunExecutor, origin=com.google.common.io.Files.write\r\n[error]       Files.write(header, stderr, StandardCharsets.UTF_8)\r\n[error]             ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala:36:16: method getAllStatistics in class FileSystem is deprecated\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.executor.ExecutorSource.fileStats, origin=org.apache.hadoop.fs.FileSystem.getAllStatistics\r\n[error]     FileSystem.getAllStatistics.asScala.find(s => s.getScheme.equals(scheme))\r\n[error]                ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala:121:7: method newTaskTempFile in class FileCommitProtocol is deprecated (since 3.3.0): use newTaskTempFile(..., spec: FileNameSpec) instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.internal.io.FileCommitProtocol.newTaskTempFile, origin=org.apache.spark.internal.io.FileCommitProtocol.newTaskTempFile, version=3.3.0\r\n[error]       newTaskTempFile(taskContext, dir, spec.suffix)\r\n[error]       ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala:157:7: method newTaskTempFileAbsPath in class FileCommitProtocol is deprecated (since 3.3.0): use newTaskTempFileAbsPath(..., spec: FileNameSpec) instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.internal.io.FileCommitProtocol.newTaskTempFileAbsPath, origin=org.apache.spark.internal.io.FileCommitProtocol.newTaskTempFileAbsPath, version=3.3.0\r\n[error]       newTaskTempFileAbsPath(taskContext, absoluteDir, spec.suffix)\r\n[error]       ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala:193:45: class SparkListenerExecutorBlacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.EventLoggingListener.onExecutorBlacklisted.event, origin=org.apache.spark.scheduler.SparkListenerExecutorBlacklisted, version=3.1.0\r\n[error]   override def onExecutorBlacklisted(event: SparkListenerExecutorBlacklisted): Unit = {\r\n[error]                                             ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala:202:14: class SparkListenerExecutorBlacklistedForStage in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorExcludedForStage instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.EventLoggingListener.onExecutorBlacklistedForStage.event, origin=org.apache.spark.scheduler.SparkListenerExecutorBlacklistedForStage, version=3.1.0\r\n[error]       event: SparkListenerExecutorBlacklistedForStage): Unit = {\r\n[error]              ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala:211:49: class SparkListenerNodeBlacklistedForStage in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeExcludedForStage instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.EventLoggingListener.onNodeBlacklistedForStage.event, origin=org.apache.spark.scheduler.SparkListenerNodeBlacklistedForStage, version=3.1.0\r\n[error]   override def onNodeBlacklistedForStage(event: SparkListenerNodeBlacklistedForStage): Unit = {\r\n[error]                                                 ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala:219:47: class SparkListenerExecutorUnblacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorUnexcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.EventLoggingListener.onExecutorUnblacklisted.event, origin=org.apache.spark.scheduler.SparkListenerExecutorUnblacklisted, version=3.1.0\r\n[error]   override def onExecutorUnblacklisted(event: SparkListenerExecutorUnblacklisted): Unit = {\r\n[error]                                               ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala:228:41: class SparkListenerNodeBlacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.EventLoggingListener.onNodeBlacklisted.event, origin=org.apache.spark.scheduler.SparkListenerNodeBlacklisted, version=3.1.0\r\n[error]   override def onNodeBlacklisted(event: SparkListenerNodeBlacklisted): Unit = {\r\n[error]                                         ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala:236:43: class SparkListenerNodeUnblacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeUnexcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.EventLoggingListener.onNodeUnblacklisted.event, origin=org.apache.spark.scheduler.SparkListenerNodeUnblacklisted, version=3.1.0\r\n[error]   override def onNodeUnblacklisted(event: SparkListenerNodeUnblacklisted): Unit = {\r\n[error]                                           ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/HealthTracker.scala:120:28: class SparkListenerExecutorUnblacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorUnexcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.HealthTracker.applyExcludeOnFailureTimeout.$anonfun, origin=org.apache.spark.scheduler.SparkListenerExecutorUnblacklisted, version=3.1.0\r\n[error]           listenerBus.post(SparkListenerExecutorUnblacklisted(now, exec))\r\n[error]                            ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/HealthTracker.scala:136:28: class SparkListenerNodeUnblacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeUnexcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.HealthTracker.applyExcludeOnFailureTimeout.$anonfun, origin=org.apache.spark.scheduler.SparkListenerNodeUnblacklisted, version=3.1.0\r\n[error]           listenerBus.post(SparkListenerNodeUnblacklisted(now, node))\r\n[error]                            ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/HealthTracker.scala:240:28: class SparkListenerNodeBlacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.HealthTracker.updateExcludedForFetchFailure, origin=org.apache.spark.scheduler.SparkListenerNodeBlacklisted, version=3.1.0\r\n[error]           listenerBus.post(SparkListenerNodeBlacklisted(now, host, 1))\r\n[error]                            ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/HealthTracker.scala:253:26: class SparkListenerExecutorBlacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.HealthTracker.updateExcludedForFetchFailure, origin=org.apache.spark.scheduler.SparkListenerExecutorBlacklisted, version=3.1.0\r\n[error]         listenerBus.post(SparkListenerExecutorBlacklisted(now, exec, 1))\r\n[error]                          ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/HealthTracker.scala:290:26: class SparkListenerExecutorBlacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.HealthTracker.updateExcludedForSuccessfulTaskSet.$anonfun, origin=org.apache.spark.scheduler.SparkListenerExecutorBlacklisted, version=3.1.0\r\n[error]         listenerBus.post(SparkListenerExecutorBlacklisted(now, exec, newTotal))\r\n[error]                          ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/HealthTracker.scala:309:28: class SparkListenerNodeBlacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.HealthTracker.updateExcludedForSuccessfulTaskSet.$anonfun, origin=org.apache.spark.scheduler.SparkListenerNodeBlacklisted, version=3.1.0\r\n[error]           listenerBus.post(SparkListenerNodeBlacklisted(now, node, excludedExecsOnNode.size))\r\n[error]                            ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala:545:28: class SparkListenerExecutorBlacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListener.onExecutorBlacklisted.executorBlacklisted, origin=org.apache.spark.scheduler.SparkListenerExecutorBlacklisted, version=3.1.0\r\n[error]       executorBlacklisted: SparkListenerExecutorBlacklisted): Unit = { }\r\n[error]                            ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala:550:36: class SparkListenerExecutorBlacklistedForStage in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorExcludedForStage instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListener.onExecutorBlacklistedForStage.executorBlacklistedForStage, origin=org.apache.spark.scheduler.SparkListenerExecutorBlacklistedForStage, version=3.1.0\r\n[error]       executorBlacklistedForStage: SparkListenerExecutorBlacklistedForStage): Unit = { }\r\n[error]                                    ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala:555:32: class SparkListenerNodeBlacklistedForStage in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeExcludedForStage instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListener.onNodeBlacklistedForStage.nodeBlacklistedForStage, origin=org.apache.spark.scheduler.SparkListenerNodeBlacklistedForStage, version=3.1.0\r\n[error]       nodeBlacklistedForStage: SparkListenerNodeBlacklistedForStage): Unit = { }\r\n[error]                                ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala:560:30: class SparkListenerExecutorUnblacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorUnexcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListener.onExecutorUnblacklisted.executorUnblacklisted, origin=org.apache.spark.scheduler.SparkListenerExecutorUnblacklisted, version=3.1.0\r\n[error]       executorUnblacklisted: SparkListenerExecutorUnblacklisted): Unit = { }\r\n[error]                              ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala:565:24: class SparkListenerNodeBlacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListener.onNodeBlacklisted.nodeBlacklisted, origin=org.apache.spark.scheduler.SparkListenerNodeBlacklisted, version=3.1.0\r\n[error]       nodeBlacklisted: SparkListenerNodeBlacklisted): Unit = { }\r\n[error]                        ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala:570:26: class SparkListenerNodeUnblacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeUnexcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListener.onNodeUnblacklisted.nodeUnblacklisted, origin=org.apache.spark.scheduler.SparkListenerNodeUnblacklisted, version=3.1.0\r\n[error]       nodeUnblacklisted: SparkListenerNodeUnblacklisted): Unit = { }\r\n[error]                          ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala:66:41: class SparkListenerExecutorBlacklistedForStage in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorExcludedForStage instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListenerBus.doPostEvent, origin=org.apache.spark.scheduler.SparkListenerExecutorBlacklistedForStage, version=3.1.0\r\n[error]       case executorBlacklistedForStage: SparkListenerExecutorBlacklistedForStage =>\r\n[error]                                         ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala:67:18: method onExecutorBlacklistedForStage in trait SparkListenerInterface is deprecated (since 3.1.0): use onExecutorExcludedForStage instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListenerBus.doPostEvent, origin=org.apache.spark.scheduler.SparkListenerInterface.onExecutorBlacklistedForStage, version=3.1.0\r\n[error]         listener.onExecutorBlacklistedForStage(executorBlacklistedForStage)\r\n[error]                  ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala:68:37: class SparkListenerNodeBlacklistedForStage in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeExcludedForStage instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListenerBus.doPostEvent, origin=org.apache.spark.scheduler.SparkListenerNodeBlacklistedForStage, version=3.1.0\r\n[error]       case nodeBlacklistedForStage: SparkListenerNodeBlacklistedForStage =>\r\n[error]                                     ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala:69:18: method onNodeBlacklistedForStage in trait SparkListenerInterface is deprecated (since 3.1.0): use onNodeExcludedForStage instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListenerBus.doPostEvent, origin=org.apache.spark.scheduler.SparkListenerInterface.onNodeBlacklistedForStage, version=3.1.0\r\n[error]         listener.onNodeBlacklistedForStage(nodeBlacklistedForStage)\r\n[error]                  ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala:70:33: class SparkListenerExecutorBlacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListenerBus.doPostEvent, origin=org.apache.spark.scheduler.SparkListenerExecutorBlacklisted, version=3.1.0\r\n[error]       case executorBlacklisted: SparkListenerExecutorBlacklisted =>\r\n[error]                                 ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala:71:18: method onExecutorBlacklisted in trait SparkListenerInterface is deprecated (since 3.1.0): use onExecutorExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListenerBus.doPostEvent, origin=org.apache.spark.scheduler.SparkListenerInterface.onExecutorBlacklisted, version=3.1.0\r\n[error]         listener.onExecutorBlacklisted(executorBlacklisted)\r\n[error]                  ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala:72:35: class SparkListenerExecutorUnblacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorUnexcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListenerBus.doPostEvent, origin=org.apache.spark.scheduler.SparkListenerExecutorUnblacklisted, version=3.1.0\r\n[error]       case executorUnblacklisted: SparkListenerExecutorUnblacklisted =>\r\n[error]                                   ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala:73:18: method onExecutorUnblacklisted in trait SparkListenerInterface is deprecated (since 3.1.0): use onExecutorUnexcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListenerBus.doPostEvent, origin=org.apache.spark.scheduler.SparkListenerInterface.onExecutorUnblacklisted, version=3.1.0\r\n[error]         listener.onExecutorUnblacklisted(executorUnblacklisted)\r\n[error]                  ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala:74:29: class SparkListenerNodeBlacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListenerBus.doPostEvent, origin=org.apache.spark.scheduler.SparkListenerNodeBlacklisted, version=3.1.0\r\n[error]       case nodeBlacklisted: SparkListenerNodeBlacklisted =>\r\n[error]                             ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala:75:18: method onNodeBlacklisted in trait SparkListenerInterface is deprecated (since 3.1.0): use onNodeExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListenerBus.doPostEvent, origin=org.apache.spark.scheduler.SparkListenerInterface.onNodeBlacklisted, version=3.1.0\r\n[error]         listener.onNodeBlacklisted(nodeBlacklisted)\r\n[error]                  ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala:76:31: class SparkListenerNodeUnblacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeUnexcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListenerBus.doPostEvent, origin=org.apache.spark.scheduler.SparkListenerNodeUnblacklisted, version=3.1.0\r\n[error]       case nodeUnblacklisted: SparkListenerNodeUnblacklisted =>\r\n[error]                               ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala:77:18: method onNodeUnblacklisted in trait SparkListenerInterface is deprecated (since 3.1.0): use onNodeUnexcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.SparkListenerBus.doPostEvent, origin=org.apache.spark.scheduler.SparkListenerInterface.onNodeUnblacklisted, version=3.1.0\r\n[error]         listener.onNodeUnblacklisted(nodeUnblacklisted)\r\n[error]                  ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/TaskSetExcludeList.scala:150:11: class SparkListenerExecutorBlacklistedForStage in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorExcludedForStage instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.TaskSetExcludelist.updateExcludedForFailedTask, origin=org.apache.spark.scheduler.SparkListenerExecutorBlacklistedForStage, version=3.1.0\r\n[error]           SparkListenerExecutorBlacklistedForStage(now, exec, numFailures, stageId, stageAttemptId))\r\n[error]           ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/scheduler/TaskSetExcludeList.scala:161:15: class SparkListenerNodeBlacklistedForStage in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeExcludedForStage instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.scheduler.TaskSetExcludelist.updateExcludedForFailedTask, origin=org.apache.spark.scheduler.SparkListenerNodeBlacklistedForStage, version=3.1.0\r\n[error]               SparkListenerNodeBlacklistedForStage(now, host, numFailExec, stageId, stageAttemptId))\r\n[error]               ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala:79:6: class AccessController in package security is deprecated (since 17)\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.serializer.SerializationDebugger.enableDebugging, origin=java.security.AccessController, version=17\r\n[error]     !AccessController.doPrivileged(action).booleanValue()\r\n[error]      ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala:293:45: class SparkListenerExecutorBlacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.AppStatusListener.onExecutorBlacklisted.event, origin=org.apache.spark.scheduler.SparkListenerExecutorBlacklisted, version=3.1.0\r\n[error]   override def onExecutorBlacklisted(event: SparkListenerExecutorBlacklisted): Unit = {\r\n[error]                                             ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala:302:14: class SparkListenerExecutorBlacklistedForStage in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorExcludedForStage instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.AppStatusListener.onExecutorBlacklistedForStage.event, origin=org.apache.spark.scheduler.SparkListenerExecutorBlacklistedForStage, version=3.1.0\r\n[error]       event: SparkListenerExecutorBlacklistedForStage): Unit = {\r\n[error]              ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala:311:49: class SparkListenerNodeBlacklistedForStage in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeExcludedForStage instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.AppStatusListener.onNodeBlacklistedForStage.event, origin=org.apache.spark.scheduler.SparkListenerNodeBlacklistedForStage, version=3.1.0\r\n[error]   override def onNodeBlacklistedForStage(event: SparkListenerNodeBlacklistedForStage): Unit = {\r\n[error]                                                 ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala:334:47: class SparkListenerExecutorUnblacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerExecutorUnexcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.AppStatusListener.onExecutorUnblacklisted.event, origin=org.apache.spark.scheduler.SparkListenerExecutorUnblacklisted, version=3.1.0\r\n[error]   override def onExecutorUnblacklisted(event: SparkListenerExecutorUnblacklisted): Unit = {\r\n[error]                                               ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala:342:41: class SparkListenerNodeBlacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.AppStatusListener.onNodeBlacklisted.event, origin=org.apache.spark.scheduler.SparkListenerNodeBlacklisted, version=3.1.0\r\n[error]   override def onNodeBlacklisted(event: SparkListenerNodeBlacklisted): Unit = {\r\n[error]                                         ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala:350:43: class SparkListenerNodeUnblacklisted in package scheduler is deprecated (since 3.1.0): use SparkListenerNodeUnexcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.AppStatusListener.onNodeUnblacklisted.event, origin=org.apache.spark.scheduler.SparkListenerNodeUnblacklisted, version=3.1.0\r\n[error]   override def onNodeUnblacklisted(event: SparkListenerNodeUnblacklisted): Unit = {\r\n[error]                                           ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala:400:35: value BLACKLISTED_EXECUTORS in class AppStatusSource is deprecated (since 3.1.0): use excludedExecutors instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.AppStatusListener.updateExecExclusionStatus.$anonfun, origin=org.apache.spark.status.AppStatusSource.BLACKLISTED_EXECUTORS, version=3.1.0\r\n[error]         appStatusSource.foreach(_.BLACKLISTED_EXECUTORS.inc())\r\n[error]                                   ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala:403:35: value UNBLACKLISTED_EXECUTORS in class AppStatusSource is deprecated (since 3.1.0): use unexcludedExecutors instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.AppStatusListener.updateExecExclusionStatus.$anonfun, origin=org.apache.spark.status.AppStatusSource.UNBLACKLISTED_EXECUTORS, version=3.1.0\r\n[error]         appStatusSource.foreach(_.UNBLACKLISTED_EXECUTORS.inc())\r\n[error]                                   ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala:121:40: value isBlacklisted in class ExecutorSummary is deprecated (since 3.1.0): use isExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.AppStatusStore.replaceDriverGcTime, origin=org.apache.spark.status.api.v1.ExecutorSummary.isBlacklisted, version=3.1.0\r\n[error]       source.totalShuffleWrite, source.isBlacklisted, source.maxMemory, source.addTime,\r\n[error]                                        ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala:123:14: value blacklistedInStages in class ExecutorSummary is deprecated (since 3.1.0): use excludedInStages instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.AppStatusStore.replaceDriverGcTime, origin=org.apache.spark.status.api.v1.ExecutorSummary.blacklistedInStages, version=3.1.0\r\n[error]       source.blacklistedInStages, source.peakMemoryMetrics, source.attributes, source.resources,\r\n[error]              ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/api/v1/api.scala:529:12: value holdingLocks in class ThreadStackTrace is deprecated (since 4.0.0): using synchronizers and monitors instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.api.v1.ThreadStackTrace.productElement, origin=org.apache.spark.status.api.v1.ThreadStackTrace.holdingLocks, version=4.0.0\r\n[error] case class ThreadStackTrace(\r\n[error]            ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/protobuf/ExecutorStageSummarySerializer.scala:41:39: value isBlacklistedForStage in class ExecutorStageSummary is deprecated (since 3.1.0): use isExcludedForStage instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.protobuf.ExecutorStageSummarySerializer.serialize.builder, origin=org.apache.spark.status.api.v1.ExecutorStageSummary.isBlacklistedForStage, version=3.1.0\r\n[error]       .setIsBlacklistedForStage(input.isBlacklistedForStage)\r\n[error]                                       ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/protobuf/ExecutorSummaryWrapperSerializer.scala:64:31: value isBlacklisted in class ExecutorSummary is deprecated (since 3.1.0): use isExcluded instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.protobuf.ExecutorSummaryWrapperSerializer.serializeExecutorSummary.builder, origin=org.apache.spark.status.api.v1.ExecutorSummary.isBlacklisted, version=3.1.0\r\n[error]       .setIsBlacklisted(input.isBlacklisted)\r\n[error]                               ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/status/protobuf/ExecutorSummaryWrapperSerializer.scala:79:11: value blacklistedInStages in class ExecutorSummary is deprecated (since 3.1.0): use excludedInStages instead\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.status.protobuf.ExecutorSummaryWrapperSerializer.serializeExecutorSummary, origin=org.apache.spark.status.api.v1.ExecutorSummary.blacklistedInStages, version=3.1.0\r\n[error]     input.blacklistedInStages.foreach { stage =>\r\n[error]           ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala:211:45: method murmur3_32 in class Hashing is deprecated\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.util.collection.AppendOnlyMap.rehash, origin=com.google.common.hash.Hashing.murmur3_32\r\n[error]   private def rehash(h: Int): Int = Hashing.murmur3_32().hashInt(h).asInt()\r\n[error]                                             ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala:269:47: method murmur3_32 in class Hashing is deprecated\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.util.collection.OpenHashSet.hashcode, origin=com.google.common.hash.Hashing.murmur3_32\r\n[error]   private def hashcode(h: Int): Int = Hashing.murmur3_32().hashInt(h).asInt()\r\n[error]                                               ^\r\n[error] 61 errors found\r\n[error] /Users/panbingkun/Developer/spark/spark-community/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/SparkSession.scala:156:12: method addAllPosArguments in class Builder is deprecated\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.SparkSession.sql.$anonfun.cmd.$anonfun, origin=org.apache.spark.connect.proto.SqlCommand.Builder.addAllPosArguments\r\n[error]           .addAllPosArguments(args.map(lit(_).expr).toImmutableArraySeq.asJava)))\r\n[error]            ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/SparkSession.scala:155:12: method setSql in class Builder is deprecated\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.SparkSession.sql.$anonfun.cmd.$anonfun, origin=org.apache.spark.connect.proto.SqlCommand.Builder.setSql\r\n[error]           .setSql(sqlText)\r\n[error]            ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/SparkSession.scala:188:14: method putAllNamedArguments in class Builder is deprecated\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.SparkSession.sql.$anonfun.cmd.$anonfun, origin=org.apache.spark.connect.proto.SqlCommand.Builder.putAllNamedArguments\r\n[error]             .putAllNamedArguments(args.asScala.map { case (k, v) => (k, lit(v).expr) }.asJava)))\r\n[error]              ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/SparkSession.scala:187:14: method setSql in class Builder is deprecated\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.SparkSession.sql.$anonfun.cmd.$anonfun, origin=org.apache.spark.connect.proto.SqlCommand.Builder.setSql\r\n[error]             .setSql(sqlText)\r\n[error]              ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala:101:11: method createExternalTable in class Catalog is deprecated (since 2.2.0): use createTable instead.\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.catalog.Catalog.createExternalTable, origin=org.apache.spark.sql.api.Catalog.createExternalTable, version=2.2.0\r\n[error]     super.createExternalTable(tableName, path)\r\n[error]           ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala:105:11: method createExternalTable in class Catalog is deprecated (since 2.2.0): use createTable instead.\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.catalog.Catalog.createExternalTable, origin=org.apache.spark.sql.api.Catalog.createExternalTable, version=2.2.0\r\n[error]     super.createExternalTable(tableName, path, source)\r\n[error]           ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala:112:11: method createExternalTable in class Catalog is deprecated (since 2.2.0): use createTable instead.\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.catalog.Catalog.createExternalTable, origin=org.apache.spark.sql.api.Catalog.createExternalTable, version=2.2.0\r\n[error]     super.createExternalTable(tableName, source, options)\r\n[error]           ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala:126:11: method createExternalTable in class Catalog is deprecated (since 2.2.0): use createTable instead.\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.catalog.Catalog.createExternalTable, origin=org.apache.spark.sql.api.Catalog.createExternalTable, version=2.2.0\r\n[error]     super.createExternalTable(tableName, source, options)\r\n[error]           ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala:134:11: method createExternalTable in class Catalog is deprecated (since 2.2.0): use createTable instead.\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.catalog.Catalog.createExternalTable, origin=org.apache.spark.sql.api.Catalog.createExternalTable, version=2.2.0\r\n[error]     super.createExternalTable(tableName, source, schema, options)\r\n[error]           ^\r\n[error] /Users/panbingkun/Developer/spark/spark-community/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala:158:11: method createExternalTable in class Catalog is deprecated (since 2.2.0): use createTable instead.\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.catalog.Catalog.createExternalTable, origin=org.apache.spark.sql.api.Catalog.createExternalTable, version=2.2.0\r\n[error]     super.createExternalTable(tableName, source, schema, options)\r\n[error]           ^\r\n[error] 10 errors found\r\n[error] (core / Compile / compileIncremental) Compilation failed\r\n[error] (connect-client-jvm / Compile / compileIncremental) Compilation failed\r\n[error] Total time: 121 s (02:01), completed Sep 24, 2024, 3:39:40 PM\r\n\r\n```

```shell\r\n[error] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/compression/CompressionSchemeBenchmark.scala:94:73: method formatted in class StringFormat is deprecated (since 2.12.16): Use `formatString.format(value)` instead of `value.formatted(formatString)`,\r\n[error] or use the `f""` string interpolator. In Java 15 and later, `formatted` resolves to the new method in String which has reversed parameters.\r\n[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.execution.columnar.compression.CompressionSchemeBenchmark.runEncodeBenchmark.$anonfun.label, origin=scala.Predef.StringFormat.formatted, version=2.12.16\r\n[error]       val label = s"${getFormattedClassName(scheme)}(${compressionRatio.formatted("%.3f")})"\r\n[error]                                                                \r\n```

cc @dongjoon-hyun @LuciferYang 

Currently, the testing based on `sbt` has passed: https://github.com/panbingkun/spark/runs/30569167386

Merged to master.

cc: @cloud-fan @dbatomic to take a look.

cc @viirya 

As you already have a e2e test in the PR description, maybe also add it to a unit test?

> As you already have a e2e test in the PR description, maybe also add it to a unit test?\r\n\r\nSure. Will add.

thanks, merging to master/3.5!

Merged to master.

Master branch is guarded from the `NullPointerException` caused by [SPARK-49739](https://issues.apache.org/jira/browse/SPARK-49739) [here](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala#L59). We should consider backporting the fix from https://github.com/apache/spark/pull/46955.

Closing the pr because #46955 solves it. 

@MaxGekk could you review please?

cc @uros-db 

@MaxGekk regenerated golden files

+1, LGTM. Merging to master.\r\nThank you, @viktorluc-db and @uros-db for review.

@MaxGekk @viktorluc-db @uros-db , the results are different after the fix. We should also update the section: "Does this PR introduce any user-facing change?"

Waiting for CI.

+1, LGTM. Merging to master.\r\nThank you, @stefankandic.

@HeartSaVioR \r\nCan you take a look ?\r\n

@anishshri-db \r\nCan you take a look ?

merged to master

> Should we preserve the version in `since`: `1.0.0`?\r\n\r\nThat makes sense, let me update it.

> Should we preserve the version in `since`: `1.0.0`?\r\n\r\nUpdated, thanks!

I believe [Run / Run Docker integration tests](https://github.com/panbingkun/spark/actions/runs/10957994215/job/30427318577#logs) is not related to the changes.\r\n\r\n+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

Also, I revised the PR description by adding K8s reference links because this is a community patch.

I have added a check for `subPath` and `subPathExpr` not being set together to provide a useful error message early (noting the exact Spark config key cases the failure).

> Could you add a test coverage for this?\r\n\r\nI have added more tests.

there is a new panda version https://pandas.pydata.org/pandas-docs/version/2.2.3/whatsnew/v2.2.3.html that have support for numpy 2.1https://github.com/pandas-dev/pandas/pull/59444

Thank you @bjornjorgensen!\r\nI think we can separate the pandas upgrade from the numpy upgrade, as the current pandas version should be compatible with numpy 2.1.0 as well. 

The test failures we are trying to fix here are almost all related to [this issue](https://github.com/pandas-dev/pandas/issues/59838#event-14332587978). Thank you @codesorcery for sharing! 

Retriggered irrelevant tests

```\r\n[info] - interrupt all - background queries, foreground interrupt *** FAILED *** (20 seconds, 50 milliseconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 30 times over 20.046569918 seconds. Last failure message: q2Interrupted was false. (SparkSessionE2ESuite.scala:71)\r\n[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:\r\n```\r\nRetriggering

@HyukjinKwon @zhengruifeng @dongjoon-hyun would you please review?

Merged to master for Apache Spark 4.0.0 on February 2025.

Thank you @dongjoon-hyun !

cc @HyukjinKwon @zhengruifeng @allisonwang-db @xinrong-meng @MaxGekk 

+1, LGTM. Merging to master.\r\nThank you, @panbingkun.

Merged to master.

+1, LGTM.

@chenhao-db @cloud-fan This PR follows up on [the previous PR](https://github.com/apache/spark/pull/47920) that added support for duplicate keys where one corner case was left out. Can you review this PR? Thanks!

Merged to master.

Merged to master.

+1, LGTM. Thank you!

Merged to master.\r\n\r\nThe website is now live. https://apache.github.io/spark/\r\n\r\nBTW, This patch also brings the unexpected [action](https://github.com/apache/spark/actions/workflows/pages/pages-build-deployment) alive again. I will ask INFRA to disable it one more time\r\n

Thanks for doing this!

The INFRA team has terminated these unexpected actions.\r\n\r\nFYI,https://issues.apache.org/jira/browse/INFRA-26133

Not attempting to fix this behaviour  as it seems its like that since very begining

Converting this back to draft as we think this change is a breaking change and there is a better way of doing it.

@cloud-fan Can you go over this PR again whenever you have time? Thanks!

Could you review this PR when you have some time, @viirya ?

Thank you, @viirya .

Thank you, @HyukjinKwon !

Merged to master.

Thanks, merging to master

Merged to master.

Can you link the PR with test changes here please?

+1, LGTM. Merging to master.\r\nThank you, @uros-db.

Merged to master. Thank you all.

Late LGTM, thank you!

thanks, merged to master

@cloud-fan ready for merge

thanks, merging to master!

Thank you @dongjoon-hyun !

@MaxGekk @yaooqinn @LuciferYang @guykhazma , this is a backport to 3.4 for #44524 can u please review this

@MaxGekk I have enabled GAs in my fork

thanks, merged to master

thanks @dongjoon-hyun @xinrong-meng and @HyukjinKwon \r\n\r\nmerged to master

ready to go?

org.apache.spark.MapStatusesConvertBenchmark\r\nJDK 17: https://github.com/panbingkun/spark/actions/runs/11431485373\r\nJDK 21: https://github.com/panbingkun/spark/actions/runs/11431488212

> ready to go?\r\n\r\nReady already, thanks!

Merged into master for Spark 4.0. Thanks @panbingkun 

any update on this one @panbingkun ?

Ya, please ping us when the PR is ready. :)

> Ya, please ping us when the PR is ready. :)\r\n\r\nThanks â¤ï¸

The benchmark: StateStoreBasicOperationsBenchmark\r\nJDK17: https://github.com/panbingkun/spark/actions/runs/11540712452\r\nJDK21: https://github.com/panbingkun/spark/actions/runs/11540715108

Merged into master for Spark 4.0. Thanks @panbingkun and @dongjoon-hyun 

+1, LGTM.\r\n\r\nThank you again, @panbingkun and @LuciferYang .

Thanks all @LuciferYang @dongjoon-hyun â¤ï¸

Or, we can remove the benchmark code from this PR.

@HeartSaVioR \r\nCan you take a look ?

@panbingkun for reference, this mistake slipped in

+1, LGTM. Merging to master.\r\nThank you, @mihailom-db and @panbingkun @dbatomic for review.

Merging to master.

Not needed.

> If PruneFilters triggers, we can lose a stateful operator and fail a query.\r\n\r\nCould we explain this better? Why and how the stateful operator is lost, and how does it fail?

cc - @jingz-db @HeartSaVioR - PTAL, thx !

Thanks! Merging to master.

Merging to master.

This one broke master...

Reverted it ðŸ‘ 

Second try :)

Merging to master

@andylam-db @sigmod @cloud-fan 

Do all the existing optimizer rules work fine with this single join? I understand that we need to implement the single-match check in all the physical join nodes, but semantic wise, is there anything we need to take care? 

> Do all the existing optimizer rules work fine with this single join? I understand that we need to implement the single-match check in all the physical join nodes, but semantic wise, is there anything we need to take care?\r\n\r\n@cloud-fan \r\n\r\nI\

thanks, merging to master!

Can we add some tests?

Added the test, let me know if this is in order.

+1, LGTM. Merging to master/3.5/3.4.\r\nThank you, @andrej-db and @urosstan-db @RaleSapic @PetarVasiljevic-DB @milastdbx @dongjoon-hyun for review.

@andrej-db Could you open separate PRs for 3.4 and 3.5 because your changes cause conflicts in the branches.

To @MaxGekk and @andrej-db , Apache Spark 3.4 reached the End-Of-Support. \r\n\r\nOnly `branch-3.5` is open for backporting. \r\n\r\nAlso, cc @LuciferYang since he is the release manager for Apache Spark 3.5.4.

Thanks @dongjoon-hyun 

cc @HyukjinKwon @MaxGekk here is the DataFrame support for the new `randstr` and `uniform` functions :)

thanks, merged to master

@jdesjean Hi, nice to meet you! Could you review this trivial change? Thanks!

Also, cc @hvanhovell , too.

Merged to master.

Thank you, @changgyoopark-db and @HyukjinKwon .

Merged to master to recover CIs.

The remaining failure is under discussion here.\r\n- https://github.com/apache/spark/pull/48136#issuecomment-2358700242

Test first

I guess we need to skip the UTs when `plotly` not installed, like\r\n\r\nhttps://github.com/apache/spark/blob/f3edc0c3570ea4b28f68607d6404c6b8d0801161/python/pyspark/pandas/tests/plot/test_frame_plot_plotly.py#L39

Thanks @zhengruifeng . The tests below passed even when Plotly was not installed:\r\n- pyspark.sql.tests.connect.test_parity_frame_plot\r\n- pyspark.sql.tests.connect.test_parity_frame_plot_plotly\r\n- pyspark.sql.tests.plot.test_frame_plot\r\n- pyspark.sql.tests.plot.test_frame_plot_plotly

Thank you, @xinrong-meng . 

I made changes related to CI in [3a553c2](https://github.com/apache/spark/pull/48139/commits/3a553c240bfdb4f7c242cea200be20a1d089b59d) to make the review easier.

Also cc @HyukjinKwon please

~However, as you see https://github.com/apache/spark/pull/48139#discussion_r1766111840 (and https://github.com/apache/spark/pull/48139#issuecomment-2357825412), the test case seems to still run (not skipped) without `plotly` and pass without throwing `ImportError` on this PR. Did I misunderstand?~\r\n> If plotly is not installed, the df.plot fails with ImportError error, but the remaining part of pyspark should be still available.

May I ask what is your result of the following?\r\n```\r\npip3 uninstall plotly\r\npython/run-tests.py --python-executables python3 --modules pyspark-sql\r\n```\r\nI see the following still. (with this PR). IIUC, `optional dependency` means no this kind of test failures .\r\n```\r\n======================================================================\r\nERROR [0.434s]: test_map_in_arrow_with_profile (pyspark.sql.tests.test_resources.ResourceProfileTests.test_map_in_arrow_with_profile)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File "/Users/dongjoon/APACHE/spark-merge/python/pyspark/sql/utils.py", line 124, in require_minimum_plotly_version\r\n    import plotly  # noqa: F401\r\n    ^^^^^^^^^^^^^\r\nModuleNotFoundError: No module named \

Hi @dongjoon-hyun after [ed93866](https://github.com/apache/spark/pull/48139/commits/ed93866847a3014bff221b984d4c5430df98b150) the command you shared should pass.

Could you fix the linter failure, @xinrong-meng ?\r\n\r\n![Screenshot 2024-09-19 at 07 26 39](https://github.com/user-attachments/assets/20e3c049-4689-45bc-b861-491c0797e198)\r\n

I verified those are skipped correctly. Merged to master.\r\n```\r\nStarting test(python3): pyspark.sql.tests.plot.test_frame_plot (temp output: /Users/dongjoon/APACHE/spark-merge/python/target/66424a1d-63aa-4fa6-94aee91d79c1dcc0/python3__pyspark.sql.tests.plot.test_frame_plot__yfubd8z7.log)\r\nStarting test(python3): pyspark.sql.tests.plot.test_frame_plot_plotly (temp output: /Users/dongjoon/APACHE/spark-merge/python/target/9b5518b0-417e-46bd-8ead-b5b9f1a2e411/python3__pyspark.sql.tests.plot.test_frame_plot_plotly__jhwyzczj.log)\r\nFinished test(python3): pyspark.sql.tests.plot.test_frame_plot (4s) ... 5 tests were skipped\r\nFinished test(python3): pyspark.sql.tests.plot.test_frame_plot_plotly (4s) ... 2 tests were skipped\r\n```

Thank you so much @dongjoon-hyun !

Mind keeping the PR description template (https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE) and filing a JIRA?

thanks @xinrong-meng , merged to master

Merged to master.\r\n\r\nThank you very much @dongjoon-hyun 

It seems to fail again in Apache Spark repo, @yaooqinn . Note that `Pandoc` passed. A new failure happens at `Setup Pages` step.\r\n\r\n- https://github.com/apache/spark/actions/runs/10916383676/job/30297716064\r\n![Screenshot 2024-09-18 at 07 41 21](https://github.com/user-attachments/assets/93164a19-f09b-4557-8648-e87d60feb57b)\r\n

I merged your follow-up. \r\n- https://github.com/apache/spark/pull/48141

It still fails at a new step, `Jekyll Build`. ðŸ˜¢ \r\n\r\n![Screenshot 2024-09-18 at 07 48 19](https://github.com/user-attachments/assets/6a6bcd31-a387-43ad-9891-a219099200c6)\r\n

This task seems to become tricky because we have three commits with no success. Do you think there is a way to verify this in a single shot, @yaooqinn ?\r\n```\r\n$ git log --oneline --format=%s | grep SPARK-49495\r\n[SPARK-49495][DOCS][FOLLOWUP] Enable GitHub Pages settings via .asf.yml\r\n[SPARK-49495][DOCS][FOLLOWUP] Fix Pandoc installation for GitHub Pages publication action\r\n[SPARK-49495][DOCS] Document and Feature Preview on the master branch via Live GitHub Pages Updates\r\n```

Nice fix, thank you!

thanks all for reviews!

cc @HyukjinKwon 

Thank you, @viirya .

Sorry, @viirya and @HyukjinKwon . I closed this PR and will add a comment to the original PR.